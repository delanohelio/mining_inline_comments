{"pr_number": 4805, "pr_title": "kafka connector: add internal columns filter pushdown", "pr_createdAt": "2020-08-13T07:50:12Z", "pr_url": "https://github.com/trinodb/trino/pull/4805", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1MTkzMg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476051932", "bodyText": "This should be Optional.empty().", "author": "aalbu", "createdAt": "2020-08-25T01:48:14Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -265,7 +293,8 @@ public ConnectorInsertTableHandle beginInsert(ConnectorSession session, Connecto\n                 table.getMessageDataFormat(),\n                 table.getKeyDataSchemaLocation(),\n                 table.getMessageDataSchemaLocation(),\n-                actualColumns);\n+                actualColumns,\n+                table.getConstraint());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MTAxNg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476091016", "bodyText": "KafkaConsumer#offsetsForTimes() will give you the earliest offset whose timestamp is greater than or equal to the argument you're passing.  That's what you want for the lower bound of a range, but using it for the upper bound leads to the issue that @gschmutz pointed out.  I think the right thing to do is to not try to limit the end offset using a _timestamp filter.", "author": "aalbu", "createdAt": "2020-08-25T02:46:56Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -74,12 +79,25 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n \n             Map<TopicPartition, Long> partitionBeginOffsets = kafkaConsumer.beginningOffsets(topicPartitions);\n             Map<TopicPartition, Long> partitionEndOffsets = kafkaConsumer.endOffsets(topicPartitions);\n+            KafkaFilter kafkaFilter = getKafkaFilter(kafkaTableHandle, new KafkaFilter(partitionInfos, partitionBeginOffsets, partitionEndOffsets, null, null));\n+            partitionInfos = kafkaFilter.getPartitionInfos();\n+            partitionBeginOffsets = kafkaFilter.getPartitionBeginOffsets();\n+            partitionEndOffsets = kafkaFilter.getPartitionEndOffsets();\n+            if (kafkaFilter.getBeginOffsetTs() != null) {\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> findOffsetsByTimestamp(kafkaConsumer, p, kafkaFilter.getBeginOffsetTs()));\n+            }\n+            if (kafkaFilter.getEndOffsetTs() != null) {\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> findOffsetsByTimestamp(kafkaConsumer, p, kafkaFilter.getEndOffsetTs()));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjUxMDQ1NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476510455", "bodyText": "@aalbu yes I agree that only having a lower bound \"pushed down\" to Kafka would solve the issue I have pointed out. Haven't thought about that. The only downside is, that you can no longer efficiently get a range of data (i.e. getting data from January 2020 in August 2020), but there is no other solution I can think of, except adding another _timestamp_offset field for the filter pushdown and leaving the _timestamp just for filtering on the timestamp header (without pushdown to kafka), but as I mentioned above, there is no meaningful value to return for that column, if this scenario would be used (we can't translate an offset into a timestamp unfortunately).", "author": "gschmutz", "createdAt": "2020-08-25T14:50:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MTAxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU2MzA5Ng==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478563096", "bodyText": "The config property @wangli-td proposed achieves the functionality you are proposing, right?  Just one column mapping, but the push-down behavior is controlled by a config/session property.", "author": "aalbu", "createdAt": "2020-08-27T16:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MTAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MjA2MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476092060", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                .collect(Collectors.toList());\n          \n          \n            \n                                .collect(toImmutableList());", "author": "aalbu", "createdAt": "2020-08-25T02:48:32Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5NDE2OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476094168", "bodyText": "Please spell out the argument name defKafkaFilter -> defaultKafkaFilter (I assume).", "author": "aalbu", "createdAt": "2020-08-25T02:51:43Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5NjA2OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476096068", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n          \n          \n            \n                        Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n          \n      \n    \n    \n  \n\nSame suggestion in several other places.", "author": "aalbu", "createdAt": "2020-08-25T02:54:37Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5Njk5Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476096992", "bodyText": "No need for the else, just return defKafkaFilter.", "author": "aalbu", "createdAt": "2020-08-25T02:57:43Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getEnd() >= 0 ? offsetTimestampRanged.get().getEnd() : null);\n+        }\n+        else {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5ODY2MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476098661", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n          \n          \n            \n                                offsetTimestampRanged.map(Range::getBegin).filter(begin -> begin >= 0).orElse(null),", "author": "aalbu", "createdAt": "2020-08-25T03:04:06Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwMDAyMA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476100020", "bodyText": "Perhaps you can find a more descriptive name for genBeginOffset, something like computeOffset.", "author": "aalbu", "createdAt": "2020-08-25T03:06:17Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getEnd() >= 0 ? offsetTimestampRanged.get().getEnd() : null);\n+        }\n+        else {\n+            return defKafkaFilter;\n+        }\n+    }\n+\n+    public static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> genBeginOffset)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwNzg5NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476107895", "bodyText": "Use TimestampType createTimestampType(3) instead of the deprecated TimestampType.TIMESTAMP.", "author": "aalbu", "createdAt": "2020-08-25T03:18:04Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -75,7 +76,12 @@\n     /**\n      * <tt>_key_length</tt> - length in bytes of the key.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n+    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\"),\n+\n+    /**\n+     * <tt>_timestamp</tt> - offset timestamp\n+     */\n+    OFFSET_TIMESTAMP_FIELD(\"_timestamp\", TimestampType.TIMESTAMP, \"Offset Timestamp\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjExMDYxMQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476110611", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n          \n          \n            \n                    return description != null;\n          \n          \n            \n                    return BY_COLUMN_NAME.containsKey(columnName);", "author": "aalbu", "createdAt": "2020-08-25T03:22:10Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -88,6 +94,12 @@ public static KafkaInternalFieldDescription forColumnName(String columnName)\n         return description;\n     }\n \n+    public static boolean isInternalColumn(String columnName)\n+    {\n+        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n+        return description != null;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478423507", "bodyText": "I think what we concluded was that we can only push down upper bounds when the topic has message.timestamp.type=LogAppendTime.  So we should name the property to reflect that, maybe something like kafka.timestamp-type with allowable values LogAppendTime and CreateTime (this should be the default, equivalent to false as it is written now, since it is the 'safe' setting - queries will return correct results, even though performance might suffer).\nIdeally, Presto would obtain this value from Kafka, but I am not sure it is exposed.", "author": "aalbu", "createdAt": "2020-08-27T13:35:15Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaConfig.java", "diffHunk": "@@ -153,4 +154,17 @@ public KafkaConfig setMessagesPerSplit(int messagesPerSplit)\n         this.messagesPerSplit = messagesPerSplit;\n         return this;\n     }\n+\n+    public boolean isTimestampUpperBoundPushDownEnabled()\n+    {\n+        return timestampUpperBoundPushDownEnabled;\n+    }\n+\n+    @Config(\"kafka.timestamp-upper-bound-push-down-enabled\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2Njk0Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478666942", "bodyText": "Yes that could be a way to do it, if it is LogAppendTime, then there it is guaranteed that offset = timestamp. This is a setting on the topic which can be retrieved via the Kafka AdminClient. And small example project on how to retrieve message.timestamp.type from a topic can be found here: https://github.com/gschmutz/various-kafka-examples/tree/master/kafka-adminclient-test. Here is the relevant code:\n    String topic = \"test-topic\";\n    Properties config = new Properties();\n    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, \"dataplatform:9092,dataplatform:9093,dataplatform:9094\");\n    AdminClient admin = KafkaAdminClient.create(config);\n\n    ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n\n    System.out.println(\"Getting topic \"+topic+\" configuration\");\n    DescribeConfigsResult describeResult = admin.describeConfigs(Collections.singleton(topicResource));\n    Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n    Config c = topicConfig.get(topicResource);\n    System.out.println(c.get(\"message.timestamp.type\").value());", "author": "gschmutz", "createdAt": "2020-08-27T20:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwMTA2NA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478701064", "bodyText": "Thank you for providing the code sample.  This would allow us to get rid of the property altogether and determine on a topic-by-topic case whether it's safe to push down the upper bound of a _timestamp range.", "author": "aalbu", "createdAt": "2020-08-27T21:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5NDAzOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478794039", "bodyText": "@aalbu@gschmutz Thank you for your suggestion. I think decided by \"message.timestamp.type\" is a nice option, but the property is also needed, because even for CreateTime case(default case), pushing down the upper bound of  timestamp is also a normal operation. So I suggest keep property but default not pushing  unless configured.", "author": "wangli-td", "createdAt": "2020-08-28T02:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODg1MzM1OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478853358", "bodyText": "The problem with CreateTime is that you are not really sure what the Kafka producer is doing (is he overwriting the timestamp or not, is the system time of the producer client exactly the same as the server time ....) and if you allow the upper bound limit then it is just up to the behavior of the producer client if you can get into the issue I have mentioned or not. So not sure if it is event worth having an upper bound for the CreateTime type and if the property not only makes it more complex for a user to decide if it is really worth it.", "author": "gschmutz", "createdAt": "2020-08-28T06:18:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODk0ODI1Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478948252", "bodyText": "@gschmutz hi, gschmutz, the default operation of CreateTime mode is that it will be generated by Producer API. So the mode as default will work like what LogAppendTime mode does.  The problem with not supporting pushing down the upper bound is that the timestamp filter will be difficult to use for users with CreateTime mode (may very slow for million of rows maybe).For them, they have no option to push down the upper bound. Would we keep the property, but not pushing the upper bound as default unless configured for createTime mode.", "author": "wangli-td", "createdAt": "2020-08-28T08:37:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTI1NzEyOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r479257129", "bodyText": "@wangli-td hi, I don't agree that by default CreateTime works the same as LogAppendTime: if the system clock on client and Kafka cluster is not synchronized/not the same (which easily can happen/ I don't talk of timezone, just system clock sync) then even if the client program does not set the timestamp when producing a record to Kafka, the timestamp the producer client will assign might not exactly be the same as on the server (not even mentioning the lag on the network). Of course the difference would not be seconds but there will be a difference to the time used for the offset (for the index). But I also agree that a push down on upper bound would be nice to have, so we efficiently can select a \"time bucket\" way in the past. So maybe having a property where you can force the push down of the upper bound when topic is set to CreateTimeand the default not to do it? I think it is always better if a user has to manually \"push\" for a feature, which might potentially harm the result.", "author": "gschmutz", "createdAt": "2020-08-28T12:52:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ5MDc2MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478490760", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (topicPartitionOffsets == null || topicPartitionOffsets.values().size() == 0) {\n          \n          \n            \n                        if (topicPartitionOffsets.isEmpty()) {\n          \n      \n    \n    \n  \n\nI don't think kafkaConsumer.offsetsForTimes() returns null.", "author": "aalbu", "createdAt": "2020-08-27T15:06:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +118,25 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            if (topicPartitionOffsets == null || topicPartitionOffsets.values().size() == 0) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU1MzkwNw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480553907", "bodyText": "I don't think you need parentheses; if you feel they improve readability, you could have something like (p -> range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty()", "author": "aalbu", "createdAt": "2020-09-01T01:29:51Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU2OTE5NA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480569194", "bodyText": "You can have this on a single line.", "author": "aalbu", "createdAt": "2020-09-01T01:44:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU3NzU3Nw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480577577", "bodyText": "getColumnDomains() has a comment that says: // Available for Jackson serialization only!. Please use getDomains() instead.", "author": "aalbu", "createdAt": "2020-09-01T01:51:51Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU3ODQyMQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480578421", "bodyText": "There is no need to check there are domains present - you already did that earlier with verify(!constraint.isNone(), \"constraint is none\").", "author": "aalbu", "createdAt": "2020-09-01T01:52:43Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU4NDAwNQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480584005", "bodyText": "Can you please use descriptive names fo k and v?", "author": "aalbu", "createdAt": "2020-09-01T01:57:47Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> genBeginOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((k, v) -> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYxODI5Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480618292", "bodyText": "No need to split declaration and assignment.", "author": "aalbu", "createdAt": "2020-09-01T02:28:10Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,49 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig;\n+            topicConfig = describeResult.all().get();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzMjY4Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480632683", "bodyText": "requireNonNull() for non-primitive arguments.", "author": "aalbu", "createdAt": "2020-09-01T02:38:36Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzMzk4MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480633980", "bodyText": "computeOffsetByTimestamp should never be null.", "author": "aalbu", "createdAt": "2020-09-01T02:39:33Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAyNDk0MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486024940", "bodyText": "Typo: hasPartitonOffset -> hasPartitionOffset", "author": "aalbu", "createdAt": "2020-09-10T02:26:26Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.testng.annotations.Test;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.plugin.kafka.KafkaFilterManager.getKafkaFilter;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestKafkaFilterManager\n+{\n+    private static final int PARTITION_0 = 0;\n+    private static final int PARTITION_1 = 1;\n+    private static final int PARTITION_2 = 2;\n+    private static final long START_TIMESTAMP = 1599035000;\n+    private static final long END_TIMESTAMP = 1599035650;\n+    private static final String TOPIC_NAME = \"topic1\";\n+    private static final String SCHEMA_NAME = \"kafka\";\n+\n+    private final Map<Integer, Range> defaultPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(10, 100),\n+            PARTITION_1, new Range(11, 200),\n+            PARTITION_2, new Range(12, 300));\n+    private final Range offsetFilterPartitionValues = new Range(20, 80);\n+    private final Map<Integer, Range> timestampFilterPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(30, 70),\n+            PARTITION_1, new Range(31, 170),\n+            PARTITION_2, new Range(32, 270));\n+\n+    @Test\n+    public void testGetKafkaFilter()\n+    {\n+        // test partition only\n+        testFilter(true, false, false, false,\n+                filter -> assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size()),\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+                });\n+\n+        // test offset only\n+        testFilter(false, true, false, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+\n+        // test timestamp only without pushing down upper bound\n+        testFilter(false, false, true, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                });\n+\n+        // test timestamp only with pushing down upper bound\n+        testFilter(false, false, true, true,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getEnd());\n+                });\n+\n+        // test both without pushing down upper bound\n+        testFilter(true, true, true, false,\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size());\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+\n+        // test both with pushing down upper bound\n+        testFilter(true, true, true, true,\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size());\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+    }\n+\n+    private void testFilter(boolean hasPartition,\n+                                     boolean hasPartitonOffset,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMDYwNg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487120606", "bodyText": "Thanks aalbu, will resolve using integration test instead.", "author": "wangli-td", "createdAt": "2020-09-11T15:26:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAyNDk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAzNDcxMQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486034711", "bodyText": "We're making this set of assertions multiple times.  Since the filter is the same every time (result of buildDefaultKafkaFilter()), it would seem sufficient to assert just once (if we need them at all).", "author": "aalbu", "createdAt": "2020-09-10T03:03:21Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.testng.annotations.Test;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.plugin.kafka.KafkaFilterManager.getKafkaFilter;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestKafkaFilterManager\n+{\n+    private static final int PARTITION_0 = 0;\n+    private static final int PARTITION_1 = 1;\n+    private static final int PARTITION_2 = 2;\n+    private static final long START_TIMESTAMP = 1599035000;\n+    private static final long END_TIMESTAMP = 1599035650;\n+    private static final String TOPIC_NAME = \"topic1\";\n+    private static final String SCHEMA_NAME = \"kafka\";\n+\n+    private final Map<Integer, Range> defaultPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(10, 100),\n+            PARTITION_1, new Range(11, 200),\n+            PARTITION_2, new Range(12, 300));\n+    private final Range offsetFilterPartitionValues = new Range(20, 80);\n+    private final Map<Integer, Range> timestampFilterPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(30, 70),\n+            PARTITION_1, new Range(31, 170),\n+            PARTITION_2, new Range(32, 270));\n+\n+    @Test\n+    public void testGetKafkaFilter()\n+    {\n+        // test partition only\n+        testFilter(true, false, false, false,\n+                filter -> assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size()),\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+                });\n+\n+        // test offset only\n+        testFilter(false, true, false, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE0Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486487143", "bodyText": "This is a utility class. If it was to stay such please rename to KafkaFilters.\nBut I think it should be converted class which we instantiate.", "author": "losipiuk", "createdAt": "2020-09-10T16:44:31Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMTQwNw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487121407", "bodyText": "Thanks losipiuk, will resolving as a singleton component.", "author": "wangli-td", "createdAt": "2020-09-11T15:27:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NDA0Nw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486494047", "bodyText": "I suggest that you rename KafkaFilter to KafkaFilteringResult and use it only as a return value of getKafkaFilter.\nThen change signature of getKafkaFilter to\n    public  KafkaFilteringResult getKafkaFilter(\n            ConnectorSession session,\n            KafkaTableHandle kafkaTableHandle,\n            List<PartitionInfo> partitionInfos,\n            Map<TopicPartition, Long> partitionBeginOffsets,\n            Map<TopicPartition, Long> partitionEndOffsets) {\nDerive needPushTimeStampUpperBound internally based on injected configuration object and session passed as an argument.\nThe computeOffsetByTimestamp can be injected to KafkaFilterManager at construction time if we need that paremetrization at all.", "author": "losipiuk", "createdAt": "2020-09-10T16:55:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMTcxNw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487121717", "bodyText": "Thanks. resovled", "author": "wangli-td", "createdAt": "2020-09-11T15:28:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NDA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMjE0NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487122145", "bodyText": "Thanks. Resolved", "author": "wangli-td", "createdAt": "2020-09-11T15:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NDA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NTEzMg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486495132", "bodyText": "inline s1", "author": "losipiuk", "createdAt": "2020-09-10T16:57:40Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+public final class KafkaSessionProperties\n+{\n+    private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    {\n+        PropertyMetadata<Boolean> s1 = PropertyMetadata.booleanProperty(\n+                TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n+                \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n+                kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false);\n+\n+        sessionProperties = ImmutableList.of(s1);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMjM5Nw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487122397", "bodyText": "Thanks. Resolved", "author": "wangli-td", "createdAt": "2020-09-11T15:29:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NTEzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjkwMw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486496903", "bodyText": "Just inline contents into KafkaConnectorModule", "author": "losipiuk", "createdAt": "2020-09-10T17:00:25Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaAdminModule.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+\n+public class KafkaAdminModule", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NzYyMg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486497622", "bodyText": "do defensive copies using ImmutableList.copyOf()/ImmutableMap.copyOf()", "author": "losipiuk", "createdAt": "2020-09-10T17:01:37Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class KafkaFilter\n+{\n+    private final List<PartitionInfo> partitionInfos;\n+    private final Map<TopicPartition, Long> partitionBeginOffsets;\n+    private final Map<TopicPartition, Long> partitionEndOffsets;\n+\n+    public KafkaFilter(List<PartitionInfo> partitionInfos,\n+                       Map<TopicPartition, Long> partitionBeginOffsets,\n+                       Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        this.partitionInfos = partitionInfos;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMjY0OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487122648", "bodyText": "Thanks. Resolved", "author": "wangli-td", "createdAt": "2020-09-11T15:29:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NzYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUwNDU4MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486504581", "bodyText": "rename to config", "author": "losipiuk", "createdAt": "2020-09-10T17:13:34Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyNTM1OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486525358", "bodyText": "This should be isTimestampUpperBoundPushdownEnabled (drop force).\nPlease also confirm that we are certain that if topic is in LOG_APPEND mode, we are sure that timestamps are alway recored in non-decreasing order when looking at messages by offset.\nI would expect so, but I do not know details of Kafka, so maybe if there is time skew between machines participating in the communication this does not need to always be true.\nIf above does not hold we cannot filter agains upper bound at all.\nAs for the CREATE_TIME mode. I am not sure that we want to expose the config property which enables upper bound filtering in that mode.\nIt is easy to shoot oneself in the foot with the property, if user does not understand it's data, and filtering logic very well.\nGiven fact that upper bound filtering is not that useful IMO (I would expect that you mostly need filtering to incrementally process new batches of data which are added to the topic; and for that lower bound filtering is enough), I would disable upper bound filter for CREATE_TIME mode.", "author": "losipiuk", "createdAt": "2020-09-10T17:49:06Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3MDcxMQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486570711", "bodyText": "Please also confirm that we are certain that if topic is in LOG_APPEND mode, we are sure that timestamps are alway recored in non-decreasing order when looking at messages by offset.\nI would expect so, but I do not know details of Kafka, so maybe if there is time skew between machines participating in the communication this does not need to always be true.\nIf above does not hold we cannot filter agains upper bound at all.\n\nIn LogAppendTime  mode, the timestamp is a monotonically increasing function of the offset.  Note that this is in the context of a single partition.\n\nAs for the CREATE_TIME mode. I am not sure that we want to expose the config property which enables upper bound filtering in that mode.\nIt is easy to shoot oneself in the foot with the property, if user does not understand it's data, and filtering logic very well.\nGiven fact that upper bound filtering is not that useful IMO (I would expect that you mostly need filtering to incrementally process new batches of data which are added to the topic; and for that lower bound filtering is enough), I would disable upper bound filter for CREATE_TIME mode.\n\nThat was my initial reaction, as well, but the reason the toggle was added is because one might have an application that is guaranteed to publish messages in the order they were created and users might want to take advantage of that.  By default, upper bound filtering is disabled in CreateTime mode.", "author": "aalbu", "createdAt": "2020-09-10T19:02:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyNTM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYwMjYwMA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486602600", "bodyText": "I still think it is a bad idea to use an upper bound filter in the CreateTimemode as there is absolutely no guarantee that the header timestamp created on the client side will be in sync with the timestamp used to create the timestamp-to-offset index on the server side. But in the query push-down that index is used to translate to an offset whereas in the result the value of the timestamp in the record is shown, which might be the same, but could also diverge quite a bit. So that might be very confusing for a user and lead to situations i have shown before, even if programmer does not overwrite the timestamp header and just uses the default behavior.", "author": "gschmutz", "createdAt": "2020-09-10T20:05:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyNTM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyNjU4NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487126585", "bodyText": "@gschmutz Doesn't KafkaConsumer.offsetsForTimes work in a way that it resolves the timestamp to offset mapping on the server side. And resolution is based on exactly the one timestamp field on messages which is set either on log appending (in LOG_APPEND mode) or on message creation (in CREATE_MODE).\nAre there two different timestamp fields, one reported in the message, and other one used for resolution? I don't think there are.", "author": "losipiuk", "createdAt": "2020-09-11T15:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyNTM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyODI4NA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486528284", "bodyText": "This should be moved to KafkaFilteringManager.class.\nIt should be used by default for standard setup of KafkaFilterManager.class. You can add another package visible constructor for KafkaFilterManager where you can pass BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp strictly for sake of unit tests. Annotate the costructor with @VisibleForTesting", "author": "losipiuk", "createdAt": "2020-09-10T17:53:56Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzMTg4MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486531880", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n          \n          \n            \n                        if (topicPartitionOffsets.isEmpty()) {\n          \n          \n            \n                            return Optional.empty();\n          \n          \n            \n                        }\n          \n          \n            \n                        OffsetAndTimestamp offsetAndTimestamp = topicPartitionOffsets.values().iterator().next();\n          \n          \n            \n                        if (offsetAndTimestamp == null) {\n          \n          \n            \n                            return Optional.empty();\n          \n          \n            \n                        }\n          \n          \n            \n            \n          \n          \n            \n                        return Optional.of(offsetAndTimestamp.offset());\n          \n          \n            \n                        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n          \n          \n            \n                        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);", "author": "losipiuk", "createdAt": "2020-09-10T18:00:07Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            if (topicPartitionOffsets.isEmpty()) {\n+                return Optional.empty();\n+            }\n+            OffsetAndTimestamp offsetAndTimestamp = topicPartitionOffsets.values().iterator().next();\n+            if (offsetAndTimestamp == null) {\n+                return Optional.empty();\n+            }\n+\n+            return Optional.of(offsetAndTimestamp.offset());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEyMzMwNQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487123305", "bodyText": "Thanks. Resolved", "author": "wangli-td", "createdAt": "2020-09-11T15:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzMTg4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0OTU5Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486549593", "bodyText": "add a comment that it also covers empty range list", "author": "losipiuk", "createdAt": "2020-09-10T18:27:02Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MTk1OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486551958", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n          \n          \n            \n                                long high = sourceValues.stream().max(Long::compareTo).orElse(0L);\n          \n          \n            \n                                long low = 0;\n          \n          \n            \n                                long high = Long.MAX_VALUE;", "author": "losipiuk", "createdAt": "2020-09-10T18:30:08Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n+                    long high = sourceValues.stream().max(Long::compareTo).orElse(0L);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MzM2OQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486553369", "bodyText": "Maybe define helper method long min(long a, Optional<Marker> b) and max .... and use it here and in getRange above", "author": "losipiuk", "createdAt": "2020-09-10T18:32:11Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n+                    long high = sourceValues.stream().max(Long::compareTo).orElse(0L);\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+                    if (lowByLowMark.isPresent()) {\n+                        low = Long.max(lowByLowMark.get(), low);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzExNjAxOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487116019", "bodyText": "Thanks. will resolve that.", "author": "wangli-td", "createdAt": "2020-09-11T15:18:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MzM2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1NTkyMQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486555921", "bodyText": "adding OFFSET_TIMESTAMP_FIELD could be a separate commit.", "author": "losipiuk", "createdAt": "2020-09-10T18:35:33Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -75,7 +77,12 @@\n     /**\n      * <tt>_key_length</tt> - length in bytes of the key.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n+    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\"),\n+\n+    /**\n+     * <tt>_timestamp</tt> - offset timestamp\n+     */\n+    OFFSET_TIMESTAMP_FIELD(\"_timestamp\", TimestampType.createTimestampType(DEFAULT_PRECISION), \"Offset Timestamp\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzExNTkwOA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r487115908", "bodyText": "Thanks. will resolve that.", "author": "wangli-td", "createdAt": "2020-09-11T15:18:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1NTkyMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUyODI4MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492528281", "bodyText": "Rename commit message to Add internal Kafka column for OFFSET_TIMESTAMP_FIELD", "author": "losipiuk", "createdAt": "2020-09-22T07:34:20Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldManager.java", "diffHunk": "@@ -18,12 +18,14 @@\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n+import io.prestosql.spi.type.TimestampType;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUzMzA0MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492533041", "bodyText": "Rename commit message to Add predicate push down support for internal Kafka columns. The reminder can be phrased as:\nAdd support for predicate pushdown for following Kafka internal columns\n * _timestamp \n * _partition_offset\n * _partition_id\n\nIf predicate specifies lower bound on _timestamp column (_timestamp > XXXX), it is always pushed down. \nThe upper bound predicate is pushed down only for topics using ``LogAppendTime`` mode. \nFor topics using ``CreateTime`` mode, upper bound pushdown must be explicitly \nallowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\nor ``timestamp_upper_bound_force_push_down_enabled`` session property.", "author": "losipiuk", "createdAt": "2020-09-22T07:43:26Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaAdminFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUzOTA5NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492539095", "bodyText": "Add comment describing logic and motivation here. The phrasing I suggested for commit message will do.", "author": "losipiuk", "createdAt": "2020-09-22T07:54:24Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU2Nzg3Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492567873", "bodyText": "This is super cryptic. Can you explain in a comment what is happening here?", "author": "losipiuk", "createdAt": "2020-09-22T08:42:29Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4MTE2NA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492581164", "bodyText": "when I rebase code from latest. It seems the timestamp precision become 16(us) other than 13(ms)", "author": "wangli-td", "createdAt": "2020-09-22T09:03:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU2Nzg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU2ODMwOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492568309", "bodyText": "What exceptions are we expecting here. And why at all?\nAs a rule of thumb we should propagate exception. Not mask it with Optional.empty()", "author": "losipiuk", "createdAt": "2020-09-22T08:43:09Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTczNA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492571734", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n          \n          \n            \n                                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n          \n          \n            \n                private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n          \n          \n            \n                                                                                         Function<TopicPartition, Optional<Long>> overrideFunction)", "author": "losipiuk", "createdAt": "2020-09-22T08:48:22Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3Mjc5Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492572793", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n          \n          \n            \n                                                                                       Function<TopicPartition, Optional<Long>> computeOffset)\n          \n          \n            \n                private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n          \n          \n            \n                                                                                       Function<TopicPartition, Optional<Long>> overrideFunction)", "author": "losipiuk", "createdAt": "2020-09-22T08:50:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzE3OQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573179", "bodyText": "Use ImmutableMap.builder instead", "author": "losipiuk", "createdAt": "2020-09-22T08:50:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzMzOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573339", "bodyText": "use ImmutableMap.builder", "author": "losipiuk", "createdAt": "2020-09-22T08:50:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzY0OA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573648", "bodyText": "rename filterIndex to newOffset", "author": "losipiuk", "createdAt": "2020-09-22T08:51:25Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3Mzc2MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573761", "bodyText": "rename filterIndex to newOffset", "author": "losipiuk", "createdAt": "2020-09-22T08:51:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4MTg0MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492581840", "bodyText": "This loop feels wrong we are breaking whole process on the first non-internal column.\nAnd even if all constraints are on internal columns we are only processing arbitrary first one.\nInstead we should skip unsupported columns. And for internal once, gradually build up the predicate to be pushed down to kafka.", "author": "losipiuk", "createdAt": "2020-09-22T09:04:51Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4NzUwMw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492587503", "bodyText": "It feels to me that if !highMark.isUpperUnbounded() then highMark.getValueBlock().isPresent() should always be true. Can we make it checkArgument instead?", "author": "losipiuk", "createdAt": "2020-09-22T09:14:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = 0;\n+                    long high = Long.MAX_VALUE;\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = maxLow(low, lowMark);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = minHigh(high, highMark);\n+                    final long finalLow = low;\n+                    final long finalHigh = high;\n+                    return sourceValues.stream()\n+                            .filter(item -> item >= finalLow && item <= finalHigh)\n+                            .collect(toImmutableSet());\n+                }\n+            }\n+        }\n+        return sourceValues;\n+    }\n+\n+    private static long minHigh(long high, Marker highMark)\n+    {\n+        Optional<Long> highByHighMark = getHighByHighMark(highMark);\n+        if (highByHighMark.isPresent()) {\n+            high = Long.min(highByHighMark.get(), high);\n+        }\n+        return high;\n+    }\n+\n+    private static long maxLow(long low, Marker lowMark)\n+    {\n+        Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+        if (lowByLowMark.isPresent()) {\n+            low = Long.max(lowByLowMark.get(), low);\n+        }\n+        return low;\n+    }\n+\n+    private static Optional<Long> getHighByHighMark(Marker highMark)\n+    {\n+        if (!highMark.isUpperUnbounded() && highMark.getValueBlock().isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4ODc3NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492588775", "bodyText": "Why do we need && high - 1 >= 0? Is that legal state?", "author": "losipiuk", "createdAt": "2020-09-22T09:16:05Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = 0;\n+                    long high = Long.MAX_VALUE;\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = maxLow(low, lowMark);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = minHigh(high, highMark);\n+                    final long finalLow = low;\n+                    final long finalHigh = high;\n+                    return sourceValues.stream()\n+                            .filter(item -> item >= finalLow && item <= finalHigh)\n+                            .collect(toImmutableSet());\n+                }\n+            }\n+        }\n+        return sourceValues;\n+    }\n+\n+    private static long minHigh(long high, Marker highMark)\n+    {\n+        Optional<Long> highByHighMark = getHighByHighMark(highMark);\n+        if (highByHighMark.isPresent()) {\n+            high = Long.min(highByHighMark.get(), high);\n+        }\n+        return high;\n+    }\n+\n+    private static long maxLow(long low, Marker lowMark)\n+    {\n+        Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+        if (lowByLowMark.isPresent()) {\n+            low = Long.max(lowByLowMark.get(), low);\n+        }\n+        return low;\n+    }\n+\n+    private static Optional<Long> getHighByHighMark(Marker highMark)\n+    {\n+        if (!highMark.isUpperUnbounded() && highMark.getValueBlock().isPresent()) {\n+            long high = (Long) highMark.getValue();\n+            if (BELOW == highMark.getBound() && high - 1 >= 0) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU5MzU1MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492593551", "bodyText": "Are EndOffsets inclusive or exclusive?\nYou are calling calcTopicPartitionEndOffsetMap as if they were exclusive. Is that the case?\nedit:\nActually it looks like you are calling calcTopicPartitionEndOffsetMap differently in different contexts.\nFor PARTITION_OFFSET_FIELD the computeOffset will return a value for partition which we do not care about.\nBut for OFFSET_TIMESTAMP_FIELD it will return id which we should still scan.\nPlease clean it up.", "author": "losipiuk", "createdAt": "2020-09-22T09:23:52Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgzMTA5Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492831093", "bodyText": "Yes\uff0cendOffsets is exclusive. Actually it is hard judging timestamp range as judging partition_offset", "author": "wangli-td", "createdAt": "2020-09-22T15:28:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU5MzU1MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4OTMwNA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495089304", "bodyText": "Use TimestampType.TIMESTAMP_MILLIS here explicitly. The DEFAULT_PRECISION may change in the future and type of this column should not.", "author": "losipiuk", "createdAt": "2020-09-25T16:07:41Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldManager.java", "diffHunk": "@@ -169,6 +176,10 @@ public KafkaInternalFieldManager(TypeManager typeManager)\n                         KEY_LENGTH_FIELD,\n                         \"Total number of key bytes\",\n                         BigintType.BIGINT))\n+                .put(OFFSET_TIMESTAMP_FIELD, new InternalField(\n+                        OFFSET_TIMESTAMP_FIELD,\n+                        \"Message timestamp\",\n+                        TimestampType.createTimestampType(DEFAULT_PRECISION)))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MTI4NA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495091284", "bodyText": "Would you be fine with adding documentation for property to kafka.rst?", "author": "losipiuk", "createdAt": "2020-09-25T16:11:08Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaConfig.java", "diffHunk": "@@ -153,4 +154,17 @@ public KafkaConfig setMessagesPerSplit(int messagesPerSplit)\n         this.messagesPerSplit = messagesPerSplit;\n         return this;\n     }\n+\n+    public boolean isTimestampUpperBoundPushDownEnabled()\n+    {\n+        return timestampUpperBoundPushDownEnabled;\n+    }\n+\n+    @Config(\"kafka.timestamp-upper-bound-force-push-down-enabled\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQzODcyOA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495438728", "bodyText": "will add", "author": "wangli-td", "createdAt": "2020-09-26T09:44:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MTI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MzQwNA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495093404", "bodyText": "Use partition instead of p", "author": "losipiuk", "createdAt": "2020-09-25T16:15:03Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NjMwNQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495096305", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (AdminClient adminClient = adminFactory.create()) {\n          \n          \n            \n                        ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n          \n          \n            \n            \n          \n          \n            \n                        DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n          \n          \n            \n                        Map<ConfigResource, Config> config = describeResult.all().get();\n          \n          \n            \n            \n          \n          \n            \n                        if (config != null) {\n          \n          \n            \n                            Config c = config.get(topicResource);\n          \n          \n            \n                            String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n          \n          \n            \n                            if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n          \n          \n            \n                                return true;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    catch (Exception e) {\n          \n          \n            \n                        throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n          \n          \n            \n                    }\n          \n          \n            \n                    try (AdminClient adminClient = adminFactory.create()) {\n          \n          \n            \n                        ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n          \n          \n            \n            \n          \n          \n            \n                        DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n          \n          \n            \n                        Map<ConfigResource, Config> configMap = describeResult.all().get();\n          \n          \n            \n            \n          \n          \n            \n                        if (configMap != null) {\n          \n          \n            \n                            Config config = configMap.get(topicResource);\n          \n          \n            \n                            String timestampType = config.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n          \n          \n            \n                            if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n          \n          \n            \n                                return true;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    catch (Exception e) {\n          \n          \n            \n                        throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get configuration for topic '%s'\", topic), e);\n          \n          \n            \n                    }", "author": "losipiuk", "createdAt": "2020-09-25T16:20:26Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNjAxNw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495106017", "bodyText": "I think it is wrong. Take a look at this example\ntimestamps:     10  20  20  20  21 \noffsets:        1   2   3   4   5    \n\nIf I search for 21 you would like this code to return 4. But it will return 3. findOffsetsByTimestamp(...21-1) will return 3. And then we will increment it.\nI think the correct code would search for first offset for upperbound. And then move back one offset.\n        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp);\n        return offsetsByTimestamp.map(aLong -> aLong - 1);", "author": "losipiuk", "createdAt": "2020-09-25T16:38:22Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTM3MDM3NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495370375", "bodyText": "@losipiuk Hi losipiuk, \"timestamp - 1\" means the timestamp of upper bound is exclusive, aLong -> aLong + 1 means we should make offset result exclusive. Please correct me. Thanks.", "author": "wangli-td", "createdAt": "2020-09-26T01:39:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNjAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgyOTczNg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495829736", "bodyText": "See the example above. Where I list message timestamps and matching offsets.\nIf we have same message timestamp for multiple offsets the formula you propose seems not sound.\nIf we call findOffsetsForTimestampLessThan(..., 21) for above example data, we want to get offset 4 as a result.\nAnd with current code we get:\n\nfindOffsetsByTimestamp(...., 21 - 1) -> 2 (given https://kafka.apache.org/0102/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes(java.util.Map)\nand then we shift it by +1.\n\nSo final result is 3.\nProbably it is not the end of the world, as we will filter out in Presto anyway. But the formula I proposed seems to work fine unless I am missing something.", "author": "losipiuk", "createdAt": "2020-09-28T10:08:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNjAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg0MzA5NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495843095", "bodyText": "Seems it depends exclusive first or last. I think your propose is also fine. I'll change it.\nBy the way, if using exclusive last. The fetch way of timestamp will same for both  lower bound and upper bound. timestamps:  [10  21)  ->  offsets: [1, 5) .\nBoth using findOffsetsForTimestampGreaterOrEqual is fine.", "author": "wangli-td", "createdAt": "2020-09-28T10:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNjAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMDY2Mw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495110663", "bodyText": "Rename to findOffsetsForTimestampGreaterOrEqual", "author": "losipiuk", "createdAt": "2020-09-25T16:47:14Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTAzMA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111030", "bodyText": "rename to findOffsetsForTimestampLessThan. Unless I get the expected semantics wrong.", "author": "losipiuk", "createdAt": "2020-09-25T16:47:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTI3MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111270", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n          \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = ImmutableMap.builder();", "author": "losipiuk", "createdAt": "2020-09-25T16:48:31Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTQ3NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111475", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();\n          \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = ImmutableMap.builder();", "author": "losipiuk", "createdAt": "2020-09-25T16:48:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMzc3Nw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495113777", "bodyText": "same here", "author": "losipiuk", "createdAt": "2020-09-25T16:53:22Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMzc5MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495113790", "bodyText": "range.getBegin() >= 0 -> range.getBegin() != INVALID_KAFKA_RANGE_INDEX", "author": "losipiuk", "createdAt": "2020-09-25T16:53:23Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDU4MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495114580", "bodyText": "Make signature:\nprivate static Set<Long> filterValuesByDomain(Set<Long> values, Domain domain)", "author": "losipiuk", "createdAt": "2020-09-25T16:54:49Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = computeOffset.apply(partition);\n+            partitionFilteredEndOffsetsBuilder.put(partition, newOffset.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsetsBuilder.build();\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExODUzNg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495118536", "bodyText": "Also this is a bit tricky. Could you write unit-test for it? To do that make it non-private and tag with @VisibleForTesting", "author": "losipiuk", "createdAt": "2020-09-25T17:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDU4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQzOTAyOQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495439029", "bodyText": "already add comments and test cases for static function", "author": "wangli-td", "createdAt": "2020-09-26T09:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExOTg1NQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495119855", "bodyText": "sessionWithUpperBoundPushDownEnabled", "author": "losipiuk", "createdAt": "2020-09-25T17:04:35Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMDc0OQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495120749", "bodyText": "do not catch", "author": "losipiuk", "createdAt": "2020-09-25T17:06:14Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+                try {\n+                    if (j < TIMESTAMP_TEST_COUNT) {\n+                        if (j == TIMESTAMP_TEST_START_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            startTime = r.timestamp();\n+                        }\n+                        else if (j == TIMESTAMP_TEST_END_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            endTime = r.timestamp();\n+                        }\n+                        else {\n+                            send.get();\n+                        }\n+                        Thread.sleep(1100);\n+                    }\n+                }\n+                catch (Exception e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMTYwMA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495121600", "bodyText": "I totally do not understand what is happening. Here.\nAlso why do we need such long sleeps here. How much time can this loop take.", "author": "losipiuk", "createdAt": "2020-09-25T17:07:41Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQwNTQyMw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495405423", "bodyText": "Hi losipiuk, it is due to the case that I'm using from_unixtime to build test sql. The precision is second level. I may think a new way.", "author": "wangli-td", "createdAt": "2020-09-26T03:11:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMTYwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQzOTQ1MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495439450", "bodyText": "I will use timestamp format instead of \"from_unixtime\" which do not need to make sure the second precision.", "author": "wangli-td", "createdAt": "2020-09-26T09:47:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMTYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyNDI1OQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495124259", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private void createMessages(String topicName)\n          \n          \n            \n                {\n          \n          \n            \n                    try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n          \n          \n            \n                        for (long j = 0; j < MESSAGE_NUM; j++) {\n          \n          \n            \n                            Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n          \n            \n                private void createMessages(String topicName)\n          \n          \n            \n                        throws ExecutionException, InterruptedException\n          \n          \n            \n                {\n          \n          \n            \n                    Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n          \n          \n            \n                    try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n          \n          \n            \n                        for (long messageNum = 0; messageNum < MESSAGE_NUM; messageNum++) {\n          \n          \n            \n                            long key = messageNum;\n          \n          \n            \n                            long value = messageNum;\n          \n          \n            \n                            lastSendFuture = producer.send(new ProducerRecord<>(topicName, key, value));\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    lastSendFuture.get();\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nA little more code makes it more readable IMO", "author": "losipiuk", "createdAt": "2020-09-25T17:13:09Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+                try {\n+                    if (j < TIMESTAMP_TEST_COUNT) {\n+                        if (j == TIMESTAMP_TEST_START_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            startTime = r.timestamp();\n+                        }\n+                        else if (j == TIMESTAMP_TEST_END_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            endTime = r.timestamp();\n+                        }\n+                        else {\n+                            send.get();\n+                        }\n+                        Thread.sleep(1100);\n+                    }\n+                }\n+                catch (Exception e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        return Pair.of(startTime, endTime);\n+    }\n+\n+    private void createMessages(String topicName)\n+    {\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+            }\n+        }\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "1ee66e841ce0e5710ca2aa3e36c9ba9065f600be", "url": "https://github.com/trinodb/trino/commit/1ee66e841ce0e5710ca2aa3e36c9ba9065f600be", "message": "Add internal Kafka column for OFFSET_TIMESTAMP_FIELD\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-09-28T11:23:40Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwNTY3Ng==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496505676", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls whether push down For topics using ``CreateTime`` mode\n          \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls if upper bound timestamp push down is enabled for topics using ``CreateTime`` mode", "author": "losipiuk", "createdAt": "2020-09-29T08:09:13Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -56,17 +56,18 @@ Configuration Properties\n \n The following configuration properties are available:\n \n-=============================== ==============================================================\n-Property Name                   Description\n-=============================== ==============================================================\n-``kafka.table-names``           List of all tables provided by the catalog\n-``kafka.default-schema``        Default schema name for tables\n-``kafka.nodes``                 List of nodes in the Kafka cluster\n-``kafka.buffer-size``           Kafka read buffer size\n-``kafka.table-description-dir`` Directory containing topic description files\n-``kafka.hide-internal-columns`` Controls whether internal columns are part of the table schema or not\n-``kafka.messages-per-split``    Number of messages that are processed by each Presto split, defaults to 100000\n-=============================== ==============================================================\n+========================================================== ==============================================================================\n+Property Name                                              Description\n+========================================================== ==============================================================================\n+``kafka.table-names``                                      List of all tables provided by the catalog\n+``kafka.default-schema``                                   Default schema name for tables\n+``kafka.nodes``                                            List of nodes in the Kafka cluster\n+``kafka.buffer-size``                                      Kafka read buffer size\n+``kafka.table-description-dir``                            Directory containing topic description files\n+``kafka.hide-internal-columns``                            Controls whether internal columns are part of the table schema or not\n+``kafka.messages-per-split``                               Number of messages that are processed by each Presto split, defaults to 100000\n+``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls whether push down For topics using ``CreateTime`` mode", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwNzAyOA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496507028", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``\n          \n          \n            \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          \n          \n            \n            \n          \n          \n            \n            The upper bound predicate is pushed down only for topics using ``LogAppendTime``\n          \n          \n            \n            mode.\n          \n          \n            \n            \n          \n          \n            \n            For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n          \n          \n            \n            allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n          \n          \n            \n            or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n          \n          \n            \n            \n          \n          \n            \n            This property is optional; the default is ``false``.\n          \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``\n          \n          \n            \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          \n          \n            \n            \n          \n          \n            \n            The upper bound predicate on ``_timestamp`` column \n          \n          \n            \n            is pushed down only for topics using ``LogAppendTime`` mode.\n          \n          \n            \n            \n          \n          \n            \n            For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n          \n          \n            \n            allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n          \n          \n            \n            or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n          \n          \n            \n            \n          \n          \n            \n            This property is optional; the default is ``false``.", "author": "losipiuk", "createdAt": "2020-09-29T08:10:31Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -121,6 +122,18 @@ files (must end with ``.json``) which contain table description files.\n \n This property is optional; the default is ``etc/kafka``.\n \n+``kafka.timestamp-upper-bound-force-push-down-enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+The upper bound predicate is pushed down only for topics using ``LogAppendTime``\n+mode.\n+\n+For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n+allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n+or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n+\n+This property is optional; the default is ``false``.\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwODk2MA==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496508960", "bodyText": "rename computeOffset to overrideFunction for consistency.", "author": "losipiuk", "createdAt": "2020-09-29T08:12:16Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = filterRangeByDomain(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = filterValuesByDomain(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = filterRangeByDomain(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        partition -> (range.getBegin() != INVALID_KAFKA_RANGE_INDEX) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        partition -> (range.getEnd() != INVALID_KAFKA_RANGE_INDEX) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            partition -> findOffsetsForTimestampGreaterOrEqual(kafkaConsumer, partition, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                partition -> findOffsetsForTimestampGreaterOrEqual(kafkaConsumer, partition, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> configMap = describeResult.all().get();\n+\n+            if (configMap != null) {\n+                Config config = configMap.get(topicResource);\n+                String timestampType = config.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get configuration for topic '%s'\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsForTimestampGreaterOrEqual(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = ImmutableMap.builder();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNzc1Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496537752", "bodyText": "Can you make createTopic call out to createTopicWithConfig?", "author": "losipiuk", "createdAt": "2020-09-29T08:38:38Z", "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafka.java", "diffHunk": "@@ -72,6 +72,32 @@ private void createTopic(int partitions, int replication, String topic)\n         }\n     }\n \n+    public void createTopicWithConfig(int partitions, int replication, String topic, boolean enableLogAppendTime)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU4OTk4Mg==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496589982", "bodyText": "I'm not sure. It seems other module will having some test conflicts if using same createTopicWithConfig with identifying the local zookeeper config. So I make new one independently.", "author": "wangli-td", "createdAt": "2020-09-29T09:53:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNzc1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzODU0Nw==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496538547", "bodyText": "Raneme to TestKafkaFilterManager", "author": "losipiuk", "createdAt": "2020-09-29T08:39:23Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaPushDownStatic.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Range;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import org.testng.annotations.Test;\n+\n+import java.util.Set;\n+\n+import static io.prestosql.spi.predicate.Domain.multipleValues;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestKafkaPushDownStatic", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU0NTk0MQ==", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496545941", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                // Make timestamp more expected.\n          \n          \n            \n                                Thread.sleep(20);\n          \n          \n            \n                                // Sleep for a while to ensure different timestamps for different messages.\n          \n          \n            \n                                Thread.sleep(20);\n          \n      \n    \n    \n  \n\nActually you can add an assertion that the timestamps for different messages are different in the tests which use createTimestampTestMessages", "author": "losipiuk", "createdAt": "2020-09-29T08:47:12Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.util.concurrent.Futures;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.time.format.DateTimeFormatter;\n+import java.util.UUID;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown() throws ExecutionException, InterruptedException\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown() throws ExecutionException, InterruptedException\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n+    {\n+        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, timePair.first(), timePair.second());\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n+    {\n+        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, timePair.first(), timePair.second());\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    {\n+        String startTime = null;\n+        String endTime = null;\n+        Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long messageNum = 0; messageNum < MESSAGE_NUM; messageNum++) {\n+                long key = messageNum;\n+                long value = messageNum;\n+                lastSendFuture = producer.send(new ProducerRecord<>(topicName, key, value));\n+                // Record timestamp to build expected timestamp\n+                if (messageNum < TIMESTAMP_TEST_COUNT) {\n+                    if (messageNum == TIMESTAMP_TEST_START_INDEX) {\n+                        RecordMetadata r = lastSendFuture.get();\n+                        startTime = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")\n+                                .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n+                    }\n+                    else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n+                        RecordMetadata r = lastSendFuture.get();\n+                        endTime = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")\n+                                .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n+                    }\n+                    else {\n+                        lastSendFuture.get();\n+                    }\n+                    // Make timestamp more expected.\n+                    Thread.sleep(20);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "599c11fbeb41b83f4274fac21c90cbec3d3e251b", "url": "https://github.com/trinodb/trino/commit/599c11fbeb41b83f4274fac21c90cbec3d3e251b", "message": "Add predicate push down support for internal Kafka columns.\n\nAdd support for predicate pushdown for following Kafka internal columns\n * _timestamp\n * _partition_offset\n * _partition_id\n\n If predicate specifies lower bound on _timestamp column (_timestamp >\n    XXXX), it is always pushed down.\n The upper bound predicate is pushed down only for topics using ``LogAppendTime`` mode.\n For topics using ``CreateTime`` mode, upper bound pushdown must be explicitly\n    allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n    or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-09-29T09:43:00Z", "type": "commit"}, {"oid": "71ccf492010fb20475ba92a217e5d5ea0af67961", "url": "https://github.com/trinodb/trino/commit/71ccf492010fb20475ba92a217e5d5ea0af67961", "message": "kafka connector: add filter pushing down integration test\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-09-29T09:43:01Z", "type": "commit"}, {"oid": "71ccf492010fb20475ba92a217e5d5ea0af67961", "url": "https://github.com/trinodb/trino/commit/71ccf492010fb20475ba92a217e5d5ea0af67961", "message": "kafka connector: add filter pushing down integration test\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-09-29T09:43:01Z", "type": "forcePushed"}]}