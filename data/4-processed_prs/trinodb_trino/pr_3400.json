{"pr_number": 3400, "pr_title": "Support native parquet writer in hive module and Misc fixes", "pr_createdAt": "2020-04-10T03:12:27Z", "pr_url": "https://github.com/trinodb/trino/pull/3400", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTk1Mg==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r419695952", "bodyText": "Consider caching this value if it is expensive to calculate.  In the ORC writer, we update the cached value after each write operation is processed.", "author": "dain", "createdAt": "2020-05-04T20:06:42Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/ParquetWriter.java", "diffHunk": "@@ -85,6 +90,16 @@ public ParquetWriter(OutputStream outputStream, List<String> columnNames, List<T\n         this.chunkMaxLogicalBytes = max(1, writerOption.getMaxBlockSize() / 2);\n     }\n \n+    public long getWrittenBytes()\n+    {\n+        return outputStream.size();\n+    }\n+\n+    public long getBufferedBytes()\n+    {\n+        return columnWriters.stream().mapToLong(ColumnWriter::getBufferedBytes).sum();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5NjE5Mg==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r429596192", "bodyText": "I resolved this. Also, I don't quite understand why in OrcWriter:\n    @Override\n    public long getWrittenBytes()\n    {\n        return orcWriter.getWrittenBytes() + orcWriter.getBufferedBytes();\n    }\n\n\nwhy bufferedBytes is part of written bytes?", "author": "qqibrow", "createdAt": "2020-05-24T03:42:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM3NzE3Nw==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420377177", "bodyText": "The commit message has a typo Add paruqet writer in hive module", "author": "dain", "createdAt": "2020-05-05T20:12:12Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveModule.java", "diffHunk": "@@ -27,6 +27,7 @@\n import io.prestosql.plugin.hive.orc.OrcPageSourceFactory;\n import io.prestosql.plugin.hive.orc.OrcReaderConfig;\n import io.prestosql.plugin.hive.orc.OrcWriterConfig;\n+import io.prestosql.plugin.hive.parquet.ParquetFileWriterFactory;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NDkwNA==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420384904", "bodyText": "Consider adding a toString for debugging.", "author": "dain", "createdAt": "2020-05-05T20:26:43Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.parquet.writer.ParquetWriter;\n+import io.prestosql.parquet.writer.ParquetWriterOptions;\n+import io.prestosql.plugin.hive.FileWriter;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.block.RunLengthEncodedBlock;\n+import io.prestosql.spi.type.Type;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.openjdk.jol.info.ClassLayout;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements FileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName);\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore\n+            }\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error committing write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void rollback()\n+    {\n+        try {\n+            try {\n+                parquetWriter.close();\n+            }\n+            finally {\n+                rollbackAction.call();\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error rolling back write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public long getValidationCpuNanos()\n+    {\n+        return 0;\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NTY1OA==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420385658", "bodyText": "This should be in the previous commit", "author": "dain", "createdAt": "2020-05-05T20:28:14Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -18,6 +18,7 @@\n import io.prestosql.plugin.hive.HdfsEnvironment;\n import io.prestosql.plugin.hive.HiveConfig;\n import io.prestosql.plugin.hive.HiveFileWriterFactory;\n+import io.prestosql.plugin.hive.HiveSessionProperties;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NjA3OA==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420386078", "bodyText": "Capitalize Parquet in description", "author": "dain", "createdAt": "2020-05-05T20:28:57Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetWriterConfig.java", "diffHunk": "@@ -47,6 +50,19 @@ public ParquetWriterConfig setPageSize(DataSize pageSize)\n         return this;\n     }\n \n+    public boolean isParquetOptimizedWriterEnabled()\n+    {\n+        return parquetOptimizedWriterEnabled;\n+    }\n+\n+    @Config(\"hive.parquet.optimized-writer.enabled\")\n+    @ConfigDescription(\"Enable optimized parquet writer\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NjU3Nw==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420386577", "bodyText": "This should be in the commit Support getRetainedBytes in ParquetWriter", "author": "dain", "createdAt": "2020-05-05T20:29:51Z", "path": "presto-parquet/pom.xml", "diffHunk": "@@ -82,6 +82,12 @@\n             <artifactId>slice</artifactId>\n         </dependency>\n \n+        <dependency>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NzMxNA==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420387314", "bodyText": "I don't think this scope is correct.  In the ORC module it not provided.", "author": "dain", "createdAt": "2020-05-05T20:31:08Z", "path": "presto-parquet/pom.xml", "diffHunk": "@@ -82,6 +82,12 @@\n             <artifactId>slice</artifactId>\n         </dependency>\n \n+        <dependency>\n+            <groupId>org.openjdk.jol</groupId>\n+            <artifactId>jol-core</artifactId>\n+            <scope>provided</scope>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5MzUwMA==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420393500", "bodyText": "I would squash this into the commit that added getBufferedBytes.  I think it was Expose written bytes and buffered bytes in ParquetWriter", "author": "dain", "createdAt": "2020-05-05T20:42:46Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "diffHunk": "@@ -296,7 +296,10 @@ private void flushCurrentPageToBuffer()\n     @Override\n     public long getBufferedBytes()\n     {\n-        return pageBuffer.stream().mapToLong(ParquetDataOutput::size).sum() + definitionLevelEncoder.getBufferedSize() + repetitionLevelEncoder.getBufferedSize();\n+        return pageBuffer.stream().mapToLong(ParquetDataOutput::size).sum() +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5NjMyMQ==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420396321", "bodyText": "Are there statistics for non-primitive types?", "author": "dain", "createdAt": "2020-05-05T20:47:59Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "diffHunk": "@@ -102,6 +103,8 @@ public PrimitiveColumnWriter(Type type, ColumnDescriptor columnDescriptor, Primi\n         this.compressionCodec = requireNonNull(compressionCodecName, \"compressionCodecName is null\");\n         this.compressor = getCompressor(compressionCodecName);\n         this.pageSizeThreshold = pageSizeThreshold;\n+\n+        this.columnStatistics = Statistics.createStats(columnDescriptor.getPrimitiveType());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5NjgzMQ==", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r429596831", "bodyText": "No. This aims to fill the stats part for primitive columns https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift#L740 . There is no support for this on non-primitive types.", "author": "qqibrow", "createdAt": "2020-05-24T03:58:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5NjMyMQ=="}], "type": "inlineReview"}, {"oid": "4e48c0fc07d38b053d3f4e1f34309672fc47b882", "url": "https://github.com/trinodb/trino/commit/4e48c0fc07d38b053d3f4e1f34309672fc47b882", "message": "Expose written bytes and buffered bytes in ParquetWriter", "committedDate": "2020-05-24T04:09:22Z", "type": "commit"}, {"oid": "fb5053ae576c07823b8f4505d09a66f95361749b", "url": "https://github.com/trinodb/trino/commit/fb5053ae576c07823b8f4505d09a66f95361749b", "message": "Support getRetainedBytes in ParquetWriter", "committedDate": "2020-05-24T04:09:22Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "330ec3fb1ec6c19776aca816a87c264e430da197", "url": "https://github.com/trinodb/trino/commit/330ec3fb1ec6c19776aca816a87c264e430da197", "message": "Add parquet writer in hive module", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "971ad008d8750e5909dffa1b34e8f8a1b2d92edf", "url": "https://github.com/trinodb/trino/commit/971ad008d8750e5909dffa1b34e8f8a1b2d92edf", "message": "Add test for optimized parquet writer in hive module", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "a5f0ec561553ddf74387cd59a6aef3f9b2d991e3", "url": "https://github.com/trinodb/trino/commit/a5f0ec561553ddf74387cd59a6aef3f9b2d991e3", "message": "Add compression in ParquetFileWriterFactory", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "b9ce66a58f82b9e01dd478433edac8421a44124e", "url": "https://github.com/trinodb/trino/commit/b9ce66a58f82b9e01dd478433edac8421a44124e", "message": "Set statistics in RowGroup metadata", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "17546f03d3f83af5603624cf9e5bafa080df9de7", "url": "https://github.com/trinodb/trino/commit/17546f03d3f83af5603624cf9e5bafa080df9de7", "message": "resetDictionary right after get dictionary page\n\nresetDictionary in reset() cleaned dictionary regardless of the fact\nthat dictionary page is null or not. resetDictionary should happen\nonly after get dictionary page and the page is not null.", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "bb27f1ce64628cb0910a16208781aaa793837ef9", "url": "https://github.com/trinodb/trino/commit/bb27f1ce64628cb0910a16208781aaa793837ef9", "message": "Add encoding should be called after getBytes() and before reset()", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "455914306019c375ad64cea9d938d3162f143ba7", "url": "https://github.com/trinodb/trino/commit/455914306019c375ad64cea9d938d3162f143ba7", "message": "Support setting page size and row group size in parquet writer", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "8c825415e52a3df9beed2ecda41bc0d3735ef5ce", "url": "https://github.com/trinodb/trino/commit/8c825415e52a3df9beed2ecda41bc0d3735ef5ce", "message": "Set statistics in row group metadata", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "d8dcc5d7f6c1617780e0c2f507f82c50f64ed6cc", "url": "https://github.com/trinodb/trino/commit/d8dcc5d7f6c1617780e0c2f507f82c50f64ed6cc", "message": "Set parquet writer page size and row group size in ParquetTester\n\nSet both setting to smaller value could force writer to write multiple\nrow groups and multiple pages in each row group, which could better\nsimulate real world examples.", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "a5f7df3d2b4f3c4861d90bdc0d91565638d58fee", "url": "https://github.com/trinodb/trino/commit/a5f7df3d2b4f3c4861d90bdc0d91565638d58fee", "message": "Set ParquetWriterOptions based on session parameters", "committedDate": "2020-05-24T05:33:19Z", "type": "commit"}, {"oid": "a5f7df3d2b4f3c4861d90bdc0d91565638d58fee", "url": "https://github.com/trinodb/trino/commit/a5f7df3d2b4f3c4861d90bdc0d91565638d58fee", "message": "Set ParquetWriterOptions based on session parameters", "committedDate": "2020-05-24T05:33:19Z", "type": "forcePushed"}]}