{"pr_number": 3396, "pr_title": "Add support for dereference pushdown to parquet reader", "pr_createdAt": "2020-04-09T16:14:05Z", "pr_url": "https://github.com/trinodb/trino/pull/3396", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgyMjE5MA==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406822190", "bodyText": "Nit: Optional.ofNullable(type)?", "author": "lhofhansl", "createdAt": "2020-04-10T15:58:51Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -264,6 +278,50 @@ public static ParquetPageSource createParquetPageSource(\n         }\n     }\n \n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());\n+        }\n+\n+        if (type == null) {\n+            return Optional.empty();\n+        }\n+        return Optional.of(type);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTI1OA==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965258", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    .map(projection -> projection.getReaderColumns())\n          \n          \n            \n                                    .map(ReaderProjections::getReaderColumns)", "author": "martint", "createdAt": "2020-04-10T22:14:32Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns\n+                        .map(projection -> projection.getReaderColumns())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTM2Mg==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965362", "bodyText": "Not sure I understand what the TODO is saying", "author": "martint", "createdAt": "2020-04-10T22:14:59Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTU1OQ==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965559", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            projectedReaderColumns.map(projection -> projection.getReaderColumns()).orElse(columns),\n          \n          \n            \n                            projectedReaderColumns.map(ReaderProjections::getReaderColumns).orElse(columns),", "author": "martint", "createdAt": "2020-04-10T22:15:43Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns\n+                        .map(projection -> projection.getReaderColumns())\n                         .orElse(columns),\n-                isUseParquetColumnNames(session),\n+                useParquetColumnNames,\n                 options\n                         .withFailOnCorruptedStatistics(isFailOnCorruptedParquetStatistics(session))\n                         .withMaxReadBlockSize(getParquetMaxReadBlockSize(session)),\n                 effectivePredicate,\n                 stats);\n \n+        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n+        ConnectorPageSource parquetPageSource = createPageSource(\n+                parquetReader,\n+                projectedReaderColumns.map(projection -> projection.getReaderColumns()).orElse(columns),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NzI5NA==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406967294", "bodyText": "This is no longer used", "author": "martint", "createdAt": "2020-04-10T22:23:04Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -215,28 +244,13 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             MessageColumnIO messageColumnIO = getColumnIO(fileSchema, requestedSchema);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2ODM0Nw==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406968347", "bodyText": "These are unnecessary. They are there only so that createPageSource can get them out of the reader. If, instead, you inlined the createPageSource method or structured it differently, you could use them directly in the caller.", "author": "martint", "createdAt": "2020-04-10T22:27:12Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ParquetReader.java", "diffHunk": "@@ -122,6 +129,16 @@ public ParquetReader(\n         chunkReaders = dataSource.planRead(ranges);\n     }\n \n+    public MessageType getFileSchema()\n+    {\n+        return fileSchema;\n+    }\n+\n+    public MessageColumnIO getMessageColumn()\n+    {\n+        return this.messageColumn;\n+    }\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI5MjA4Mg==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412292082", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n          \n          \n            \n                        return Optional.of(getParquetTypeByName(column.getBaseColumnName(), groupType));", "author": "martint", "createdAt": "2020-04-21T15:41:41Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNTU2NA==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412335564", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        type = groupType.getType(column.getBaseHiveColumnIndex());\n          \n          \n            \n                        Optional.of(groupType.getType(column.getBaseHiveColumnIndex()));", "author": "martint", "createdAt": "2020-04-21T17:07:35Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNTc4Mg==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412335782", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return Optional.ofNullable(type);\n          \n          \n            \n                    return Optional.empty();", "author": "martint", "createdAt": "2020-04-21T17:07:53Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());\n+        }\n+\n+        return Optional.ofNullable(type);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNjIwNQ==", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412336205", "bodyText": "Why is this commented out?", "author": "martint", "createdAt": "2020-04-21T17:08:28Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestHiveIntegrationSmokeTest.java", "diffHunk": "@@ -6303,11 +6321,11 @@ private void testSelectWithNoColumns(Session session, HiveStorageFormat storageF\n     public void testColumnPruning()\n     {\n         Session session = Session.builder(getSession())\n-                .setCatalogSessionProperty(\"hive\", \"orc_use_column_names\", \"true\")\n-                .setCatalogSessionProperty(\"hive\", \"parquet_use_column_names\", \"true\")\n+                .setCatalogSessionProperty(catalog, \"orc_use_column_names\", \"true\")\n+                .setCatalogSessionProperty(catalog, \"parquet_use_column_names\", \"true\")\n                 .build();\n \n-        testWithStorageFormat(new TestingHiveStorageFormat(session, HiveStorageFormat.ORC), this::testColumnPruning);\n+        //testWithStorageFormat(new TestingHiveStorageFormat(session, HiveStorageFormat.ORC), this::testColumnPruning);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "url": "https://github.com/trinodb/trino/commit/6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "message": "Add support for dereference pushdown to parquet reader", "committedDate": "2020-04-22T01:47:27Z", "type": "commit"}, {"oid": "6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "url": "https://github.com/trinodb/trino/commit/6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "message": "Add support for dereference pushdown to parquet reader", "committedDate": "2020-04-22T01:47:27Z", "type": "forcePushed"}]}