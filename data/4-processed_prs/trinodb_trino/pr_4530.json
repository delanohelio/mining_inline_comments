{"pr_number": 4530, "pr_title": "Update Kafka docs with encoder info", "pr_createdAt": "2020-07-22T03:09:21Z", "pr_url": "https://github.com/trinodb/trino/pull/4530", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3MjI2Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r458672263", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The CSV encoder formats the values for each row as a CSV (comma-separated value)\n          \n          \n            \n            line and then converts the formatted values to bytes using UTF-8 encoding.\n          \n          \n            \n            The CSV encoder formats the values for each row as a CSV (comma-separated value)\n          \n          \n            \n            line using UTF-8 encoding.", "author": "losipiuk", "createdAt": "2020-07-22T09:48:28Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,78 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line and then converts the formatted values to bytes using UTF-8 encoding.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NDEyMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r458674120", "bodyText": "Slice.toStringUtf8() is not super widely known. I would just say Unicode string encoded in UTF-8", "author": "losipiuk", "createdAt": "2020-07-22T09:51:39Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,78 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line and then converts the formatted values to bytes using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Formatted using Airlift ``Slice.toStringUtf8()``", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3Njk2Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r458676962", "bodyText": "Maybe just\nTo use ``avro`` encoder, the ``dataSchema`` attribue must be defined.", "author": "losipiuk", "createdAt": "2020-07-22T09:56:35Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,78 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line and then converts the formatted values to bytes using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Formatted using Airlift ``Slice.toStringUtf8()``\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder maps the column values according to the Avro schema.\n+It converts the column values and Avro schema to bytes and sends them to the Kafka topic.\n+Presto does not support schema-less Avro encoding.\n+\n+For key/message, using ``avro`` encoder, the ``dataSchema`` must be defined.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3ODA2MQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r458678061", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This should point to the location of a valid Avro schema file of the message which needs to be encoded. This location can be a remote web server\n          \n          \n            \n            (e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n          \n          \n            \n            The encoder fails if this location is not accessible from the Presto coordinator node.\n          \n          \n            \n            This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n          \n          \n            \n            (e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n          \n          \n            \n            The encoder fails if this location is not accessible from the Presto coordinator node.\n          \n      \n    \n    \n  \n\nAlso fails or will fail?", "author": "losipiuk", "createdAt": "2020-07-22T09:58:20Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,78 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line and then converts the formatted values to bytes using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Formatted using Airlift ``Slice.toStringUtf8()``\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder maps the column values according to the Avro schema.\n+It converts the column values and Avro schema to bytes and sends them to the Kafka topic.\n+Presto does not support schema-less Avro encoding.\n+\n+For key/message, using ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file of the message which needs to be encoded. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder fails if this location is not accessible from the Presto coordinator node.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MzU3MA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459673570", "bodyText": "Do not use future tense..", "author": "mosabua", "createdAt": "2020-07-23T19:19:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3ODA2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3Mzk2Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459673966", "bodyText": "Also ... doesnt it also have to be available on the workers? Isnt write operation also distributed?", "author": "mosabua", "createdAt": "2020-07-23T19:19:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3ODA2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc2Nzg3Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r458767873", "bodyText": "Technically, the encoder does not send the messages to the topic \ud83d\ude09. Perhaps we can simply say:\nThis encoder serializes rows to Avro records as defined by the Avro schema.\nAnd details on how columns are mapped to fields are provided below.", "author": "aalbu", "createdAt": "2020-07-22T12:52:10Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,78 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line and then converts the formatted values to bytes using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Formatted using Airlift ``Slice.toStringUtf8()``\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder maps the column values according to the Avro schema.\n+It converts the column values and Avro schema to bytes and sends them to the Kafka topic.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0OTAzNQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459649035", "bodyText": "Row encoding", "author": "mosabua", "createdAt": "2020-07-23T18:33:45Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDExOQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459650119", "bodyText": "A row encoder defines how Kafka key and message data onto table columns in Presto.", "author": "mosabua", "createdAt": "2020-07-23T18:35:50Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NDc3Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459654777", "bodyText": "Also need to add what encoding is for .. it allows SQL insert statements to be used and defines how data is encoded into the Kafka stream... or something along those lines", "author": "mosabua", "createdAt": "2020-07-23T18:44:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NTM2Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459655363", "bodyText": "Link to the section and also detail there how that is supposed to be done? E.g. where do you put the definition file once you written it and how to you configure Presto to be aware of it and use it.", "author": "mosabua", "createdAt": "2020-07-23T18:45:04Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjEwMQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459656101", "bodyText": "CSV encoder", "author": "mosabua", "createdAt": "2020-07-23T18:46:24Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjY0OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459656649", "bodyText": "as a line of comma-separated values (CSV) using UTF-8 encoding.", "author": "mosabua", "createdAt": "2020-07-23T18:47:25Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NzI1MQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459657251", "bodyText": "The type and mapping attributes must be defined for each field:", "author": "mosabua", "createdAt": "2020-07-23T18:48:25Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NzU4Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459657583", "bodyText": "remove  \"(see table below for list of supported data types)\"", "author": "mosabua", "createdAt": "2020-07-23T18:49:06Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Nzg1NA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459657854", "bodyText": "what does that mean ? A integer number?", "author": "mosabua", "createdAt": "2020-07-23T18:49:33Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODA2Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459658062", "bodyText": "The following table lists...", "author": "mosabua", "createdAt": "2020-07-23T18:49:55Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODM0Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459658346", "bodyText": "Need to add an example for a simple encoding config", "author": "mosabua", "createdAt": "2020-07-23T18:50:24Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODUzOA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459658538", "bodyText": "Avro encoder", "author": "mosabua", "createdAt": "2020-07-23T18:50:44Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODY5OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459658698", "bodyText": "The Avro encoder ...", "author": "mosabua", "createdAt": "2020-07-23T18:51:01Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTA0Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459659047", "bodyText": "The dataSchema must be defined to use the avro encoder.", "author": "mosabua", "createdAt": "2020-07-23T18:51:39Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTU3Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459659577", "bodyText": "The following field attributes are supported:", "author": "mosabua", "createdAt": "2020-07-23T18:52:41Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder will fail if this location is not accessible from the Presto coordinator node.\n+\n+For fields, the following attributes are supported:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTc3NQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459659775", "bodyText": "wrap at 80 char", "author": "mosabua", "createdAt": "2020-07-23T18:53:03Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder will fail if this location is not accessible from the Presto coordinator node.\n+\n+For fields, the following attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the Avro schema. If field specified in ``mapping`` does not exist in the original Avro schema then a write operation fails.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MDEwNw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459660107", "bodyText": "If the field specified ..", "author": "mosabua", "createdAt": "2020-07-23T18:53:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTk5Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459661996", "bodyText": "It points to the location of the Avro schema file for the messages.\nFiles can be retrieved via HTTP or HTTPS from remote server with the syntax dataSchema: 'http://example.org/schema/avro_data.avsc'.  Local files need to be available on all Presto nodes and use a absolute path in the syntax, for example  dataSchema: '/usr/local/schema/avro_data.avsc'.", "author": "mosabua", "createdAt": "2020-07-23T18:57:02Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MjE1NA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459662154", "bodyText": "Also need to add an example", "author": "mosabua", "createdAt": "2020-07-23T18:57:21Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder will fail if this location is not accessible from the Presto coordinator node.\n+\n+For fields, the following attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the Avro schema. If field specified in ``mapping`` does not exist in the original Avro schema then a write operation fails.\n+\n+Table below lists supported Presto types which can be used in ``type`` for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Allowed Avro data type\n+===================================== =======================================\n+``BIGINT``                            ``INT``, ``LONG``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``REAL``                              ``FLOAT``\n+``BOOLEAN``                           ``BOOLEAN``\n+``VARCHAR`` / ``VARCHAR(x)``          ``STRING``\n+===================================== =======================================\n+\n Row Decoding", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzcxOA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459663718", "bodyText": "remove \"Allowed\"", "author": "mosabua", "createdAt": "2020-07-23T19:00:17Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder will fail if this location is not accessible from the Presto coordinator node.\n+\n+For fields, the following attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the Avro schema. If field specified in ``mapping`` does not exist in the original Avro schema then a write operation fails.\n+\n+Table below lists supported Presto types which can be used in ``type`` for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Allowed Avro data type", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzkzMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459663930", "bodyText": "The following table list supported Presto types, which", "author": "mosabua", "createdAt": "2020-07-23T19:00:43Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +259,77 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row Encoding\n+------------\n+\n+For key and message, an encoder is used to map message and key data onto table columns.\n+\n+The Kafka connector contains the following encoders:\n+\n+* ``csv`` - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* ``avro`` - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work.\n+\n+``csv`` Encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a CSV (comma-separated value)\n+line using UTF-8 encoding.\n+\n+For fields, the ``type`` and ``mapping`` attributes must be defined:\n+\n+* ``type`` - Presto data type (see table below for list of supported data types)\n+* ``mapping`` - the index of the field in the CSV record\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+Table below lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+``avro`` Encoder\n+^^^^^^^^^^^^^^^^\n+\n+This encoder serializes rows to Avro records as defined by the Avro schema.\n+Presto does not support schema-less Avro encoding.\n+\n+To use ``avro`` encoder, the ``dataSchema`` must be defined.\n+This should point to the location of a valid Avro schema file matching the messages. This location can be a remote web server\n+(e.g.: ``dataSchema: 'http://example.org/schema/avro_data.avsc'``) or local file system(e.g.: ``dataSchema: '/usr/local/schema/avro_data.avsc'``).\n+The encoder will fail if this location is not accessible from the Presto coordinator node.\n+\n+For fields, the following attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the Avro schema. If field specified in ``mapping`` does not exist in the original Avro schema then a write operation fails.\n+\n+Table below lists supported Presto types which can be used in ``type`` for the equivalent Avro field type/s.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NzY3OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459747678", "bodyText": "remove backticks here and below...", "author": "mosabua", "createdAt": "2020-07-23T21:49:39Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,133 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* `CSV encoder`_ - Kafka message is formatted as comma separated message, and fields are mapped to table columns", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0ODIxOA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459748218", "bodyText": "So where do you put that file and how do you configure Presto / the connector to use it", "author": "mosabua", "createdAt": "2020-07-23T21:50:59Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,133 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* `CSV encoder`_ - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* `Avro encoder`_ - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of comma-separated-values (CSV)\n+using UTF-8 encoding.\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition for a Kafka message:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0ODYyNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r459748624", "bodyText": "Place the file in the directory configure with the kafka.table-description-dir property.", "author": "mosabua", "createdAt": "2020-07-23T21:51:59Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -178,10 +178,12 @@ this data must be mapped into columns to allow queries against the data.\n     use any table definition files, but instead use the Presto\n     :doc:`/functions/json` to parse the ``_message`` column which contains\n     the bytes mapped into an UTF-8 string. This is, however, pretty\n-    cumbersome and makes it difficult to write SQL queries.\n+    cumbersome and makes it difficult to write SQL queries. This only works\n+    when reading data.\n \n A table definition file consists of a JSON definition for a table. The\n-name of the file can be arbitrary but must end in ``.json``.\n+name of the file can be arbitrary but must end in ``.json``. This file lives", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzMTkzNg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460131936", "bodyText": "Table column bytes -> Column values?", "author": "aalbu", "createdAt": "2020-07-24T15:39:19Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.\n+\n+For fields, the following attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of dataFormat can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of the bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon (``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined, otherwise there is no way to know how many bytes\n+    the message contains. The raw format mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are used.\n+\n+If no ``mapping`` attribute is specified, it is equivalent to setting start position to 0 and leaving end position undefined.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``, ``TINYINT``, ``DOUBLE``) is straightforward.\n+Table column bytes are encoded to bytes according to either:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzNjYyMg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460136622", "bodyText": "Use single quotes.", "author": "aalbu", "createdAt": "2020-07-24T15:47:18Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.\n+\n+For fields, the following attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of dataFormat can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of the bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon (``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined, otherwise there is no way to know how many bytes\n+    the message contains. The raw format mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are used.\n+\n+If no ``mapping`` attribute is specified, it is equivalent to setting start position to 0 and leaving end position undefined.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``, ``TINYINT``, ``DOUBLE``) is straightforward.\n+Table column bytes are encoded to bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a table definition file (`Table Definition Files`_) for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4) VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of comma-separated-values (CSV)\n+using UTF-8 encoding.\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type`` and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a table definition file (`Table Definition Files`_) for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3) VALUES (123456789, \"example text\", TRUE);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NTcxNQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460175715", "bodyText": "does that reference style work? this makes a local anchor for a local title ... in this case I think its fine not to link it and instead use double backtick it as code. Otherwise use :ref: syntax", "author": "mosabua", "createdAt": "2020-07-24T16:58:11Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -178,10 +178,12 @@ this data must be mapped into columns to allow queries against the data.\n     use any table definition files, but instead use the Presto\n     :doc:`/functions/json` to parse the ``_message`` column which contains\n     the bytes mapped into an UTF-8 string. This is, however, pretty\n-    cumbersome and makes it difficult to write SQL queries.\n+    cumbersome and makes it difficult to write SQL queries. This only works\n+    when reading data.\n \n A table definition file consists of a JSON definition for a table. The\n-name of the file can be arbitrary but must end in ``.json``.\n+name of the file can be arbitrary but must end in ``.json``. Place the\n+file in the directory configured with the `kafka.table-description-dir`_ property.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NjY4MQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460176681", "bodyText": "https://developers.google.com/style/link-text .. so in this case set an anchor and the use :ref:in the table definition file <my-anchor> .. ping me on slack if you want to edit together", "author": "mosabua", "createdAt": "2020-07-24T17:00:00Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3Njc3Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460176777", "bodyText": "new paragraph", "author": "mosabua", "createdAt": "2020-07-24T17:00:14Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NzE4Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460177182", "bodyText": "message encoding:\n\nraw\nCSV\nAvro\n\nThese data...", "author": "mosabua", "createdAt": "2020-07-24T17:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3Njc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3ODgxNg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460178816", "bodyText": ".. supports the use of :doc:/sql/insert statements ..", "author": "mosabua", "createdAt": "2020-07-24T17:04:34Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3ODkyMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460178920", "bodyText": "remove", "author": "mosabua", "createdAt": "2020-07-24T17:04:46Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3OTEzMQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460179131", "bodyText": "remove (since you got example in context below ..)", "author": "mosabua", "createdAt": "2020-07-24T17:05:07Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MDIyMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460180220", "bodyText": "comma separated values\nits CSV not CSM ;-)", "author": "mosabua", "createdAt": "2020-07-24T17:07:27Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MDM1MA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460180350", "bodyText": "again .. descriptive link with anchor", "author": "mosabua", "createdAt": "2020-07-24T17:07:47Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MDk2OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460180969", "bodyText": "descriptive link", "author": "mosabua", "createdAt": "2020-07-24T17:09:03Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MTA5Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460181093", "bodyText": "The following field attributes are supported:", "author": "mosabua", "createdAt": "2020-07-24T17:09:19Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.\n+\n+For fields, the following attributes are supported:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MTc4NQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460181785", "bodyText": "Depending on the Presto type assigned to a column different values of dataFormat can be used:", "author": "mosabua", "createdAt": "2020-07-24T17:10:36Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.\n+\n+For fields, the following attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of dataFormat can be used:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MjA5Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r460182093", "bodyText": "defined. Otherwise", "author": "mosabua", "createdAt": "2020-07-24T17:11:14Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +261,266 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of SQL ``INSERT INTO`` statements to write data to a Kafka topic.\n+Table column data is mapped to Kafka messages as defined in the table definition file, `Table Definition Files`_.\n+There are currently three supported data formats for key and message encoding, raw, CSV, and Avro. These data\n+formats each have an encoder that handles mapping table column values.\n+\n+Example ``INSERT INTO`` query for a hypothetical \"product\" table with fields name (VARCHAR), and key (BIGINT):\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO product (name, key) VALUES ('product name', 12345);\n+\n+Row encoding\n+------------\n+\n+Encoding allows SQL insert statements to be used and defines how table columns in Presto\n+map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* raw encoder - Table columns are mapped to Kafka message as raw bytes\n+* CSV encoder - Kafka message is formatted as comma separated message, and fields are mapped to table columns\n+* Avro encoder - Kafka message is mapped to Avro fields based on an Avro schema\n+\n+.. note::\n+\n+    A table definition file must be defined for the encoder to work. `Table Definition Files`_\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping information specified in the\n+table definition file, `Table Definition Files`_.\n+\n+For fields, the following attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of dataFormat can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of the bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon (``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined, otherwise there is no way to know how many bytes", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTUyNg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r464679526", "bodyText": "shorten to be same as title Raw encoder", "author": "mosabua", "createdAt": "2020-08-03T21:47:13Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<kafka-raw-encoder>`\n+* :ref:`CSV format<kafka-csv-encoder>`\n+* :ref:`JSON format<kafka-json-encoder>`\n+* :ref:`Avro format<kafka-avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<kafka-raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<kafka-csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<kafka-json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<kafka-avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _kafka-raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIzOTU4Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465239583", "bodyText": "Title line indicators should be same width as the text", "author": "electrum", "createdAt": "2020-08-04T18:15:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTUyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTcyMw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r464679723", "bodyText": "shorten", "author": "mosabua", "createdAt": "2020-08-03T21:47:40Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<kafka-raw-encoder>`\n+* :ref:`CSV format<kafka-csv-encoder>`\n+* :ref:`JSON format<kafka-json-encoder>`\n+* :ref:`Avro format<kafka-avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<kafka-raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<kafka-csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<kafka-json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<kafka-avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _kafka-raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _kafka-csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTc2MA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r464679760", "bodyText": "shorten", "author": "mosabua", "createdAt": "2020-08-03T21:47:49Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<kafka-raw-encoder>`\n+* :ref:`CSV format<kafka-csv-encoder>`\n+* :ref:`JSON format<kafka-json-encoder>`\n+* :ref:`Avro format<kafka-avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<kafka-raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<kafka-csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<kafka-json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<kafka-avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _kafka-raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _kafka-csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _kafka-json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTgzOQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r464679839", "bodyText": "Shorten", "author": "mosabua", "createdAt": "2020-08-03T21:47:59Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<kafka-raw-encoder>`\n+* :ref:`CSV format<kafka-csv-encoder>`\n+* :ref:`JSON format<kafka-json-encoder>`\n+* :ref:`Avro format<kafka-avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<kafka-raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<kafka-csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<kafka-json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<kafka-avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _kafka-raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _kafka-csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _kafka-json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _kafka-avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NzI3OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257278", "bodyText": "Fix header width", "author": "electrum", "createdAt": "2020-08-04T18:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTgzOQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIzODkzNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465238934", "bodyText": "No need to add references for links in the same document. You can link here as\n`table definition file <#table-definition-files>`__", "author": "electrum", "createdAt": "2020-08-04T18:14:49Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -165,6 +165,8 @@ Column name             Type      Description\n For tables without a table definition file, the ``_key_corrupt`` and\n ``_message_corrupt`` columns will always be ``false``.\n \n+.. _kafka-table-definition-files:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIzOTkzNw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465239937", "bodyText": "Use inline links for all of these. Care needs to be taken when adding references, as these names are global. In this case, we don't need them at all.", "author": "electrum", "createdAt": "2020-08-04T18:16:40Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0MDY3Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465240676", "bodyText": "Make these uppercase \"Kafka Inserts\" to match the existing titles in the document. While the new style guide says to use lowercase, that should only be followed for new documents, as we don't want to be inconsistent in the same document.", "author": "electrum", "createdAt": "2020-08-04T18:17:55Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0MTE4Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465241186", "bodyText": "We could shorten to\n\nThese data formats each have an encoder that maps column values into bytes to be sent to a Kafka topic.", "author": "electrum", "createdAt": "2020-08-04T18:18:52Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0MjM2OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465242369", "bodyText": "Comma before \"it\"", "author": "electrum", "createdAt": "2020-08-04T18:20:57Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0MjkwOA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465242908", "bodyText": "Avoid ending in preposition\n\nThe Kafka connector does not allow the user to define which partition will be used as the target for a message. If a message includes a key, the producer will use a hash algorithm to choose the target partition for the message. The same key will always be assigned the same partition.", "author": "electrum", "createdAt": "2020-08-04T18:21:53Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NDMwMw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465244303", "bodyText": "How about\n\nEncoding is required to allow writing data and defines how table columns in Presto map to Kafka keys and message data.", "author": "electrum", "createdAt": "2020-08-04T18:24:12Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NDc2OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465244768", "bodyText": "message -> messages\nOr\n\"to a Kafka message\"", "author": "electrum", "createdAt": "2020-08-04T18:24:59Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NTM5Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465245392", "bodyText": "Should this be \"Table columns are mapped to\"", "author": "electrum", "createdAt": "2020-08-04T18:25:50Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NTY4NA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465245684", "bodyText": "Selects -> Specifies", "author": "electrum", "createdAt": "2020-08-04T18:26:18Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NjM2NQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465246365", "bodyText": "* ``mapping`` - start and optional end position of bytes to convert (specified as ``start`` or ``start:end``)", "author": "electrum", "createdAt": "2020-08-04T18:27:42Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NzA0OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465247049", "bodyText": "Add big-endian to both of these (assuming it is big endian)", "author": "electrum", "createdAt": "2020-08-04T18:28:55Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NzE4OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465247188", "bodyText": "Remove \"are\"", "author": "electrum", "createdAt": "2020-08-04T18:29:10Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NzY0OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465247649", "bodyText": "Different values of ``dataFormat`` are supported, depending on the Presto data type:", "author": "electrum", "createdAt": "2020-08-04T18:29:58Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NzgyNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465247824", "bodyText": "Do we have a mapping for REAL?", "author": "electrum", "createdAt": "2020-08-04T18:30:14Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0ODA0Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465248047", "bodyText": "Maybe put FLOAT first since it is shorter", "author": "electrum", "createdAt": "2020-08-04T18:30:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0NzgyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0ODM3Mw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465248373", "bodyText": "Let's remove this second sentence since we specify it above", "author": "electrum", "createdAt": "2020-08-04T18:31:11Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0ODU3Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465248577", "bodyText": "Flip first sentence (assuming \"end\" is required)\nBoth a start and end position must be defined for ``VARCHAR`` types", "author": "electrum", "createdAt": "2020-08-04T18:31:34Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0ODk5Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465248992", "bodyText": "Comma after \"otherwise\"", "author": "electrum", "createdAt": "2020-08-04T18:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0ODU3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0OTIyNw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465249227", "bodyText": "Comma after \"types\"", "author": "electrum", "createdAt": "2020-08-04T18:32:45Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0OTI5NQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465249295", "bodyText": "\"If both\"", "author": "electrum", "createdAt": "2020-08-04T18:32:52Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0OTM5MQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465249391", "bodyText": "Comma after \"types\"", "author": "electrum", "createdAt": "2020-08-04T18:33:03Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI0OTQzNg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465249436", "bodyText": "Comma after varchar", "author": "electrum", "createdAt": "2020-08-04T18:33:09Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MDE0Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465250147", "bodyText": "It's not clear what this applies to. If for varchar, it seems redundant with the above note.", "author": "electrum", "createdAt": "2020-08-04T18:34:31Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MDUyMg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465250522", "bodyText": "Perhaps this should be\n\nAll mappings must include a start position for encoding to work.\n\nWhat happens if it does not have one?", "author": "electrum", "createdAt": "2020-08-04T18:35:13Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3NDk3OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465274979", "bodyText": "We could \"guess\" the width of columns with a defined dataFormat and compute the start position of columns based on the previous widths, but the decoder doesn't do this. The raw format is very specific about how data has to be serialized, and there isn't really any room for error. I didn't want to make any big changes to the raw format when implementing the encoder. The raw format needs to be rethought, I figured that would be better left for a future pr.", "author": "charlesjmorgan", "createdAt": "2020-08-04T19:21:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MDUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MTQzNQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465251435", "bodyText": "The encoding for numeric data types (...) is straightforward. All numeric types use big-endian. Floating point types use IEEE 754 format.", "author": "electrum", "createdAt": "2020-08-04T18:36:51Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MTU1OA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465251558", "bodyText": "Should this be type 'json'", "author": "electrum", "createdAt": "2020-08-04T18:37:08Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI4ODE0Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465288142", "bodyText": "json doesn't seem to work, not sure why", "author": "charlesjmorgan", "createdAt": "2020-08-04T19:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MTU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI4OTY1Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465289652", "bodyText": "it might be because it isn't valid json, it's a fragment of a json file", "author": "charlesjmorgan", "createdAt": "2020-08-04T19:45:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MTU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MjEwMw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465252103", "bodyText": "The indentation seems off here. Let's use two-space indent to be consistent with other documentation.", "author": "electrum", "createdAt": "2020-08-04T18:38:05Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MjcyNw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465252727", "bodyText": "Let's make this a non-note, since this seems like normal explanatory text. Having too many notes makes documentation harder to read (and reduces the values of a note).", "author": "electrum", "createdAt": "2020-08-04T18:39:21Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MzIzMQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465253231", "bodyText": "Let's just say \"Example insert query\" (no need to use SQL keyword)", "author": "electrum", "createdAt": "2020-08-04T18:40:20Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MzUwNg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465253506", "bodyText": "Note that SQL is the default language type, so we can use the shorthand (double colon)\nExample insert query for the above table definition::", "author": "electrum", "createdAt": "2020-08-04T18:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MzIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1Mzc2Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465253767", "bodyText": "Same comment about note", "author": "electrum", "createdAt": "2020-08-04T18:41:15Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1MzgxMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465253810", "bodyText": "Comma after \"types\"", "author": "electrum", "createdAt": "2020-08-04T18:41:22Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NDMxNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465254314", "bodyText": "Make this a second sentence or include it in the above sentence.", "author": "electrum", "createdAt": "2020-08-04T18:42:17Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NTUyNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465255524", "bodyText": "We shouldn't talk about Java implementation here. For integers, these are simply converted to strings. For floating point, these are converted to strings as if casted to varchar in Presto. Perhaps it's fine to simply say \"converted to a string\"", "author": "electrum", "createdAt": "2020-08-04T18:44:43Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NzAxMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257010", "bodyText": "json", "author": "electrum", "createdAt": "2020-08-04T18:47:22Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NzA4MA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257080", "bodyText": "Same comment about indentation", "author": "electrum", "createdAt": "2020-08-04T18:47:30Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NzE4Ng==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257186", "bodyText": "Same comment as above", "author": "electrum", "createdAt": "2020-08-04T18:47:43Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1NzM3OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257379", "bodyText": "Remove index.html and use https", "author": "electrum", "createdAt": "2020-08-04T18:48:05Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1Nzg4Mg==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465257882", "bodyText": "Comma after \"schema\"", "author": "electrum", "createdAt": "2020-08-04T18:49:03Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1ODAzMQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465258031", "bodyText": "Remove /s", "author": "electrum", "createdAt": "2020-08-04T18:49:19Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+for the equivalent Avro field type/s.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1ODIxNA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465258214", "bodyText": "Put REAL before double", "author": "electrum", "createdAt": "2020-08-04T18:49:39Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Avro data type\n+===================================== =======================================\n+``BIGINT``                            ``INT``, ``LONG``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``REAL``                              ``FLOAT``", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1ODI4OQ==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465258289", "bodyText": "Put FLOAT first since it's smaller", "author": "electrum", "createdAt": "2020-08-04T18:49:47Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Avro data type\n+===================================== =======================================\n+``BIGINT``                            ``INT``, ``LONG``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1ODQwMA==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465258400", "bodyText": "Same comments about JSON", "author": "electrum", "createdAt": "2020-08-04T18:50:00Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Avro data type\n+===================================== =======================================\n+``BIGINT``                            ``INT``, ``LONG``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``REAL``                              ``FLOAT``\n+``BOOLEAN``                           ``BOOLEAN``\n+``VARCHAR`` / ``VARCHAR(x)``          ``STRING``\n+===================================== =======================================\n+\n+Example Avro field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI1ODQ2Nw==", "url": "https://github.com/trinodb/trino/pull/4530#discussion_r465258467", "bodyText": "Same comment", "author": "electrum", "createdAt": "2020-08-04T18:50:07Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -259,6 +264,430 @@ Field           Required  Type      Description\n \n There is no limit on field descriptions for either key or message.\n \n+Kafka inserts\n+-------------\n+\n+The Kafka connector supports the use of :doc:`/sql/insert` statements to write\n+data to a Kafka topic. Table column data is mapped to Kafka messages as defined\n+in the :ref:`table definition file<kafka-table-definition-files>`. There are\n+four supported data formats for key and message encoding:\n+\n+* :ref:`raw format<raw-encoder>`\n+* :ref:`CSV format<csv-encoder>`\n+* :ref:`JSON format<json-encoder>`\n+* :ref:`Avro format<avro-encoder>`\n+\n+These data formats each have an encoder that handles mapping table column values.\n+Table column values are mapped by the encoders and sent to Kafka topics as bytes.\n+\n+Presto supports at-least-once delivery for Kafka producers. This means that\n+messages are guaranteed to be sent to Kafka topics at least once. If a producer\n+acknowledgement times out or if the producer receives an error it might retry\n+sending the message. This could result in a duplicate message being sent to the\n+Kafka topic.\n+\n+The Kafka connector does not allow the user to define which partition to send a\n+message to. If a key is inserted with the message the producer will use a hash\n+algorithm to decide which partition to send the message to. The same key will\n+always be assigned the same partition.\n+\n+Row encoding\n+------------\n+\n+Encoding allows :doc:`/sql/insert` statements to be used and defines how table\n+columns in Presto map to Kafka key and message data.\n+\n+The Kafka connector contains the following encoders:\n+\n+* :ref:`raw encoder<raw-encoder>` - Table columns are mapped to Kafka\n+  message as raw bytes\n+* :ref:`CSV encoder<csv-encoder>` - Kafka message is formatted as comma\n+  separated value\n+* :ref:`JSON encoder<json-encoder>` - Table columns are mapped to JSON\n+  fields\n+* :ref:`Avro encoder<avro-encoder>` - Kafka message is mapped to Avro\n+  fields based on an Avro schema\n+\n+.. note::\n+\n+    A :ref:`table definition file<kafka-table-definition-files>` must be defined\n+    for the encoder to work.\n+\n+.. _raw-encoder:\n+\n+raw encoder\n+^^^^^^^^^^^^^^^\n+\n+The raw encoder formats the table columns as raw bytes using the mapping\n+information specified in the\n+:ref:`table definition file<kafka-table-definition-files>`.\n+\n+The following field attributes are supported:\n+\n+* ``dataFormat`` - Selects the width of the column data type\n+* ``type`` - Presto data type\n+* ``mapping`` - ``<start>[:<end>]``; start and end position of bytes to convert\n+\n+The ``dataFormat`` attribute selects the number of bytes converted.\n+If absent, ``BYTE`` is assumed. All values are signed.\n+\n+Supported values are:\n+\n+* ``BYTE`` - one byte\n+* ``SHORT`` - two bytes (big-endian)\n+* ``INT`` - four bytes (big-endian)\n+* ``LONG`` - eight bytes (big-endian)\n+* ``FLOAT`` - four bytes (IEEE 754 format)\n+* ``DOUBLE`` - eight bytes (IEEE 754 format)\n+\n+The ``type`` attribute defines the Presto data type.\n+\n+Depending on Presto type assigned to column different values of ``dataFormat``\n+can be used:\n+\n+===================================== =======================================\n+Presto data type                      ``dataFormat`` values\n+===================================== =======================================\n+``BIGINT``                            ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``INTEGER``                           ``BYTE``, ``SHORT``, ``INT``\n+``SMALLINT``                          ``BYTE``, ``SHORT``\n+``TINYINT``                           ``BYTE``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``BOOLEAN``                           ``BYTE``, ``SHORT``, ``INT``, ``LONG``\n+``VARCHAR`` / ``VARCHAR(x)``          ``BYTE``\n+===================================== =======================================\n+\n+The ``mapping`` attribute specifies the range of bytes in a key or\n+message used for encoding. It can be one or two numbers separated by a colon\n+(``<start>[:<end>]``).\n+\n+.. note::\n+\n+    For ``VARCHAR`` types a start and end position must be defined. Otherwise\n+    there is no way to know how many bytes the message contains. The raw format\n+    mapping information is static and cannot be dynamically changed to fit the\n+    variable width of some Presto data types.\n+\n+If only a start position is given:\n+\n+* For fixed width types the appropriate number of bytes are used for the\n+  specified ``dateFormat`` (see above).\n+\n+If start and end position are given, then:\n+\n+* For fixed width types the size must be equal to number of bytes used by\n+  specified ``dataFormat``.\n+* For ``VARCHAR`` all bytes between start (inclusive) and end (exclusive) are\n+  used.\n+\n+.. note::\n+\n+    At the very least a start value must be present in the mapping for encoding\n+    to work.\n+\n+Encoding scheme of numeric data types (``BIGINT``, ``INTEGER``, ``SMALLINT``,\n+``TINYINT``, ``DOUBLE``) is straightforward. Table column values are encoded to\n+bytes according to either:\n+\n+* big-endian encoding (for integer types)\n+* IEEE 754 format for (for ``DOUBLE``).\n+\n+Example raw field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"raw\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"INTEGER\",\n+                    \"dataFormat\": \"INT\"\n+                    \"mapping\": \"8\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"SMALLINT\",\n+                    \"dataFormat\": \"LONG\",\n+                    \"mapping\": \"12\"\n+                },\n+                {\n+                    \"name\": \"field4\",\n+                    \"type\": \"VARCHAR(6)\",\n+                    \"dataFormat\": \"BYTE\",\n+                    \"mapping\": \"20:26\"\n+                }\n+            ]\n+    }\n+\n+.. note::\n+\n+    Columns should be defined in the same order they are mapped. There can be\n+    no gaps or overlaps between column mappings. The width of the column as\n+    defined by the column mapping must be equivalent to the width of the\n+    ``dataFormat`` for all types except for variable width types.\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_raw_table (field1, field2, field3, field4)\n+      VALUES (123456789, 123456, 1234, 'abcdef');\n+\n+.. note::\n+\n+    When inserting variable width types the value must be exactly equal to the\n+    width defined in the table definition file. If the inserted value is longer\n+    it gets truncated, resulting in data loss. If the inserted value is shorter\n+    the decoder will not be able to properly read the value because there is no\n+    defined padding character. Due to these constraints the encoder fails if the\n+    width of the inserted value is not equal to the mapping width.\n+\n+.. _csv-encoder:\n+\n+CSV encoder\n+^^^^^^^^^^^^^^^\n+\n+The CSV encoder formats the values for each row as a line of\n+comma-separated-values (CSV) using UTF-8 encoding.\n+\n+.. note::\n+\n+    The CSV line is formatted with a comma ',' as the column delimiter\n+\n+The ``type`` and ``mapping`` attributes must be defined for each field:\n+\n+* ``type`` - Presto data type\n+* ``mapping`` - The integer index of the column in the CSV line (the first\n+  column is 0, the second is 1, and so on)\n+\n+``dataFormat`` and ``formatHint`` are not supported and must be omitted.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+and encoding scheme:\n+\n+============================= ================================================\n+Presto data type              Encoding rules\n+============================= ================================================\n+``BIGINT``                    Formatted using Java ``Long.toString()``\n+``INTEGER``                   Formatted using Java ``Integer.toString()``\n+``SMALLINT``                  Formatted using Java ``Short.toString()``\n+``TINYINT``                   Formatted using Java ``Byte.toString()``\n+``DOUBLE``                    Formatted using Java ``Double.toString()``\n+``REAL``                      Formatted using Java ``Float.toString()``\n+``BOOLEAN``                   Formatted using Java ``Boolean.toString()``\n+``VARCHAR`` / ``VARCHAR(x)``  Unicode String encoded in UTF-8\n+============================= ================================================\n+\n+Example CSV field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"csv\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"0\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"1\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"2\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_csv_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _json-encoder:\n+\n+JSON encoder\n+^^^^^^^^^^^^^^^^\n+\n+The JSON encoder maps table columns to JSON fields defined in the\n+:ref:`table definition file<kafka-table-definition-files>` according to\n+:rfc:`4627`.\n+\n+For fields, the following attributes are supported:\n+\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  JSON object\n+\n+The following Presto data types are supported by the JSON encoder\n+\n+* ``BIGINT``\n+* ``INTEGER``\n+* ``SMALLINT``\n+* ``TINYINT``\n+* ``DOUBLE``\n+* ``REAL``\n+* ``BOOLEAN``\n+* ``VARCHAR``\n+\n+Example JSON field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"json\",\n+            \"fields\": [\n+                {\n+                    \"name\": \"field1\",\n+                    \"type\": \"BIGINT\",\n+                    \"mapping\": \"field1\"\n+                },\n+                {\n+                    \"name\": \"field2\",\n+                    \"type\": \"VARCHAR\",\n+                    \"mapping\": \"field2\"\n+                },\n+                {\n+                    \"name\": \"field3\",\n+                    \"type\": \"BOOLEAN\",\n+                    \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:\n+\n+.. code-block:: SQL\n+\n+    INSERT INTO example_json_table (field1, field2, field3)\n+      VALUES (123456789, 'example text', TRUE);\n+\n+.. _avro-encoder:\n+\n+Avro encoder\n+^^^^^^^^^^^^^^^^\n+\n+The Avro encoder serializes rows to Avro records as defined by the\n+`Avro schema <http://avro.apache.org/docs/current/index.html>`_.\n+Presto does not support schema-less Avro encoding.\n+\n+.. note::\n+\n+    The Avro schema is encoded with the table column values in each Kafka message\n+\n+The ``dataSchema`` must be defined in the table definition file to use the Avro\n+encoder. It points to the location of the Avro schema file for the key or message\n+\n+Avro schema files can be retrieved via HTTP or HTTPS from remote server with the\n+syntax:\n+\n+``\"dataSchema\": \"http://example.org/schema/avro_data.avsc\"``\n+\n+Local files need to be available on all Presto nodes and use an absolute path in\n+the syntax, for example:\n+\n+``\"dataSchema\": \"/usr/local/schema/avro_data.avsc\"``\n+\n+The following field attributes are supported:\n+\n+* ``name`` - Name of the column in the Presto table.\n+* ``type`` - Presto type of column.\n+* ``mapping`` - slash-separated list of field names to select a field from the\n+  Avro schema. If the field specified in ``mapping`` does not exist\n+  in the original Avro schema then a write operation fails.\n+\n+The following table lists supported Presto types, which can be used in ``type``\n+for the equivalent Avro field type/s.\n+\n+===================================== =======================================\n+Presto data type                      Avro data type\n+===================================== =======================================\n+``BIGINT``                            ``INT``, ``LONG``\n+``DOUBLE``                            ``DOUBLE``, ``FLOAT``\n+``REAL``                              ``FLOAT``\n+``BOOLEAN``                           ``BOOLEAN``\n+``VARCHAR`` / ``VARCHAR(x)``          ``STRING``\n+===================================== =======================================\n+\n+Example Avro field definition in a :ref:`table definition file<kafka-table-definition-files>`\n+for a Kafka message:\n+\n+.. code-block:: none\n+\n+    \"message\": {\n+            \"dataFormat\": \"avro\",\n+            \"dataSchema\": \"/avro_message_schema.avsc\",\n+            \"fields\": [\n+                {\n+                \"name\": \"field1\",\n+                \"type\": \"BIGINT\",\n+                \"mapping\": \"field1\"\n+                },\n+                {\n+                \"name\": \"field2\",\n+                \"type\": \"VARCHAR\",\n+                \"mapping\": \"field2\"\n+                },\n+                {\n+                \"name\": \"field3\",\n+                \"type\": \"BOOLEAN\",\n+                \"mapping\": \"field3\"\n+                }\n+            ]\n+    }\n+\n+Example Avro schema definition for the above table definition:\n+\n+.. code-block:: none\n+\n+    {\n+      \"type\" : \"record\",\n+      \"name\" : \"example_avro_message\",\n+      \"namespace\" : \"io.prestosql.plugin.kafka\",\n+      \"fields\" :\n+      [\n+                  {\n+                      \"name\":\"field1\",\n+                      \"type\":[\"null\", \"long\"],\n+                      \"default\": null\n+                  },\n+                  {\n+                      \"name\": \"field2\",\n+                      \"type\":[\"null\", \"string\"],\n+                      \"default\": null\n+                  },\n+                  {\n+                      \"name\":\"field3\",\n+                      \"type\":[\"null\", \"boolean\"],\n+                      \"default\": null\n+                  }\n+      ],\n+      \"doc:\" : \"A basic avro schema\"\n+    }\n+\n+Example ``INSERT INTO`` query for the above table definition:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a1f973cfd4487ccf0773b9f0c44cca8902086e82", "url": "https://github.com/trinodb/trino/commit/a1f973cfd4487ccf0773b9f0c44cca8902086e82", "message": "Update Kafka docs with encoder info", "committedDate": "2020-08-04T20:13:26Z", "type": "commit"}, {"oid": "a1f973cfd4487ccf0773b9f0c44cca8902086e82", "url": "https://github.com/trinodb/trino/commit/a1f973cfd4487ccf0773b9f0c44cca8902086e82", "message": "Update Kafka docs with encoder info", "committedDate": "2020-08-04T20:13:26Z", "type": "forcePushed"}]}