{"pr_number": 4460, "pr_title": "Fix HDFS impersonation in Iceberg Connector", "pr_createdAt": "2020-07-15T17:14:01Z", "pr_url": "https://github.com/trinodb/trino/pull/4460", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkyNzY1Nw==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455927657", "bodyText": "Is this needed? We don't other delete calls.", "author": "electrum", "createdAt": "2020-07-16T16:47:56Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/HdfsFileIo.java", "diffHunk": "@@ -44,26 +41,24 @@ public HdfsFileIo(HdfsEnvironment environment, HdfsContext context)\n     @Override\n     public InputFile newInputFile(String path)\n     {\n-        Configuration configuration = environment.getConfiguration(context, new Path(path));\n-        return HadoopInputFile.fromLocation(path, configuration);\n+        return new PrestoHadoopInputFile(new Path(path), environment, context);\n     }\n \n     @Override\n     public OutputFile newOutputFile(String path)\n     {\n-        Configuration configuration = environment.getConfiguration(context, new Path(path));\n-        return HadoopOutputFile.fromPath(new Path(path), configuration);\n+        return new PrestoHadoopOutputFile(new Path(path), environment, context);\n     }\n \n     @Override\n     public void deleteFile(String pathString)\n     {\n         Path path = new Path(pathString);\n         try {\n-            environment.getFileSystem(context, path).delete(path, false);\n+            environment.doAs(context.getIdentity().getUser(), () -> environment.getFileSystem(context, path).delete(path, false));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2MjE3NQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455962175", "bodyText": "I think whether we should wrap a call is not super clear, and it'd be safer to wrap everything than not. We had a discussion in https://prestosql.slack.com/archives/CP1MUNEUX/p1593662755069900.", "author": "lxynov", "createdAt": "2020-07-16T17:45:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkyNzY1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MTcyOQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456081729", "bodyText": "I believe for delete operation it is not required, but +1 for be consistent for all operations.", "author": "manishmalhotrawork", "createdAt": "2020-07-16T21:14:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkyNzY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkyODI2MQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455928261", "bodyText": "Same, we don't do this for OrcFileWriterFactory", "author": "electrum", "createdAt": "2020-07-16T16:48:59Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergFileWriterFactory.java", "diffHunk": "@@ -186,9 +186,9 @@ private IcebergFileWriter createOrcWriter(\n                     try {\n                         return new HdfsOrcDataSource(\n                                 new OrcDataSourceId(outputPath.toString()),\n-                                fileSystem.getFileStatus(outputPath).getLen(),\n+                                hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.getFileStatus(outputPath).getLen()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2MjI5NQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455962295", "bodyText": "Same above", "author": "lxynov", "createdAt": "2020-07-16T17:45:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkyODI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkzMzM4Nw==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455933387", "bodyText": "We could name this HdfsInputFile to match HdfsFileIo", "author": "electrum", "createdAt": "2020-07-16T16:57:25Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/PrestoHadoopInputFile.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.SeekableInputStream;\n+\n+import java.io.IOException;\n+\n+import static io.prestosql.plugin.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoHadoopInputFile", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkzMzc0MA==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r455933740", "bodyText": "toString() not needed for concatenation", "author": "electrum", "createdAt": "2020-07-16T16:57:52Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/PrestoHadoopInputFile.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.SeekableInputStream;\n+\n+import java.io.IOException;\n+\n+import static io.prestosql.plugin.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoHadoopInputFile\n+        implements InputFile\n+{\n+    private final InputFile delegate;\n+    private final HdfsEnvironment environment;\n+    private final String user;\n+\n+    public PrestoHadoopInputFile(Path path, HdfsEnvironment environment, HdfsContext context)\n+    {\n+        requireNonNull(path, \"path is null\");\n+        this.environment = requireNonNull(environment, \"environment is null\");\n+        requireNonNull(context, \"context is null\");\n+        try {\n+            this.delegate = HadoopInputFile.fromPath(path, environment.getFileSystem(context, path), environment.getConfiguration(context, path));\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(ICEBERG_FILESYSTEM_ERROR, \"Failed to create input file: \" + path.toString(), e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456090719", "bodyText": "should we also add recoverAction to IcebergRecordFileWriter  created-at\nas otherwise.\n[this] (https://github.com/prestosql/presto/blob/master/presto-hive/src/main/java/io/prestosql/plugin/hive/RecordFileWriter.java#L197) might fail as its running with Authentication ( doAs )", "author": "manishmalhotrawork", "createdAt": "2020-07-16T21:33:47Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergFileWriterFactory.java", "diffHunk": "@@ -165,9 +165,9 @@ private IcebergFileWriter createOrcWriter(\n     {\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(session.getUser(), outputPath, jobConf);\n-            OrcDataSink orcDataSink = new OutputStreamOrcDataSink(fileSystem.create(outputPath));\n+            OrcDataSink orcDataSink = hdfsEnvironment.doAs(session.getUser(), () -> new OutputStreamOrcDataSink(fileSystem.create(outputPath)));\n             Callable<Void> rollbackAction = () -> {\n-                fileSystem.delete(outputPath, false);\n+                hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.delete(outputPath, false));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MTAwNg==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456091006", "bodyText": "Sorry for commenting bit late.", "author": "manishmalhotrawork", "createdAt": "2020-07-16T21:34:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5NDkxOQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456094919", "bodyText": "2nd comment is:\nNot sure, if that can create problem.\nthe same above problem could occur at this point as well in RecordFileWriter: code\nfinalWrittenBytes = path.getFileSystem(conf).getFileStatus(path).getLen();\nFlow to reach this methods are like HiveWriter1,\nHiveWrtier2\nand IcebergPageSink", "author": "manishmalhotrawork", "createdAt": "2020-07-16T21:42:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwNzI2Nw==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456107267", "bodyText": "Hmm makes sense. We can add a rollbackAction in RecordFileWriter like what we have in OrcFileWriter. It actually depends on whether we should wrap deletes in doAs. I feel we can do it in a separate PR.", "author": "lxynov", "createdAt": "2020-07-16T22:10:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE0Mjg2OA==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456142868", "bodyText": "Sure, though but we are wrapping deletes in doAs, thats why I pointed.\nalso in the second comment,  path.getFileSystem(conf).getFileStatus(path).getLen(); is called without doAs. So, that can also be a problem. WDYT ?", "author": "manishmalhotrawork", "createdAt": "2020-07-16T23:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYyODY3Ng==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r456628676", "bodyText": "If the above case is not true, and we don't need to use doAs.\nThen its LG for me, and thanks for working on this !!", "author": "manishmalhotrawork", "createdAt": "2020-07-17T19:15:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU5MTM4Mw==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r457591383", "bodyText": "@manishmalhotrawork Thanks for letting us know. I think we can do the fix in a separate PR as it involves changes in presto-hive, so we're good for merging this one as of now.", "author": "lxynov", "createdAt": "2020-07-20T17:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MDk1OQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r457770959", "bodyText": "@lxynov thanks.\nbut if we see: #4460 (comment)\nthat flow is being called from IcebergSink as well.\nSo, iceberg writes can still create problem.", "author": "manishmalhotrawork", "createdAt": "2020-07-21T00:47:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE4ODE4Ng==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r461188186", "bodyText": "@electrum can you please see my comment as well, and if it's not a problem( or can be taken care in future).\nthen can we please merge this fix.", "author": "manishmalhotrawork", "createdAt": "2020-07-27T21:44:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYxNDA0MQ==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r462614041", "bodyText": "We can follow up if it's a problem.", "author": "electrum", "createdAt": "2020-07-29T21:58:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MjUzMg==", "url": "https://github.com/trinodb/trino/pull/4460#discussion_r462642532", "bodyText": "@electrum thanks !", "author": "manishmalhotrawork", "createdAt": "2020-07-29T23:14:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5MDcxOQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b8215f85f1a647f2c579fa4d11fe08eda35f0678", "url": "https://github.com/trinodb/trino/commit/b8215f85f1a647f2c579fa4d11fe08eda35f0678", "message": "Add Iceberg in Kerberos product tests", "committedDate": "2020-07-28T03:24:49Z", "type": "commit"}, {"oid": "6878f023420216d06fd100fe56b146a2abc2acff", "url": "https://github.com/trinodb/trino/commit/6878f023420216d06fd100fe56b146a2abc2acff", "message": "Fix HDFS impersonation in Iceberg Connector", "committedDate": "2020-07-28T03:24:49Z", "type": "commit"}, {"oid": "6878f023420216d06fd100fe56b146a2abc2acff", "url": "https://github.com/trinodb/trino/commit/6878f023420216d06fd100fe56b146a2abc2acff", "message": "Fix HDFS impersonation in Iceberg Connector", "committedDate": "2020-07-28T03:24:49Z", "type": "forcePushed"}]}