{"pr_number": 5545, "pr_title": "Add PagesSerdeContext abstraction to enable temporary buffer reuse", "pr_createdAt": "2020-10-13T15:27:33Z", "pr_url": "https://github.com/trinodb/trino/pull/5545", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjY0MzQ2OA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r506643468", "bodyText": "Don't abbreviate variable names.", "author": "martint", "createdAt": "2020-10-16T18:09:23Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjY1NDEwNg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r506654106", "bodyText": "Capitalize constant: ROW_TYPE", "author": "martint", "createdAt": "2020-10-16T18:31:16Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)\n+    {\n+        Page[] pages = data.dataPages;\n+        PagesSerde serde = data.serde;\n+        for (Page p : pages) {\n+            bh.consume(serde.serialize(p));\n+        }\n+    }\n+\n+    @Benchmark\n+    public void deserialize(BenchmarkData data, Blackhole bh)\n+    {\n+        SerializedPage[] pages = data.serializedPages;\n+        PagesSerde serde = data.serde;\n+        for (SerializedPage p : pages) {\n+            bh.consume(serde.deserialize(p));\n+        }\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class BenchmarkData\n+    {\n+        private static final int ROW_COUNT = 10000;\n+        private static final RowType rowType = RowType.anonymous(ImmutableList.of(VARCHAR, VARCHAR, VARCHAR, VARCHAR));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjY1ODM0OQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r506658349", "bodyText": "It seems like this is now used only by tests. We should be able to get rid of it or turn it into a utility method to discourage inadvertent use of it.", "author": "martint", "createdAt": "2020-10-16T18:40:29Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,213 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n+    public PagesSerdeContext newContext()\n+    {\n+        return new PagesSerdeContext();\n+    }\n+\n     public SerializedPage serialize(Page page)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTMyMzg4MA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509323880", "bodyText": "Removed it and updated tests.", "author": "pettyjamesm", "createdAt": "2020-10-21T14:14:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjY1ODM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg2ODQ5NA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r508868494", "bodyText": "The first time a buffer is released, largerBuffer will be set unconditionally. The second time it's released, smallerBuffer will be set unconditionally. If the second buffer is larger than the first one, we end up with the variables tracking the opposite buffers they should be tracking:\nPagesSerdeContext context = new PagesSerdeContext();\n\ncontext.releaseBuffer(context.acquireBuffer(5));\ncontext.releaseBuffer(context.acquireBuffer(10));\n\nSystem.out.println(\"smaller: \" + context.smallerBuffer.length);\nSystem.out.println(\"larger:  \" + context.largerBuffer.length);\nproduces:\nsmaller: 10\nlarger:  5", "author": "martint", "createdAt": "2020-10-20T22:05:25Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,225 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n+    public PagesSerdeContext newContext()\n+    {\n+        return new PagesSerdeContext();\n+    }\n+\n     public SerializedPage serialize(Page page)\n     {\n-        SliceOutput serializationBuffer = new DynamicSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n-        writeRawPage(page, serializationBuffer, blockEncodingSerde);\n-        Slice slice = serializationBuffer.slice();\n-        int uncompressedSize = serializationBuffer.size();\n-        MarkerSet markers = MarkerSet.empty();\n-\n-        if (compressor.isPresent()) {\n-            byte[] compressed = new byte[compressor.get().maxCompressedLength(uncompressedSize)];\n-            int compressedSize = compressor.get().compress(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    uncompressedSize,\n-                    compressed,\n-                    0,\n-                    compressed.length);\n-\n-            if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n-                slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n-                markers.add(COMPRESSED);\n-            }\n+        try (PagesSerdeContext context = newContext()) {\n+            return serialize(context, page);\n         }\n+    }\n \n-        if (spillCipher.isPresent()) {\n-            byte[] encrypted = new byte[spillCipher.get().encryptedMaxLength(slice.length())];\n-            int encryptedSize = spillCipher.get().encrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    encrypted,\n-                    0);\n-\n-            slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n-            markers.add(ENCRYPTED);\n-        }\n+    public SerializedPage serialize(PagesSerdeContext context, Page page)\n+    {\n+        DynamicSliceOutput serializationBuffer = context.acquireSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            writeRawPage(page, serializationBuffer, blockEncodingSerde);\n+            Slice slice = serializationBuffer.slice();\n+            int uncompressedSize = serializationBuffer.size();\n+            MarkerSet markers = MarkerSet.empty();\n \n-        if (!slice.isCompact()) {\n-            slice = Slices.copyOf(slice);\n-        }\n+            if (compressor.isPresent()) {\n+                byte[] compressed = context.acquireBuffer(compressor.get().maxCompressedLength(uncompressedSize));\n+                int compressedSize = compressor.get().compress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        uncompressedSize,\n+                        compressed,\n+                        0,\n+                        compressed.length);\n+\n+                if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n+                    slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n+                    markers.add(COMPRESSED);\n+                    inUseTempBuffer = compressed; // Track the compression buffer as in use\n+                }\n+                else {\n+                    // Eager release of the compression buffer to enable reusing it for encryption without an extra allocation\n+                    context.releaseBuffer(compressed);\n+                }\n+            }\n+\n+            if (spillCipher.isPresent()) {\n+                byte[] encrypted = context.acquireBuffer(spillCipher.get().encryptedMaxLength(slice.length()));\n+                int encryptedSize = spillCipher.get().encrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        encrypted,\n+                        0);\n \n-        return new SerializedPage(slice, markers, page.getPositionCount(), uncompressedSize);\n+                slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n+                markers.add(ENCRYPTED);\n+                //  Previous buffer is no longer in use and can be released\n+                if (inUseTempBuffer != null) {\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = encrypted;\n+            }\n+            //  Resulting slice *must* be copied to ensure the shared buffers aren't referenced after method exit\n+            return new SerializedPage(Slices.copyOf(slice), markers, page.getPositionCount(), uncompressedSize);\n+        }\n+        finally {\n+            context.releaseSliceOutput(serializationBuffer);\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n     }\n \n     public Page deserialize(SerializedPage serializedPage)\n+    {\n+        try (PagesSerdeContext context = newContext()) {\n+            return deserialize(context, serializedPage);\n+        }\n+    }\n+\n+    public Page deserialize(PagesSerdeContext context, SerializedPage serializedPage)\n     {\n         checkArgument(serializedPage != null, \"serializedPage is null\");\n \n         Slice slice = serializedPage.getSlice();\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            if (serializedPage.isEncrypted()) {\n+                checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+\n+                byte[] decrypted = context.acquireBuffer(spillCipher.get().decryptedMaxLength(slice.length()));\n+                int decryptedSize = spillCipher.get().decrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decrypted,\n+                        0);\n+\n+                slice = Slices.wrappedBuffer(decrypted, 0, decryptedSize);\n+                inUseTempBuffer = decrypted;\n+            }\n+\n+            if (serializedPage.isCompressed()) {\n+                checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n+\n+                int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n+                byte[] decompressed = context.acquireBuffer(uncompressedSize);\n+                checkState(decompressor.get().decompress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decompressed,\n+                        0,\n+                        uncompressedSize) == uncompressedSize);\n \n-        if (serializedPage.isEncrypted()) {\n-            checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+                slice = Slices.wrappedBuffer(decompressed, 0, uncompressedSize);\n+                if (inUseTempBuffer != null) {\n+                    //  Previous buffer is no longer in use\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = decompressed;\n+            }\n+\n+            return readRawPage(serializedPage.getPositionCount(), slice.getInput(), blockEncodingSerde);\n+        }\n+        finally {\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n+    }\n \n-            byte[] decrypted = new byte[spillCipher.get().decryptedMaxLength(slice.length())];\n-            int decryptedSize = spillCipher.get().decrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    decrypted,\n-                    0);\n+    public static final class PagesSerdeContext\n+            implements AutoCloseable\n+    {\n+        //  Limit retained buffers to 4x the default max page size\n+        private static final int MAX_BUFFER_RETAINED_SIZE = DEFAULT_MAX_PAGE_SIZE_IN_BYTES * 4;\n+\n+        private DynamicSliceOutput sliceOutput;\n+        //  Wraps two buffers since encryption + decryption will use at most 2 buffers at once. Buffers are kept in relative order\n+        //  based on length so that they can be used for compression or encryption, maximizing reuse opportunities\n+        private byte[] largerBuffer;\n+        private byte[] smallerBuffer;\n+        private boolean closed;\n+\n+        private void checkNotClosed()\n+        {\n+            if (closed) {\n+                throw new IllegalStateException(\"PagesSerdeContext is already closed\");\n+            }\n+        }\n \n-            slice = Slices.wrappedBuffer(decrypted, 0, decryptedSize);\n+        private DynamicSliceOutput acquireSliceOutput(int estimatedSize)\n+        {\n+            checkNotClosed();\n+            if (sliceOutput != null && sliceOutput.writableBytes() >= estimatedSize) {\n+                DynamicSliceOutput result = this.sliceOutput;\n+                this.sliceOutput = null;\n+                return result;\n+            }\n+            this.sliceOutput = null; // Clear any existing slice output that might be smaller than the request\n+            return new DynamicSliceOutput(estimatedSize);\n         }\n \n-        if (serializedPage.isCompressed()) {\n-            checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n+        private void releaseSliceOutput(DynamicSliceOutput sliceOutput)\n+        {\n+            if (closed) {\n+                return;\n+            }\n+            sliceOutput.reset();\n+            if (sliceOutput.writableBytes() <= MAX_BUFFER_RETAINED_SIZE) {\n+                this.sliceOutput = sliceOutput;\n+            }\n+        }\n \n-            int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n-            byte[] decompressed = new byte[uncompressedSize];\n-            checkState(decompressor.get().decompress(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    decompressed,\n-                    0,\n-                    uncompressedSize) == uncompressedSize);\n+        private byte[] acquireBuffer(int size)\n+        {\n+            checkNotClosed();\n+            byte[] result;\n+            //  Check the smallest buffer first\n+            if (smallerBuffer != null && smallerBuffer.length >= size) {\n+                result = smallerBuffer;\n+                smallerBuffer = null;\n+                return result;\n+            }\n+            if (largerBuffer != null && largerBuffer.length >= size) {\n+                result = largerBuffer;\n+                largerBuffer = smallerBuffer;\n+                smallerBuffer = null;\n+                return result;\n+            }\n+            return new byte[size];\n+        }\n \n-            slice = Slices.wrappedBuffer(decompressed);\n+        private void releaseBuffer(byte[] buffer)\n+        {\n+            if (closed || buffer.length > MAX_BUFFER_RETAINED_SIZE) {\n+                return;\n+            }\n+            if (largerBuffer == null) {\n+                largerBuffer = buffer;\n+            }\n+            else if (smallerBuffer == null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIxMDIxNg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509210216", "bodyText": "Very good catch. This logical bug could only serve to increase the allocation rate slightly so I re-ran the benchmarks after fixing.\nGetting this logic correct reduces allocation rate when both compression and encryption are enabled. Here's the previous benchmark output vs the output when fixed: Bugfix Improvement\nAnd against baseline: Updated Benchmark vs Baseline", "author": "pettyjamesm", "createdAt": "2020-10-21T11:45:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg2ODQ5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTMzNg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509115336", "bodyText": "You can simply return serialized pages. They don't have to be consumed by Blackhole", "author": "sopel39", "createdAt": "2020-10-21T09:07:47Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM2NDM2NQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509364365", "bodyText": "It actually has to be written this way. The point of this code is to benchmark serialization of multiple pages with the same \"context\" to showcase the benefits of reusing those buffers.", "author": "martint", "createdAt": "2020-10-21T15:01:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM3MjcyMg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509372722", "bodyText": "You can collect serialized pages into list an return entire list. Why blackhole is needed?", "author": "sopel39", "createdAt": "2020-10-21T15:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4Njk5Mw==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509386993", "bodyText": "Woops, I changed it to return an array briefly but went back and made it use Blackhole#consume again.", "author": "pettyjamesm", "createdAt": "2020-10-21T15:29:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTMzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ5OA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509115498", "bodyText": "please return deserialized pages", "author": "sopel39", "createdAt": "2020-10-21T09:08:01Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)\n+    {\n+        Page[] pages = data.dataPages;\n+        PagesSerde serde = data.serde;\n+        for (Page p : pages) {\n+            bh.consume(serde.serialize(p));\n+        }\n+    }\n+\n+    @Benchmark\n+    public void deserialize(BenchmarkData data, Blackhole bh)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509116668", "bodyText": "use different types (instead of just VACHAR)", "author": "sopel39", "createdAt": "2020-10-21T09:09:48Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)\n+    {\n+        Page[] pages = data.dataPages;\n+        PagesSerde serde = data.serde;\n+        for (Page p : pages) {\n+            bh.consume(serde.serialize(p));\n+        }\n+    }\n+\n+    @Benchmark\n+    public void deserialize(BenchmarkData data, Blackhole bh)\n+    {\n+        SerializedPage[] pages = data.serializedPages;\n+        PagesSerde serde = data.serde;\n+        for (SerializedPage p : pages) {\n+            bh.consume(serde.deserialize(p));\n+        }\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class BenchmarkData\n+    {\n+        private static final int ROW_COUNT = 10000;\n+        private static final RowType rowType = RowType.anonymous(ImmutableList.of(VARCHAR, VARCHAR, VARCHAR, VARCHAR));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIyMjU5MQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509222591", "bodyText": "For this particular PR I don't really care about the types being used except that I can control the compressibility to exercise both the compression succeeds path vs the compression fails path when serializing the page (as written and using the current seed value, 1/2 of pages compress successfully).\nAs long as the serialization time is constant, then they serve their purpose for now, but I agree that we should have the ability exercise more aspects of serialization and deserialization performance. Maybe we can add the other cases toBenchmarkPagesSerde after this PR?", "author": "pettyjamesm", "createdAt": "2020-10-21T12:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIyNDM5MQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509224391", "bodyText": "Maybe we can add the other cases toBenchmarkPagesSerde after this PR?\n\nWhat about the snippet I attached in comment? Could we use it as a base? It should be easy to extend it with other block types.\nThe reason I think it's valuable is because there are other optimization in PagesSerde/BlockSerde area which we most likely want to explore.", "author": "sopel39", "createdAt": "2020-10-21T12:10:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM0NjczOQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509346739", "bodyText": "I completely agree that we should be able to use BenchmarkPagesSerde to benchmark different block encodings and data types in the future. A quick look at your example seems ok, but not necessary for this PR. I see a couple problems:\n\nBenchmarking the performance of buffer reuse is very sensitive to the size of the pages being serialized / deserialized, the compressibility of the data, and the number of pages serialized in each batch. You'll need a way to control those factors explicitly and target \"realistic\" scenarios in those dimensions.\nDon't use TestingPagesSerdeFactory().createPagesSerde() to create the PagesSerde since it wraps all of the operations in synchronized blocks\n\nI'd still rather land this first and then come back to enhance it with a full suite of types, channel counts, and encodings.", "author": "pettyjamesm", "createdAt": "2020-10-21T14:40:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4OTM0Mg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509389342", "bodyText": "The problem with the current setup is that we are testing a very specific case (4 varchar columns, not too many rows). It's hard to judge how much it helps in general case. It's also not obvious how to extend this benchmarks for different types which makes this benchmark sealed for single particular use-case.\nWhy there are 4 columns? Wouldn't one suffice?", "author": "sopel39", "createdAt": "2020-10-21T15:32:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDA3MjAzMA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r510072030", "bodyText": "Let's make it use single VARCHAR column.", "author": "sopel39", "createdAt": "2020-10-22T11:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2NDkxOA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r510264918", "bodyText": "Changed to a single VARCHAR column. Benchmarks still show the same basic shape modulo the new page size constraints. I had a much harder time picking values that would compress ~1/2 of the pages the way that the previous iteration did, so in this version almost all (26/30) pages compress successfully.\nThere's also more variability in some of the benchmark runs which I couldn't seem to avoid after repeated runs, but the updated benchmarks only \"regress\" when one or both sides have large error bars.", "author": "pettyjamesm", "createdAt": "2020-10-22T15:41:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNjY2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExODM1MA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509118350", "bodyText": "Just make it a field as in BenchmarkData, see: io.prestosql.orc.BenchmarkColumnReaders.BenchmarkData#random", "author": "sopel39", "createdAt": "2020-10-21T09:12:26Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)\n+    {\n+        Page[] pages = data.dataPages;\n+        PagesSerde serde = data.serde;\n+        for (Page p : pages) {\n+            bh.consume(serde.serialize(p));\n+        }\n+    }\n+\n+    @Benchmark\n+    public void deserialize(BenchmarkData data, Blackhole bh)\n+    {\n+        SerializedPage[] pages = data.serializedPages;\n+        PagesSerde serde = data.serde;\n+        for (SerializedPage p : pages) {\n+            bh.consume(serde.deserialize(p));\n+        }\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class BenchmarkData\n+    {\n+        private static final int ROW_COUNT = 10000;\n+        private static final RowType rowType = RowType.anonymous(ImmutableList.of(VARCHAR, VARCHAR, VARCHAR, VARCHAR));\n+        private static final List<Type> TYPES = ImmutableList.of(BIGINT, rowType, rowType, rowType);\n+        @Param({\"true\", \"false\"})\n+        private boolean encrypted;\n+        @Param({\"true\", \"false\"})\n+        private boolean compressed;\n+        @Param(\"1000\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExOTA5OQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509119099", "bodyText": "add a test method for benchmarks, see: io.prestosql.orc.BenchmarkOrcDecimalReader#testReadDecimal", "author": "sopel39", "createdAt": "2020-10-21T09:13:35Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.execution.buffer;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PageBuilder;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spiller.AesSpillCipher;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Random;\n+\n+import static io.airlift.slice.Slices.utf8Slice;\n+import static io.prestosql.metadata.MetadataManager.createTestMetadataManager;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.nio.charset.StandardCharsets.ISO_8859_1;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+@State(Scope.Thread)\n+@OutputTimeUnit(SECONDS)\n+@Fork(1)\n+@Warmup(iterations = 10, time = 1, timeUnit = SECONDS)\n+@Measurement(iterations = 10, time = 1, timeUnit = SECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+public class BenchmarkPagesSerde\n+{\n+    @Benchmark\n+    public void serialize(BenchmarkData data, Blackhole bh)\n+    {\n+        Page[] pages = data.dataPages;\n+        PagesSerde serde = data.serde;\n+        for (Page p : pages) {\n+            bh.consume(serde.serialize(p));\n+        }\n+    }\n+\n+    @Benchmark\n+    public void deserialize(BenchmarkData data, Blackhole bh)\n+    {\n+        SerializedPage[] pages = data.serializedPages;\n+        PagesSerde serde = data.serde;\n+        for (SerializedPage p : pages) {\n+            bh.consume(serde.deserialize(p));\n+        }\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class BenchmarkData\n+    {\n+        private static final int ROW_COUNT = 10000;\n+        private static final RowType rowType = RowType.anonymous(ImmutableList.of(VARCHAR, VARCHAR, VARCHAR, VARCHAR));\n+        private static final List<Type> TYPES = ImmutableList.of(BIGINT, rowType, rowType, rowType);\n+        @Param({\"true\", \"false\"})\n+        private boolean encrypted;\n+        @Param({\"true\", \"false\"})\n+        private boolean compressed;\n+        @Param(\"1000\")\n+        private int randomSeed = 1000;\n+\n+        private PagesSerde serde;\n+        private Page[] dataPages;\n+        private SerializedPage[] serializedPages;\n+\n+        @Setup\n+        public void initialize()\n+        {\n+            serde = createPagesSerde();\n+            dataPages = createPages();\n+            serializedPages = createSerializedPages();\n+        }\n+\n+        public Page[] getDataPages()\n+        {\n+            return dataPages;\n+        }\n+\n+        private PagesSerde createPagesSerde()\n+        {\n+            PagesSerdeFactory serdeFactory = new PagesSerdeFactory(createTestMetadataManager().getBlockEncodingSerde(), compressed);\n+            return encrypted ? serdeFactory.createPagesSerdeForSpill(Optional.of(new AesSpillCipher())) : serdeFactory.createPagesSerde();\n+        }\n+\n+        private SerializedPage[] createSerializedPages()\n+        {\n+            SerializedPage[] result = new SerializedPage[dataPages.length];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = serde.serialize(dataPages[i]);\n+            }\n+            return result;\n+        }\n+\n+        private Page[] createPages()\n+        {\n+            Random random = new Random(randomSeed);\n+            List<Page> pages = new ArrayList<>();\n+            int remainingRows = ROW_COUNT;\n+            PageBuilder pageBuilder = new PageBuilder(TYPES);\n+            while (remainingRows > 0) {\n+                int rows = 50 + random.nextInt(450); // 50 - 500 rows per pass\n+                List<Object>[] testRows = generateTestRows(random, ImmutableList.of(VARCHAR, VARCHAR, VARCHAR, VARCHAR), rows);\n+                remainingRows -= rows;\n+                for (int i = 0; i < testRows.length; i++) {\n+                    BIGINT.writeLong(pageBuilder.getBlockBuilder(0), i);\n+                    writeRow(testRows[i], pageBuilder.getBlockBuilder(1));\n+                    writeRow(testRows[i], pageBuilder.getBlockBuilder(2));\n+                    writeRow(testRows[i], pageBuilder.getBlockBuilder(3));\n+                }\n+                pageBuilder.declarePositions(rows);\n+                pages.add(pageBuilder.build());\n+                pageBuilder.reset();\n+            }\n+            return pages.toArray(Page[]::new);\n+        }\n+\n+        private void writeRow(List<Object> testRow, BlockBuilder rowBlockBuilder)\n+        {\n+            BlockBuilder singleRowBlockWriter = rowBlockBuilder.beginBlockEntry();\n+            for (Object fieldValue : testRow) {\n+                if (fieldValue == null) {\n+                    singleRowBlockWriter.appendNull();\n+                }\n+                else if (fieldValue instanceof String) {\n+                    VARCHAR.writeSlice(singleRowBlockWriter, utf8Slice((String) fieldValue));\n+                }\n+                else {\n+                    throw new UnsupportedOperationException();\n+                }\n+            }\n+            rowBlockBuilder.closeEntry();\n+        }\n+\n+        // copied & modifed from TestRowBlock\n+        private List<Object>[] generateTestRows(Random random, List<Type> fieldTypes, int numRows)\n+        {\n+            @SuppressWarnings(\"unchecked\")\n+            List<Object>[] testRows = new List[numRows];\n+            for (int i = 0; i < numRows; i++) {\n+                List<Object> testRow = new ArrayList<>(fieldTypes.size());\n+                for (int j = 0; j < fieldTypes.size(); j++) {\n+                    if (fieldTypes.get(j) == VARCHAR) {\n+                        byte[] data = new byte[random.nextInt(128)];\n+                        if (data.length == 0) {\n+                            testRow.add(null);\n+                        }\n+                        else if (data.length < 32 && i > 0) {\n+                            // Repeat values to make compression more interesting\n+                            testRow.add(testRows[i - 1].get(j));\n+                        }\n+                        else {\n+                            random.nextBytes(data);\n+                            testRow.add(new String(data, ISO_8859_1));\n+                        }\n+                    }\n+                    else {\n+                        throw new UnsupportedOperationException();\n+                    }\n+                }\n+                testRows[i] = testRow;\n+            }\n+            return testRows;\n+        }\n+    }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTMwNTQ3OA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509305478", "bodyText": "Test added.", "author": "pettyjamesm", "createdAt": "2020-10-21T13:52:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExOTA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyMTc5OQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509121799", "bodyText": "Please model benchmarks after io/prestosql/orc/BenchmarkColumnReaders.java. This means benchmarking different column serdes (we can start with Long block only). Block serialization/deserialization consumes large chunk of time when serializing pages.", "author": "sopel39", "createdAt": "2020-10-21T09:17:32Z", "path": "presto-main/src/test/java/io/prestosql/execution/buffer/BenchmarkPagesSerde.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyMjM3OQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509122379", "bodyText": "I actually started working on similar benchmarks, here is my draft:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            /*\n          \n          \n            \n            /*\n          \n          \n            \n             * Licensed under the Apache License, Version 2.0 (the \"License\");\n          \n          \n            \n             * you may not use this file except in compliance with the License.\n          \n          \n            \n             * You may obtain a copy of the License at\n          \n          \n            \n             *\n          \n          \n            \n             *     http://www.apache.org/licenses/LICENSE-2.0\n          \n          \n            \n             *\n          \n          \n            \n             * Unless required by applicable law or agreed to in writing, software\n          \n          \n            \n             * distributed under the License is distributed on an \"AS IS\" BASIS,\n          \n          \n            \n             * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n          \n          \n            \n             * See the License for the specific language governing permissions and\n          \n          \n            \n             * limitations under the License.\n          \n          \n            \n             */\n          \n          \n            \n            package io.prestosql.execution.buffer;\n          \n          \n            \n            \n          \n          \n            \n            import com.google.common.collect.ImmutableList;\n          \n          \n            \n            import io.airlift.slice.BasicSliceInput;\n          \n          \n            \n            import io.airlift.slice.OutputStreamSliceOutput;\n          \n          \n            \n            import io.airlift.slice.Slice;\n          \n          \n            \n            import io.airlift.slice.Slices;\n          \n          \n            \n            import io.prestosql.spi.Page;\n          \n          \n            \n            import io.prestosql.spi.block.Block;\n          \n          \n            \n            import io.prestosql.spi.block.BlockBuilder;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Benchmark;\n          \n          \n            \n            import org.openjdk.jmh.annotations.BenchmarkMode;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Fork;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Measurement;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Mode;\n          \n          \n            \n            import org.openjdk.jmh.annotations.OperationsPerInvocation;\n          \n          \n            \n            import org.openjdk.jmh.annotations.OutputTimeUnit;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Scope;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Setup;\n          \n          \n            \n            import org.openjdk.jmh.annotations.State;\n          \n          \n            \n            import org.openjdk.jmh.annotations.Warmup;\n          \n          \n            \n            import org.openjdk.jmh.runner.Runner;\n          \n          \n            \n            import org.openjdk.jmh.runner.options.Options;\n          \n          \n            \n            import org.openjdk.jmh.runner.options.OptionsBuilder;\n          \n          \n            \n            import org.openjdk.jmh.runner.options.VerboseMode;\n          \n          \n            \n            import org.testng.annotations.Test;\n          \n          \n            \n            \n          \n          \n            \n            import java.io.File;\n          \n          \n            \n            import java.io.FileOutputStream;\n          \n          \n            \n            import java.nio.ByteBuffer;\n          \n          \n            \n            import java.nio.channels.FileChannel;\n          \n          \n            \n            import java.util.ArrayList;\n          \n          \n            \n            import java.util.Iterator;\n          \n          \n            \n            import java.util.List;\n          \n          \n            \n            import java.util.Random;\n          \n          \n            \n            import java.util.concurrent.TimeUnit;\n          \n          \n            \n            \n          \n          \n            \n            import static com.google.common.io.Files.createTempDir;\n          \n          \n            \n            import static io.prestosql.execution.buffer.PagesSerdeUtil.readPages;\n          \n          \n            \n            import static io.prestosql.execution.buffer.PagesSerdeUtil.writePages;\n          \n          \n            \n            import static io.prestosql.spi.type.BigintType.BIGINT;\n          \n          \n            \n            import static java.nio.channels.FileChannel.MapMode.READ_ONLY;\n          \n          \n            \n            import static java.nio.file.Files.readAllBytes;\n          \n          \n            \n            import static java.util.UUID.randomUUID;\n          \n          \n            \n            import static java.util.concurrent.TimeUnit.MILLISECONDS;\n          \n          \n            \n            \n          \n          \n            \n            @State(Scope.Thread)\n          \n          \n            \n            @OutputTimeUnit(TimeUnit.NANOSECONDS)\n          \n          \n            \n            @Fork(3)\n          \n          \n            \n            @Warmup(iterations = 30, time = 500, timeUnit = MILLISECONDS)\n          \n          \n            \n            @Measurement(iterations = 20, time = 500, timeUnit = MILLISECONDS)\n          \n          \n            \n            @BenchmarkMode(Mode.AverageTime)\n          \n          \n            \n            @OperationsPerInvocation(BenchmarkPagesSerde.ROWS)\n          \n          \n            \n            public class BenchmarkPagesSerde\n          \n          \n            \n            {\n          \n          \n            \n                public static final int ROWS = 10_000_000;\n          \n          \n            \n                private static final PagesSerde SERDE = new TestingPagesSerdeFactory().createPagesSerde();\n          \n          \n            \n            \n          \n          \n            \n                @Benchmark\n          \n          \n            \n                public Object readLongWithNull(BenchmarkData data)\n          \n          \n            \n                {\n          \n          \n            \n                    return ImmutableList.copyOf(readPages(SERDE, new BasicSliceInput(data.getDataSource())));\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                @State(Scope.Thread)\n          \n          \n            \n                public static class BenchmarkData\n          \n          \n            \n                {\n          \n          \n            \n                    private final Random random = new Random(0);\n          \n          \n            \n                    private File temporaryDirectory;\n          \n          \n            \n                    private File file;\n          \n          \n            \n                    private Slice dataSource;\n          \n          \n            \n            \n          \n          \n            \n                    @Setup\n          \n          \n            \n                    public void setup()\n          \n          \n            \n                            throws Exception\n          \n          \n            \n                    {\n          \n          \n            \n                        temporaryDirectory = createTempDir();\n          \n          \n            \n                        file = new File(temporaryDirectory, randomUUID().toString());\n          \n          \n            \n                        BlockBuilder blockBuilder = BIGINT.createBlockBuilder(null, 1024);\n          \n          \n            \n                        Iterator<?> values = createValues();\n          \n          \n            \n                        while (values.hasNext()) {\n          \n          \n            \n                            Object value = values.next();\n          \n          \n            \n                            if (value == null) {\n          \n          \n            \n                                blockBuilder.appendNull();\n          \n          \n            \n                            }\n          \n          \n            \n                            else {\n          \n          \n            \n                                BIGINT.writeLong(blockBuilder, ((Number) value).longValue());\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        Page page = new Page(blockBuilder.build());\n          \n          \n            \n                        writePages(SERDE, new OutputStreamSliceOutput(new FileOutputStream(file)), page);\n          \n          \n            \n                        dataSource = Slices.wrappedBuffer(readAllBytes(file.toPath()));\n          \n          \n            \n                    }\n          \n          \n            \n            \n          \n          \n            \n                    public Slice getDataSource()\n          \n          \n            \n                    {\n          \n          \n            \n                        return dataSource;\n          \n          \n            \n                    }\n          \n          \n            \n            \n          \n          \n            \n                    protected Iterator<?> createValues()\n          \n          \n            \n                    {\n          \n          \n            \n                        List<Long> values = new ArrayList<>();\n          \n          \n            \n                        for (int i = 0; i < ROWS; ++i) {\n          \n          \n            \n                            if (random.nextBoolean()) {\n          \n          \n            \n                                values.add(random.nextLong());\n          \n          \n            \n                            }\n          \n          \n            \n                            else {\n          \n          \n            \n                                values.add(null);\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return values.iterator();\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                @Test\n          \n          \n            \n                public void testReadLongWithNull()\n          \n          \n            \n                        throws Exception\n          \n          \n            \n                {\n          \n          \n            \n                    BenchmarkPagesSerde benchmark = new BenchmarkPagesSerde();\n          \n          \n            \n                    BenchmarkData data = new BenchmarkData();\n          \n          \n            \n                    data.setup();\n          \n          \n            \n                    benchmark.readLongWithNull(data);\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                public static void main(String[] args)\n          \n          \n            \n                        throws Exception\n          \n          \n            \n                {\n          \n          \n            \n                    Options options = new OptionsBuilder()\n          \n          \n            \n                            .verbosity(VerboseMode.NORMAL)\n          \n          \n            \n                            .include(\".*\" + BenchmarkPagesSerde.class.getSimpleName() + \".*\")\n          \n          \n            \n                            .build();\n          \n          \n            \n            \n          \n          \n            \n                    new Runner(options).run();\n          \n          \n            \n                }\n          \n          \n            \n            }", "author": "sopel39", "createdAt": "2020-10-21T09:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyMTc5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTMyNjkyMQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509326921", "bodyText": "I would rather address the broader problem of benchmarking the entire serialization / deserialization performance in a variety of block encodings and data types after this PR. In the context of this PR, it doesn't especially matter what the data type and block encoding performance is as long as it remains constant overhead between the baseline and with reused buffers.", "author": "pettyjamesm", "createdAt": "2020-10-21T14:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyMTc5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyNzExNg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509127116", "bodyText": "why do we need to close it? Java will automatically GC unused objects", "author": "sopel39", "createdAt": "2020-10-21T09:25:01Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,225 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n+    public PagesSerdeContext newContext()\n+    {\n+        return new PagesSerdeContext();\n+    }\n+\n     public SerializedPage serialize(Page page)\n     {\n-        SliceOutput serializationBuffer = new DynamicSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n-        writeRawPage(page, serializationBuffer, blockEncodingSerde);\n-        Slice slice = serializationBuffer.slice();\n-        int uncompressedSize = serializationBuffer.size();\n-        MarkerSet markers = MarkerSet.empty();\n-\n-        if (compressor.isPresent()) {\n-            byte[] compressed = new byte[compressor.get().maxCompressedLength(uncompressedSize)];\n-            int compressedSize = compressor.get().compress(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    uncompressedSize,\n-                    compressed,\n-                    0,\n-                    compressed.length);\n-\n-            if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n-                slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n-                markers.add(COMPRESSED);\n-            }\n+        try (PagesSerdeContext context = newContext()) {\n+            return serialize(context, page);\n         }\n+    }\n \n-        if (spillCipher.isPresent()) {\n-            byte[] encrypted = new byte[spillCipher.get().encryptedMaxLength(slice.length())];\n-            int encryptedSize = spillCipher.get().encrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    encrypted,\n-                    0);\n-\n-            slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n-            markers.add(ENCRYPTED);\n-        }\n+    public SerializedPage serialize(PagesSerdeContext context, Page page)\n+    {\n+        DynamicSliceOutput serializationBuffer = context.acquireSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            writeRawPage(page, serializationBuffer, blockEncodingSerde);\n+            Slice slice = serializationBuffer.slice();\n+            int uncompressedSize = serializationBuffer.size();\n+            MarkerSet markers = MarkerSet.empty();\n \n-        if (!slice.isCompact()) {\n-            slice = Slices.copyOf(slice);\n-        }\n+            if (compressor.isPresent()) {\n+                byte[] compressed = context.acquireBuffer(compressor.get().maxCompressedLength(uncompressedSize));\n+                int compressedSize = compressor.get().compress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        uncompressedSize,\n+                        compressed,\n+                        0,\n+                        compressed.length);\n+\n+                if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n+                    slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n+                    markers.add(COMPRESSED);\n+                    inUseTempBuffer = compressed; // Track the compression buffer as in use\n+                }\n+                else {\n+                    // Eager release of the compression buffer to enable reusing it for encryption without an extra allocation\n+                    context.releaseBuffer(compressed);\n+                }\n+            }\n+\n+            if (spillCipher.isPresent()) {\n+                byte[] encrypted = context.acquireBuffer(spillCipher.get().encryptedMaxLength(slice.length()));\n+                int encryptedSize = spillCipher.get().encrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        encrypted,\n+                        0);\n \n-        return new SerializedPage(slice, markers, page.getPositionCount(), uncompressedSize);\n+                slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n+                markers.add(ENCRYPTED);\n+                //  Previous buffer is no longer in use and can be released\n+                if (inUseTempBuffer != null) {\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = encrypted;\n+            }\n+            //  Resulting slice *must* be copied to ensure the shared buffers aren't referenced after method exit\n+            return new SerializedPage(Slices.copyOf(slice), markers, page.getPositionCount(), uncompressedSize);\n+        }\n+        finally {\n+            context.releaseSliceOutput(serializationBuffer);\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n     }\n \n     public Page deserialize(SerializedPage serializedPage)\n+    {\n+        try (PagesSerdeContext context = newContext()) {\n+            return deserialize(context, serializedPage);\n+        }\n+    }\n+\n+    public Page deserialize(PagesSerdeContext context, SerializedPage serializedPage)\n     {\n         checkArgument(serializedPage != null, \"serializedPage is null\");\n \n         Slice slice = serializedPage.getSlice();\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            if (serializedPage.isEncrypted()) {\n+                checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+\n+                byte[] decrypted = context.acquireBuffer(spillCipher.get().decryptedMaxLength(slice.length()));\n+                int decryptedSize = spillCipher.get().decrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decrypted,\n+                        0);\n+\n+                slice = Slices.wrappedBuffer(decrypted, 0, decryptedSize);\n+                inUseTempBuffer = decrypted;\n+            }\n+\n+            if (serializedPage.isCompressed()) {\n+                checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n+\n+                int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n+                byte[] decompressed = context.acquireBuffer(uncompressedSize);\n+                checkState(decompressor.get().decompress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decompressed,\n+                        0,\n+                        uncompressedSize) == uncompressedSize);\n \n-        if (serializedPage.isEncrypted()) {\n-            checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+                slice = Slices.wrappedBuffer(decompressed, 0, uncompressedSize);\n+                if (inUseTempBuffer != null) {\n+                    //  Previous buffer is no longer in use\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = decompressed;\n+            }\n+\n+            return readRawPage(serializedPage.getPositionCount(), slice.getInput(), blockEncodingSerde);\n+        }\n+        finally {\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n+    }\n \n-            byte[] decrypted = new byte[spillCipher.get().decryptedMaxLength(slice.length())];\n-            int decryptedSize = spillCipher.get().decrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    decrypted,\n-                    0);\n+    public static final class PagesSerdeContext\n+            implements AutoCloseable\n+    {\n+        //  Limit retained buffers to 4x the default max page size\n+        private static final int MAX_BUFFER_RETAINED_SIZE = DEFAULT_MAX_PAGE_SIZE_IN_BYTES * 4;\n+\n+        private DynamicSliceOutput sliceOutput;\n+        //  Wraps two buffers since encryption + decryption will use at most 2 buffers at once. Buffers are kept in relative order\n+        //  based on length so that they can be used for compression or encryption, maximizing reuse opportunities\n+        private byte[] largerBuffer;\n+        private byte[] smallerBuffer;\n+        private boolean closed;\n+\n+        private void checkNotClosed()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTMwOTc0Mw==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509309743", "bodyText": "In some usages (eg: PagesSerdeUtil#PageReader) you would have to null out the PagesSerdeContext field instead of closing it to actually release the buffers, since you don't exactly know how long the iterator might still be reachable after reading the final element.\nI think either approach could work, but implementing AutoClosable seemed like a  stronger signal about how the class should be used. If the context is kept for long periods of time then you have to start worrying about (and probably tracking) it's memory consumption which can be non-trivial.", "author": "pettyjamesm", "createdAt": "2020-10-21T13:57:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyNzExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4NTg4Mg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509385882", "bodyText": "since you don't exactly know how long the iterator might still be reachable after reading the final element.\n\nThe same applies to nullyfing buffers, they won't be released immediately.\n\nsince you don't exactly know how long the iterator might still be reachable after reading the final element.\n\nIt's responsibility of iterator owner to release it after final element has been read.", "author": "sopel39", "createdAt": "2020-10-21T15:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyNzExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyODM1MA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509128350", "bodyText": "can slice output be aquired twice?", "author": "sopel39", "createdAt": "2020-10-21T09:26:47Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,225 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n+    public PagesSerdeContext newContext()\n+    {\n+        return new PagesSerdeContext();\n+    }\n+\n     public SerializedPage serialize(Page page)\n     {\n-        SliceOutput serializationBuffer = new DynamicSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n-        writeRawPage(page, serializationBuffer, blockEncodingSerde);\n-        Slice slice = serializationBuffer.slice();\n-        int uncompressedSize = serializationBuffer.size();\n-        MarkerSet markers = MarkerSet.empty();\n-\n-        if (compressor.isPresent()) {\n-            byte[] compressed = new byte[compressor.get().maxCompressedLength(uncompressedSize)];\n-            int compressedSize = compressor.get().compress(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    uncompressedSize,\n-                    compressed,\n-                    0,\n-                    compressed.length);\n-\n-            if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n-                slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n-                markers.add(COMPRESSED);\n-            }\n+        try (PagesSerdeContext context = newContext()) {\n+            return serialize(context, page);\n         }\n+    }\n \n-        if (spillCipher.isPresent()) {\n-            byte[] encrypted = new byte[spillCipher.get().encryptedMaxLength(slice.length())];\n-            int encryptedSize = spillCipher.get().encrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    encrypted,\n-                    0);\n-\n-            slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n-            markers.add(ENCRYPTED);\n-        }\n+    public SerializedPage serialize(PagesSerdeContext context, Page page)\n+    {\n+        DynamicSliceOutput serializationBuffer = context.acquireSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            writeRawPage(page, serializationBuffer, blockEncodingSerde);\n+            Slice slice = serializationBuffer.slice();\n+            int uncompressedSize = serializationBuffer.size();\n+            MarkerSet markers = MarkerSet.empty();\n \n-        if (!slice.isCompact()) {\n-            slice = Slices.copyOf(slice);\n-        }\n+            if (compressor.isPresent()) {\n+                byte[] compressed = context.acquireBuffer(compressor.get().maxCompressedLength(uncompressedSize));\n+                int compressedSize = compressor.get().compress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        uncompressedSize,\n+                        compressed,\n+                        0,\n+                        compressed.length);\n+\n+                if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n+                    slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n+                    markers.add(COMPRESSED);\n+                    inUseTempBuffer = compressed; // Track the compression buffer as in use\n+                }\n+                else {\n+                    // Eager release of the compression buffer to enable reusing it for encryption without an extra allocation\n+                    context.releaseBuffer(compressed);\n+                }\n+            }\n+\n+            if (spillCipher.isPresent()) {\n+                byte[] encrypted = context.acquireBuffer(spillCipher.get().encryptedMaxLength(slice.length()));\n+                int encryptedSize = spillCipher.get().encrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        encrypted,\n+                        0);\n \n-        return new SerializedPage(slice, markers, page.getPositionCount(), uncompressedSize);\n+                slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n+                markers.add(ENCRYPTED);\n+                //  Previous buffer is no longer in use and can be released\n+                if (inUseTempBuffer != null) {\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = encrypted;\n+            }\n+            //  Resulting slice *must* be copied to ensure the shared buffers aren't referenced after method exit\n+            return new SerializedPage(Slices.copyOf(slice), markers, page.getPositionCount(), uncompressedSize);\n+        }\n+        finally {\n+            context.releaseSliceOutput(serializationBuffer);\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n     }\n \n     public Page deserialize(SerializedPage serializedPage)\n+    {\n+        try (PagesSerdeContext context = newContext()) {\n+            return deserialize(context, serializedPage);\n+        }\n+    }\n+\n+    public Page deserialize(PagesSerdeContext context, SerializedPage serializedPage)\n     {\n         checkArgument(serializedPage != null, \"serializedPage is null\");\n \n         Slice slice = serializedPage.getSlice();\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            if (serializedPage.isEncrypted()) {\n+                checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+\n+                byte[] decrypted = context.acquireBuffer(spillCipher.get().decryptedMaxLength(slice.length()));\n+                int decryptedSize = spillCipher.get().decrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decrypted,\n+                        0);\n+\n+                slice = Slices.wrappedBuffer(decrypted, 0, decryptedSize);\n+                inUseTempBuffer = decrypted;\n+            }\n+\n+            if (serializedPage.isCompressed()) {\n+                checkState(decompressor.isPresent(), \"Page is compressed, but decompressor is missing\");\n+\n+                int uncompressedSize = serializedPage.getUncompressedSizeInBytes();\n+                byte[] decompressed = context.acquireBuffer(uncompressedSize);\n+                checkState(decompressor.get().decompress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        decompressed,\n+                        0,\n+                        uncompressedSize) == uncompressedSize);\n \n-        if (serializedPage.isEncrypted()) {\n-            checkState(spillCipher.isPresent(), \"Page is encrypted, but spill cipher is missing\");\n+                slice = Slices.wrappedBuffer(decompressed, 0, uncompressedSize);\n+                if (inUseTempBuffer != null) {\n+                    //  Previous buffer is no longer in use\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = decompressed;\n+            }\n+\n+            return readRawPage(serializedPage.getPositionCount(), slice.getInput(), blockEncodingSerde);\n+        }\n+        finally {\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n+        }\n+    }\n \n-            byte[] decrypted = new byte[spillCipher.get().decryptedMaxLength(slice.length())];\n-            int decryptedSize = spillCipher.get().decrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    decrypted,\n-                    0);\n+    public static final class PagesSerdeContext\n+            implements AutoCloseable\n+    {\n+        //  Limit retained buffers to 4x the default max page size\n+        private static final int MAX_BUFFER_RETAINED_SIZE = DEFAULT_MAX_PAGE_SIZE_IN_BYTES * 4;\n+\n+        private DynamicSliceOutput sliceOutput;\n+        //  Wraps two buffers since encryption + decryption will use at most 2 buffers at once. Buffers are kept in relative order\n+        //  based on length so that they can be used for compression or encryption, maximizing reuse opportunities\n+        private byte[] largerBuffer;\n+        private byte[] smallerBuffer;\n+        private boolean closed;\n+\n+        private void checkNotClosed()\n+        {\n+            if (closed) {\n+                throw new IllegalStateException(\"PagesSerdeContext is already closed\");\n+            }\n+        }\n \n-            slice = Slices.wrappedBuffer(decrypted, 0, decryptedSize);\n+        private DynamicSliceOutput acquireSliceOutput(int estimatedSize)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIyNDU5OA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509224598", "bodyText": "As long as the requested size is smaller than it's current size and it's current size stays smaller than MAX_BUFFER_RETAINED_SIZE, then yes- these can be reused in multiple serialization runs which is why even the \"no compression, no encryption\" case must copy the slice contents before returning.", "author": "pettyjamesm", "createdAt": "2020-10-21T12:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEyODM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509131843", "bodyText": "PagesSerde is not thread-safe. It's mean to be used from single-thread. Therefore, we don't need extra PagesSerdeContext. We can keep temporary buffers within PagesSerde itself.", "author": "sopel39", "createdAt": "2020-10-21T09:31:52Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,225 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n+    public PagesSerdeContext newContext()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTMyMjgzMw==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509322833", "bodyText": "You could do that, and that's what prestodb/presto#13232 added to prestodb for compression buffers. I think there are a couple problems with that approach:\n\nThe memory consumed by these can be non-trivial. In fact, the biggest benefit comes from cases where the serialization buffers are large so then you have to strike a balance with how big you let these buffers become. You also then have to track the memory explicitly since it's no longer \"ephemeral\"\nJust because PagesSerde is not thread safe doesn't mean that it isn't accidentally used from multiple threads on occassion. A quick survey of PagesSerde usages around prestodb has some worrying occurrences of PagesSerde usages very near (but not inside of) synchronized blocks which suggests that they are susceptible to simultaneously using that buffer from multiple threads (eg: PrestoSparkSerializedPageInput). In my opinion, that's far riskier and more subtle than the this approach of caller managed temporary buffers.\nI suspect that running on machines with non-uniform memory access (NUMA) performance might be worse than this approach in aggregate, even though buffer reuse would be much better. On those machines, the calling thread created PagesContext would be allocated within its current core's NUMA node whereas keeping the buffer indefinitely inside of the PagesSerde could much more easily result in the thread being scheduled on another core (or socket) than where the buffer was originally allocated and as a result perform very poorly. I haven't attempted to test on a machine with multiple NUMA nodes, so I don't know what the performance is actually like in that scenario.", "author": "pettyjamesm", "createdAt": "2020-10-21T14:12:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4Mzk2Nw==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509383967", "bodyText": "The memory consumed by these can be non-trivial. In fact, the biggest benefit comes from cases where the serialization buffers are large so then you have to strike a balance with how big you let these buffers become. You also then have to track the memory explicitly since it's no longer \"ephemeral\"\n\nDo you mean that PagesSerde is long lived while context is short lived? If we care about nullyfying buffers, we could add PagesSerde#reset method. Having buffers in PagesSerde allows to avoid complex aquire/release logic.\n\nJust because PagesSerde is not thread safe doesn't mean that it isn't accidentally used from multiple threads on occassion.\n\nThe reason PagesSerde is not thread-safe is because compressors are not thread safe. See io.airlift.compress.lz4.Lz4Compressor for example. If PagesSerde is used in multi-threaded context I would consider it a bug.\n\nI suspect that running on machines with non-uniform memory access (NUMA) performance might be worse\n\nThis could be solved (if it is a problem) by optinal PagesSerde#reset method", "author": "sopel39", "createdAt": "2020-10-21T15:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQxNzA3Mg==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509417072", "bodyText": "Do you mean that PagesSerde is long lived while context is short lived? If we care about nullyfying buffers, we could add PagesSerde#reset method. Having buffers in PagesSerde allows to avoid complex aquire/release logic.\n\nYes, that's the biggest point here. I'm not sure having a PagesSerde#reset() method actually simplifies the situation because:\n\nit becomes ambiguous about when and why to call reset()\nif you let some PagesSerde instances be longer-lived, how large do you let those buffers get? Do you need to reduce the max buffer size in those scenarios to avoid consuming too much memory? Do you need to track that memory somewhere?\n\nI agree that adding a try-with-resources block around all of the calls to serialize / deserialize isn't exactly pretty or convenient, but I think it's a better option than allocation all of these buffers on-demand (like happens today) and that it's still probably better than PagesSerde owning the buffers and having to wire each instance up into memory tracking and memory revoking (ie: reset()) even though that would in general allow more reuse than this strategy. If the idea is that you would just call reset() on every use, then I think this is still the clearer idiom.", "author": "pettyjamesm", "createdAt": "2020-10-21T16:09:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQzMDc0OQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r509430749", "bodyText": "if you let some PagesSerde instances be longer-lived, how large do you let those buffers get?\n\nWe don't have to make PagesSerde instances long lived. Instead of passing PagesSerde to operator (like ExchangeOperator), we could pass PagesSerdeFactory or Supplier<PagesSerde>. This way PagesSerde becomes a PagesSerdeContext itself.", "author": "sopel39", "createdAt": "2020-10-21T16:29:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk3MjY3Ng==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r510972676", "bodyText": "It seems to me like this would just be shifting the same problem around compared to having a PagesSerdeContext. Sure, you could make PagesSerde instances relatively ephemeral instead of having a PagesSerdeContext but then you have to either re-create the Compressor / Decompressor each time you create a PagesSerde (which would not be particularly efficient since Lz4Compressor has that internal 16KiB \"table\") or keep those around on some other object and pass them in each time along with the AesSpillCipher since those keys are also ephemeral and only associated with a single SpillCipher instance for it's lifespan. You still have the long-lived stateful entity (in my case, PagesSerde in your case Supplier<PagesSerde>) and a short lived temporary buffer reusing entity.", "author": "pettyjamesm", "createdAt": "2020-10-23T15:40:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgxNDEwOQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r511814109", "bodyText": "if you let some PagesSerde instances be longer-lived, how large do you let those buffers get? Do you need to reduce the max buffer size in those scenarios to avoid consuming too much memory? Do you need to track that memory somewhere?\n\nWhen we have long-lived objects of larger size, ideally we should account for their memory using LocalMemoryContext or AggregatedMemoryContext.\n\nwhich would not be particularly efficient since Lz4Compressor has that internal 16KiB \"table\"\n\nThat could be a concern (albeit a small one), but serialized page/compression/decompression buffers would be much larger than that. Generally, that would be a problem only when a single page is serialized/deserialized (which is the case of ExchangeOperator or TaskOutputOperator), but we should avoid that anyway.\nAdditional notes:\n\nPagesSerde is contextual object. No need to have another one.\nPagesSerdeContext contains aquire/release logic which won't be needed if PagesSerde stored buffers itself.\nPagesSerdeContext is PagesSerde specific. We would rather have generic BufferPool that could be also used in BlockSerde", "author": "sopel39", "createdAt": "2020-10-26T09:16:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMTg0Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODIzOQ==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r511808239", "bodyText": "ExchangeOperator might benefit from having buffers long-lived.", "author": "sopel39", "createdAt": "2020-10-26T09:06:46Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/PagesSerde.java", "diffHunk": "@@ -56,86 +56,216 @@ public PagesSerde(BlockEncodingSerde blockEncodingSerde, Optional<Compressor> co\n         this.spillCipher = requireNonNull(spillCipher, \"spillCipher is null\");\n     }\n \n-    public SerializedPage serialize(Page page)\n+    public PagesSerdeContext newContext()\n     {\n-        SliceOutput serializationBuffer = new DynamicSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n-        writeRawPage(page, serializationBuffer, blockEncodingSerde);\n-        Slice slice = serializationBuffer.slice();\n-        int uncompressedSize = serializationBuffer.size();\n-        MarkerSet markers = MarkerSet.empty();\n-\n-        if (compressor.isPresent()) {\n-            byte[] compressed = new byte[compressor.get().maxCompressedLength(uncompressedSize)];\n-            int compressedSize = compressor.get().compress(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    uncompressedSize,\n-                    compressed,\n-                    0,\n-                    compressed.length);\n-\n-            if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n-                slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n-                markers.add(COMPRESSED);\n+        return new PagesSerdeContext();\n+    }\n+\n+    public SerializedPage serialize(PagesSerdeContext context, Page page)\n+    {\n+        DynamicSliceOutput serializationBuffer = context.acquireSliceOutput(toIntExact(page.getSizeInBytes() + Integer.BYTES)); // block length is an int\n+        byte[] inUseTempBuffer = null;\n+        try {\n+            writeRawPage(page, serializationBuffer, blockEncodingSerde);\n+            Slice slice = serializationBuffer.slice();\n+            int uncompressedSize = serializationBuffer.size();\n+            MarkerSet markers = MarkerSet.empty();\n+\n+            if (compressor.isPresent()) {\n+                byte[] compressed = context.acquireBuffer(compressor.get().maxCompressedLength(uncompressedSize));\n+                int compressedSize = compressor.get().compress(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        uncompressedSize,\n+                        compressed,\n+                        0,\n+                        compressed.length);\n+\n+                if ((((double) compressedSize) / uncompressedSize) <= MINIMUM_COMPRESSION_RATIO) {\n+                    slice = Slices.wrappedBuffer(compressed, 0, compressedSize);\n+                    markers.add(COMPRESSED);\n+                    inUseTempBuffer = compressed; // Track the compression buffer as in use\n+                }\n+                else {\n+                    // Eager release of the compression buffer to enable reusing it for encryption without an extra allocation\n+                    context.releaseBuffer(compressed);\n+                }\n             }\n-        }\n \n-        if (spillCipher.isPresent()) {\n-            byte[] encrypted = new byte[spillCipher.get().encryptedMaxLength(slice.length())];\n-            int encryptedSize = spillCipher.get().encrypt(\n-                    slice.byteArray(),\n-                    slice.byteArrayOffset(),\n-                    slice.length(),\n-                    encrypted,\n-                    0);\n-\n-            slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n-            markers.add(ENCRYPTED);\n-        }\n+            if (spillCipher.isPresent()) {\n+                byte[] encrypted = context.acquireBuffer(spillCipher.get().encryptedMaxLength(slice.length()));\n+                int encryptedSize = spillCipher.get().encrypt(\n+                        slice.byteArray(),\n+                        slice.byteArrayOffset(),\n+                        slice.length(),\n+                        encrypted,\n+                        0);\n \n-        if (!slice.isCompact()) {\n-            slice = Slices.copyOf(slice);\n+                slice = Slices.wrappedBuffer(encrypted, 0, encryptedSize);\n+                markers.add(ENCRYPTED);\n+                //  Previous buffer is no longer in use and can be released\n+                if (inUseTempBuffer != null) {\n+                    context.releaseBuffer(inUseTempBuffer);\n+                }\n+                inUseTempBuffer = encrypted;\n+            }\n+            //  Resulting slice *must* be copied to ensure the shared buffers aren't referenced after method exit\n+            return new SerializedPage(Slices.copyOf(slice), markers, page.getPositionCount(), uncompressedSize);\n+        }\n+        finally {\n+            context.releaseSliceOutput(serializationBuffer);\n+            if (inUseTempBuffer != null) {\n+                context.releaseBuffer(inUseTempBuffer);\n+            }\n         }\n-\n-        return new SerializedPage(slice, markers, page.getPositionCount(), uncompressedSize);\n     }\n \n     public Page deserialize(SerializedPage serializedPage)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgxMjA0NA==", "url": "https://github.com/trinodb/trino/pull/5545#discussion_r511812044", "bodyText": "we are creating temp buffers for a single page, we most likely want them to be long-lived.", "author": "sopel39", "createdAt": "2020-10-26T09:13:13Z", "path": "presto-main/src/main/java/io/prestosql/operator/TaskOutputOperator.java", "diffHunk": "@@ -142,14 +142,22 @@ public void addInput(Page page)\n \n         page = pagePreprocessor.apply(page);\n \n-        List<SerializedPage> serializedPages = splitPage(page, DEFAULT_MAX_PAGE_SIZE_IN_BYTES).stream()\n-                .map(serde::serialize)\n-                .collect(toImmutableList());\n-\n-        outputBuffer.enqueue(serializedPages);\n+        outputBuffer.enqueue(splitAndSerializePage(page));\n         operatorContext.recordOutput(page.getSizeInBytes(), page.getPositionCount());\n     }\n \n+    private List<SerializedPage> splitAndSerializePage(Page page)\n+    {\n+        List<Page> split = splitPage(page, DEFAULT_MAX_PAGE_SIZE_IN_BYTES);\n+        ImmutableList.Builder<SerializedPage> builder = ImmutableList.builderWithExpectedSize(split.size());\n+        try (PagesSerde.PagesSerdeContext context = serde.newContext()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "3b82e00eb9a2796614c2cf3dad40e5691b39e4a0", "url": "https://github.com/trinodb/trino/commit/3b82e00eb9a2796614c2cf3dad40e5691b39e4a0", "message": "Add BenchmarkPagesSerde\n\nAlso makes AesSpillCipher visible for the purposes of testing and\nbenchmarking", "committedDate": "2020-11-05T11:52:15Z", "type": "commit"}, {"oid": "51cff0b7102e03d3c413da114362a7199decd345", "url": "https://github.com/trinodb/trino/commit/51cff0b7102e03d3c413da114362a7199decd345", "message": "Add PagesSerdeContext abstraction to enable temporary buffer reuse\n\nReduces the allocation rate of PagesSerde when multiple pages are\nserialized or deserialized in a batch by reusing byte array buffers\nand DynamicSliceOutput instances through the new PagesSerdeContext.", "committedDate": "2020-11-05T11:52:15Z", "type": "commit"}, {"oid": "51cff0b7102e03d3c413da114362a7199decd345", "url": "https://github.com/trinodb/trino/commit/51cff0b7102e03d3c413da114362a7199decd345", "message": "Add PagesSerdeContext abstraction to enable temporary buffer reuse\n\nReduces the allocation rate of PagesSerde when multiple pages are\nserialized or deserialized in a batch by reusing byte array buffers\nand DynamicSliceOutput instances through the new PagesSerdeContext.", "committedDate": "2020-11-05T11:52:15Z", "type": "forcePushed"}]}