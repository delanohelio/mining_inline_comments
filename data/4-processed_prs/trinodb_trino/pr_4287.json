{"pr_number": 4287, "pr_title": "Implement base functionality for kafka connector inserts", "pr_createdAt": "2020-06-30T22:20:12Z", "pr_url": "https://github.com/trinodb/trino/pull/4287", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MDI5NA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448170294", "bodyText": "Do we need to pass serializers here? could this be internal implementation detail of KafkaProducerFactory? You could change KafkaProducerFactory to always create KafkaProducer<byte[], byte[]> (no need for generics).", "author": "kokosing", "createdAt": "2020-07-01T07:29:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTEwMQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171101", "bodyText": "it is not used here.", "author": "kokosing", "createdAt": "2020-07-01T07:31:21Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4OTk2Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448489967", "bodyText": "added to ProducerMessage", "author": "charlesjmorgan", "createdAt": "2020-07-01T16:45:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTEwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTY4NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171685", "bodyText": "Why is this user error?", "author": "kokosing", "createdAt": "2020-07-01T07:32:29Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        checkArgument(tableHandle instanceof KafkaTableHandle, \"tableHandle is not an instance of KafkaTableHandle\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new PrestoException(GENERIC_USER_ERROR, \"unexpected internal column\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5MDUzMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448490532", "bodyText": "Good point, I added a few new KafkaErrorCodes and changed that exception", "author": "charlesjmorgan", "createdAt": "2020-07-01T16:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTY4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTk4Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171986", "bodyText": "I think this check is not needed.", "author": "kokosing", "createdAt": "2020-07-01T07:33:06Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        checkArgument(tableHandle instanceof KafkaTableHandle, \"tableHandle is not an instance of KafkaTableHandle\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MjcyOQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448172729", "bodyText": "PlainTextKafkaProducerFactory is not extendable, there is no need to install this module here. Install it in KafkaConnectorFactory in a normal way.", "author": "kokosing", "createdAt": "2020-07-01T07:34:28Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPlugin.java", "diffHunk": "@@ -25,6 +25,7 @@\n {\n     public static final Module DEFAULT_EXTENSION = binder -> {\n         binder.install(new KafkaConsumerModule());\n+        binder.install(new KafkaProducerModule());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mjk1Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448172953", "bodyText": "Why do you need this try-catch?", "author": "kokosing", "createdAt": "2020-07-01T07:34:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaProducerModule.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.util.Properties;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+\n+public class KafkaProducerModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        try {\n+            binder.bind(Properties.class).toInstance(new Properties());\n+            binder.bind(PlainTextKafkaProducerFactory.class).toConstructor(PlainTextKafkaProducerFactory.class.getConstructor(KafkaConfig.class, Properties.class)).in(Scopes.SINGLETON);\n+        }\n+        catch (NoSuchMethodException e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ0NzY3NA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448447674", "bodyText": "NoSuchMethodException is thrown by class.getConstructor() if the constructor doesn't exist", "author": "charlesjmorgan", "createdAt": "2020-07-01T15:34:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mjk1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0ODQyNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449048427", "bodyText": "Why not just\n        binder.bind(PlainTextKafkaProducerFactory.class).in(Scopes.SINGLETON);", "author": "losipiuk", "createdAt": "2020-07-02T14:36:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mjk1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyMjU5Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449122593", "bodyText": "Injection is new to me, didn't know I could do that lol", "author": "charlesjmorgan", "createdAt": "2020-07-02T16:07:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mjk1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MzEzMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448173133", "bodyText": "Why is it an user error?", "author": "kokosing", "createdAt": "2020-07-01T07:35:12Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaProducerModule.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.util.Properties;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+\n+public class KafkaProducerModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        try {\n+            binder.bind(Properties.class).toInstance(new Properties());\n+            binder.bind(PlainTextKafkaProducerFactory.class).toConstructor(PlainTextKafkaProducerFactory.class.getConstructor(KafkaConfig.class, Properties.class)).in(Scopes.SINGLETON);\n+        }\n+        catch (NoSuchMethodException e) {\n+            throw new PrestoException(GENERIC_USER_ERROR, e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mzk5Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448173992", "bodyText": "make sure you are using unique code:\nkokosing@m16:~/presto$ git grep '0x0101_0000'\npresto-record-decoder/src/main/java/io/prestosql/decoder/DecoderErrorCode.java:        errorCode = new ErrorCode(code + 0x0101_0000, name(), type);\n\nWhy do you use io.prestosql.plugin.kafka.KafkaErrorCode?", "author": "kokosing", "createdAt": "2020-07-01T07:37:01Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/EncoderErrorCode.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.ErrorCode;\n+import io.prestosql.spi.ErrorCodeSupplier;\n+import io.prestosql.spi.ErrorType;\n+\n+import static io.prestosql.spi.ErrorType.EXTERNAL;\n+\n+public enum EncoderErrorCode\n+        implements ErrorCodeSupplier\n+{\n+    /** A requested data conversion is not supported */\n+    ENCODER_CONVERSION_NOT_SUPPORTED(0, EXTERNAL);\n+\n+    private final ErrorCode errorCode;\n+\n+    EncoderErrorCode(int code, ErrorType type)\n+    {\n+        errorCode = new ErrorCode(code + 0x0101_0000, name(), type);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NzQxMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448497413", "bodyText": "It is not unique right now. Same value is used in DecoderErrorCode. I think @findepi  pointed that out in previous review.", "author": "losipiuk", "createdAt": "2020-07-01T16:58:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mzk5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUwNjI1Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448506257", "bodyText": "I deleted EncoderErrorCode because I don't use the error code in this pr, I think it might be better to just add a KAFKA_ENCODER_ERROR to KafkaErrorCode in the future", "author": "charlesjmorgan", "createdAt": "2020-07-01T17:16:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mzk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDI1MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174251", "bodyText": "separate commit", "author": "kokosing", "createdAt": "2020-07-01T07:37:32Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -148,7 +148,7 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n             Map<String, String> kafkaProperties = new HashMap<>(ImmutableMap.copyOf(extraKafkaProperties));\n             kafkaProperties.putIfAbsent(\"kafka.nodes\", testingKafka.getConnectString());\n             kafkaProperties.putIfAbsent(\"kafka.connect-timeout\", \"120s\");\n-            kafkaProperties.putIfAbsent(\"kafka.default-schema\", \"default\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDMwMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174302", "bodyText": "separate commit", "author": "kokosing", "createdAt": "2020-07-01T07:37:37Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -203,6 +203,7 @@ public static void main(String[] args)\n         Logging.initialize();\n         DistributedQueryRunner queryRunner = builder(new TestingKafka())\n                 .setTables(TpchTable.getTables())\n+                .setExtraProperties(ImmutableMap.of(\"http-server.http.port\", \"8080\"))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDY2Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174666", "bodyText": "what is a raw topic name? how does it differ to topic name?", "author": "kokosing", "createdAt": "2020-07-01T07:38:20Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -44,18 +50,17 @@\n         extends AbstractTestIntegrationSmokeTest\n {\n     private TestingKafka testingKafka;\n-    private String topicName;\n+    private String rawTopicName;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ1NTQ4OQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448455489", "bodyText": "I can see how that might be confusing. It is meant to stand for the raw data format, I'll rename it.", "author": "charlesjmorgan", "createdAt": "2020-07-01T15:46:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDY2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NTAwNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448175004", "bodyText": "You might want to use io.prestosql.testing.TestngUtils#toDataProvider", "author": "kokosing", "createdAt": "2020-07-01T07:39:00Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjEwNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176107", "bodyText": "I would inline these", "author": "kokosing", "createdAt": "2020-07-01T07:41:03Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjIzNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176237", "bodyText": "roundTripTestSetup -> testCase?", "author": "kokosing", "createdAt": "2020-07-01T07:41:17Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjM1Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176356", "bodyText": "RoundTripTestSetup -> RoundTripTestCase?", "author": "kokosing", "createdAt": "2020-07-01T07:41:34Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjgxMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176812", "bodyText": "please remove this try-catch. IllegalArgumentException is already a runtime exception, also it is test code so there is no need to use PrestoException", "author": "kokosing", "createdAt": "2020-07-01T07:42:25Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Njk0Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176946", "bodyText": "Don't you need to create a topic here?", "author": "kokosing", "createdAt": "2020-07-01T07:42:41Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ2MTc4MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448461781", "bodyText": "I didn't include the actual test cases because the encoder formats aren't included in this pr. Maybe it would be best to not include this test in this pr either.", "author": "charlesjmorgan", "createdAt": "2020-07-01T15:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Njk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NzUxMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448177512", "bodyText": "The best would be have simple use case already implemented (possibly in separate commit) to see that all of this actually work.", "author": "kokosing", "createdAt": "2020-07-01T07:43:48Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5MTUzNg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448491536", "bodyText": "I have one for each format when I open encoder prs", "author": "charlesjmorgan", "createdAt": "2020-07-01T16:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NzUxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODA2Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178062", "bodyText": "fieldNames.indexOf(..)?", "author": "kokosing", "createdAt": "2020-07-01T07:44:50Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODE2NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178165", "bodyText": "remove try-catch", "author": "kokosing", "createdAt": "2020-07-01T07:45:03Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODM1MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178351", "bodyText": "use checkArgument instead", "author": "kokosing", "createdAt": "2020-07-01T07:45:26Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODU5Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178592", "bodyText": "String.join()?", "author": "kokosing", "createdAt": "2020-07-01T07:45:53Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODcwOQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178709", "bodyText": "remove try-catch", "author": "kokosing", "createdAt": "2020-07-01T07:46:08Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODgxNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178814", "bodyText": "use checkArgument instead", "author": "kokosing", "createdAt": "2020-07-01T07:46:20Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODkxOQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178919", "bodyText": "String.join()?", "author": "kokosing", "createdAt": "2020-07-01T07:46:30Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldValues()\n+        {\n+            StringBuilder sb = new StringBuilder();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3OTExMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448179112", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return tableName;  // for test case label in IDE\n          \n          \n            \n                        return tableName; // for test case label in IDE", "author": "kokosing", "createdAt": "2020-07-01T07:46:46Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldValues()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldValue(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldValues.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        @Override\n+        public String toString()\n+        {\n+            return tableName;  // for test case label in IDE", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ1ODAwNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448458004", "bodyText": "Please file an issue to support transactional inserts in Kafka and reference it here as //TODO", "author": "losipiuk", "createdAt": "2020-07-01T15:50:38Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,28 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NTAxOA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448475018", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n          \n          \n            \n                    this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");", "author": "losipiuk", "createdAt": "2020-07-01T16:19:07Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3Njg2NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448476865", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    else if (type == VARBINARY) {\n          \n          \n            \n                    else if (VarbinaryType.isVarbinaryType(type)) {", "author": "losipiuk", "createdAt": "2020-07-01T16:22:03Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();\n+        return NOT_BLOCKED;\n+    }\n+\n+    private void appendColumn(RowEncoder rowEncoder, Page page, int channel, int position)\n+    {\n+        Block block = page.getBlock(channel);\n+        EncoderColumnHandle columnHandle = columns.get(channel);\n+        Type type = columns.get(channel).getType();\n+        if (block.isNull(position)) {\n+            rowEncoder.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            rowEncoder.put(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            rowEncoder.put(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            rowEncoder.put(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            rowEncoder.put(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            rowEncoder.put(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            rowEncoder.put(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            rowEncoder.put(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (type instanceof VarcharType) {\n+            rowEncoder.put(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (type == VARBINARY) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3OTU1OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448479558", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    else if (type instanceof VarcharType) {\n          \n          \n            \n                    isVarcharType(type)\n          \n      \n    \n    \n  \n\nio.prestosql.spi.type.Varchars#isVarcharType", "author": "losipiuk", "createdAt": "2020-07-01T16:26:44Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();\n+        return NOT_BLOCKED;\n+    }\n+\n+    private void appendColumn(RowEncoder rowEncoder, Page page, int channel, int position)\n+    {\n+        Block block = page.getBlock(channel);\n+        EncoderColumnHandle columnHandle = columns.get(channel);\n+        Type type = columns.get(channel).getType();\n+        if (block.isNull(position)) {\n+            rowEncoder.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            rowEncoder.put(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            rowEncoder.put(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            rowEncoder.put(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            rowEncoder.put(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            rowEncoder.put(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            rowEncoder.put(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            rowEncoder.put(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (type instanceof VarcharType) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448482386", "bodyText": "Do we need to be synchronous here? Maybe we should just wake up sender here and return the future we got from last producer.send call?", "author": "losipiuk", "createdAt": "2020-07-01T16:31:29Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxNzczOA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448517738", "bodyText": "producer.send returns a Future, not a CompletableFuture", "author": "charlesjmorgan", "createdAt": "2020-07-01T17:37:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAzMDk3OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449030978", "bodyText": "is there a way to wake up the sender using the producer?", "author": "charlesjmorgan", "createdAt": "2020-07-02T14:11:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0MzQxNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449043414", "bodyText": "Maybe sth like this would do?\n    private static class CountingSendCallback\n            implements Callback\n    {\n        private final AtomicLong remaining;\n        private final CompletableFuture<?> future;\n\n        private CountingSendCallback(long pageSize) {\n            this.remaining = new AtomicLong(pageSize);\n            this.future = new CompletableFuture();\n        }\n\n        @Override\n        public void onCompletion(RecordMetadata metadata, Exception exception)\n        {\n            // TODO possibly handle exception here\n            remaining.decrementAndGet();\n            completeFutureIfDone();\n        }\n\n        private void completeFutureIfDone()\n        {\n            if (remaining.get() <= 0) {\n                future.complete(null);\n            }\n        }\n\n        public CompletableFuture<?> getFuture()\n        {\n            return future;\n        }\n    }\n\n    @Override\n    public CompletableFuture<?> appendPage(Page page)\n    {\n        CountingSendCallback callback = new CountingSendCallback(page.getPositionCount());\n        for (int position = 0; position < page.getPositionCount(); position++) {\n   ....\n            producer.send(new ProducerRecord<>(topicName, keyEncoder.toByteArray(), messageEncoder.toByteArray()), callback);\n  .....\n        }\n        return callback.getFuture();\n    }", "author": "losipiuk", "createdAt": "2020-07-02T14:29:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0NDY3MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449044671", "bodyText": "This also brings my attention to error handling. What would happen in current codebase if writing operation fails? Would that be propagated to INSERT query in presto. Maybe producer.close(); would fail in such case but I am not sure. Do you know?", "author": "losipiuk", "createdAt": "2020-07-02T14:30:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NzA3NA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449067074", "bodyText": "is there a way to wake up the sender using the producer?\n\nGood point. Without it the code I pasted will not work. As the sending may basically not happen ever.\nSo maybe we need explicit flush() here or in finish().\nDoes producer.close() we call in finish() also flush the sender queue. I skimmed Kafka code and it does not seem so.\nIt feels the safest for now to get back to what you had originally - flush after each page. And improve in followup PRs as we get more understanding.", "author": "losipiuk", "createdAt": "2020-07-02T15:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NzczNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449077737", "bodyText": "that looks great, thank you! I think that you're right about handling exceptions in the onCompletion callback method. The docs for KafkaProducer say that producer.close() blocks until all previously sent requests complete.", "author": "charlesjmorgan", "createdAt": "2020-07-02T15:19:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA5MTAzNQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449091035", "bodyText": "that looks great\n\nNot that much really :) - unless we can trigger actual sending to happen It is prone to starvation. As I wrote above. I think safest would be to do what you did originally to flush after each page or only in finish.", "author": "losipiuk", "createdAt": "2020-07-02T15:33:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA5NDI2NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449094265", "bodyText": "ok, we can come back to this later. Seems like it would still be smart to have a callback with an onCompletion method to handle any errors.", "author": "charlesjmorgan", "createdAt": "2020-07-02T15:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NDE3Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448494172", "bodyText": "Defensive copy properties. Or maybe just drop the parameter because you are binding it to empty properties anyway.\nIf we actually need the parameter add the marker annotation @ForKafkaProducer as Properties commonly used type and you can easily need to bind multiple instances at different places in the codebase. As an example look for @ForBaseJdbc", "author": "losipiuk", "createdAt": "2020-07-01T16:53:04Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Set<HostAddress> nodes;\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig, Properties properties)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyNDU4Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448524583", "bodyText": "removed properties from constructor", "author": "charlesjmorgan", "createdAt": "2020-07-01T17:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NDE3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NDUxOQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448494519", "bodyText": "prepare properties in the constructor.", "author": "losipiuk", "createdAt": "2020-07-01T16:53:41Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Set<HostAddress> nodes;\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig, Properties properties)\n+    {\n+        requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+        this.nodes = ImmutableSet.copyOf(kafkaConfig.getNodes());\n+        this.properties = requireNonNull(properties, \"properties is null\");\n+    }\n+\n+    public <K, V> KafkaProducer<K, V> create(Serializer<K> keySerializer, Serializer<V> messageSerializer)\n+    {\n+        properties.put(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5OTE1NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448499155", "bodyText": "please rename the put methods to putObject, putLong, putInt ...\nHaving that many overrides is very error prone. Especially when there are built-in type conversions in the language between the types for which you provide overrides.", "author": "losipiuk", "createdAt": "2020-07-01T17:01:51Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    RowEncoder putNullValue(EncoderColumnHandle columnHandle);\n+\n+    default RowEncoder put(EncoderColumnHandle columnHandle, Object value)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM5Mjc3OQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448392779", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                            .map(columnHandle -> (KafkaColumnHandle) columnHandle)\n          \n          \n            \n                                            .map(KafkaColumnHandle.class::cast)", "author": "aalbu", "createdAt": "2020-07-01T14:13:42Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -82,7 +87,10 @@ public KafkaTableHandle getTableHandle(ConnectorSession session, SchemaTableName\n                         getDataFormat(kafkaTopicDescription.getKey()),\n                         getDataFormat(kafkaTopicDescription.getMessage()),\n                         kafkaTopicDescription.getKey().flatMap(KafkaTopicFieldGroup::getDataSchema),\n-                        kafkaTopicDescription.getMessage().flatMap(KafkaTopicFieldGroup::getDataSchema)))\n+                        kafkaTopicDescription.getMessage().flatMap(KafkaTopicFieldGroup::getDataSchema),\n+                        getColumnHandles(schemaTableName).values().stream()\n+                                .map(columnHandle -> (KafkaColumnHandle) columnHandle)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NjE0Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448576147", "bodyText": "Maybe we should have a constant for this?", "author": "aalbu", "createdAt": "2020-07-01T19:37:23Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_AVRO_SCHEMA_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new IllegalArgumentException(format(\"unexpected internal column handle '%s'\", col.getName()));\n+            }\n+            if (col.isKeyDecoder()) {\n+                keyColumns.add(col);\n+            }\n+            else {\n+                messageColumns.add(col);\n+            }\n+        });\n+\n+        RowEncoder keyEncoder = encoderFactory.create(\n+                handle.getKeyDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getKeyDataSchemaLocation())),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5MTE4Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448591187", "bodyText": "if the encoder params are used just for the avro data schema, would it make sense to just pass an Optional<String> that stores the dataSchema? The map seems like overkill, and is kinda clunky, for just one variable.", "author": "charlesjmorgan", "createdAt": "2020-07-01T20:11:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NjE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NjU1NA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448576554", "bodyText": "It would seem more appropriate for this method to return an Optional<String>.", "author": "aalbu", "createdAt": "2020-07-01T19:38:17Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_AVRO_SCHEMA_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new IllegalArgumentException(format(\"unexpected internal column handle '%s'\", col.getName()));\n+            }\n+            if (col.isKeyDecoder()) {\n+                keyColumns.add(col);\n+            }\n+            else {\n+                messageColumns.add(col);\n+            }\n+        });\n+\n+        RowEncoder keyEncoder = encoderFactory.create(\n+                handle.getKeyDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getKeyDataSchemaLocation())),\n+                keyColumns.build());\n+\n+        RowEncoder messageEncoder = encoderFactory.create(\n+                handle.getMessageDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getMessageDataSchemaLocation())),\n+                messageColumns.build());\n+\n+        return new KafkaPageSink(\n+                handle.getTopicName(),\n+                handle.getColumns(),\n+                keyEncoder,\n+                messageEncoder,\n+                session,\n+                producerFactory);\n+    }\n+\n+    private String getDataSchema(Optional<String> dataSchemaLocation)\n+    {\n+        String dataSchema = \"\";\n+        try {\n+            if (dataSchemaLocation.isPresent()) {\n+                dataSchema = Files.readString(Paths.get(dataSchemaLocation.get()));\n+            }\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(KAFKA_AVRO_SCHEMA_ERROR, format(\"Unable to read data schema at '%s'\", dataSchemaLocation.get()), e);\n+        }\n+        return dataSchema;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1MjkwNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449052907", "bodyText": "return dataSchemaLocation.map(location -> {\n            try {\n                return Files.readString(Paths.get(location));\n            }\n            catch (IOException e) {\n                throw new PrestoException(KAFKA_AVRO_SCHEMA_ERROR, format(\"Unable to read data schema at '%s'\", location), e);\n            }\n        });", "author": "losipiuk", "createdAt": "2020-07-02T14:42:37Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -95,17 +94,18 @@ public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHa\n                 producerFactory);\n     }\n \n-    private String getDataSchema(Optional<String> dataSchemaLocation)\n+    private Optional<String> getDataSchema(Optional<String> dataSchemaLocation)\n     {\n-        String dataSchema = \"\";\n-        try {\n-            if (dataSchemaLocation.isPresent()) {\n-                dataSchema = Files.readString(Paths.get(dataSchemaLocation.get()));\n+        if (dataSchemaLocation.isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTA4OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449059088", "bodyText": "Maybe putValueFromBlock? I do not know why but when I see putValueAt method which takes block I think it will be putting something into the block (though blocks are not mutable).", "author": "losipiuk", "createdAt": "2020-07-02T14:51:47Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -29,7 +31,12 @@\n \n     void clear();\n \n-    RowEncoder putNullValue(EncoderColumnHandle columnHandle);\n+    RowEncoder putValueAt(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTQ2NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449059465", "bodyText": "Also can't you put default implementation already here?", "author": "losipiuk", "createdAt": "2020-07-02T14:52:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTA4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3ODg2Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449078866", "bodyText": "right, that could definitely be confusing", "author": "charlesjmorgan", "createdAt": "2020-07-02T15:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTA4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA5NTgzNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449095834", "bodyText": "I think so, would just be the same thing as what was in KafkaPageSink#appendColumn", "author": "charlesjmorgan", "createdAt": "2020-07-02T15:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTA4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA4MTQyNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449081424", "bodyText": "We should probably close the producer here, as well.", "author": "aalbu", "createdAt": "2020-07-02T15:24:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        int partition;\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            partition = 1;\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (!columns.get(channel).isInternal()) {\n+                    if (columns.get(channel).isKeyDecoder()) {\n+                        keyEncoder.putValueAt(columns.get(channel), session, page.getBlock(channel), position);\n+                    }\n+                    else {\n+                        messageEncoder.putValueAt(columns.get(channel), session, page.getBlock(channel), position);\n+                    }\n+                }\n+                else {\n+                    partition = toIntExact(columns.get(channel).getType().getLong(page.getBlock(channel), position));\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, partition, keyEncoder.toByteArray(), messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        return NOT_BLOCKED;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Collection<Slice>> finish()\n+    {\n+        producer.close();\n+        return completedFuture(ImmutableList.of());\n+    }\n+\n+    @Override\n+    public void abort() {}", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3Nzc5MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449177791", "bodyText": "You don't need this.", "author": "aalbu", "createdAt": "2020-07-02T17:47:46Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return this.putNullValue(columnHandle);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODEwMA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449178100", "bodyText": "Do we need this method?", "author": "aalbu", "createdAt": "2020-07-02T17:48:23Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return this.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            return this.putBoolean(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            return this.putLong(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            return this.putInt(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            return this.putShort(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            return this.putByte(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            return this.putDouble(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            return this.putFloat(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            return this.putString(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            return this.putByteBuffer(columnHandle, type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            return this.putSqlDate(columnHandle, (SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            return this.putSqlTime(columnHandle, (SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            return this.putSqlTimeWithTimeZone(columnHandle, (SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            return this.putSqlTimestamp(columnHandle, (SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            return this.putSqlTimestampWithTimeZone(columnHandle, (SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+        }\n+    }\n+\n+    default RowEncoder putNullValue(EncoderColumnHandle columnHandle)\n+    {\n+        throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+    }\n+\n+    default RowEncoder putObject(EncoderColumnHandle columnHandle, Object value)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIxMjA3Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449212072", "bodyText": "no, I'll remove it", "author": "charlesjmorgan", "createdAt": "2020-07-02T18:59:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MDUyOQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449180529", "bodyText": "This is not absolutely required, as close() calls flush().", "author": "aalbu", "createdAt": "2020-07-02T17:53:14Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);\n+                checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnName);\n+                checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnName);\n+\n+                checkArgument(isSupportedType(columnType), \"unsupported column type '%s' for column '%s'\", columnType, columnName);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIxNjQzMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449216432", "bodyText": "Even if I get the byte array before it's closed?", "author": "charlesjmorgan", "createdAt": "2020-07-02T19:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MDUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyMjM3Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449222377", "bodyText": "No, I missed the fact that the return is in the try-with-resources.", "author": "aalbu", "createdAt": "2020-07-02T19:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MDUyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MTM3Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449181372", "bodyText": "The check for whether the column is valid happens at a higher level.  I don't think this should be the encoder's concern.", "author": "aalbu", "createdAt": "2020-07-02T17:54:56Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIxMjUwMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449212502", "bodyText": "right, I noticed that internal columns don't ever seem to make it to the encoder", "author": "charlesjmorgan", "createdAt": "2020-07-02T18:59:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MTM3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MTg4OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449181888", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n          \n          \n            \n                private void validateColumns(Set<EncoderColumnHandle> columnHandles)", "author": "aalbu", "createdAt": "2020-07-02T17:56:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4Mzc2OQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449183769", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        throw new RuntimeException(e);\n          \n          \n            \n                        throw new UncheckedIOException(e);", "author": "aalbu", "createdAt": "2020-07-02T17:59:11Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);\n+                checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnName);\n+                checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnName);\n+\n+                checkArgument(isSupportedType(columnType), \"unsupported column type '%s' for column '%s'\", columnType, columnName);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE5MzY2OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449193668", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n          \n          \n            \n                            if (tableTemplate.getKey().isPresent()) {\n          \n          \n            \n                                if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n          \n          \n            \n                                    KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n          \n          \n            \n                                    key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n          \n          \n            \n                                }\n          \n          \n            \n                            }\n          \n          \n            \n                            Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey()\n          \n          \n            \n                                    .map(keyTemplate -> new KafkaTopicFieldGroup(\n          \n          \n            \n                                            keyTemplate.getDataFormat(),\n          \n          \n            \n                                            keyTemplate.getDataSchema().map(schema -> KafkaQueryRunner.class.getResource(schema).getPath()),\n          \n          \n            \n                                            keyTemplate.getFields()));", "author": "aalbu", "createdAt": "2020-07-02T18:19:43Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -134,9 +138,42 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n \n             Map<SchemaTableName, KafkaTopicDescription> tpchTopicDescriptions = createTpchTopicDescriptions(queryRunner.getCoordinator().getMetadata(), tables);\n \n+            List<String> tableNames = new ArrayList<>(4);\n+            tableNames.add(\"all_datatypes_csv\");\n+\n+            JsonCodec<KafkaTopicDescription> topicDescriptionJsonCodec = new CodecSupplier<>(KafkaTopicDescription.class, queryRunner.getMetadata()).get();\n+\n+            ImmutableMap.Builder<SchemaTableName, KafkaTopicDescription> testTopicDescriptions = ImmutableMap.builder();\n+            for (String tableName : tableNames) {\n+                testingKafka.createTopics(\"write_test.\" + tableName);\n+                SchemaTableName table = new SchemaTableName(\"write_test\", tableName);\n+                KafkaTopicDescription tableTemplate = topicDescriptionJsonCodec.fromJson(toByteArray(TestMinimalFunctionality.class.getResourceAsStream(format(\"/write_test/%s.json\", tableName))));\n+                Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n+                if (tableTemplate.getKey().isPresent()) {\n+                    if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n+                        KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n+                        key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n+                    }\n+                }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIxMDAyNg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449210026", "bodyText": "You could use functional style here, too.", "author": "aalbu", "createdAt": "2020-07-02T18:54:40Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -134,9 +138,42 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n \n             Map<SchemaTableName, KafkaTopicDescription> tpchTopicDescriptions = createTpchTopicDescriptions(queryRunner.getCoordinator().getMetadata(), tables);\n \n+            List<String> tableNames = new ArrayList<>(4);\n+            tableNames.add(\"all_datatypes_csv\");\n+\n+            JsonCodec<KafkaTopicDescription> topicDescriptionJsonCodec = new CodecSupplier<>(KafkaTopicDescription.class, queryRunner.getMetadata()).get();\n+\n+            ImmutableMap.Builder<SchemaTableName, KafkaTopicDescription> testTopicDescriptions = ImmutableMap.builder();\n+            for (String tableName : tableNames) {\n+                testingKafka.createTopics(\"write_test.\" + tableName);\n+                SchemaTableName table = new SchemaTableName(\"write_test\", tableName);\n+                KafkaTopicDescription tableTemplate = topicDescriptionJsonCodec.fromJson(toByteArray(TestMinimalFunctionality.class.getResourceAsStream(format(\"/write_test/%s.json\", tableName))));\n+                Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n+                if (tableTemplate.getKey().isPresent()) {\n+                    if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n+                        KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n+                        key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n+                    }\n+                }\n+                Optional<KafkaTopicFieldGroup> message = tableTemplate.getMessage();\n+                if (tableTemplate.getMessage().isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUwODYzMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449508632", "bodyText": "Would you mind moving to the previous place, before getName, to match ctor params?", "author": "findepi", "createdAt": "2020-07-03T10:25:46Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaColumnHandle.java", "diffHunk": "@@ -152,6 +147,13 @@ public boolean isInternal()\n         return internal;\n     }\n \n+    @Override\n+    @JsonProperty\n+    public int getOrdinalPosition()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUwOTI2OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449509268", "bodyText": "Unused, remove.\nFYI #4330", "author": "findepi", "createdAt": "2020-07-03T10:27:19Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/EncoderColumnHandle.java", "diffHunk": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.type.Type;\n+\n+public interface EncoderColumnHandle\n+        extends ColumnHandle\n+{\n+    boolean isInternal();\n+\n+    String getFormatHint();\n+\n+    Type getType();\n+\n+    String getName();\n+\n+    String getMapping();\n+\n+    String getDataFormat();\n+\n+    int getOrdinalPosition();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxMTE3Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449511172", "bodyText": "Since order of columnHandles is of utmost importance here,\nthis should be a list.", "author": "findepi", "createdAt": "2020-07-03T10:31:34Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_ENCODER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.validateColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxMTkyMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449511923", "bodyText": "reorder methods in the order in which they are used (logically):\n\nputValueFromBlock (or appendValue)\ntoByteArray\nclear", "author": "findepi", "createdAt": "2020-07-03T10:33:15Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449515203", "bodyText": "The encoders will see every column and some encoders (like CSV or RAW) care getting data\nin order. Since encoders are initialized with the list of columns, they know the columns.\nBy passing column index rather than column handle, we can save lookup like here\nio.prestosql.plugin.kafka.encoder.csv.CsvRowEncoder#getPosition\nFurthermore, we can guarantee that columns are being provided in order.\nThis would unlock certain optimizations in encoders like CSV or RAW, which\nwould be writing to a byte buffer directly.\nTo express this in the method signature, I would remove the column parameter\naltogether.\nThird, we could weave in the ConnectorSession session during encoder initialization,\nthis would allow encoder to precalculate (\"compile\") encoding strategies for each\nof the columns, if it wishes to do so.\nTherefore I propose to have this as the signature:\n/**\n * Adds value for the next column to the row being encoded.\n */\nvoid appendValue(Block block, int position)", "author": "findepi", "createdAt": "2020-07-03T10:41:12Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2NjE2Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450166163", "bodyText": "The position argument here represents the index of the column from the table schema, which is not necessarily the same as the position in the result set, so we will still need a mapping (though it would be just an array lookup).\n\nFurthermore, we can guarantee that columns are being provided in order.\nThis would unlock certain optimizations in encoders like CSV or RAW, which\nwould be writing to a byte buffer directly.\n\nI don't know if that's necessarily the case.  It would be pretty constraining for the API to require values be provided in the order columns appear in the table schema.\nPerhaps we would be better off passing the entire page, instead of individual blocks, and let the encoder decide how to iterate over columns.", "author": "aalbu", "createdAt": "2020-07-06T11:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDIzMjIxMQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450232211", "bodyText": "The position argument here represents the index of the column from the table schema\n\nblock+position represents a value (position is the row)", "author": "findepi", "createdAt": "2020-07-06T13:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDIzMzc3MA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450233770", "bodyText": "void appendValue(Block block, int position)\n\nAs discussed offline w @charlesjmorgan  -- appendNextColumnValue\n\nI don't know if that's necessarily the case. It would be pretty constraining for the API to require values be provided in the order columns appear in the table schema.\n\nWhy is that constraining?\nWha would be the advantage of relaxing this?", "author": "findepi", "createdAt": "2020-07-06T13:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI1OTI1MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450259251", "bodyText": "The position argument here represents the index of the column from the table schema\n\nblock+position represents a value (position is the row)\n\nSorry, that was a dumb comment.\nWhere my head was at is that you might have a table schema like foo (x varchar, y bigint, z boolean) and you run INSERT INTO foo (z, y) VALUES (true, 42), so if you don't specify the column, you need to perform a mapping for the column index from the result set schema to the table schema.\n\nWhy is that constraining?\n\nIn the example above, you'd have to append null, then 42, then true.  It seems like it requires a lot of work for the API client.", "author": "aalbu", "createdAt": "2020-07-06T14:26:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI4Mjk2Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450282967", "bodyText": "this would allow encoder to precalculate (\"compile\") encoding strategies for each\nof the columns\n\nBy this do you (@findepi) mean that the encoder could create a list of write functions for the different column types similar to the jdbc connector?\nAre columns in pages always in the same order?", "author": "charlesjmorgan", "createdAt": "2020-07-06T15:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM1MDQ5NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450350495", "bodyText": "I'm worried that key columns and message columns being in the same page might mess up the ordering", "author": "charlesjmorgan", "createdAt": "2020-07-06T16:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2NDM1Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450364357", "bodyText": "It looks like pages always have the columns in the same (correct) order which is good. I could track the index of the key and message columns separately with two different ints.\n\nIt seems like it requires a lot of work for the API client.\n\n@aalbu I think that this is a good point. My only worry is that if we pass the entire page to the key/message encoder then the page includes both the key and message columns so it would need some way to distinguish these. Maybe we could pass a range of index values in the method too? Or would it be better to construct two Block[] for the key and message columns and then pass each of those once to their respective encoders to minimize the number of api calls needed.", "author": "charlesjmorgan", "createdAt": "2020-07-06T17:09:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2ODM2Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450368366", "bodyText": "also I'm not sure if the name append... really fits if we specify the position to put the value at.\nedit: on second thought I think append makes sense if we have an internal index counter. So a method that appends an array of blocks and increments an index counter field and then helper methods that append individual values and use the index field to put the values in the right places", "author": "charlesjmorgan", "createdAt": "2020-07-06T17:17:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM3NDIzMA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450374230", "bodyText": "It looks like pages always have the columns in the same (correct) order which is good\n\nAh, my incorrect assumption was that only the columns specified in the insert are going to be present on the page.  That makes my concern irrelevant.", "author": "aalbu", "createdAt": "2020-07-06T17:28:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQyMzM5NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450423395", "bodyText": "By this do you (@findepi) mean that the encoder could create a list of write functions for the different column types similar to the jdbc connector?\n\nit can, if it wishes to do so\ni am not saying \"do it\". I am just saying \"it is possible\"\n\nAre columns in pages always in the same order?\n\nyes", "author": "findepi", "createdAt": "2020-07-06T19:04:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQyMzkzNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450423937", "bodyText": "@aalbu I think that this is a good point. My only worry is that if we pass the entire page to the key/message encoder then the page includes both the key and message columns so it would need some way to distinguish these.\n\nThat's a very good point, which actually rules out passing full Page (+ position)", "author": "findepi", "createdAt": "2020-07-06T19:06:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTQwNA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449515404", "bodyText": "The interface has 3 actual methods:\n\nputValueFromBlock (or appendValue)\ntoByteArray\nclear\n\nAll the other, default methods are not used by users of the interface.\nThey are convenience methods added for the sake of the implementations.\nMove them to AbstractRowEncoder (as protected).", "author": "findepi", "createdAt": "2020-07-03T10:41:42Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            return putBoolean(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            return putLong(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            return putInt(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            return putShort(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            return putByte(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            return putDouble(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            return putFloat(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            return putString(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            return putByteBuffer(columnHandle, type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            return putSqlDate(columnHandle, (SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            return putSqlTime(columnHandle, (SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            return putSqlTimeWithTimeZone(columnHandle, (SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            return putSqlTimestamp(columnHandle, (SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            return putSqlTimestampWithTimeZone(columnHandle, (SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+        }\n+    }\n+\n+    default RowEncoder putNullValue(EncoderColumnHandle columnHandle)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNjE5Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449516193", "bodyText": "As noted elsewhere:\n\nSet columnHandles -> List columnHandles\nadd ConnectorSession session (as first arg perhaps)", "author": "findepi", "createdAt": "2020-07-03T10:43:30Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoderFactory.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+\n+public interface RowEncoderFactory\n+{\n+    RowEncoder create(Optional<String> dataSchema, Set<EncoderColumnHandle> columnHandles);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNzYyMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449517623", "bodyText": "KAFKA_SCHEMA_ERROR", "author": "findepi", "createdAt": "2020-07-03T10:46:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -18,11 +18,16 @@\n import io.prestosql.spi.ErrorType;\n \n import static io.prestosql.spi.ErrorType.EXTERNAL;\n+import static io.prestosql.spi.ErrorType.INTERNAL_ERROR;\n \n public enum KafkaErrorCode\n         implements ErrorCodeSupplier\n {\n-    KAFKA_SPLIT_ERROR(0, EXTERNAL);\n+    KAFKA_SPLIT_ERROR(0, EXTERNAL),\n+    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxODYxNg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449518616", "bodyText": "KAFKA_ENCODER_ERROR is probably not EXTERNAL\nanyway, i am not convinced there is a value in discerning these", "author": "findepi", "createdAt": "2020-07-03T10:48:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -18,11 +18,16 @@\n import io.prestosql.spi.ErrorType;\n \n import static io.prestosql.spi.ErrorType.EXTERNAL;\n+import static io.prestosql.spi.ErrorType.INTERNAL_ERROR;\n \n public enum KafkaErrorCode\n         implements ErrorCodeSupplier\n {\n-    KAFKA_SPLIT_ERROR(0, EXTERNAL);\n+    KAFKA_SPLIT_ERROR(0, EXTERNAL),\n+    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),\n+    KAFKA_PRODUCER_ERROR(2, INTERNAL_ERROR),\n+    KAFKA_ENCODER_ERROR(3, EXTERNAL)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI0NDU4Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450244583", "bodyText": "Would it be better to just throw some sort of runtime exception in these cases or have a catch all KAFKA_ERROR?", "author": "charlesjmorgan", "createdAt": "2020-07-06T14:06:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxODYxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxOTY5Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449519693", "bodyText": "why?", "author": "findepi", "createdAt": "2020-07-03T10:51:13Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -107,13 +115,16 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n                 .collect(toImmutableList());\n     }\n \n-    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")\n     @Override\n     public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         KafkaTableHandle kafkaTableHandle = convertTableHandle(tableHandle);\n+        return getColumnHandles(kafkaTableHandle.toSchemaTableName());\n+    }\n \n-        SchemaTableName schemaTableName = kafkaTableHandle.toSchemaTableName();\n+    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI0MzM1OQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450243359", "bodyText": "I need to get the columns using just the schema table name so I split the function into two parts. Would it be a good idea to give them different names? like getColumnHandlesHelper or something? The suppress warnings annotation was there originally.", "author": "charlesjmorgan", "createdAt": "2020-07-06T14:04:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxOTY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk4NTE2MA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450985160", "bodyText": "Naming is fine. As for the annotation,  drop it (and one below) in separate commit.", "author": "losipiuk", "createdAt": "2020-07-07T16:16:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxOTY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMDUzOA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449520538", "bodyText": "add\n// TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n\nhere as well", "author": "findepi", "createdAt": "2020-07-03T10:53:31Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,29 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        // TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;\n+\n+        return new KafkaTableHandle(\n+                table.getSchemaName(),\n+                table.getTableName(),\n+                table.getTopicName(),\n+                table.getKeyDataFormat(),\n+                table.getMessageDataFormat(),\n+                table.getKeyDataSchemaLocation(),\n+                table.getMessageDataSchemaLocation(),\n+                columns.stream()\n+                        .map(KafkaColumnHandle.class::cast)\n+                        .collect(toImmutableList()));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishInsert(ConnectorSession session, ConnectorInsertTableHandle insertHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n+    {\n+        return Optional.empty();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMTA4Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449521087", "bodyText": "This should be (and must be) equivalent to table.getColumns() except for internal columns.\nAdd something like\n// Encoders must be give all columns\nList<KafkaColumnHandle> actualColumns = ((KafkaTableHandle) tableHandle).getColumns().stream()\n        .filter(column -> !column.isInternal())\n        .collect(toImmutableList());\ncheckArgument(columns.equals(actualColumns), \"Unexpected columns, expected: %s, got: %s\", actualColumns, columns);\n\nthen you can use actualColumns here.", "author": "findepi", "createdAt": "2020-07-03T10:54:57Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,29 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        // TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;\n+\n+        return new KafkaTableHandle(\n+                table.getSchemaName(),\n+                table.getTableName(),\n+                table.getTopicName(),\n+                table.getKeyDataFormat(),\n+                table.getMessageDataFormat(),\n+                table.getKeyDataSchemaLocation(),\n+                table.getMessageDataSchemaLocation(),\n+                columns.stream()\n+                        .map(KafkaColumnHandle.class::cast)\n+                        .collect(toImmutableList()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMzk2Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449523962", "bodyText": "properties is mutable, so defensive copy would be needed here\n\nchange Properties properties field to Map<...>  properties and initialize it using ImmutableMap\nby using Map<String, Object> you can call the overload of KafkaProducer that does not require you to convert to Properties", "author": "findepi", "createdAt": "2020-07-03T11:02:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig)\n+    {\n+        requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+        Set<HostAddress> nodes = ImmutableSet.copyOf(kafkaConfig.getNodes());\n+        this.properties = new Properties();\n+        this.properties.put(\n+                BOOTSTRAP_SERVERS_CONFIG,\n+                nodes.stream()\n+                        .map(HostAddress::toString)\n+                        .collect(joining(\",\")));\n+        this.properties.put(ACKS_CONFIG, \"all\");\n+        this.properties.put(LINGER_MS_CONFIG, 5);  // reduces number of requests sent, adds 5ms of latency in the absence of load\n+    }\n+\n+    public KafkaProducer<byte[], byte[]> create()\n+    {\n+        return new KafkaProducer<>(properties, new ByteArraySerializer(), new ByteArraySerializer());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2ODEyMA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450168120", "bodyText": "I think we should do some additional validation on the columns here.  For example, if the table schema specifies a key, we would need to ensure that all key columns are present.  We could also do the check for internal columns here - make sure that only supported ones are present.", "author": "aalbu", "createdAt": "2020-07-06T11:51:05Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM5MTUwNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450391507", "bodyText": "All columns are present by means of io.prestosql.spi.connector.ConnectorMetadata#supportsMissingColumnsOnInsert", "author": "findepi", "createdAt": "2020-07-06T18:02:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2ODEyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQyMzY1NA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450423654", "bodyText": "Thanks.", "author": "aalbu", "createdAt": "2020-07-06T19:05:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2ODEyMA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk2NjI2OQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450966269", "bodyText": "This should go to previous commit.", "author": "losipiuk", "createdAt": "2020-07-07T15:48:16Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -24,7 +24,7 @@\n         implements ErrorCodeSupplier\n {\n     KAFKA_SPLIT_ERROR(0, EXTERNAL),\n-    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),\n+    KAFKA_SCHEMA_ERROR(1, EXTERNAL),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk3MDMyNQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450970325", "bodyText": "right, thank you", "author": "charlesjmorgan", "createdAt": "2020-07-07T15:54:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk2NjI2OQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5MjM2MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450992361", "bodyText": "The callback can be run in very different thread than the one calling producer.send. Do we know if exception thrown by the callback will be always propagated back to Presto thread. I.e if we do not get exception directly here, will we still get it on producer.flush() called from finish()?", "author": "losipiuk", "createdAt": "2020-07-07T16:28:15Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (columns.get(channel).isKeyCodec()) {\n+                    keyEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+                else {\n+                    messageEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, keyEncoder.toByteArray(), messageEncoder.toByteArray()), (recordMetadata, e) -> {\n+                if (e != null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxNTkzNg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451015936", "bodyText": "the callback is executed in the I/O thread of the producer. It doesn't look like flush throws any exceptions other than InterruptException", "author": "charlesjmorgan", "createdAt": "2020-07-07T17:07:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5MjM2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM2NzY3Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451367672", "bodyText": "Indeed it looks so. If the callback throws exception in later stage of sending it will be caught by try-catch in ProducerBatch.completeFutureAndFireCallbacks and just logged. See quoted method body below.\n    private void completeFutureAndFireCallbacks(long baseOffset, long logAppendTime, RuntimeException exception) {\n        // Set the future before invoking the callbacks as we rely on its state for the `onCompletion` call\n        produceFuture.set(baseOffset, logAppendTime, exception);\n\n        // execute callbacks\n        for (Thunk thunk : thunks) {\n            try {\n                if (exception == null) {\n                    RecordMetadata metadata = thunk.future.value();\n                    if (thunk.callback != null)\n                        thunk.callback.onCompletion(metadata, null);\n                } else {\n                    if (thunk.callback != null)\n                        thunk.callback.onCompletion(null, exception);\n                }\n            } catch (Exception e) {\n                log.error(\"Error executing user-provided callback on message for topic-partition '{}'\", topicPartition, e);\n            }\n        }\n\n        produceFuture.done();\n    }\nGiven such logic I think that instead of throwing exception in from our callback, we should just record that message sending failure happened. A simple counter, how many message-sending failures we observed should do.\nThen in KafkaPageSink.finish(), after producer.close(), we can introspect the counter, and if it is >0 we can throw PrestoException there", "author": "losipiuk", "createdAt": "2020-07-08T08:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5MjM2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTY0NTQ5MA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451645490", "bodyText": "that sounds good!", "author": "charlesjmorgan", "createdAt": "2020-07-08T15:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5MjM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5ODg1NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450998855", "bodyText": "It cannot return different list than one we pass to it as an argument, right? Otherwise the columHandles stored here would not be compatible with loop which calls to appendColumnValue. Change to void (unless I miss something).", "author": "losipiuk", "createdAt": "2020-07-07T16:38:49Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxMTY2OA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451011668", "bodyText": "It only returns a different list as a defensive copy, I don't think that this should affect how it functions", "author": "charlesjmorgan", "createdAt": "2020-07-07T16:59:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5ODg1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxNTAzMQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451015031", "bodyText": "I think we should do the defensive copy in AbstractRowEncoder and make validate return void so it is obvious it cannot change the list.", "author": "losipiuk", "createdAt": "2020-07-07T17:05:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5ODg1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxODI0MA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451018240", "bodyText": "right, would be best to have validateColumns only validate the columns rather than validate-and-copy the columns", "author": "charlesjmorgan", "createdAt": "2020-07-07T17:11:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5ODg1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMDYzNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451000637", "bodyText": "checkState that currentColumnIndex < columnHandles.size()", "author": "losipiuk", "createdAt": "2020-07-07T16:41:45Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);\n+\n+    @Override\n+    public RowEncoder appendColumnValue(Block block, int position)\n+    {\n+        Type type = columnHandles.get(currentColumnIndex).getType();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMTE0Nw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451001147", "bodyText": "should we checkState that currentColumnIndex == columnHandles.size()?", "author": "losipiuk", "createdAt": "2020-07-07T16:42:32Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);\n+\n+    @Override\n+    public RowEncoder appendColumnValue(Block block, int position)\n+    {\n+        Type type = columnHandles.get(currentColumnIndex).getType();\n+        if (block.isNull(position)) {\n+            appendNullValue();\n+        }\n+        else if (type == BOOLEAN) {\n+            appendBoolean(type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            appendLong(type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            appendInt(toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            appendShort(Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            appendByte(SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            appendDouble(type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            appendFloat(intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            appendString(type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            appendByteBuffer(type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            appendSqlDate((SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            appendSqlTime((SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            appendSqlTimeWithTimeZone((SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            appendSqlTimestamp((SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            appendSqlTimestampWithTimeZone((SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandles.get(currentColumnIndex).getName()));\n+        }\n+        currentColumnIndex++;\n+        return this;\n+    }\n+\n+    // these append value methods should be overridden for each row encoder\n+    // only the methods with types supported by the data format should be overridden\n+    protected void appendNullValue()\n+    {\n+        throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendLong(long value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", long.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendInt(int value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", int.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendShort(short value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", short.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendByte(byte value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", byte.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendDouble(double value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", double.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendFloat(float value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", float.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendBoolean(boolean value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", boolean.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendString(String value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendByteBuffer(ByteBuffer value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlDate(SqlDate value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTime(SqlTime value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimeWithTimeZone(SqlTimeWithTimeZone value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimestamp(SqlTimestamp value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimestampWithTimeZone(SqlTimestampWithTimeZone value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    @Override\n+    public void reset()\n+    {\n+        currentColumnIndex = 0;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwOTUyOA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451009528", "bodyText": "yes, that would be a good idea", "author": "charlesjmorgan", "createdAt": "2020-07-07T16:56:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMTE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTY5MjExMw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451692113", "bodyText": "I actually think this belongs in toByteArray, not in reset. So that if not all columns are appended previous row values can't be duplicated.", "author": "charlesjmorgan", "createdAt": "2020-07-08T16:58:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMTE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMzQ4Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451003486", "bodyText": "not really needed right?", "author": "losipiuk", "createdAt": "2020-07-07T16:46:10Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+\n+public class CsvRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final int size;\n+    private String[] row;\n+\n+    public CsvRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    @Override\n+    protected List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        return ImmutableList.copyOf(columnHandles);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        row[currentColumnIndex] = null;\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        row[currentColumnIndex] = Long.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        row[currentColumnIndex] = Integer.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        row[currentColumnIndex] = Short.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        row[currentColumnIndex] = Byte.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        row[currentColumnIndex] = Double.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        row[currentColumnIndex] = Float.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        row[currentColumnIndex] = Boolean.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        row[currentColumnIndex] = value;\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    protected void clear()\n+    {\n+        row = new String[size];", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwOTI5NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451009295", "bodyText": "If we make sure that every index is updated for every row then no it is not needed", "author": "charlesjmorgan", "createdAt": "2020-07-07T16:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMzQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwNDMwMA==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451004300", "bodyText": "do we support overriding special characters on read? If so we should also support them on write.", "author": "losipiuk", "createdAt": "2020-07-07T16:47:35Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+\n+public class CsvRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final int size;\n+    private String[] row;\n+\n+    public CsvRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    @Override\n+    protected List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        return ImmutableList.copyOf(columnHandles);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        row[currentColumnIndex] = null;\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        row[currentColumnIndex] = Long.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        row[currentColumnIndex] = Integer.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        row[currentColumnIndex] = Short.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        row[currentColumnIndex] = Byte.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        row[currentColumnIndex] = Double.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        row[currentColumnIndex] = Float.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        row[currentColumnIndex] = Boolean.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        row[currentColumnIndex] = value;\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwODg3Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451008876", "bodyText": "You mean like using something other than a comma to separate the values?", "author": "charlesjmorgan", "createdAt": "2020-07-07T16:55:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwNDMwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxMTExMg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451011112", "bodyText": "Yep. Or different quote char.", "author": "losipiuk", "createdAt": "2020-07-07T16:59:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwNDMwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAxNjYxNw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451016617", "bodyText": "I don't see anything about that in the docs for read so I'm gonna say no it doesn't. Would be a nice feature tho", "author": "charlesjmorgan", "createdAt": "2020-07-07T17:08:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwNDMwMA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5MjM1Ng==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452192356", "bodyText": "make it return long.", "author": "losipiuk", "createdAt": "2020-07-09T12:49:44Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();\n+    }\n+\n+    private static class ErrorCountingCallback\n+            implements Callback\n+    {\n+        private final AtomicLong errorCounter;\n+\n+        public ErrorCountingCallback()\n+        {\n+            this.errorCounter = new AtomicLong(0);\n+        }\n+\n+        @Override\n+        public void onCompletion(RecordMetadata recordMetadata, Exception e)\n+        {\n+            if (e != null) {\n+                errorCounter.incrementAndGet();\n+            }\n+        }\n+\n+        public AtomicLong getErrorCount()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5Mjg5MQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452192891", "bodyText": "nit s/'%d'/%d/", "author": "losipiuk", "createdAt": "2020-07-09T12:50:38Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();\n+    }\n+\n+    private static class ErrorCountingCallback\n+            implements Callback\n+    {\n+        private final AtomicLong errorCounter;\n+\n+        public ErrorCountingCallback()\n+        {\n+            this.errorCounter = new AtomicLong(0);\n+        }\n+\n+        @Override\n+        public void onCompletion(RecordMetadata recordMetadata, Exception e)\n+        {\n+            if (e != null) {\n+                errorCounter.incrementAndGet();\n+            }\n+        }\n+\n+        public AtomicLong getErrorCount()\n+        {\n+            return errorCounter;\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (columns.get(channel).isKeyCodec()) {\n+                    keyEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+                else {\n+                    messageEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, keyEncoder.toByteArray(), messageEncoder.toByteArray()), errorCounter);\n+\n+            keyEncoder.reset();\n+            messageEncoder.reset();\n+        }\n+        return NOT_BLOCKED;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Collection<Slice>> finish()\n+    {\n+        producer.flush();\n+        producer.close();\n+        if (errorCounter.getErrorCount().get() > 0) {\n+            throw new PrestoException(KAFKA_PRODUCER_ERROR, format(\"'%d' producer record('s) failed to send\", errorCounter.getErrorCount().get()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5NDg2Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452194863", "bodyText": "Capitalize Kafka in commit messages and (if there are occurences) comments.", "author": "losipiuk", "createdAt": "2020-07-09T12:54:02Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaColumnHandle.java", "diffHunk": "@@ -25,7 +26,7 @@\n import static java.util.Objects.requireNonNull;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5Nzc1Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452197753", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    validateColumns(columnHandles);\n          \n          \n            \n                    this.columnHandles = ImmutableList.copyOf(columnHandles);\n          \n          \n            \n                    this.columnHandles = ImmutableList.copyOf(columnHandles);\n          \n          \n            \n                    validateColumns(this.columnHandles);", "author": "losipiuk", "createdAt": "2020-07-09T12:58:44Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        validateColumns(columnHandles);\n+        this.columnHandles = ImmutableList.copyOf(columnHandles);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjIwMDk0NQ==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452200945", "bodyText": "Any idea if we can write a test which triggers errorcounter?", "author": "losipiuk", "createdAt": "2020-07-09T13:03:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjI3MDc0Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452270742", "bodyText": "hmm not that I can think of, I'm not sure how to intentionally make a producer fail to send a record.", "author": "charlesjmorgan", "createdAt": "2020-07-09T14:43:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjIwMDk0NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2NzI1Mg==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452367252", "bodyText": "I wonder if we need this method at all.  Perhaps the contract could be: add data, then generate the record, rinse, repeat.  Generating the byte[] could reset things (that would be an implementation detail).", "author": "aalbu", "createdAt": "2020-07-09T17:10:12Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.block.Block;\n+\n+public interface RowEncoder\n+{\n+    /**\n+     * Adds the value from the given block/position to the row being encoded\n+     */\n+    RowEncoder appendColumnValue(Block block, int position);\n+\n+    /**\n+     * Returns the encoded values as a byte array\n+     */\n+    byte[] toByteArray();\n+\n+    /**\n+     * Resets the encoder so that it can be used for the next row\n+     */\n+    void reset();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM5NjU2Mw==", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452396563", "bodyText": "I was thinking a similar thing. I just went through and updated all the encoders with the changes I've made and I realized that the clear method  was only used once by the RawRowEncoder.", "author": "charlesjmorgan", "createdAt": "2020-07-09T18:03:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2NzI1Mg=="}], "type": "inlineReview"}, {"oid": "b67102f9096300bcf67354cd0c151e5b6a91faa2", "url": "https://github.com/trinodb/trino/commit/b67102f9096300bcf67354cd0c151e5b6a91faa2", "message": "Implement inserts for Kafka connector", "committedDate": "2020-07-09T18:14:02Z", "type": "commit"}, {"oid": "1ebec263807acd62541aa83289e083298617c9d6", "url": "https://github.com/trinodb/trino/commit/1ebec263807acd62541aa83289e083298617c9d6", "message": "Add CSV encoder and CSV roundtrip test", "committedDate": "2020-07-09T18:14:06Z", "type": "commit"}, {"oid": "54be340f460d656aefb2f5959b0aa915fba59e89", "url": "https://github.com/trinodb/trino/commit/54be340f460d656aefb2f5959b0aa915fba59e89", "message": "Remove suppress warnings labels", "committedDate": "2020-07-09T18:14:09Z", "type": "commit"}, {"oid": "54be340f460d656aefb2f5959b0aa915fba59e89", "url": "https://github.com/trinodb/trino/commit/54be340f460d656aefb2f5959b0aa915fba59e89", "message": "Remove suppress warnings labels", "committedDate": "2020-07-09T18:14:09Z", "type": "forcePushed"}]}