{"pr_number": 2823, "pr_title": "Support decimal casting in Parquet", "pr_createdAt": "2020-02-13T15:53:51Z", "pr_url": "https://github.com/trinodb/trino/pull/2823", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEwNjU0MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382106540", "bodyText": "That's interesting, asserting on the data provider?", "author": "aalbu", "createdAt": "2020-02-20T16:19:07Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,479 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.RoundingMode;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private final java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"different-rescalings\")\n+    public void testReadingRescaledDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        assertTrue((schemaPrecision - schemaScale) >= (precision - scale), \"Test case is invalid - significant truncation\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjExOTQ4Ng==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382119486", "bodyText": "Not that it matters, but you could throw an UncheckedIOException.", "author": "aalbu", "createdAt": "2020-02-20T16:39:01Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,479 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.RoundingMode;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private final java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"different-rescalings\")\n+    public void testReadingRescaledDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        assertTrue((schemaPrecision - schemaScale) >= (precision - scale), \"Test case is invalid - significant truncation\");\n+        assertTrue(schemaScale >= scale, \"Test case is invalid - truncation\");\n+\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"roundable-decimals\")\n+    public void testReadingRoundedDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertRoundedValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"non-rescalable-decimals\")\n+    public void testReadingNonRescalableDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertQueryFails(format(\"SELECT * FROM %s\", tableName), format(\"Cannot cast DECIMAL\\\\(%d, %d\\\\) '.*' to DECIMAL\\\\(%d, %d\\\\)\", precision, scale, schemaPrecision, schemaScale));\n+\n+        dropTable(tableName);\n+    }\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, expectedValues\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    @DataProvider(name = \"different-rescalings\")\n+    public Object[][] differentFittingRescaling()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale, expectedValues\n+        return new Object[][] {\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4, ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        13, 5, ImmutableList.of(\"10.01000\", \"10.0000\", \"1.23000\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 4, ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_rescale2\", false,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 2, ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        11, 3, ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4, ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5, ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle_rescale\", true,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5, ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2_rescale\", false,\n+                        14, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        20, 3, ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3_rescale\", false,\n+                        6, 3, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6, ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle_rescale\", true,\n+                        6, 3, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6, ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_10_2_rescale_max\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        38, 4, ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_18_4_rescale_max\", false,\n+                        18, 4, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(18, 4), minimumValue(18, 4)),\n+                        38, 14, ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(18, 4), minimumValue(18, 4))}\n+        };\n+    }\n+\n+    @DataProvider(name = \"roundable-decimals\")\n+    public Object[][] roundableDecimals()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2_round\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_9_2_fle_round\", true,\n+                        9, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2)),\n+                        12, 1, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2))},\n+                {\"decimal_4_2_round\", false,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        7, 1, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_round2\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))}\n+        };\n+    }\n+\n+    @DataProvider(name = \"non-rescalable-decimals\")\n+    public Object[][] differentUnscalable()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale\n+        return new Object[][] {\n+                {\"decimal_4_2_rescale_fail\", false,\n+                        4, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        4, 3},\n+                {\"decimal_10_2_rescale_fail\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        10, 3},\n+                {\"decimal_10_2_rescale_fail2\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        3, 2},\n+                {\"decimal_10_2_fle_rescale_fail\", true,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        14, 7},\n+                {\"decimal_10_2_rescale_fail3\", false,\n+                        10, 2, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        10, 4},\n+                {\"decimal_18_8_rescale_fail\", false,\n+                        18, 8, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(18, 8), minimumValue(18, 8)),\n+                        32, 23},\n+                {\"decimal_20_8_rescale_fail\", false,\n+                        20, 8, ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(20, 8), minimumValue(20, 8)),\n+                        32, 21}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", new BigDecimal(s).setScale(scale).toString())).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    protected void assertRoundedValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", new BigDecimal(s).setScale(scale, RoundingMode.HALF_EVEN).toString())).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5NzI1Mw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382197253", "bodyText": "In Presto, the naming convention is to have Test as the class name prefix (except for test base classes).", "author": "aalbu", "createdAt": "2020-02-20T19:03:30Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,479 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.RoundingMode;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test\n+public class ParquetDecimalScalingTest", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5NDU3OQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382694579", "bodyText": "bump; you fixed the name in the second commit", "author": "findepi", "createdAt": "2020-02-21T16:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5NzI1Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5NTE3Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382695177", "bodyText": "avoid IO in test init / cinit (surefire is notorious about misreporting errors during this phase)\nuse @BeforeClass instead (here this would need to be done in createQueryRunner method actually)", "author": "findepi", "createdAt": "2020-02-21T16:57:59Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5NTg3MQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382695871", "bodyText": "move ImmutableList.of( to next line", "author": "findepi", "createdAt": "2020-02-21T16:59:17Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5NjIzNw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382696237", "bodyText": "i strongly prefer using default @DataProvider name (ie method name). One name less.", "author": "findepi", "createdAt": "2020-02-21T17:00:00Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5Nzk4OQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382697989", "bodyText": "In theory format may mask problems. Just SELECT * would be better.\nSince you have expected  values at hand, \"VALUES \" doesn't really help you (joiner, ,etc). you can computeActual and you should get BigDecimals in return", "author": "findepi", "createdAt": "2020-02-21T17:03:28Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5ODYyNQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382698625", "bodyText": "irrelevant, so you use default (new SecurityConfig().getSecuritySystem())", "author": "findepi", "createdAt": "2020-02-21T17:04:46Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzAxOTk2MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r383019960", "bodyText": "Default throws exception during table drop: java.lang.RuntimeException: Access Denied: Cannot drop table tpch.decimal_4_2_fle_rescale", "author": "wendigo", "createdAt": "2020-02-23T16:46:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5ODYyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5ODY3MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382698670", "bodyText": "relevant?", "author": "findepi", "createdAt": "2020-02-21T17:04:51Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5OTQ4Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382699487", "bodyText": "what does \"order\" stand for here?", "author": "findepi", "createdAt": "2020-02-21T17:06:27Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private Path getParquetWritePath(String tableName)\n+    {\n+        return new Path(basePath.toString(), format(\"hive_data/tpch/%s/%s\", tableName, UUID.randomUUID().toString()));\n+    }\n+\n+    private static void createParquetFile(\n+            Path path,\n+            StandardStructObjectInspector inspector,\n+            Iterator<?>[] iterators,\n+            MessageType parquetSchema,\n+            List<String> columnNames)\n+    {\n+        Properties tableProperties = createTableProperties(columnNames, Collections.singletonList(inspector));\n+\n+        JobConf jobConf = new JobConf();\n+        jobConf.setEnum(COMPRESSION, UNCOMPRESSED);\n+        jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+        jobConf.setEnum(WRITER_VERSION, PARQUET_2_0);\n+\n+        try {\n+            FileSinkOperator.RecordWriter recordWriter = new TestMapredParquetOutputFormat(Optional.of(parquetSchema), true)\n+                    .getHiveRecordWriter(\n+                            jobConf,\n+                            path,\n+                            Text.class,\n+                            false,\n+                            tableProperties,\n+                            () -> {});\n+\n+            Object row = inspector.create();\n+            List<StructField> fields = ImmutableList.copyOf(inspector.getAllStructFieldRefs());\n+\n+            while (stream(iterators).allMatch(Iterator::hasNext)) {\n+                for (int i = 0; i < fields.size(); i++) {\n+                    Object value = iterators[i].next();\n+                    inspector.setStructFieldData(row, fields.get(i), value);\n+                }\n+\n+                ParquetHiveSerDe serde = new ParquetHiveSerDe();\n+                serde.initialize(jobConf, tableProperties, null);\n+                Writable record = serde.serialize(row, inspector);\n+                recordWriter.write(record);\n+            }\n+\n+            recordWriter.close(false);\n+        }\n+        catch (IOException | SerDeException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void writeParquetDecimalsRecord(Path output, List<ParquetDecimalInsert> inserts)\n+    {\n+        List<String> fields = inserts.stream().map(ParquetDecimalInsert::schemaFieldDeclaration).collect(toImmutableList());\n+        MessageType schema = parseMessageType(format(\"message record { %s; }\", Joiner.on(\"; \").join(fields)));\n+        List<ObjectInspector> inspectors = inserts.stream().map(ParquetDecimalInsert::getParquetObjectInspector).collect(toImmutableList());\n+        List<String> columnNames = inserts.stream().map(ParquetDecimalInsert::getColumnName).collect(toImmutableList());\n+        Iterator<?>[] values = inserts.stream().map(ParquetDecimalInsert::getValues).map(Iterable::iterator).toArray(Iterator[]::new);\n+\n+        createParquetFile(\n+                output,\n+                getStandardStructObjectInspector(columnNames, inspectors),\n+                values,\n+                schema,\n+                Collections.singletonList(\"record\"));\n+    }\n+\n+    private static Properties createTableProperties(List<String> columnNames, List<ObjectInspector> objectInspectors)\n+    {\n+        Properties orderTableProperties = new Properties();\n+        orderTableProperties.setProperty(\"columns\", Joiner.on(',').join(columnNames));\n+        orderTableProperties.setProperty(\"columns.types\", Joiner.on(',').join(transform(objectInspectors, ObjectInspector::getTypeName)));\n+        return orderTableProperties;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5OTg3Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382699877", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n          \n          \n            \n                            throw new IllegalArgumentException(\"Scale cannot be greater than 38 or less than 0\");", "author": "findepi", "createdAt": "2020-02-21T17:07:12Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private Path getParquetWritePath(String tableName)\n+    {\n+        return new Path(basePath.toString(), format(\"hive_data/tpch/%s/%s\", tableName, UUID.randomUUID().toString()));\n+    }\n+\n+    private static void createParquetFile(\n+            Path path,\n+            StandardStructObjectInspector inspector,\n+            Iterator<?>[] iterators,\n+            MessageType parquetSchema,\n+            List<String> columnNames)\n+    {\n+        Properties tableProperties = createTableProperties(columnNames, Collections.singletonList(inspector));\n+\n+        JobConf jobConf = new JobConf();\n+        jobConf.setEnum(COMPRESSION, UNCOMPRESSED);\n+        jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+        jobConf.setEnum(WRITER_VERSION, PARQUET_2_0);\n+\n+        try {\n+            FileSinkOperator.RecordWriter recordWriter = new TestMapredParquetOutputFormat(Optional.of(parquetSchema), true)\n+                    .getHiveRecordWriter(\n+                            jobConf,\n+                            path,\n+                            Text.class,\n+                            false,\n+                            tableProperties,\n+                            () -> {});\n+\n+            Object row = inspector.create();\n+            List<StructField> fields = ImmutableList.copyOf(inspector.getAllStructFieldRefs());\n+\n+            while (stream(iterators).allMatch(Iterator::hasNext)) {\n+                for (int i = 0; i < fields.size(); i++) {\n+                    Object value = iterators[i].next();\n+                    inspector.setStructFieldData(row, fields.get(i), value);\n+                }\n+\n+                ParquetHiveSerDe serde = new ParquetHiveSerDe();\n+                serde.initialize(jobConf, tableProperties, null);\n+                Writable record = serde.serialize(row, inspector);\n+                recordWriter.write(record);\n+            }\n+\n+            recordWriter.close(false);\n+        }\n+        catch (IOException | SerDeException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void writeParquetDecimalsRecord(Path output, List<ParquetDecimalInsert> inserts)\n+    {\n+        List<String> fields = inserts.stream().map(ParquetDecimalInsert::schemaFieldDeclaration).collect(toImmutableList());\n+        MessageType schema = parseMessageType(format(\"message record { %s; }\", Joiner.on(\"; \").join(fields)));\n+        List<ObjectInspector> inspectors = inserts.stream().map(ParquetDecimalInsert::getParquetObjectInspector).collect(toImmutableList());\n+        List<String> columnNames = inserts.stream().map(ParquetDecimalInsert::getColumnName).collect(toImmutableList());\n+        Iterator<?>[] values = inserts.stream().map(ParquetDecimalInsert::getValues).map(Iterable::iterator).toArray(Iterator[]::new);\n+\n+        createParquetFile(\n+                output,\n+                getStandardStructObjectInspector(columnNames, inspectors),\n+                values,\n+                schema,\n+                Collections.singletonList(\"record\"));\n+    }\n+\n+    private static Properties createTableProperties(List<String> columnNames, List<ObjectInspector> objectInspectors)\n+    {\n+        Properties orderTableProperties = new Properties();\n+        orderTableProperties.setProperty(\"columns\", Joiner.on(',').join(columnNames));\n+        orderTableProperties.setProperty(\"columns.types\", Joiner.on(',').join(transform(objectInspectors, ObjectInspector::getTypeName)));\n+        return orderTableProperties;\n+    }\n+\n+    public static class ParquetDecimalInsert\n+    {\n+        private final String columnName;\n+        private final boolean forceFixedLengthArray;\n+        private final int precision;\n+        private final int scale;\n+        private final List<String> values;\n+\n+        public ParquetDecimalInsert(String columnName, boolean forceFixedLengthArray, int precision, int scale, List<String> values)\n+        {\n+            this.columnName = columnName;\n+            this.forceFixedLengthArray = forceFixedLengthArray;\n+            this.precision = precision;\n+            this.scale = scale;\n+            this.values = values;\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n+\n+        public String parquetStorage()\n+        {\n+            if (!forceFixedLengthArray && precision > 0 && precision < 10) {\n+                return \"INT32\";\n+            }\n+\n+            if (!forceFixedLengthArray && precision >= 10 && precision < 18) {\n+                return \"INT64\";\n+            }\n+\n+            if (precision > 38 || precision < 0) {\n+                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDE1Mg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382700152", "bodyText": "is it suitable also for scale!=0 ?", "author": "findepi", "createdAt": "2020-02-21T17:07:44Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private Path getParquetWritePath(String tableName)\n+    {\n+        return new Path(basePath.toString(), format(\"hive_data/tpch/%s/%s\", tableName, UUID.randomUUID().toString()));\n+    }\n+\n+    private static void createParquetFile(\n+            Path path,\n+            StandardStructObjectInspector inspector,\n+            Iterator<?>[] iterators,\n+            MessageType parquetSchema,\n+            List<String> columnNames)\n+    {\n+        Properties tableProperties = createTableProperties(columnNames, Collections.singletonList(inspector));\n+\n+        JobConf jobConf = new JobConf();\n+        jobConf.setEnum(COMPRESSION, UNCOMPRESSED);\n+        jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+        jobConf.setEnum(WRITER_VERSION, PARQUET_2_0);\n+\n+        try {\n+            FileSinkOperator.RecordWriter recordWriter = new TestMapredParquetOutputFormat(Optional.of(parquetSchema), true)\n+                    .getHiveRecordWriter(\n+                            jobConf,\n+                            path,\n+                            Text.class,\n+                            false,\n+                            tableProperties,\n+                            () -> {});\n+\n+            Object row = inspector.create();\n+            List<StructField> fields = ImmutableList.copyOf(inspector.getAllStructFieldRefs());\n+\n+            while (stream(iterators).allMatch(Iterator::hasNext)) {\n+                for (int i = 0; i < fields.size(); i++) {\n+                    Object value = iterators[i].next();\n+                    inspector.setStructFieldData(row, fields.get(i), value);\n+                }\n+\n+                ParquetHiveSerDe serde = new ParquetHiveSerDe();\n+                serde.initialize(jobConf, tableProperties, null);\n+                Writable record = serde.serialize(row, inspector);\n+                recordWriter.write(record);\n+            }\n+\n+            recordWriter.close(false);\n+        }\n+        catch (IOException | SerDeException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void writeParquetDecimalsRecord(Path output, List<ParquetDecimalInsert> inserts)\n+    {\n+        List<String> fields = inserts.stream().map(ParquetDecimalInsert::schemaFieldDeclaration).collect(toImmutableList());\n+        MessageType schema = parseMessageType(format(\"message record { %s; }\", Joiner.on(\"; \").join(fields)));\n+        List<ObjectInspector> inspectors = inserts.stream().map(ParquetDecimalInsert::getParquetObjectInspector).collect(toImmutableList());\n+        List<String> columnNames = inserts.stream().map(ParquetDecimalInsert::getColumnName).collect(toImmutableList());\n+        Iterator<?>[] values = inserts.stream().map(ParquetDecimalInsert::getValues).map(Iterable::iterator).toArray(Iterator[]::new);\n+\n+        createParquetFile(\n+                output,\n+                getStandardStructObjectInspector(columnNames, inspectors),\n+                values,\n+                schema,\n+                Collections.singletonList(\"record\"));\n+    }\n+\n+    private static Properties createTableProperties(List<String> columnNames, List<ObjectInspector> objectInspectors)\n+    {\n+        Properties orderTableProperties = new Properties();\n+        orderTableProperties.setProperty(\"columns\", Joiner.on(',').join(columnNames));\n+        orderTableProperties.setProperty(\"columns.types\", Joiner.on(',').join(transform(objectInspectors, ObjectInspector::getTypeName)));\n+        return orderTableProperties;\n+    }\n+\n+    public static class ParquetDecimalInsert\n+    {\n+        private final String columnName;\n+        private final boolean forceFixedLengthArray;\n+        private final int precision;\n+        private final int scale;\n+        private final List<String> values;\n+\n+        public ParquetDecimalInsert(String columnName, boolean forceFixedLengthArray, int precision, int scale, List<String> values)\n+        {\n+            this.columnName = columnName;\n+            this.forceFixedLengthArray = forceFixedLengthArray;\n+            this.precision = precision;\n+            this.scale = scale;\n+            this.values = values;\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n+\n+        public String parquetStorage()\n+        {\n+            if (!forceFixedLengthArray && precision > 0 && precision < 10) {\n+                return \"INT32\";\n+            }\n+\n+            if (!forceFixedLengthArray && precision >= 10 && precision < 18) {\n+                return \"INT64\";\n+            }\n+\n+            if (precision > 38 || precision < 0) {\n+                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n+            }\n+\n+            return format(\"fixed_len_byte_array(%d)\", ParquetHiveSerDe.PRECISION_TO_BYTE_COUNT[precision - 1]);\n+        }\n+\n+        public ObjectInspector getParquetObjectInspector()\n+        {\n+            if (!forceFixedLengthArray && precision > 0 && precision < 10) {\n+                return javaIntObjectInspector;\n+            }\n+\n+            if (!forceFixedLengthArray && precision >= 10 && precision < 18) {\n+                return javaLongObjectInspector;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzAxOTc0Ng==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r383019746", "bodyText": "yes, it's how the decimal is represented in parquet (decimals got converted to longs)", "author": "wendigo", "createdAt": "2020-02-23T16:43:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDI0NA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382700244", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n          \n          \n            \n                            throw new IllegalArgumentException(\"Scale cannot be greater than 38 or less than 0\");", "author": "findepi", "createdAt": "2020-02-21T17:07:54Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private Path getParquetWritePath(String tableName)\n+    {\n+        return new Path(basePath.toString(), format(\"hive_data/tpch/%s/%s\", tableName, UUID.randomUUID().toString()));\n+    }\n+\n+    private static void createParquetFile(\n+            Path path,\n+            StandardStructObjectInspector inspector,\n+            Iterator<?>[] iterators,\n+            MessageType parquetSchema,\n+            List<String> columnNames)\n+    {\n+        Properties tableProperties = createTableProperties(columnNames, Collections.singletonList(inspector));\n+\n+        JobConf jobConf = new JobConf();\n+        jobConf.setEnum(COMPRESSION, UNCOMPRESSED);\n+        jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+        jobConf.setEnum(WRITER_VERSION, PARQUET_2_0);\n+\n+        try {\n+            FileSinkOperator.RecordWriter recordWriter = new TestMapredParquetOutputFormat(Optional.of(parquetSchema), true)\n+                    .getHiveRecordWriter(\n+                            jobConf,\n+                            path,\n+                            Text.class,\n+                            false,\n+                            tableProperties,\n+                            () -> {});\n+\n+            Object row = inspector.create();\n+            List<StructField> fields = ImmutableList.copyOf(inspector.getAllStructFieldRefs());\n+\n+            while (stream(iterators).allMatch(Iterator::hasNext)) {\n+                for (int i = 0; i < fields.size(); i++) {\n+                    Object value = iterators[i].next();\n+                    inspector.setStructFieldData(row, fields.get(i), value);\n+                }\n+\n+                ParquetHiveSerDe serde = new ParquetHiveSerDe();\n+                serde.initialize(jobConf, tableProperties, null);\n+                Writable record = serde.serialize(row, inspector);\n+                recordWriter.write(record);\n+            }\n+\n+            recordWriter.close(false);\n+        }\n+        catch (IOException | SerDeException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void writeParquetDecimalsRecord(Path output, List<ParquetDecimalInsert> inserts)\n+    {\n+        List<String> fields = inserts.stream().map(ParquetDecimalInsert::schemaFieldDeclaration).collect(toImmutableList());\n+        MessageType schema = parseMessageType(format(\"message record { %s; }\", Joiner.on(\"; \").join(fields)));\n+        List<ObjectInspector> inspectors = inserts.stream().map(ParquetDecimalInsert::getParquetObjectInspector).collect(toImmutableList());\n+        List<String> columnNames = inserts.stream().map(ParquetDecimalInsert::getColumnName).collect(toImmutableList());\n+        Iterator<?>[] values = inserts.stream().map(ParquetDecimalInsert::getValues).map(Iterable::iterator).toArray(Iterator[]::new);\n+\n+        createParquetFile(\n+                output,\n+                getStandardStructObjectInspector(columnNames, inspectors),\n+                values,\n+                schema,\n+                Collections.singletonList(\"record\"));\n+    }\n+\n+    private static Properties createTableProperties(List<String> columnNames, List<ObjectInspector> objectInspectors)\n+    {\n+        Properties orderTableProperties = new Properties();\n+        orderTableProperties.setProperty(\"columns\", Joiner.on(',').join(columnNames));\n+        orderTableProperties.setProperty(\"columns.types\", Joiner.on(',').join(transform(objectInspectors, ObjectInspector::getTypeName)));\n+        return orderTableProperties;\n+    }\n+\n+    public static class ParquetDecimalInsert\n+    {\n+        private final String columnName;\n+        private final boolean forceFixedLengthArray;\n+        private final int precision;\n+        private final int scale;\n+        private final List<String> values;\n+\n+        public ParquetDecimalInsert(String columnName, boolean forceFixedLengthArray, int precision, int scale, List<String> values)\n+        {\n+            this.columnName = columnName;\n+            this.forceFixedLengthArray = forceFixedLengthArray;\n+            this.precision = precision;\n+            this.scale = scale;\n+            this.values = values;\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n+\n+        public String parquetStorage()\n+        {\n+            if (!forceFixedLengthArray && precision > 0 && precision < 10) {\n+                return \"INT32\";\n+            }\n+\n+            if (!forceFixedLengthArray && precision >= 10 && precision < 18) {\n+                return \"INT64\";\n+            }\n+\n+            if (precision > 38 || precision < 0) {\n+                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n+            }\n+\n+            return format(\"fixed_len_byte_array(%d)\", ParquetHiveSerDe.PRECISION_TO_BYTE_COUNT[precision - 1]);\n+        }\n+\n+        public ObjectInspector getParquetObjectInspector()\n+        {\n+            if (!forceFixedLengthArray && precision > 0 && precision < 10) {\n+                return javaIntObjectInspector;\n+            }\n+\n+            if (!forceFixedLengthArray && precision >= 10 && precision < 18) {\n+                return javaLongObjectInspector;\n+            }\n+\n+            if (precision > 38 || precision < 0) {\n+                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDkyOQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382700929", "bodyText": "private", "author": "findepi", "createdAt": "2020-02-21T17:09:17Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(\"hive.parquet.use-column-names\", \"true\"),\n+                Optional.of(basePath));\n+    }\n+\n+    private static java.nio.file.Path getBasePath()\n+    {\n+        try {\n+            return Files.createTempDirectory(\"parquet\");\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private Path getParquetWritePath(String tableName)\n+    {\n+        return new Path(basePath.toString(), format(\"hive_data/tpch/%s/%s\", tableName, UUID.randomUUID().toString()));\n+    }\n+\n+    private static void createParquetFile(\n+            Path path,\n+            StandardStructObjectInspector inspector,\n+            Iterator<?>[] iterators,\n+            MessageType parquetSchema,\n+            List<String> columnNames)\n+    {\n+        Properties tableProperties = createTableProperties(columnNames, Collections.singletonList(inspector));\n+\n+        JobConf jobConf = new JobConf();\n+        jobConf.setEnum(COMPRESSION, UNCOMPRESSED);\n+        jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+        jobConf.setEnum(WRITER_VERSION, PARQUET_2_0);\n+\n+        try {\n+            FileSinkOperator.RecordWriter recordWriter = new TestMapredParquetOutputFormat(Optional.of(parquetSchema), true)\n+                    .getHiveRecordWriter(\n+                            jobConf,\n+                            path,\n+                            Text.class,\n+                            false,\n+                            tableProperties,\n+                            () -> {});\n+\n+            Object row = inspector.create();\n+            List<StructField> fields = ImmutableList.copyOf(inspector.getAllStructFieldRefs());\n+\n+            while (stream(iterators).allMatch(Iterator::hasNext)) {\n+                for (int i = 0; i < fields.size(); i++) {\n+                    Object value = iterators[i].next();\n+                    inspector.setStructFieldData(row, fields.get(i), value);\n+                }\n+\n+                ParquetHiveSerDe serde = new ParquetHiveSerDe();\n+                serde.initialize(jobConf, tableProperties, null);\n+                Writable record = serde.serialize(row, inspector);\n+                recordWriter.write(record);\n+            }\n+\n+            recordWriter.close(false);\n+        }\n+        catch (IOException | SerDeException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void writeParquetDecimalsRecord(Path output, List<ParquetDecimalInsert> inserts)\n+    {\n+        List<String> fields = inserts.stream().map(ParquetDecimalInsert::schemaFieldDeclaration).collect(toImmutableList());\n+        MessageType schema = parseMessageType(format(\"message record { %s; }\", Joiner.on(\"; \").join(fields)));\n+        List<ObjectInspector> inspectors = inserts.stream().map(ParquetDecimalInsert::getParquetObjectInspector).collect(toImmutableList());\n+        List<String> columnNames = inserts.stream().map(ParquetDecimalInsert::getColumnName).collect(toImmutableList());\n+        Iterator<?>[] values = inserts.stream().map(ParquetDecimalInsert::getValues).map(Iterable::iterator).toArray(Iterator[]::new);\n+\n+        createParquetFile(\n+                output,\n+                getStandardStructObjectInspector(columnNames, inspectors),\n+                values,\n+                schema,\n+                Collections.singletonList(\"record\"));\n+    }\n+\n+    private static Properties createTableProperties(List<String> columnNames, List<ObjectInspector> objectInspectors)\n+    {\n+        Properties orderTableProperties = new Properties();\n+        orderTableProperties.setProperty(\"columns\", Joiner.on(',').join(columnNames));\n+        orderTableProperties.setProperty(\"columns.types\", Joiner.on(',').join(transform(objectInspectors, ObjectInspector::getTypeName)));\n+        return orderTableProperties;\n+    }\n+\n+    public static class ParquetDecimalInsert", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzAxOTk5Mw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r383019993", "bodyText": "importing static methods from it so protected", "author": "wendigo", "createdAt": "2020-02-23T16:46:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMTY5NQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382701695", "bodyText": "nit: we most often place createQueryRunner above the tests", "author": "findepi", "createdAt": "2020-02-21T17:10:51Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetDecimalScalingTest.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.ParquetDecimalScalingTest.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+public class ParquetDecimalScalingTest\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath = getBasePath();\n+\n+    @Test(dataProvider = \"different-precisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+\n+    @DataProvider(name = \"different-precisions\")\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n+                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMTk4Mw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382701983", "bodyText": "why change here?", "author": "findepi", "createdAt": "2020-02-21T17:11:30Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/AbstractTestParquetReader.java", "diffHunk": "@@ -844,7 +845,7 @@ private static long maxPrecision(int numBytes)\n     public void testDecimalBackedByINT32()\n             throws Exception\n     {\n-        for (int precision = 1; precision <= MAX_PRECISION_INT32; precision++) {\n+        for (int precision = 4; precision <= MAX_PRECISION_INT32; precision++) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjkyMTc5MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382921790", "bodyText": "Decimal(3, 0) cannot keep -1000..1000 ints", "author": "wendigo", "createdAt": "2020-02-22T15:50:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMTk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5NTc2MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384395760", "bodyText": "ok, make it separate commit", "author": "findepi", "createdAt": "2020-02-26T10:14:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMTk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjAwMw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382702003", "bodyText": "why change here?", "author": "findepi", "createdAt": "2020-02-21T17:11:33Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/AbstractTestParquetReader.java", "diffHunk": "@@ -861,7 +862,7 @@ public void testDecimalBackedByINT32()\n     public void testDecimalBackedByINT64()\n             throws Exception\n     {\n-        for (int precision = 1; precision <= MAX_PRECISION_INT64; precision++) {\n+        for (int precision = 4; precision <= MAX_PRECISION_INT64; precision++) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjkyMTg0Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382921847", "bodyText": "Decimal(3, 0) cannot keep -1000..1000 ints", "author": "wendigo", "createdAt": "2020-02-22T15:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjAwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5NTg0OA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384395848", "bodyText": "ok, make it separate commit", "author": "findepi", "createdAt": "2020-02-26T10:14:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjU5Mw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382702593", "bodyText": "the change here is more that removing assertThatThrownBy. Why?", "author": "findepi", "createdAt": "2020-02-21T17:12:53Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/AbstractTestParquetReader.java", "diffHunk": "@@ -876,12 +877,10 @@ public void testDecimalBackedByINT64()\n \n     @Test\n     public void testParquetShortDecimalWriteToPrestoDecimalWithNonMatchingScale()\n+            throws Exception\n     {\n-        assertThatThrownBy(() -> {\n-            MessageType parquetSchema = parseMessageType(format(\"message hive_decimal { optional INT64 test (DECIMAL(%d, %d)); }\", 10, 1));\n-            tester.testRoundTrip(javaLongObjectInspector, ImmutableList.of(1L), ImmutableList.of(1L), createDecimalType(10, 2), Optional.of(parquetSchema));\n-        }).hasMessage(\"Presto decimal column type has different scale (2) than Parquet decimal column (1)\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjkyMTkzNg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382921936", "bodyText": "Because now values are scaled (in the test the readValues were dummy) so we are actually testing a round-trip of read-write valeus", "author": "wendigo", "createdAt": "2020-02-22T15:52:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjU5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjkzNw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382702937", "bodyText": "the change here is more that removing assertThatThrownBy. Why?", "author": "findepi", "createdAt": "2020-02-21T17:13:37Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/AbstractTestParquetReader.java", "diffHunk": "@@ -910,15 +909,13 @@ public void testDecimalBackedByFixedLenByteArray()\n \n     @Test\n     public void testParquetLongDecimalWriteToPrestoDecimalWithNonMatchingScale()\n+            throws Exception\n     {\n-        assertThatThrownBy(() ->\n-                tester.testRoundTrip(\n-                        new JavaHiveDecimalObjectInspector(new DecimalTypeInfo(38, 10)),\n-                        ImmutableList.of(HiveDecimal.create(0)),\n-                        ImmutableList.of(new SqlDecimal(BigInteger.ZERO, 38, 10)),\n-                        createDecimalType(38, 9)))\n-                .hasMessage(\"Presto decimal column type has different scale (9) than Parquet decimal column (10)\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjkyMjAwNA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382922004", "bodyText": "Serializing 0 in every scale/precision yields zero so I've changed to values that actually got rescaled", "author": "wendigo", "createdAt": "2020-02-22T15:53:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMjkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMzA1NQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382703055", "bodyText": "why change here?", "author": "findepi", "createdAt": "2020-02-21T17:13:50Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/AbstractTestParquetReader.java", "diffHunk": "@@ -970,7 +967,7 @@ public void testParquetShortDecimalWriteToPrestoIntegerBlock()\n     public void testParquetShortDecimalWriteToPrestoBigintBlock()\n             throws Exception\n     {\n-        for (int precision = 1; precision <= MAX_PRECISION_INT64; precision++) {\n+        for (int precision = 4; precision <= MAX_PRECISION_INT64; precision++) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5Njg0Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384396847", "bodyText": "make it separate commit", "author": "findepi", "createdAt": "2020-02-26T10:16:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMzA1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMzIyMA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382703220", "bodyText": "redundant", "author": "findepi", "createdAt": "2020-02-21T17:14:10Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -64,10 +66,11 @@\n import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n \n-public class ParquetDecimalScalingTest\n+@Test", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMzg5Nw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382703897", "bodyText": "Would be good to add a javadoc to each method explaining what test covers, perhaps along with an example. Terms \"rescaled\", \"rounded\", \"roundable\" are not obvious.", "author": "findepi", "createdAt": "2020-02-21T17:15:37Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -83,13 +86,55 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         dropTable(tableName);\n     }\n \n+    @Test(dataProvider = \"different-rescalings\")\n+    public void testReadingRescaledDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"roundable-decimals\")\n+    public void testReadingRoundedDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNDA0OA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382704048", "bodyText": "belongs to prev commit", "author": "findepi", "createdAt": "2020-02-21T17:15:55Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -83,13 +86,55 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         dropTable(tableName);\n     }\n \n+    @Test(dataProvider = \"different-rescalings\")\n+    public void testReadingRescaledDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"roundable-decimals\")\n+    public void testReadingRoundedDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertRoundedValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"non-rescalable-decimals\")\n+    public void testReadingNonRescalableDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertQueryFails(format(\"SELECT * FROM %s\", tableName), format(\"Cannot cast DECIMAL\\\\(%d, %d\\\\) '.*' to DECIMAL\\\\(%d, %d\\\\)\", precision, scale, schemaPrecision, schemaScale));\n+\n+        dropTable(tableName);\n+    }\n \n     @DataProvider(name = \"different-precisions\")\n     public Object[][] differentPrecisions()\n     {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, expectedValues", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNDA3OA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382704078", "bodyText": "belongs to prev commit", "author": "findepi", "createdAt": "2020-02-21T17:16:00Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -83,13 +86,55 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         dropTable(tableName);\n     }\n \n+    @Test(dataProvider = \"different-rescalings\")\n+    public void testReadingRescaledDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"roundable-decimals\")\n+    public void testReadingRoundedDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale, List<String> expected)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertRoundedValues(tableName, schemaScale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @Test(dataProvider = \"non-rescalable-decimals\")\n+    public void testReadingNonRescalableDecimals(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, int schemaPrecision, int schemaScale)\n+    {\n+        createTable(tableName, schemaPrecision, schemaScale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertQueryFails(format(\"SELECT * FROM %s\", tableName), format(\"Cannot cast DECIMAL\\\\(%d, %d\\\\) '.*' to DECIMAL\\\\(%d, %d\\\\)\", precision, scale, schemaPrecision, schemaScale));\n+\n+        dropTable(tableName);\n+    }\n \n     @DataProvider(name = \"different-precisions\")\n     public Object[][] differentPrecisions()\n     {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, expectedValues\n         return new Object[][] {\n-                {\"decimal_10_2\", false, 10, 2, ImmutableList.of(\n-                        \"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                {\"decimal_10_2\", false, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNDU3OA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382704578", "bodyText": "belongs to prev commit; however, i suggested to change thus to computeActual anyway", "author": "findepi", "createdAt": "2020-02-21T17:17:04Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -127,7 +267,13 @@ protected void dropTable(String tableName)\n \n     protected void assertValues(String tableName, int scale, List<String> expected)\n     {\n-        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", s)).collect(toImmutableList()));\n+        String formattedValues = Joiner.on(\", \").join(expected.stream().map(s -> format(\"'%s'\", new BigDecimal(s).setScale(scale).toString())).collect(toImmutableList()));\n+        assertQuery(format(\"select format('%%.%df', value) FROM %s\", scale, tableName), \"VALUES \" + formattedValues);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNDYwMA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382704600", "bodyText": "belongs to prev commit", "author": "findepi", "createdAt": "2020-02-21T17:17:07Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -149,7 +295,7 @@ protected QueryRunner createQueryRunner()\n             return Files.createTempDirectory(\"parquet\");\n         }\n         catch (IOException e) {\n-            throw new RuntimeException(e);\n+            throw new UncheckedIOException(e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNDY3NA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382704674", "bodyText": "belongs to prev commit", "author": "findepi", "createdAt": "2020-02-21T17:17:19Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -310,19 +456,21 @@ public static String minimumValue(int precision, int scale)\n \n         private Object convertValue(String value)\n         {\n+            BigDecimal bigValue = new BigDecimal(value).setScale(scale);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTQyNQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382715425", "bodyText": "This should not happen (bug in the code), so now need to PrestoException.\nthrow new IllegalArgumentException(\"Unsupported type: \" + type);", "author": "findepi", "createdAt": "2020-02-21T17:40:36Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale(),\n+                        rescale,\n+                        rescale / 2);\n+\n+                prestoType.writeLong(blockBuilder, scaledValue);\n+            }\n+            else if (isLongDecimal(prestoDecimalType)) {\n+                prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else if (isNumberType(prestoType)) {\n+                if (isInValidNumberRange(prestoType, value)) {\n+                    prestoType.writeLong(blockBuilder, value);\n+                }\n+                else {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Could not coerce from %s to %s\", parquetDecimalType, prestoType));\n+                }\n             }\n         }\n         else if (isValueNull()) {\n             blockBuilder.appendNull();\n         }\n     }\n \n-    private void validateDecimal(DecimalType prestoType)\n+    protected boolean isNumberType(Type type)\n+    {\n+        return type.equals(TINYINT) || type.equals(SMALLINT) || type.equals(INTEGER) || type.equals(BIGINT);\n+    }\n+\n+    protected boolean isInValidNumberRange(Type type, long value)\n     {\n-        if (prestoType.getScale() != parquetDecimalType.getScale()) {\n-            throw new ParquetDecodingException(format(\n-                    \"Presto decimal column type has different scale (%s) than Parquet decimal column (%s)\",\n-                    prestoType.getScale(),\n-                    parquetDecimalType.getScale()));\n+        long minValue;\n+        long maxValue;\n+\n+        if (type.equals(TINYINT)) {\n+            minValue = Byte.MIN_VALUE;\n+            maxValue = Byte.MAX_VALUE;\n+        }\n+        else if (type.equals(SMALLINT)) {\n+            minValue = Short.MIN_VALUE;\n+            maxValue = Short.MAX_VALUE;\n+        }\n+        else if (type.equals(INTEGER)) {\n+            minValue = Integer.MIN_VALUE;\n+            maxValue = Integer.MAX_VALUE;\n         }\n+        else if (type.equals(BIGINT)) {\n+            minValue = Long.MIN_VALUE;\n+            maxValue = Long.MAX_VALUE;\n+        }\n+        else {\n+            throw new PrestoException(NOT_SUPPORTED, format(\"Could not coerce from %s to %s\", parquetDecimalType, type));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQwMTYyOA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384401628", "bodyText": "bump", "author": "findepi", "createdAt": "2020-02-26T10:25:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTYwNA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382715604", "bodyText": "nit: i'd inline these variables", "author": "findepi", "createdAt": "2020-02-21T17:40:56Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale(),\n+                        rescale,\n+                        rescale / 2);\n+\n+                prestoType.writeLong(blockBuilder, scaledValue);\n+            }\n+            else if (isLongDecimal(prestoDecimalType)) {\n+                prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else if (isNumberType(prestoType)) {\n+                if (isInValidNumberRange(prestoType, value)) {\n+                    prestoType.writeLong(blockBuilder, value);\n+                }\n+                else {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Could not coerce from %s to %s\", parquetDecimalType, prestoType));\n+                }\n             }\n         }\n         else if (isValueNull()) {\n             blockBuilder.appendNull();\n         }\n     }\n \n-    private void validateDecimal(DecimalType prestoType)\n+    protected boolean isNumberType(Type type)\n+    {\n+        return type.equals(TINYINT) || type.equals(SMALLINT) || type.equals(INTEGER) || type.equals(BIGINT);\n+    }\n+\n+    protected boolean isInValidNumberRange(Type type, long value)\n     {\n-        if (prestoType.getScale() != parquetDecimalType.getScale()) {\n-            throw new ParquetDecodingException(format(\n-                    \"Presto decimal column type has different scale (%s) than Parquet decimal column (%s)\",\n-                    prestoType.getScale(),\n-                    parquetDecimalType.getScale()));\n+        long minValue;\n+        long maxValue;\n+\n+        if (type.equals(TINYINT)) {\n+            minValue = Byte.MIN_VALUE;\n+            maxValue = Byte.MAX_VALUE;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQwMTcyOQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384401729", "bodyText": "thoughts?", "author": "findepi", "createdAt": "2020-02-26T10:25:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQ5MDU5Ng==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384490596", "bodyText": "Consider it done ;)", "author": "wendigo", "createdAt": "2020-02-26T13:28:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTk5OA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382715998", "bodyText": "private (here & below)\nREAL, DOUBLE and DECIMAL are also number types", "author": "findepi", "createdAt": "2020-02-21T17:41:49Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale(),\n+                        rescale,\n+                        rescale / 2);\n+\n+                prestoType.writeLong(blockBuilder, scaledValue);\n+            }\n+            else if (isLongDecimal(prestoDecimalType)) {\n+                prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else if (isNumberType(prestoType)) {\n+                if (isInValidNumberRange(prestoType, value)) {\n+                    prestoType.writeLong(blockBuilder, value);\n+                }\n+                else {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Could not coerce from %s to %s\", parquetDecimalType, prestoType));\n+                }\n             }\n         }\n         else if (isValueNull()) {\n             blockBuilder.appendNull();\n         }\n     }\n \n-    private void validateDecimal(DecimalType prestoType)\n+    protected boolean isNumberType(Type type)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzAzMTA1MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r383031050", "bodyText": "protected", "author": "wendigo", "createdAt": "2020-02-23T19:22:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNTk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNjUyMg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382716522", "bodyText": "no need for if here, since you already validate it's either decimal or \"number\"\n(otherwise there should be else branch that throws something)", "author": "findepi", "createdAt": "2020-02-21T17:42:57Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale(),\n+                        rescale,\n+                        rescale / 2);\n+\n+                prestoType.writeLong(blockBuilder, scaledValue);\n+            }\n+            else if (isLongDecimal(prestoDecimalType)) {\n+                prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else if (isNumberType(prestoType)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNzMyMw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382717323", "bodyText": "move into if branch where it's used", "author": "findepi", "createdAt": "2020-02-21T17:44:42Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNzczNw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382717737", "bodyText": "\"scaledValue\" is a wrong name, because it's \"unscaled value in presto decimal type\", right?\n\"convertedValue\"?", "author": "findepi", "createdAt": "2020-02-21T17:45:35Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxODQyNg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382718426", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            if (isInValidNumberRange(prestoType, value)) {\n          \n          \n            \n                            verify(parquetDecimalType.getScale() == 0);\n          \n          \n            \n                            if (isInValidNumberRange(prestoType, value)) {", "author": "findepi", "createdAt": "2020-02-21T17:47:08Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);\n+\n                 if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n-                            parquetDecimalType.getScale(),\n-                            prestoType));\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n-            else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+\n+            long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+\n+            if (isShortDecimal(prestoDecimalType)) {\n+                long scaledValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale(),\n+                        rescale,\n+                        rescale / 2);\n+\n+                prestoType.writeLong(blockBuilder, scaledValue);\n+            }\n+            else if (isLongDecimal(prestoDecimalType)) {\n+                prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else if (isNumberType(prestoType)) {\n+                if (isInValidNumberRange(prestoType, value)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxOTIyNg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r382719226", "bodyText": "I don't like fake information, because this is error prone.\nFortunately, the result of this assignment is never used.\nYou can restructure the code slightly and remove the fake.", "author": "findepi", "createdAt": "2020-02-21T17:48:55Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,104 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isNumberType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n+            DecimalType prestoDecimalType;\n+\n+            if (prestoType instanceof DecimalType) {\n+                prestoDecimalType = (DecimalType) prestoType;\n             }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n+            else {\n+                // for numeric types convert decimal to unscaled long\n+                prestoDecimalType = DecimalType.createDecimalType(MAX_SHORT_PRECISION, 0);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4MTI2Mw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384381263", "bodyText": "Remove empty @param lines, they don't add any value\nhere & below", "author": "findepi", "createdAt": "2020-02-26T09:49:55Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4MTM2OQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384381369", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n          \n          \n            \n                 * if Parquet decimal value could be rescaled into Presto decimal without losing most and least significant digits.", "author": "findepi", "createdAt": "2020-02-26T09:50:06Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4NDc1NA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384384754", "bodyText": "String tableName arg is unnecessary, because each test can use a random (or string.format-created) table name.\nThis serves the purpose of identifying tests runs though. It will be satisfied, if you move int precision, int scale as initial args, then forceFixedLengthArray.", "author": "findepi", "createdAt": "2020-02-26T09:55:41Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected\n+     */\n+    @Test(dataProvider = \"differentPrecisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4NTgzMg==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384385832", "bodyText": "This isn't going to be reusable, because this generates the test cases matching exactly testReadingMatchingPrecision test specification, so\n\nmove right under the test method\nconsider naming it testReadingMatchingPrecisionDataProvider", "author": "findepi", "createdAt": "2020-02-26T09:57:24Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected\n+     */\n+    @Test(dataProvider = \"differentPrecisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentPrecisions()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4NjU0MA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384386540", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n          \n          \n            \n                    assertUpdate(format(\"CREATE TABLE %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));", "author": "findepi", "createdAt": "2020-02-26T09:58:33Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected\n+     */\n+    @Test(dataProvider = \"differentPrecisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4NjczNA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384386734", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertUpdate(format(\"drop table %s\", tableName));\n          \n          \n            \n                    assertUpdate(\"DROP TABLE \" + tableName);", "author": "findepi", "createdAt": "2020-02-26T09:58:52Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected\n+     */\n+    @Test(dataProvider = \"differentPrecisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM4NzkyMw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384387923", "bodyText": "collect to list + .toArray = .toArray(BigDecimal[]::new)", "author": "findepi", "createdAt": "2020-02-26T10:00:48Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.plugin.hive.HiveQueryRunner;\n+import io.prestosql.plugin.hive.parquet.write.TestMapredParquetOutputFormat;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.Iterables.transform;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.maximumValue;\n+import static io.prestosql.plugin.hive.parquet.TestParquetDecimalScaling.ParquetDecimalInsert.minimumValue;\n+import static java.lang.String.format;\n+import static java.math.RoundingMode.UNNECESSARY;\n+import static java.util.Arrays.stream;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_2_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestParquetDecimalScaling\n+        extends AbstractTestQueryFramework\n+{\n+    private java.nio.file.Path basePath;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        basePath = getBasePath();\n+\n+        return HiveQueryRunner.createQueryRunner(\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                \"sql-standard\",\n+                ImmutableMap.of(),\n+                Optional.of(basePath));\n+    }\n+\n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param expected\n+     */\n+    @Test(dataProvider = \"differentPrecisions\")\n+    public void testReadingMatchingPrecision(String tableName, boolean forceFixedLengthArray, int precision, int scale, List<String> values, List<String> expected)\n+    {\n+        createTable(tableName, precision, scale);\n+\n+        writeParquetDecimalsRecord(\n+                getParquetWritePath(tableName),\n+                ImmutableList.of(new ParquetDecimalInsert(\"value\", forceFixedLengthArray, precision, scale, values)));\n+\n+        assertValues(tableName, scale, expected);\n+\n+        dropTable(tableName);\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentPrecisions()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2\", false, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle\", true, 10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2\", false, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle\", true, 4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2\", false, 14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3\", false, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle\", true, 6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_38_4\", false, 38, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(38, 4), minimumValue(38, 4)),\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(38, 4), minimumValue(38, 4))}\n+        };\n+    }\n+\n+    protected void createTable(String tableName, int precision, int scale)\n+    {\n+        assertUpdate(format(\"create table %s (value decimal(%d, %d)) WITH (format = 'PARQUET')\", tableName, precision, scale));\n+    }\n+\n+    protected void dropTable(String tableName)\n+    {\n+        assertUpdate(format(\"drop table %s\", tableName));\n+    }\n+\n+    protected void assertValues(String tableName, int scale, List<String> expected)\n+    {\n+        MaterializedResult materializedRows = computeActual(format(\"select value FROM %s\", tableName));\n+\n+        List<BigDecimal> actualValues = materializedRows.getMaterializedRows().stream()\n+                .map(row -> row.getField(0))\n+                .map(BigDecimal.class::cast)\n+                .collect(toImmutableList());\n+\n+        BigDecimal[] expectedValues = expected.stream()\n+                .map(value -> new BigDecimal(value).setScale(scale, UNNECESSARY))\n+                .collect(toImmutableList())\n+                .toArray(new BigDecimal[0]);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5NzA1MQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384397051", "bodyText": "remove", "author": "findepi", "createdAt": "2020-02-26T10:16:56Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -112,6 +113,86 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         dropTable(tableName);\n     }\n \n+    /**\n+     * Tests if Parquet decimal with given precision and scale can be read into Presto decimal with different precision and scale\n+     * if Parquet decimal value could be rescaled into Presto decimal without loosing most and least significant digits.\n+     *\n+     * @param tableName\n+     * @param forceFixedLengthArray\n+     * @param precision\n+     * @param scale\n+     * @param values\n+     * @param schemaPrecision\n+     * @param schemaScale\n+     * @param expected", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5NzQyOQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384397429", "bodyText": "what does \"fle\" stand for?", "author": "findepi", "createdAt": "2020-02-26T10:17:44Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -143,6 +224,142 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         };\n     }\n \n+    @DataProvider\n+    public Object[][] differentFittingRescaling()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale, expectedValues\n+        return new Object[][] {\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        13, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.0000\", \"1.23000\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_rescale2\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 2,\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        11, 3,\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle_rescale\", true,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2_rescale\", false,\n+                        14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        20, 3,\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3_rescale\", false,\n+                        6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6,\n+                        ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle_rescale\", true,\n+                        6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6,\n+                        ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_10_2_rescale_max\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        38, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_18_4_rescale_max\", false,\n+                        18, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(18, 4), minimumValue(18, 4)),\n+                        38, 14,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(18, 4), minimumValue(18, 4))}\n+        };\n+    }\n+\n+    @DataProvider\n+    public Object[][] roundableDecimals()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2_round\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_9_2_fle_round\", true,\n+                        9, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2))},\n+                {\"decimal_4_2_round\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        7, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_round2\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))}\n+        };\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentUnscalable()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale\n+        return new Object[][] {\n+                {\"decimal_4_2_rescale_fail\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        4, 3},\n+                {\"decimal_10_2_rescale_fail\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        10, 3},\n+                {\"decimal_10_2_rescale_fail2\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        3, 2},\n+                {\"decimal_10_2_fle_rescale_fail\", true,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQ2NTMzOQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384465339", "bodyText": "fixed length encoding", "author": "wendigo", "createdAt": "2020-02-26T12:38:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5NzQyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5ODMxOQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384398319", "bodyText": "Why is this failure? is it because maximumValue(10, 2) doesn't fit? Then remove other test values, they are misleading. Maybe also use explicit value \"99999999.99\" instead if maximumValue, to make things more explicit.\nSame for other negative test cases.", "author": "findepi", "createdAt": "2020-02-26T10:19:22Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -143,6 +224,142 @@ public void testReadingMatchingPrecision(String tableName, boolean forceFixedLen\n         };\n     }\n \n+    @DataProvider\n+    public Object[][] differentFittingRescaling()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale, expectedValues\n+        return new Object[][] {\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        13, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.0000\", \"1.23000\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_rescale2\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        6, 2,\n+                        ImmutableList.of(\"10.01\", \"10.00\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_rescale\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        11, 3,\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_10_2_fle_rescale\", true,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_4_2_rescale\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_4_2_fle_rescale\", true,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        10, 5,\n+                        ImmutableList.of(\"10.01000\", \"10.00000\", \"1.23000\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_14_2_rescale\", false,\n+                        14, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(14, 2), minimumValue(14, 2)),\n+                        20, 3,\n+                        ImmutableList.of(\"10.010\", \"10.000\", \"1.230\", maximumValue(14, 2), minimumValue(14, 2))},\n+                {\"decimal_6_3_rescale\", false,\n+                        6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6,\n+                        ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_6_3_fle_rescale\", true,\n+                        6, 3,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(6, 3), minimumValue(6, 3)),\n+                        9, 6,\n+                        ImmutableList.of(\"10.010000\", \"10.000000\", \"1.230000\", maximumValue(6, 3), minimumValue(6, 3))},\n+                {\"decimal_10_2_rescale_max\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        38, 4,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_18_4_rescale_max\", false,\n+                        18, 4,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(18, 4), minimumValue(18, 4)),\n+                        38, 14,\n+                        ImmutableList.of(\"10.0100\", \"10.0000\", \"1.2300\", maximumValue(18, 4), minimumValue(18, 4))}\n+        };\n+    }\n+\n+    @DataProvider\n+    public Object[][] roundableDecimals()\n+    {\n+        return new Object[][] {\n+                {\"decimal_10_2_round\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))},\n+                {\"decimal_9_2_fle_round\", true,\n+                        9, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(9, 2), minimumValue(9, 2))},\n+                {\"decimal_4_2_round\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        7, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2))},\n+                {\"decimal_10_2_round2\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        12, 1,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2))}\n+        };\n+    }\n+\n+    @DataProvider\n+    public Object[][] differentUnscalable()\n+    {\n+        // tableName, useFixedLengthArray, parquetPrecision, parquetScale, writeValues, schemaPrecision, schemaScale\n+        return new Object[][] {\n+                {\"decimal_4_2_rescale_fail\", false,\n+                        4, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(4, 2), minimumValue(4, 2)),\n+                        4, 3},\n+                {\"decimal_10_2_rescale_fail\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        10, 3},\n+                {\"decimal_10_2_rescale_fail2\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        3, 2},\n+                {\"decimal_10_2_fle_rescale_fail\", true,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        14, 7},\n+                {\"decimal_10_2_rescale_fail3\", false,\n+                        10, 2,\n+                        ImmutableList.of(\"10.01\", \"10\", \"1.23\", maximumValue(10, 2), minimumValue(10, 2)),\n+                        10, 4},", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5ODUxNw==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384398517", "bodyText": "collect + toArray = toArray", "author": "findepi", "createdAt": "2020-02-26T10:19:45Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -170,6 +387,23 @@ protected void assertValues(String tableName, int scale, List<String> expected)\n         assertThat(actualValues).containsExactlyInAnyOrder(expectedValues);\n     }\n \n+    protected void assertRoundedValues(String tableName, int scale, List<String> expected)\n+    {\n+        MaterializedResult materializedRows = computeActual(format(\"select value FROM %s\", tableName));\n+\n+        List<BigDecimal> actualValues = materializedRows.getMaterializedRows().stream()\n+                .map(row -> row.getField(0))\n+                .map(BigDecimal.class::cast)\n+                .collect(toImmutableList());\n+\n+        BigDecimal[] expectedValues = expected.stream()\n+                .map(value -> new BigDecimal(value).setScale(scale, RoundingMode.HALF_UP))\n+                .collect(toImmutableList())\n+                .toArray(new BigDecimal[0]);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5ODU5NA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384398594", "bodyText": "prev commit", "author": "findepi", "createdAt": "2020-02-26T10:19:54Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -288,7 +522,7 @@ public String parquetStorage()\n             }\n \n             if (precision > 38 || precision < 0) {\n-                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n+                throw new IllegalArgumentException(\"Scale cannot be greater than 38 or less than 0\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM5ODYyNA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384398624", "bodyText": "prev commit", "author": "findepi", "createdAt": "2020-02-26T10:19:58Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestParquetDecimalScaling.java", "diffHunk": "@@ -305,7 +539,7 @@ public ObjectInspector getParquetObjectInspector()\n             }\n \n             if (precision > 38 || precision < 0) {\n-                throw new IllegalArgumentException(\"Scale could not be greater than 38 or less than 0\");\n+                throw new IllegalArgumentException(\"Scale cannot be greater than 38 or less than 0\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQwMDU3NQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384400575", "bodyText": "In longToLongCast, add:\nif (sourcePrecision == resultPrecision && sourceScale == resultScale) {\n    return value;\n}\n\nthis method doesn't cleanly shortcircuit when it's a no-op.", "author": "findepi", "createdAt": "2020-02-26T10:23:32Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/LongDecimalColumnReader.java", "diffHunk": "@@ -41,20 +43,32 @@\n     @Override\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n-        if (!isLongDecimal(prestoType)) {\n+        if (!(prestoType instanceof DecimalType)) {\n             throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n         }\n+\n         DecimalType prestoDecimalType = (DecimalType) prestoType;\n-        if (prestoDecimalType.getScale() != parquetDecimalType.getScale()) {\n-            throw new ParquetDecodingException(format(\n-                    \"Presto decimal column type has different scale (%s) than Parquet decimal column (%s)\",\n-                    prestoDecimalType.getScale(),\n-                    parquetDecimalType.getScale()));\n-        }\n \n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            Binary value = valuesReader.readBytes();\n-            prestoType.writeSlice(blockBuilder, Decimals.encodeUnscaledValue(new BigInteger(value.getBytes())));\n+            Binary binary = valuesReader.readBytes();\n+            Slice value = Decimals.encodeUnscaledValue(new BigInteger(binary.getBytes()));\n+\n+            if (prestoDecimalType.isShort()) {\n+                prestoType.writeLong(blockBuilder, longToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        prestoDecimalType.getPrecision(),\n+                        prestoDecimalType.getScale()));\n+            }\n+            else {\n+                prestoType.writeSlice(blockBuilder, longToLongCast(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e855a6efcf98bebeb58ac27fd7dc77b904efe0bc", "url": "https://github.com/trinodb/trino/commit/e855a6efcf98bebeb58ac27fd7dc77b904efe0bc", "message": "Fix decimal precision so it can represent test values", "committedDate": "2020-02-26T13:35:05Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0MjYwMA==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384542600", "bodyText": "redundant else", "author": "findepi", "createdAt": "2020-02-26T14:52:54Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,97 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isIntegerType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n-            }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n-                if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n+            if (prestoType instanceof DecimalType) {\n+                DecimalType prestoDecimalType = (DecimalType) prestoType;\n+\n+                if (isShortDecimal(prestoDecimalType)) {\n+                    long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+                    long convertedValue = shortToShortCast(\n+                            value,\n+                            parquetDecimalType.getPrecision(),\n+                            parquetDecimalType.getScale(),\n+                            prestoDecimalType.getPrecision(),\n+                            prestoDecimalType.getScale(),\n+                            rescale,\n+                            rescale / 2);\n+\n+                    prestoType.writeLong(blockBuilder, convertedValue);\n+                }\n+                else if (isLongDecimal(prestoDecimalType)) {\n+                    prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                            value,\n+                            parquetDecimalType.getPrecision(),\n                             parquetDecimalType.getScale(),\n-                            prestoType));\n+                            prestoDecimalType.getPrecision(),\n+                            prestoDecimalType.getScale()));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n             else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+                if (parquetDecimalType.getScale() != 0) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+                }\n+\n+                long rescale = longTenToNth(parquetDecimalType.getScale());\n+                long convertedValue = shortToShortCast(\n+                        value,\n+                        parquetDecimalType.getPrecision(),\n+                        parquetDecimalType.getScale(),\n+                        MAX_SHORT_PRECISION,\n+                        0,\n+                        rescale,\n+                        rescale / 2);\n+\n+                if (isInValidNumberRange(prestoType, convertedValue)) {\n+                    prestoType.writeLong(blockBuilder, convertedValue);\n+                }\n+                else {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Could not coerce from %s to %s\", parquetDecimalType, prestoType));\n+                }\n             }\n         }\n         else if (isValueNull()) {\n             blockBuilder.appendNull();\n         }\n     }\n \n-    private void validateDecimal(DecimalType prestoType)\n+    protected boolean isIntegerType(Type type)\n+    {\n+        return type.equals(TINYINT) || type.equals(SMALLINT) || type.equals(INTEGER) || type.equals(BIGINT);\n+    }\n+\n+    protected boolean isInValidNumberRange(Type type, long value)\n     {\n-        if (prestoType.getScale() != parquetDecimalType.getScale()) {\n-            throw new ParquetDecodingException(format(\n-                    \"Presto decimal column type has different scale (%s) than Parquet decimal column (%s)\",\n-                    prestoType.getScale(),\n-                    parquetDecimalType.getScale()));\n+        if (type.equals(TINYINT)) {\n+            return Byte.MIN_VALUE <= value && value <= Byte.MAX_VALUE;\n+        }\n+        else if (type.equals(SMALLINT)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDU0NDk5OQ==", "url": "https://github.com/trinodb/trino/pull/2823#discussion_r384544999", "bodyText": "I realized this shortToShortCast here is a no-op, because we know parquetDecimalType.getScale() == 0", "author": "findepi", "createdAt": "2020-02-26T14:56:24Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ShortDecimalColumnReader.java", "diffHunk": "@@ -47,54 +52,97 @@\n     protected void readValue(BlockBuilder blockBuilder, Type prestoType)\n     {\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n-            long decimalValue;\n+            if (!((prestoType instanceof DecimalType) || isIntegerType(prestoType))) {\n+                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+            }\n+\n+            long value;\n+\n             // When decimals are encoded with primitive types Parquet stores unscaled values\n             if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT32) {\n-                decimalValue = valuesReader.readInteger();\n+                value = valuesReader.readInteger();\n             }\n             else if (columnDescriptor.getPrimitiveType().getPrimitiveTypeName() == INT64) {\n-                decimalValue = valuesReader.readLong();\n+                value = valuesReader.readLong();\n             }\n             else {\n-                decimalValue = getShortDecimalValue(valuesReader.readBytes().getBytes());\n+                value = getShortDecimalValue(valuesReader.readBytes().getBytes());\n             }\n \n-            if (isShortDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeLong(blockBuilder, decimalValue);\n-            }\n-            else if (isLongDecimal(prestoType)) {\n-                validateDecimal((DecimalType) prestoType);\n-                // TODO: validate that value fits Presto decimal precision\n-                prestoType.writeSlice(blockBuilder, unscaledDecimal(decimalValue));\n-            }\n-            else if (prestoType.equals(TINYINT) || prestoType.equals(SMALLINT) || prestoType.equals(INTEGER) || prestoType.equals(BIGINT)) {\n-                if (parquetDecimalType.getScale() != 0) {\n-                    throw new ParquetDecodingException(format(\n-                            \"Parquet decimal column type with non-zero scale (%s) cannot be converted to Presto %s column type\",\n+            if (prestoType instanceof DecimalType) {\n+                DecimalType prestoDecimalType = (DecimalType) prestoType;\n+\n+                if (isShortDecimal(prestoDecimalType)) {\n+                    long rescale = longTenToNth(Math.abs(prestoDecimalType.getScale() - parquetDecimalType.getScale()));\n+                    long convertedValue = shortToShortCast(\n+                            value,\n+                            parquetDecimalType.getPrecision(),\n+                            parquetDecimalType.getScale(),\n+                            prestoDecimalType.getPrecision(),\n+                            prestoDecimalType.getScale(),\n+                            rescale,\n+                            rescale / 2);\n+\n+                    prestoType.writeLong(blockBuilder, convertedValue);\n+                }\n+                else if (isLongDecimal(prestoDecimalType)) {\n+                    prestoType.writeSlice(blockBuilder, shortToLongCast(\n+                            value,\n+                            parquetDecimalType.getPrecision(),\n                             parquetDecimalType.getScale(),\n-                            prestoType));\n+                            prestoDecimalType.getPrecision(),\n+                            prestoDecimalType.getScale()));\n                 }\n-                prestoType.writeLong(blockBuilder, decimalValue);\n             }\n             else {\n-                throw new ParquetDecodingException(format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+                if (parquetDecimalType.getScale() != 0) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Presto column type (%s) for Parquet column (%s)\", prestoType, columnDescriptor));\n+                }\n+\n+                long rescale = longTenToNth(parquetDecimalType.getScale());\n+                long convertedValue = shortToShortCast(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "7a06c922a65cd69e2fac66ada75eef8602f60852", "url": "https://github.com/trinodb/trino/commit/7a06c922a65cd69e2fac66ada75eef8602f60852", "message": "Test reading decimals with different parquet storage primitive", "committedDate": "2020-02-27T13:18:12Z", "type": "commit"}, {"oid": "6c71a147e1ef10008a2aa8e869be8d37ccc26724", "url": "https://github.com/trinodb/trino/commit/6c71a147e1ef10008a2aa8e869be8d37ccc26724", "message": "Short circuit when no conversion happens", "committedDate": "2020-02-27T13:18:12Z", "type": "commit"}, {"oid": "34678faf9863615d620515b4db962e09d42df204", "url": "https://github.com/trinodb/trino/commit/34678faf9863615d620515b4db962e09d42df204", "message": "Allow rescaling parquet decimals", "committedDate": "2020-02-27T13:18:12Z", "type": "commit"}, {"oid": "34678faf9863615d620515b4db962e09d42df204", "url": "https://github.com/trinodb/trino/commit/34678faf9863615d620515b4db962e09d42df204", "message": "Allow rescaling parquet decimals", "committedDate": "2020-02-27T13:18:12Z", "type": "forcePushed"}]}