{"pr_number": 4832, "pr_title": "Iceberg connector support for Materialized Views", "pr_createdAt": "2020-08-14T06:12:55Z", "pr_url": "https://github.com/trinodb/trino/pull/4832", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4NjMyMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r485886321", "bodyText": "This sequence of handling where the insert (including the table scans of source tables is planned), insert performed and then the metadata (snapshots of tables) is captured can lead to metadata and data getting out of sync. e.g. if an insert was performed on the source table in between the insert on target table (say using snapshot S1 of source table) and getting snapshot id (say, returns S2 since an insert happened on the source table), the metadata is fresh (shows S2) and will indicate the materialized view (MV) is fresh while the data in MV is actually stale (has data corresponding to S1). Here is how we can keep the data and metadata in sync.\n\nMake transaction metadata object available to IcebergMetadata\nIn BeginRefreshMaterializedView, get snapshot-ids of all source tables and store them in transaction metadata. (In our problem example, this is S1)\nPopulate TableHandle for each source table using the snapshot-ids from transaction metadata in 'getSplits'. This way, GetTableHandle will return the table as of the given snapshot to planner for planning. (S1)\nFinishRefreshMaterializedView writes the snapshot-ids from transaction metadata as MV metadata (S1) ensuring the data and the metadata correspond to the same snapshot (S1).\n\nNote that the algorithm above does not ensure that the end of refresh the MV has the latest data/metadata. MV may still be stale. Future refresh can update the MV to fresh state.\nAlso note that keeping metadata and data in sync ensures that incremental refresh (especially row-level) would work correctly.", "author": "anjalinorwood", "createdAt": "2020-09-09T19:59:27Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +697,332 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+            appendFiles.commit();\n+        }\n+\n+        // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+        UpdateProperties updateProperties = transaction.updateProperties();\n+        Map<String, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle.toString(), token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle.toString(), (long) -1);\n+            }\n+        }\n+        String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(tableSnapshotIdMap);\n+        updateProperties.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+        updateProperties.commit();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NTIxMw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r485895213", "bodyText": "This code stores the dependsOnTables metadata in the table properties of the Iceberg storage table. This solution works since the data insert and updating the table property is done in a transaction.\nAn alternative way is to store the dependsOnTables metadata as snapshot metadata. The snapshot data and metadata are written at the same time. An additional advantage is that this leaves enough bread-crumbs such that if the materialized view is rolled back to a particular snapshot,  snapshot metadata has information on which snapshots of source tables were used to compute that snapshot of MV. This can be useful for debugging purposes.", "author": "anjalinorwood", "createdAt": "2020-09-09T20:13:54Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +697,332 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+            appendFiles.commit();\n+        }\n+\n+        // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+        UpdateProperties updateProperties = transaction.updateProperties();\n+        Map<String, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle.toString(), token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle.toString(), (long) -1);\n+            }\n+        }\n+        String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(tableSnapshotIdMap);\n+        updateProperties.set(DEPENDS_ON_TABLES, tableSnapshotIDString);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MDYwNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488290606", "bodyText": "Why is this in the SPI? It's not used by anything", "author": "electrum", "createdAt": "2020-09-14T23:20:17Z", "path": "presto-spi/src/main/java/io/prestosql/spi/connector/TableToken.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.connector;\n+\n+public interface TableToken", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk1OTc4NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r492959784", "bodyText": "Currently it is only used by Iceberg connector. When materialized views are supported across connectors, the table token will need to pass through spi. Some discussion notes here: https://docs.google.com/document/d/1MWhiM5N-00YHFCg428wXEIOs91NwT6ppZHngawSGk-w/edit#heading=h.ot6j6jjndqzp\nGiven that design I put it in SPI .. please let me know if I should move to the Iceberg connector.", "author": "anjalinorwood", "createdAt": "2020-09-22T18:49:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MDYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MTY5Nw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488291697", "bodyText": "st_ might be better here. Or perhaps z_ so that they sort to the end?", "author": "electrum", "createdAt": "2020-09-14T23:24:01Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk2MDEzNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r492960137", "bodyText": "I will change it to 'st_'. (stforstorage table`).", "author": "anjalinorwood", "createdAt": "2020-09-22T18:49:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MTY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5Mzg4MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488293880", "bodyText": "Why is replace special? If we're going to store the comment, it seems like we should always do it.\nFor Presto views in Hive, we always use \"Presto View\" since these comments are not useful to non-Presto users. The same logic probably applies to Iceberg. Having a dummy comment in the Hive view will discourage users from touching it or thinking they can change the comment there.", "author": "electrum", "createdAt": "2020-09-14T23:30:42Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzY4OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493003689", "bodyText": "Sorry, Netflix implementation leaked here. I changed the code to always set the comment to \"\"Presto Materialized View\".", "author": "anjalinorwood", "createdAt": "2020-09-22T20:09:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5Mzg4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5NDAzNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488294034", "bodyText": "Use an ImmutableMap builder here.\nNaming nit: we avoid abbreviating variables, so spell out \"properties\". This could be called propertyBuilder", "author": "electrum", "createdAt": "2020-09-14T23:31:12Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5MDAyMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493790021", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-09-23T18:08:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5NDAzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzIxNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303215", "bodyText": "Pass transaction to the create() call so that the field can be final", "author": "electrum", "createdAt": "2020-09-15T00:01:50Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConnector.java", "diffHunk": "@@ -180,9 +179,11 @@ public ConnectorAccessControl getAccessControl()\n     public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel, boolean readOnly)\n     {\n         checkConnectorSupports(READ_COMMITTED, isolationLevel);\n-        ConnectorTransactionHandle transaction = new HiveTransactionHandle();\n+        IcebergTransactionHandle transaction = new IcebergTransactionHandle();\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(getClass().getClassLoader())) {\n-            transactionManager.put(transaction, metadataFactory.create());\n+            IcebergMetadata icebergMetadata = metadataFactory.create();\n+            icebergMetadata.setTransactionHandle(transaction);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTMwNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305306", "bodyText": "Actually, if we move the state into IcebergMetadata, we shouldn't need the handle at all there (see other comment).", "author": "electrum", "createdAt": "2020-09-15T00:09:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzIxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU0Mjk5NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494542994", "bodyText": "Yes, done", "author": "anjalinorwood", "createdAt": "2020-09-24T18:55:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzIxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzUxOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303518", "bodyText": "Drop the \"A\" from the method name to match the common naming convention", "author": "electrum", "createdAt": "2020-09-15T00:02:57Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5MjE0Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494292142", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-09-24T12:53:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzYzNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303634", "bodyText": "As a general note, this can be moved to the field declaration so the field can be final\nprivate final Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();", "author": "electrum", "createdAt": "2020-09-15T00:03:22Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTransactionHandle.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.spi.connector.ConnectorTableHandle;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.UUID;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergTransactionHandle\n+        implements ConnectorTransactionHandle\n+{\n+    private final UUID uuid;\n+    private Map<ConnectorTableHandle, Long> tableSnapshotIdMap; // Used for refresh of a materialized view\n+\n+    public IcebergTransactionHandle()\n+    {\n+        this(UUID.randomUUID());\n+        this.tableSnapshotIdMap = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5MjcyNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494292726", "bodyText": "yes, done.", "author": "anjalinorwood", "createdAt": "2020-09-24T12:54:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTAzOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305038", "bodyText": "The purpose of the transaction handle is so that the engine can associate the beginTransaction(), getMetadata() and commit() calls. However, it's just a handle and shouldn't the store.\nInstead, it's expected that the transaction state is stored inside IcebergMetadata. That's why IcebergConnector creates the metadata object once and stores it in a map using the handle.", "author": "electrum", "createdAt": "2020-09-15T00:08:17Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTransactionHandle.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.spi.connector.ConnectorTableHandle;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.UUID;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergTransactionHandle\n+        implements ConnectorTransactionHandle\n+{\n+    private final UUID uuid;\n+    private Map<ConnectorTableHandle, Long> tableSnapshotIdMap; // Used for refresh of a materialized view", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5Mzk4Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494293982", "bodyText": "Correct. We discussed this offline as well. Removed all changes to stored metadata on transaction handle. Moved the metadata to IcebergMetadata.", "author": "anjalinorwood", "createdAt": "2020-09-24T12:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTQ0NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305445", "bodyText": "I think this should be MaterializedViewAlreadyExistsException", "author": "electrum", "createdAt": "2020-09-15T00:09:52Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5NzEwNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494297107", "bodyText": "I made the change. Just to bring something to your notice, the previous LoC has done metastore.getTable, so tis object may be a table, a view or a materialized view.", "author": "anjalinorwood", "createdAt": "2020-09-24T13:00:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTY0OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305648", "bodyText": "This seems to be missing the \"not\" word in \"clause is specified\"\n// create command where materialized view already exists and IF NOT EXISTS clause is not specified", "author": "electrum", "createdAt": "2020-09-15T00:10:29Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0MzUwMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494643501", "bodyText": "yes, correct, good catch", "author": "anjalinorwood", "createdAt": "2020-09-24T22:23:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzQ3MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488307470", "bodyText": "This logic seems wrong. If ignoreExisting is true, we will still throw MaterializedViewAlreadyExistsException below. Should we return here instead? Something like\nif (!replace && existing.isPresent()) {\n    if (ignoreExisting) {\n        return;\n    }\n    throw new MaterializedViewAlreadyExistsException(viewName);\n}", "author": "electrum", "createdAt": "2020-09-15T00:16:44Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwODIwNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488308206", "bodyText": "How is OR REPLACE supposed to work in combination with IF NOT EXISTS? Why do we have both of these flags?\nWe don't seem to have any tests for IF NOT EXISTS (both with and without OR REPLACE)", "author": "electrum", "createdAt": "2020-09-15T00:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzQ3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI5ODY3MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494298670", "bodyText": "We talked offline. The expected semantics is:\nThe optional 'IF NOT EXISTS' clause causes the error to be suppressed if a materialized view already exists. This clause is ignored if 'OR REPLACE' is specified.\nWe agreed that if both or replace and if not exists is specified, engine should throw an error. I will add a separate commit to this PR for that change. Will also add tests.", "author": "anjalinorwood", "createdAt": "2020-09-24T13:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzY5Nw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488307697", "bodyText": "This can be removed if we handle it above", "author": "electrum", "createdAt": "2020-09-15T00:17:27Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDMwMDc3NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494300774", "bodyText": "yes, done", "author": "anjalinorwood", "createdAt": "2020-09-24T13:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwOTY0MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488309640", "bodyText": "This should use the file format from the table properties\ngetFileFormat(definition.getProperties())", "author": "electrum", "createdAt": "2020-09-15T00:24:10Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDAxNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310017", "bodyText": "Why modify the table properties here at all, rather than just passing them through?", "author": "electrum", "createdAt": "2020-09-15T00:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwOTY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUxNjEwNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494516104", "bodyText": "There are two distinct parts to create materialized view: creation of storage table and creation of view that points to the storage table. I have now restructured this function to show the two parts. Each object that gets created has its own 'table' properties e.g. view has presto_view flag and storage_table name. Storage table does not have those properties. I have now renamed the properties to indicate which object they correspond to.\nThe storage table properties are obtained from definition properties, if file format is not set, it takes the default as follows:\nMap<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n        }", "author": "anjalinorwood", "createdAt": "2020-09-24T18:14:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwOTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDUzMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310531", "bodyText": "This can be chained\n.withStorage(storage ->\n        .setStorageFormat(VIEW_STORAGE_FORMAT)\n        .setLocation(\"\"))", "author": "electrum", "createdAt": "2020-09-15T00:26:59Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjAzOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494476038", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-09-24T17:05:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDUzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDc4NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310784", "bodyText": "If we move the entire Table.Builder part down here, we could directly chain the whole thing\nTable table = Table.builder()\n        ...\n        .build();", "author": "electrum", "createdAt": "2020-09-15T00:27:54Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4NjQzNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494486437", "bodyText": "Yes. The function has now been restructured and takes care of this comment.", "author": "anjalinorwood", "createdAt": "2020-09-24T17:22:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTIxMg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488311212", "bodyText": "We shouldn't catch arbitrary checked exceptions. This should probably just be PrestoException.\nWhy do we want to ignore errors when dropping the storage table? It seems like we should log a warning, at a minimum.\ncatch (PrestoException e) {\n    log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s', storageTableName, viewName);\n}", "author": "electrum", "createdAt": "2020-09-15T00:29:44Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUxODE1Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494518152", "bodyText": "thanks. fixed.", "author": "anjalinorwood", "createdAt": "2020-09-24T18:17:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTk4NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488311985", "bodyText": "This needs to use the correct Path for the file we are going to open. The purpose of HdfsEnvironment.getConfiguration() is to create a Configuration object with the right values for the file system we are going to use. If we don't", "author": "electrum", "createdAt": "2020-09-15T00:32:29Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUyMTU5NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494521594", "bodyText": "Your comment cut off. But next comments talks about using HdfsInputFile. That change makes the getConfiguration unused, so removed the function.", "author": "anjalinorwood", "createdAt": "2020-09-24T18:23:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTk4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjMxNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312314", "bodyText": "Use HdfsInputFile instead of HadoopInputFile. See finishInsert()", "author": "electrum", "createdAt": "2020-09-15T00:33:46Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUyMDk3Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494520972", "bodyText": "Got it. done.", "author": "anjalinorwood", "createdAt": "2020-09-24T18:22:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjMxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjYwOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312608", "bodyText": "Nit: we only capitalize the first letter of words, even acronyms, so tableSnapshotIdString", "author": "electrum", "createdAt": "2020-09-15T00:34:53Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwOTI0OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490609248", "bodyText": "We shouldn't depend on the toString() method of IcebergTableHandle as the serialization format, since it could change and it's not designed to be serializable. Using SchemaTableName directly and serializing to a JSON list seems better, since that handles names with special characters, etc.", "author": "electrum", "createdAt": "2020-09-17T23:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjYwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0OTc0Nw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494649747", "bodyText": "Now I am using SchemaTableName.toString. Not serializing to json since Iceberg stores this table property in JSON metadata.", "author": "anjalinorwood", "createdAt": "2020-09-24T22:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjYwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjY5Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312693", "bodyText": "Same, drop the \"A\"", "author": "electrum", "createdAt": "2020-09-15T00:35:10Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwNTM2Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493005362", "bodyText": "Done", "author": "anjalinorwood", "createdAt": "2020-09-22T20:13:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjc1Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312753", "bodyText": "Nit: wrap all arguments or none", "author": "electrum", "createdAt": "2020-09-15T00:35:22Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwNzQzNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493007435", "bodyText": "This method looks unused. Removed it. Point noted around wrapping all arguments or null.", "author": "anjalinorwood", "createdAt": "2020-09-22T20:17:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjg1Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312852", "bodyText": "Use Guava Splitter", "author": "electrum", "createdAt": "2020-09-15T00:35:42Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {\n+            return true;\n+        }\n+\n+        if (tableToken.isPresent() && currentToken.isPresent() &&\n+                tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId()) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, tableHandle);\n+        if (refreshStateMap.size() == 0) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            String[] strings = entry.getKey().split(\"\\\\.\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgwOTEyNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493809125", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-09-23T18:41:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjk5Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312992", "bodyText": "Use isEmpty() for collections instead of size() == 0", "author": "electrum", "createdAt": "2020-09-15T00:36:15Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {\n+            return true;\n+        }\n+\n+        if (tableToken.isPresent() && currentToken.isPresent() &&\n+                tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId()) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, tableHandle);\n+        if (refreshStateMap.size() == 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwNjcxMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493006710", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-09-22T20:15:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MjM4NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490592384", "bodyText": "Nit: the indentation is off here. Continuation indent should be 8 spaces", "author": "electrum", "createdAt": "2020-09-17T22:16:25Z", "path": "presto-main/src/main/java/io/prestosql/sql/planner/optimizations/BeginTableWrite.java", "diffHunk": "@@ -189,7 +189,8 @@ private WriterTarget createWriterTarget(WriterTarget target)\n             }\n             if (target instanceof TableWriterNode.RefreshMaterializedViewReference) {\n                 TableWriterNode.RefreshMaterializedViewReference refreshMV = (TableWriterNode.RefreshMaterializedViewReference) target;\n-                return new TableWriterNode.RefreshMaterializedViewTarget(metadata.beginRefreshMaterializedView(session, refreshMV.getStorageTableHandle()),\n+                return new TableWriterNode.RefreshMaterializedViewTarget(\n+                    metadata.beginRefreshMaterializedView(session, refreshMV.getStorageTableHandle(), refreshMV.getSourceTableHandles()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwNzk5Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493007992", "bodyText": "fixed.", "author": "anjalinorwood", "createdAt": "2020-09-22T20:18:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MjM4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MzQ5MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490593490", "bodyText": "From what I can tell, the query for a materialized view can reference any connector. Passing arbitrary arbitrary table handles from other connectors is not useful (the connector can't do anything with them). It's also expected that a connector only gets their own table handles. Maybe we should filter this list to the connector and a boolean indicating that there are source tables for other connectors.", "author": "electrum", "createdAt": "2020-09-17T22:19:30Z", "path": "presto-spi/src/main/java/io/prestosql/spi/connector/ConnectorMetadata.java", "diffHunk": "@@ -454,7 +454,7 @@ default boolean supportsMissingColumnsOnInsert()\n     /**\n      * Begin materialized view query\n      */\n-    default ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    default ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzODgwNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494538807", "bodyText": "Cross referencing connectors in a materialized view will need some work and API changes. The design is not yet completely mapped out, so I hesitate to make this change as it feels incomplete change that may need further changes down the line.\nIf we think this is a good change to have, I am open to it. FYI @martint for his input as well.", "author": "anjalinorwood", "createdAt": "2020-09-24T18:48:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MzQ5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Mzk5NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490593995", "bodyText": "This type of operation can be done with a stream (though we'll need to change it anyway so we filter out tables from other connectors)", "author": "electrum", "createdAt": "2020-09-17T22:20:48Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -847,13 +847,18 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+        List<ConnectorTableHandle> sourceConnectorHandles = new ArrayList<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTQwMg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494539402", "bodyText": "Please see my response to cross-connector materialized views above. Keeping in line with that, I did not make any changes here. I rewrote the loop to use streams though. (here and in FinishRefreshMaterializedView call).", "author": "anjalinorwood", "createdAt": "2020-09-24T18:49:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Mzk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzc0Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490603743", "bodyText": "Don't abbreviate variable names", "author": "electrum", "createdAt": "2020-09-17T22:47:31Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5MjYyNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493792624", "bodyText": "Fixed now. Sorry, Martin pointed out in the previous review to not abbreviate variable names. This code was written before then and it did not occur to me to double check. Will keep in mind in future.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:12:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzc0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzg2Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490603866", "bodyText": "Why? We shouldn't change the owner stored in the view", "author": "electrum", "createdAt": "2020-09-17T22:48:01Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5NDUzMw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493794533", "bodyText": "yes, sorry, Netflix implementation crept in. Fixed.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:16:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzg2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNDk4NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490604985", "bodyText": "This can be\nreturn Optional.ofNullable(icebergTable.currentSnapshot())\n        .map(snapshot -> new TableToken(snapshot.snapshotId()));", "author": "electrum", "createdAt": "2020-09-17T22:51:26Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5NjQxNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493796416", "bodyText": "yes, done.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:19:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNDk4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNjM3Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490606373", "bodyText": "We can restructure this a bit to make the various cases more clear\nif (!tableToken.isPresent()) {\n    return false;\n}\n\nif (!currentToken.isPresent()) {\n    return tableToken.get().getSnapshotId() == -1;\n}\n\nreturn tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();", "author": "electrum", "createdAt": "2020-09-17T22:55:35Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5ODU4NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493798585", "bodyText": "ok, done. I double checked that they are semantically equivalent.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNjM3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwODE2Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490608166", "bodyText": "This should probably start with presto since it is Presto specific", "author": "electrum", "createdAt": "2020-09-17T23:01:10Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -124,10 +145,12 @@\n public class IcebergMetadata\n         implements ConnectorMetadata\n {\n+    public static final String DEPENDS_ON_TABLES = \"dependsOnTables\";", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgwMDA5OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493800098", "bodyText": "Hmm, not really presto specific. This property says that the current table was computed using these other tables with corresponding snapshot-ids. I would rather leave it as is. If you feel strongly, please let me know and I will change it.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:26:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwODE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwOTk1Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490609956", "bodyText": "This is inherently racey since the definition of the table might change between the time that we check it and when we use it later. We should fetch the table metadata, then check if it's a materialized view.", "author": "electrum", "createdAt": "2020-09-17T23:07:00Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgxNDY3OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r493814679", "bodyText": "ok, I see the race condition .. code checks if an object is a materialized view .. before it gets the metadata of the materialized view somebody goes and drops it and creates a table with the same name .. access to metadata is stale and incorrect.\nFixed the order.", "author": "anjalinorwood", "createdAt": "2020-09-23T18:47:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwOTk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxMDU3NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490610575", "bodyText": "Handles should be immutable. Update this by creating a new object with the updated value.", "author": "electrum", "createdAt": "2020-09-17T23:09:04Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTableHandle.java", "diffHunk": "@@ -84,6 +84,11 @@ public TableType getTableType()\n         return snapshotId;\n     }\n \n+    public void setSnapshotId(Optional<Long> snapshotId)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY1MzQyOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r494653428", "bodyText": "We don't really even need a new handle here. We simply need to send in the retrieved snapshot-id.", "author": "anjalinorwood", "createdAt": "2020-09-24T22:52:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxMDU3NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzAyMw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505867023", "bodyText": "This should be NOT_SUPPORTED", "author": "electrum", "createdAt": "2020-10-15T21:26:05Z", "path": "presto-main/src/main/java/io/prestosql/sql/analyzer/StatementAnalyzer.java", "diffHunk": "@@ -942,6 +943,10 @@ protected Scope visitCreateMaterializedView(CreateMaterializedView node, Optiona\n             QualifiedObjectName viewName = createQualifiedObjectName(session, node, node.getName());\n             analysis.setUpdateType(\"CREATE MATERIALIZED VIEW\", viewName);\n \n+            if (node.isReplace() && node.isNotExists()) {\n+                throw semanticException(GENERIC_USER_ERROR, node, \"'CREATE OR REPLACE' and 'IF NOT EXISTS' clauses can not be used together\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyMDUxMg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511120512", "bodyText": "It looks like the change to the error code is in the wrong commit (it's in the next one)", "author": "electrum", "createdAt": "2020-10-23T19:47:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyODY1Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511128656", "bodyText": "oh sorry, fixed now.", "author": "anjalinorwood", "createdAt": "2020-10-23T19:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzMxOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505867318", "bodyText": "Nit: uppercase SQL keywords", "author": "electrum", "createdAt": "2020-10-15T21:26:26Z", "path": "presto-main/src/test/java/io/prestosql/sql/analyzer/TestAnalyzer.java", "diffHunk": "@@ -2497,6 +2498,13 @@ public void testNullTreatment()\n         analyze(\"SELECT lag(1) IGNORE NULLS OVER (ORDER BY x) FROM (VALUES 1) t(x)\");\n     }\n \n+    @Test\n+    public void testCreateOrReplaceMaterializedView()\n+    {\n+        assertFails(\"create or replace materialized view if not exists mv1 as select * from tab1\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3MjIzMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505872230", "bodyText": "This should be indented as\n.stream()\n.flatMap(schema -> metastore.getAllViews(schema).stream()\n        .map(table -> new SchemaTableName(schema, table))\n        .collect(toList())\n        .stream())\n.collect(toList());", "author": "electrum", "createdAt": "2020-10-15T21:32:37Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -256,14 +280,25 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n     @Override\n     public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n     {\n-        return schemaName.map(Collections::singletonList)\n+        List<SchemaTableName> tablesList = schemaName.map(Collections::singletonList)\n                 .orElseGet(metastore::getAllDatabases)\n                 .stream()\n                 .flatMap(schema -> metastore.getTablesWithParameter(schema, TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE).stream()\n                         .map(table -> new SchemaTableName(schema, table))\n                         .collect(toList())\n                         .stream())\n                 .collect(toList());\n+\n+        List<SchemaTableName> viewsList = schemaName.map(Collections::singletonList)\n+                .orElseGet(metastore::getAllDatabases)\n+                .stream()\n+                .flatMap(schema -> metastore.getAllViews(schema).stream()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3MjYxOQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505872619", "bodyText": "You can remove the viewsList variable by replacing this with\n.forEach(tablesList::add);", "author": "electrum", "createdAt": "2020-10-15T21:33:09Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -256,14 +280,25 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n     @Override\n     public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n     {\n-        return schemaName.map(Collections::singletonList)\n+        List<SchemaTableName> tablesList = schemaName.map(Collections::singletonList)\n                 .orElseGet(metastore::getAllDatabases)\n                 .stream()\n                 .flatMap(schema -> metastore.getTablesWithParameter(schema, TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE).stream()\n                         .map(table -> new SchemaTableName(schema, table))\n                         .collect(toList())\n                         .stream())\n                 .collect(toList());\n+\n+        List<SchemaTableName> viewsList = schemaName.map(Collections::singletonList)\n+                .orElseGet(metastore::getAllDatabases)\n+                .stream()\n+                .flatMap(schema -> metastore.getAllViews(schema).stream()\n+                .map(table -> new SchemaTableName(schema, table))\n+                .collect(toList())\n+                .stream())\n+                .collect(toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3OTk5Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505879996", "bodyText": "Nit: we don't use the Immutable* types in declarations. Instead, declare as Map", "author": "electrum", "createdAt": "2020-10-15T21:43:29Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg4Njg2OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505886869", "bodyText": "Nit: might look nicer to write this as\n .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n .withStorage(storage -> storage.setLocation(\"\"))", "author": "electrum", "createdAt": "2020-10-15T21:54:36Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg5NzcxMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505897711", "bodyText": "Nit: we avoid abbreviations. Name this column\nAlso, I think this can be done as a stream", "author": "electrum", "createdAt": "2020-10-15T22:22:57Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ5MTY3MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508491670", "bodyText": "done", "author": "anjalinorwood", "createdAt": "2020-10-20T13:16:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg5NzcxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg5OTM1Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505899356", "bodyText": "This can use Map.putIfAbsent()", "author": "electrum", "createdAt": "2020-10-15T22:27:07Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwMDY2Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505900666", "bodyText": "I think we need to return here if ignoreExisting is present and the view already exists\nHow do we treat these flags when the view exists but storage table does not, or vice versa?", "author": "electrum", "createdAt": "2020-10-15T22:30:49Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ5NzgxNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508497817", "bodyText": "I incorporated first part of the comment.\nIf the view exists, the storage table must exist since storage table is created when view is created or replaced. If storage table was dropped by the user (using drop table storage_table), the refresh operation will fail.", "author": "anjalinorwood", "createdAt": "2020-10-20T13:23:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwMDY2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwNzc4Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505907783", "bodyText": "I don't think this needs to be fully qualified", "author": "electrum", "createdAt": "2020-10-15T22:50:57Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwODA5NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505908095", "bodyText": "We could static import VIRTUAL_VIEW", "author": "electrum", "createdAt": "2020-10-15T22:51:52Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTE4NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505909185", "bodyText": "What does it mean when the STORAGE_TABLE parameter is not present?", "author": "electrum", "createdAt": "2020-10-15T22:55:03Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDE0MQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910141", "bodyText": "The canonical way to write this is\nString oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\nif (oldStorageTable != null) {\n    ...\n}", "author": "electrum", "createdAt": "2020-10-15T22:57:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTE4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODAxOTk1Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508019953", "bodyText": "The STORAGE_TABLE parameter should always be present. This check guards against the case where somehow the metadata got corrupted.\nI rewrote the code in canonical fashion like you showed.", "author": "anjalinorwood", "createdAt": "2020-10-19T19:47:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTE4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTg1Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505909853", "bodyText": "This can be\nTable view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n        .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));", "author": "electrum", "createdAt": "2020-10-15T22:56:57Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDI0OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910249", "bodyText": "Same comment as above. Do get() and check for null", "author": "electrum", "createdAt": "2020-10-15T22:58:04Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDUwOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910508", "bodyText": "If any of the subsequent operations fail, we leave the storage table behind", "author": "electrum", "createdAt": "2020-10-15T22:58:47Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ5OTk0OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508499949", "bodyText": "correct. Not sure how we can avoid in the absence of multi-statement, multi-table transaction support.", "author": "anjalinorwood", "createdAt": "2020-10-20T13:26:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDUwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMzYwNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505913605", "bodyText": "This could be ImmutableList.of() since we don't modify it", "author": "electrum", "createdAt": "2020-10-15T23:07:55Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTIyOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915228", "bodyText": "Do we need this check? We don't have it for finishInsert(). It seems like we should still update the dependsOnTables even if the final output is empty, since we still updated the materialized view based on those input tables.", "author": "electrum", "createdAt": "2020-10-15T23:12:40Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4MDQ1Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508080456", "bodyText": "Right. The new dependsOnTables property should reflect the (potentially new) snapshot-ids of the base tables and the analysis work that determined that no data needs to be inserted (in this case).", "author": "anjalinorwood", "createdAt": "2020-10-19T21:44:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTY1OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915658", "bodyText": "This can be collected directly into a string\nString dependencies = ...\n        .collect(joining(\",\"));", "author": "electrum", "createdAt": "2020-10-15T23:14:01Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTc1OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915759", "bodyText": "This indentation is off. Please run the code formatter.", "author": "electrum", "createdAt": "2020-10-15T23:14:19Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4NDkwNg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508084906", "bodyText": "I am using the checkstyle xml according to instructions from prestosql dev page. Despite running code-formatter, I get 4 space indent rather than 8. I manually fixed it. I will look into how I can fix it in intelliJ preferences.", "author": "anjalinorwood", "createdAt": "2020-10-19T21:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNjcyMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505916720", "bodyText": "I'm not sure we should be using PRESTO_VIEW_FLAG here, since these are materialized views for Iceberg. They can't be queried as a view in the Hive connector. Thoughts?", "author": "electrum", "createdAt": "2020-10-15T23:17:05Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk1NjQ4NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508956485", "bodyText": "All views recognized by presto as views excluding the hive views (that get translated) need the PRESTO_VIEW_FLAG.\nThis code sets the flag on the view object.\nWhen we try to resolve the catalog for a materialized view, getRawSystemTable in IcebergMetadata gets called (we had an offline discussion about this). getRawSystemTable in turn calls metastore.getTable, which calls ThriftHiveMetastore.getTable. This function has code:\nif (!translateHiveViews && table.getTableType().equals(TableType.VIRTUAL_VIEW.name()) && !isPrestoView(table)) { throw new HiveViewNotSupportedException(new SchemaTableName(databaseName, tableName)); }\nThe materialized view-view object is expected to have the PRESTO_VIEW_FLAG, otherwise it throws the Hive views are not supported exception.", "author": "anjalinorwood", "createdAt": "2020-10-21T02:49:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNjcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjUxMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922511", "bodyText": "This defensive copy should be done in ConnectorMaterializedViewDefinition (although we don't need it since the map returned from Table is immutable)", "author": "electrum", "createdAt": "2020-10-15T23:35:41Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4NzA0OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508087049", "bodyText": "materializedView.getParameters() returns Map<String, String>. ConnectorMaterializedViewDefinition needs it to be Map<String, Object> hence the copy.", "author": "anjalinorwood", "createdAt": "2020-10-19T21:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjUxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjcyNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922724", "bodyText": "We shouldn't need to fully qualify here", "author": "electrum", "createdAt": "2020-10-15T23:36:28Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            Preconditions.checkState(strings.size() >= 2);\n+            String schema;\n+            String name;\n+            if (strings.size() == 3) {\n+                schema = strings.get(1);\n+                name = strings.get(2);\n+            }\n+            else {\n+                schema = strings.get(0);\n+                name = strings.get(1);\n+            }\n+            SchemaTableName schemaTableName = new SchemaTableName(schema, name);\n+            if (!isTableCurrent(session, getTableHandle(session, schemaTableName), entry.getValue())) {\n+                return new MaterializedViewFreshness(false);\n+            }\n+        }\n+        return new MaterializedViewFreshness(true);\n+    }\n+\n+    public Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> getMaterializedViewToken(ConnectorSession session, SchemaTableName name)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjgyMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922821", "bodyText": "I think this method can be private", "author": "electrum", "createdAt": "2020-10-15T23:36:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjcyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4Nzg1MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508087850", "bodyText": "yes to both, done.", "author": "anjalinorwood", "createdAt": "2020-10-19T22:00:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjcyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzI5MQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505923291", "bodyText": "We could simplify to\nif (strings.size() == 3) {\n    strings = strings.subList(1, 3);\n}\nString schema = strings.get(0);\nString name = strings.get(1);", "author": "electrum", "createdAt": "2020-10-15T23:38:25Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            Preconditions.checkState(strings.size() >= 2);\n+            String schema;\n+            String name;\n+            if (strings.size() == 3) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzkwNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505923905", "bodyText": "I think we can eliminate this class for now since we are only using it as a return type for the internal getMaterializedViewToken() method", "author": "electrum", "createdAt": "2020-10-15T23:40:30Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/TableToken.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+public class TableToken", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDE0OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508090149", "bodyText": "We do want to extend the TableToken with additional information for incremental refresh in the near future. So, I would prefer to have the class there. Please let me know if you do not agree.", "author": "anjalinorwood", "createdAt": "2020-10-19T22:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNjEzMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514426131", "bodyText": "Can we make this a private class in IcebergMetadata without JSON serialization? It's confusing to have dead code. We can always move it out later when needed.", "author": "electrum", "createdAt": "2020-10-29T17:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyNDQ3Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505924473", "bodyText": "Can we use createIcebergQueryRunner(ImmutableMap.of(), false) from IcebergQueryRunner for this? The only difference seems to be that this one doesn't install the TPCH catalog, but that shouldn't be a concern.", "author": "electrum", "createdAt": "2020-10-15T23:42:25Z", "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODczMjY2MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508732660", "bodyText": "Yes, good idea. Cleaned up the test as per this and other comments below.", "author": "anjalinorwood", "createdAt": "2020-10-20T18:04:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyNDQ3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyNDgwNA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505924804", "bodyText": "We should shorten this to test since there is no problem with collisions. A shorter name makes the tests easier to read. Or if we use createIcebergQueryRunner(), just use the default session schema and remove the schema qualification everywhere.", "author": "electrum", "createdAt": "2020-10-15T23:43:34Z", "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MjMwMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506052301", "bodyText": "Use the date constructor instead of a cast\nDATE '2019-09-09'", "author": "electrum", "createdAt": "2020-10-16T04:59:22Z", "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table1(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzM1MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053350", "bodyText": "We prefer to do the initialization close to the tests, since that makes the tests self contained and thus easier to read. Shared resources are also problematic because multiple tests could modify them (even accidentally), and tests run multithreaded in arbitrary order, leading to hard to debug failures.", "author": "electrum", "createdAt": "2020-10-16T05:03:37Z", "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table1(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (3, CAST('2019-09-09' AS DATE)), (4, CAST('2019-09-10' AS DATE)), (5, CAST('2019-09-10' AS DATE))\", 3);\n+        assertQuery(\"SELECT count(*) FROM test_materialized_views.base_table1\", \"VALUES 6\");\n+\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table2 (_varchar VARCHAR, _bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_bigint', '_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table2 VALUES ('a', 0, CAST('2019-09-08' AS DATE)), ('a', 1, CAST('2019-09-08' AS DATE)), ('a', 0, CAST('2019-09-09' AS DATE))\", 3);\n+        assertQuery(\"SELECT count(*) FROM test_materialized_views.base_table2\", \"VALUES 3\");\n+\n+        // A very simple non-partitioned materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_no_part as select * from test_materialized_views.base_table1\");\n+        // A non-partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_agg as select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1\");\n+        // A partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_part WITH (partitioning = ARRAY['_date']) as select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1\");\n+        // A non-partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join as \" +\n+                \"select t2._bigint, _varchar, t1._date from test_materialized_views.base_table1 t1, test_materialized_views.base_table2 t2 where t1._date = t2._date\");\n+        // A partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_part WITH (partitioning = ARRAY['_date', '_bigint']) as \" +\n+                \"select t1._bigint, _varchar, t2._date, sum(1) as my_sum from test_materialized_views.base_table1 t1, test_materialized_views.base_table2 t2 where t1._date = t2._date group by 1, 2, 3 order by 1, 2\");\n+\n+        // Base tables and materialized views for staleness check\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table3(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table3 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);\n+\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table4 (_varchar VARCHAR, _bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_bigint', '_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table4 VALUES ('a', 0, CAST('2019-09-08' AS DATE)), ('a', 1, CAST('2019-09-08' AS DATE)), ('a', 0, CAST('2019-09-09' AS DATE))\", 3);\n+\n+        // A partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_part_stale WITH (partitioning = ARRAY['_date']) as select _date, count(_date) as num_dates from test_materialized_views.base_table3 group by 1\");\n+        // A non-partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_stale as \" +\n+                \"select t2._bigint, _varchar, t1._date from test_materialized_views.base_table3 t1, test_materialized_views.base_table4 t2 where t1._date = t2._date\");\n+        // A partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_part_stale WITH (partitioning = ARRAY['_date', '_bigint']) as \" +\n+                \"select t1._bigint, _varchar, t2._date, sum(1) as my_sum from test_materialized_views.base_table3 t1, test_materialized_views.base_table4 t2 where t1._date = t2._date group by 1, 2, 3 order by 1, 2\");\n+\n+        // Materialized views to test SQL features\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_window WITH (partitioning = ARRAY['_date']) as select _date, \" +\n+                \"sum(_bigint) OVER (partition by _date order by _date) as sum_ints from test_materialized_views.base_table1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_window\", 6);\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_union WITH (partitioning = ARRAY['_date']) as \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1 union \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table2 group by 1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_union\", 5);\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_subquery WITH (partitioning = ARRAY['_date']) as \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table1 where _date = (select max(_date) from test_materialized_views.base_table2) group by 1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_subquery\", 1);\n+\n+        // Materialized view to test 'replace' feature", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzUyOQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053529", "bodyText": "Please move the non-Iceberg changes to a separate commit. It's best to not mix engine and connector changes.", "author": "electrum", "createdAt": "2020-10-16T05:04:17Z", "path": "presto-main/src/main/java/io/prestosql/metadata/Metadata.java", "diffHunk": "@@ -269,7 +269,7 @@\n     /**\n      * Begin refresh materialized view query\n      */\n-    InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle);\n+    InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzYwNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053607", "bodyText": "toImmutableList()", "author": "electrum", "createdAt": "2020-10-16T05:04:36Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1Mzc3OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053779", "bodyText": "This can use a method reference\n.map(Object::getClass)", "author": "electrum", "createdAt": "2020-10-16T05:05:09Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1Mzg2OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053869", "bodyText": "You can use Stream.count() here", "author": "electrum", "createdAt": "2020-10-16T05:05:28Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())\n+                .distinct()\n+                .collect(Collectors.toList()).size() > 1) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzkzMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053930", "bodyText": "Use NOT_SUPPORTED error code", "author": "electrum", "createdAt": "2020-10-16T05:05:41Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())\n+                .distinct()\n+                .collect(Collectors.toList()).size() > 1) {\n+            throw new PrestoException(GENERIC_USER_ERROR, \"Cross connector materialized views are not supported\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDA4Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054086", "bodyText": "Should this also verify that source connector handles are for the same connector as the materialized view?", "author": "electrum", "createdAt": "2020-10-16T05:06:24Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ3MDE3NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508470175", "bodyText": "Yes.", "author": "anjalinorwood", "createdAt": "2020-10-20T12:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDEzMQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054131", "bodyText": "toImmutableList()", "author": "electrum", "createdAt": "2020-10-16T05:06:35Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -864,10 +879,10 @@ public InsertTableHandle beginRefreshMaterializedView(Session session, TableHand\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         ConnectorMetadata metadata = getMetadata(session, catalogName);\n-        List<ConnectorTableHandle> sourceConnectorHandles = new ArrayList<>();\n-        for (TableHandle handle : sourceTableHandles) {\n-            sourceConnectorHandles.add(handle.getConnectorHandle());\n-        }\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDk5OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054998", "bodyText": "How is it possible for all of them to be empty? I don't think it's valid for any of them to be empty.", "author": "electrum", "createdAt": "2020-10-16T05:09:58Z", "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -1145,12 +1160,23 @@ public void dropMaterializedView(Session session, QualifiedObjectName viewName)\n     }\n \n     @Override\n-    public MaterializedViewFreshness getMaterializedViewFreshness(Session session, TableHandle tableHandle)\n+    public MaterializedViewFreshness getMaterializedViewFreshness(Session session, QualifiedObjectName viewName)\n     {\n-        CatalogName catalogName = tableHandle.getCatalogName();\n-        CatalogMetadata catalogMetadata = getCatalogMetadata(session, catalogName);\n-        ConnectorMetadata metadata = catalogMetadata.getMetadataFor(catalogName);\n-        return metadata.getMaterializedViewFreshness(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+        if (viewName.getCatalogName().isEmpty() || viewName.getSchemaName().isEmpty() || viewName.getObjectName().isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ2NzYxNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r508467615", "bodyText": "Yes, constructor for QualifiedObjectName requires the three components to be non-empty. I removed the check.", "author": "anjalinorwood", "createdAt": "2020-10-20T12:42:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NTY0Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506055643", "bodyText": "This \"else\" is redundant since the \"if\" branch returns", "author": "electrum", "createdAt": "2020-10-16T05:12:19Z", "path": "presto-main/src/main/java/io/prestosql/sql/analyzer/StatementAnalyzer.java", "diffHunk": "@@ -1177,37 +1175,31 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n \n             QualifiedObjectName name = createQualifiedObjectName(session, table, table.getName());\n             analysis.addEmptyColumnReferencesForTable(accessControl, session.getIdentity(), name);\n-            Optional<TableHandle> tableHandle = metadata.getTableHandle(session, name);\n-\n-            // If this is a materialized view, get the name of the storage table\n-            Optional<QualifiedName> storageName = getMaterializedViewStorageTableName(name);\n-            Optional<TableHandle> storageHandle = Optional.empty();\n-            if (storageName.isPresent()) {\n-                storageHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, storageName.get()));\n-            }\n-\n-            // If materialized view is current, answer the query using the storage table\n-            Identifier catalogName = new Identifier(name.getCatalogName());\n-            Identifier schemaName = new Identifier(name.getSchemaName());\n-            Identifier tableName = new Identifier(name.getObjectName());\n-            QualifiedName materializedViewName = QualifiedName.of(ImmutableList.of(catalogName, schemaName, tableName));\n-            Optional<TableHandle> materializedViewHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, materializedViewName));\n-            if (storageHandle.isPresent() && metadata.getMaterializedViewFreshness(session, materializedViewHandle.get()).isMaterializedViewFresh()) {\n-                tableHandle = storageHandle;\n-            }\n-            else {\n-                // This is a stale materialized view and should be expanded like a logical view\n-                if (storageHandle.isPresent()) {\n-                    Optional<ConnectorMaterializedViewDefinition> optionalMaterializedView = metadata.getMaterializedView(session, name);\n-                    if (optionalMaterializedView.isPresent()) {\n-                        return createScopeForMaterializedView(table, name, scope, optionalMaterializedView.get());\n+            Optional<TableHandle> tableHandle = Optional.empty();\n+\n+            Optional<ConnectorMaterializedViewDefinition> optionalMaterializedView = metadata.getMaterializedView(session, name);\n+            if (optionalMaterializedView.isPresent()) {\n+                if (metadata.getMaterializedViewFreshness(session, name).isMaterializedViewFresh()) {\n+                    // If materialized view is current, answer the query using the storage table\n+                    Optional<QualifiedName> storageName = getMaterializedViewStorageTableName(name);\n+                    if (storageName.isPresent()) {\n+                        tableHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, storageName.get()));\n                     }\n                 }\n-                // This is a reference to a logical view\n+                else {\n+                    // This is a stale materialized view and should be expanded like a logical view\n+                    return createScopeForMaterializedView(table, name, scope, optionalMaterializedView.get());\n+                }\n+            }\n+            else {\n+                // This is could be a reference to a logical view or a table\n                 Optional<ConnectorViewDefinition> optionalView = metadata.getView(session, name);\n                 if (optionalView.isPresent()) {\n                     return createScopeForView(table, name, scope, optionalView.get());\n                 }\n+                else {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyMTYxNQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511121615", "bodyText": "This is a good typo to fix, but can you do it in a separate commit, since it's unrelated to the materialized view change? It's fine to leave it in this PR as a separate commit.", "author": "electrum", "createdAt": "2020-10-23T19:49:20Z", "path": "presto-parser/src/test/java/io/prestosql/sql/parser/TestSqlParser.java", "diffHunk": "@@ -2693,7 +2693,7 @@ private static void assertStatement(String query, Statement expected)\n         assertFormattedSql(SQL_PARSER, expected);\n     }\n \n-    private static void assertInvalidStatemennt(String statement, String expectedErrorMessageRegex)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyODkwMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511128900", "bodyText": "ok, done.", "author": "anjalinorwood", "createdAt": "2020-10-23T20:00:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyMTYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjUxNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511126517", "bodyText": "The addition of this class should go in the next commit. Also, should it go in the presto-iceberg module (and package) since it's only used by Iceberg?", "author": "electrum", "createdAt": "2020-10-23T19:55:00Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/MaterializedViewAlreadyExistsException.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.SchemaTableName;\n+\n+import static io.prestosql.spi.StandardErrorCode.ALREADY_EXISTS;\n+import static java.lang.String.format;\n+\n+public class MaterializedViewAlreadyExistsException", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4MTUxMg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511181512", "bodyText": "We decided offline to throw a PRESTO_EXCEPTION. I removed this class.", "author": "anjalinorwood", "createdAt": "2020-10-23T22:27:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjUxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE2Njg3NQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511166875", "bodyText": "We don't use final on local variables", "author": "electrum", "createdAt": "2020-10-23T21:38:59Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4MTM2NA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511181364", "bodyText": "sorry, fixed.", "author": "anjalinorwood", "createdAt": "2020-10-23T22:26:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE2Njg3NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM1Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513112352", "bodyText": "This method doesn't belong in this commit.", "author": "martint", "createdAt": "2020-10-28T00:37:59Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/ViewReaderUtil.java", "diffHunk": "@@ -97,6 +102,13 @@ public static String encodeViewData(ConnectorViewDefinition definition)\n         return VIEW_PREFIX + data + VIEW_SUFFIX;\n     }\n \n+    public static String encodeMaterializedViewData(ConnectorMaterializedViewDefinition definition)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyODI0Mw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515428243", "bodyText": "Moving to iceberg connector commit", "author": "anjalinorwood", "createdAt": "2020-10-30T23:59:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM2Ng==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513112366", "bodyText": "This method doesn't belong in this commit.", "author": "martint", "createdAt": "2020-10-28T00:38:04Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/ViewReaderUtil.java", "diffHunk": "@@ -113,6 +125,16 @@ public ConnectorViewDefinition decodeViewData(String viewData, Table table, Cata\n             byte[] bytes = Base64.getDecoder().decode(viewData);\n             return VIEW_CODEC.fromJson(bytes);\n         }\n+\n+        public static ConnectorMaterializedViewDefinition decodeMaterializedViewData(String data)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyODMzOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515428338", "bodyText": "Moving to iceberg connector commit", "author": "anjalinorwood", "createdAt": "2020-10-31T00:00:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMzMxOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513113318", "bodyText": "This class doesn't belong in this commit. It's not related to \"Connector APIs for materialized views that modeled them as TableHandles are being changed to pass in materialized view name rather than handles.\"", "author": "martint", "createdAt": "2020-10-28T00:41:16Z", "path": "presto-spi/src/main/java/io/prestosql/spi/connector/MaterializedViewNotFoundException.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.connector;\n+\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class MaterializedViewNotFoundException", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyODI5Mg==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515428292", "bodyText": "Moving to iceberg connector commit", "author": "anjalinorwood", "createdAt": "2020-10-30T23:59:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMzMxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwNjMyNw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514406327", "bodyText": "This doesn't seem to be needed", "author": "electrum", "createdAt": "2020-10-29T16:41:37Z", "path": "presto-iceberg/pom.xml", "diffHunk": "@@ -212,6 +212,12 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTA4NTc0OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515085748", "bodyText": "If I remove it, I get:\n[WARNING] Used undeclared dependencies found: [WARNING]    io.prestosql:presto-parser:jar:345-SNAPSHOT:test", "author": "anjalinorwood", "createdAt": "2020-10-30T13:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwNjMyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwOTIwMA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514409200", "bodyText": "This shouldn't be done here, as beginRefreshMaterializedView() is called during planning (and even for EXPLAIN), and executeDelete() commits the delete operation. We should do the delete as part of the same Iceberg transaction that inserts the data files, in finishRefreshMaterializedView().", "author": "electrum", "createdAt": "2020-10-29T16:45:37Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQwMTk4OQ==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515401989", "bodyText": "Good catch!\nI was surprised why it did not come up in my testing before. Explaining the refresh command would delete the data and we should see problems when we try to use/refresh the materialized view.\nTurns out, when materialized view is fresh, early on, during analysis we determine that it is fresh and refresh can be skipped, so beginRefreshMaterializedView is not called and storage table data stays intact. When it is stale, explain would delete the data, but no harm done, since materialized view's storage table can't be used for querying anyway and any subsequent refresh would just delete the non-existent data.\nGiven the above and the fact that we only have full refresh so far, this problem was not seen.\nI have now moved the delete to 'finishRefresh...', I need to send in the ConnectorTableHandle for the storage table to 'finishRefresh ..', I am adding those changes in the same PR in a separate commit.\n@martint FYI for connector API changes and because this is fascinating.", "author": "anjalinorwood", "createdAt": "2020-10-30T22:02:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwOTIwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwOTMxOA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514409318", "bodyText": "Nit: wrap all arguments if any are wrapped", "author": "electrum", "createdAt": "2020-10-29T16:45:48Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNDQ1MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514424450", "bodyText": "We should be more explicit in the validation here and provide a good error message, since this value comes from the user. Let's move this into getMaterializedViewToken() and have it return SchemaTableName since that is where we are doing the parsing.\nif (strings.size() == 3) {\n    ...\n}\nelse if (strings.size() != 2) {\n    throw new PrestoException(ICEBERG_INVALID_METADATA, format(\"Invalid table name in '%s' property: %s'\", DEPENDS_ON_TABLES, name));\n}", "author": "electrum", "createdAt": "2020-10-29T17:07:09Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            checkState(strings.size() >= 2);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNTM3MA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515415370", "bodyText": "Added the validation. Leaving the code where it is as we expect GetMaterializedViewFreshness to evolve over time.", "author": "anjalinorwood", "createdAt": "2020-10-30T22:54:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNDQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNTA4OA==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514425088", "bodyText": "I don't think we need to check for empty, since splitter on empty will return an empty map.", "author": "electrum", "createdAt": "2020-10-29T17:08:07Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            checkState(strings.size() >= 2);\n+            if (strings.size() == 3) {\n+                strings = strings.subList(1, 3);\n+            }\n+            String schema = strings.get(0);\n+            String name = strings.get(1);\n+            SchemaTableName schemaTableName = new SchemaTableName(schema, name);\n+            if (!isTableCurrent(session, getTableHandle(session, schemaTableName), entry.getValue())) {\n+                return new MaterializedViewFreshness(false);\n+            }\n+        }\n+        return new MaterializedViewFreshness(true);\n+    }\n+\n+    private Map<String, Optional<TableToken>> getMaterializedViewToken(ConnectorSession session, SchemaTableName name)\n+    {\n+        Map<String, Optional<TableToken>> viewToken = new HashMap<>();\n+        Optional<ConnectorMaterializedViewDefinition> materializedViewDefinition = getMaterializedView(session, name);\n+        if (!materializedViewDefinition.isPresent()) {\n+            return viewToken;\n+        }\n+\n+        String storageTableName = materializedViewDefinition.get().getProperties().getOrDefault(STORAGE_TABLE, \"\").toString();\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, new SchemaTableName(name.getSchemaName(), storageTableName));\n+        String dependsOnTables = icebergTable.currentSnapshot().summary().getOrDefault(DEPENDS_ON_TABLES, \"\");\n+        if (!dependsOnTables.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTMwNzIxMw==", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r515307213", "bodyText": "Splitting an empty string on comma yields an empty string. But empty string does not qualify as a valid 'key-value' pair with separator '='. The 'MapSplitter' class says:\n* @throws IllegalArgumentException if the specified sequence does not split into valid map *     entries, or if there are duplicate keys\nSo, when dependsOnTables is empty, it results in the following error. Hence we need that if (!dependsOnTables.isEmpty()) { check.\nQuery 20201030_182956_00010_ttinc failed: Chunk [] is not a valid entry java.lang.IllegalArgumentException: Chunk [] is not a valid entry at com.google.common.base.Preconditions.checkArgument(Preconditions.java:217) at com.google.common.base.Splitter$MapSplitter.split(Splitter.java:526) at io.prestosql.plugin.iceberg.IcebergMetadata.getMaterializedViewToken(IcebergMetadata.java:997) at io.prestosql.plugin.iceberg.IcebergMetadata.getMaterializedViewFreshness(IcebergMetadata.java:965)", "author": "anjalinorwood", "createdAt": "2020-10-30T18:43:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNTA4OA=="}], "type": "inlineReview"}, {"oid": "f7215ec8486747b4274fa43ce30fc397b8f9970b", "url": "https://github.com/trinodb/trino/commit/f7215ec8486747b4274fa43ce30fc397b8f9970b", "message": "Disallow 'CREATE OR REPLACE' and 'IF NOT EXISTS'...\n\n... clauses together in 'CREATE MATERIALIZED VIEW' statement.\n\nThe optional 'IF NOT EXISTS' clause causes the error to be suppressed\nif a materialized view already exists. This clause was silently ignored\nif 'OR REPLACE' was specified.\nThis can be confusing to users, so this commit explicitly throws an error\nwhen the two clauses are specified together.", "committedDate": "2020-10-31T17:54:53Z", "type": "commit"}, {"oid": "4effcf7b3d75ca5fc0fa38ffb48b774a27e303b1", "url": "https://github.com/trinodb/trino/commit/4effcf7b3d75ca5fc0fa38ffb48b774a27e303b1", "message": "Materialized view connector API changes\n\nEven though materialized views are accessible through a connector and can be said to\n'belong' to a connector, they need to be treated separately from tables. Connector\nAPIs for materialized views that modeled them as TableHandles are being changed to\npass in materialized view name rather than handles.", "committedDate": "2020-10-31T17:54:53Z", "type": "commit"}, {"oid": "ecbb65943dcb1f2bd2ef9bb1594d7709cdab5c63", "url": "https://github.com/trinodb/trino/commit/ecbb65943dcb1f2bd2ef9bb1594d7709cdab5c63", "message": "Send storage table handle to finishRefreshMaterializedView", "committedDate": "2020-10-31T17:54:53Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "2081b4d1ec51261a4722e50b92c46848da1990fa", "url": "https://github.com/trinodb/trino/commit/2081b4d1ec51261a4722e50b92c46848da1990fa", "message": "Iceberg connector support for materialized views\n\nIceberg connector allows creation, refresh and drop of materialized views.\nIt provides methods to determine if a given materialized view is current\nwith respect to underlying base table(s).\nFreshness of a materialized view is determined by comparing the snapshot ID\nof base table(s) at the time the materialized view was refreshed with\nsnapshot ID of base table(s) at the time of the check.", "committedDate": "2020-11-01T18:25:37Z", "type": "commit"}, {"oid": "e473e3312b3fd432e0dfab25ffb803a983a51518", "url": "https://github.com/trinodb/trino/commit/e473e3312b3fd432e0dfab25ffb803a983a51518", "message": "Fix typo in 'assertInvalidStatemennt'", "committedDate": "2020-11-01T18:25:53Z", "type": "commit"}, {"oid": "e473e3312b3fd432e0dfab25ffb803a983a51518", "url": "https://github.com/trinodb/trino/commit/e473e3312b3fd432e0dfab25ffb803a983a51518", "message": "Fix typo in 'assertInvalidStatemennt'", "committedDate": "2020-11-01T18:25:53Z", "type": "forcePushed"}]}