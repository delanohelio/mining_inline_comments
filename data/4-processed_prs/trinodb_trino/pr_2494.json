{"pr_number": 2494, "pr_title": "Add azure configuration and tests", "pr_createdAt": "2020-01-13T21:36:30Z", "pr_url": "https://github.com/trinodb/trino/pull/2494", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNDI3MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366224270", "bodyText": "nit: this should return Optional", "author": "kokosing", "createdAt": "2020-01-14T09:16:35Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/HiveAzureConfig.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import io.airlift.configuration.Config;\n+import io.airlift.configuration.ConfigSecuritySensitive;\n+\n+public class HiveAzureConfig\n+{\n+    private String wasbStorageAccount;\n+    private String wasbAccessKey;\n+\n+    public String getWasbStorageAccount()\n+    {\n+        return wasbStorageAccount;\n+    }\n+\n+    @ConfigSecuritySensitive\n+    @Config(\"hive.azure.wasb-storage-account\")\n+    public HiveAzureConfig setWasbStorageAccount(String wasbStorageAccount)\n+    {\n+        this.wasbStorageAccount = wasbStorageAccount;\n+        return this;\n+    }\n+\n+    public String getWasbAccessKey()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNDMwMg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366224302", "bodyText": "nit: this should return Optional", "author": "kokosing", "createdAt": "2020-01-14T09:16:39Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/HiveAzureConfig.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import io.airlift.configuration.Config;\n+import io.airlift.configuration.ConfigSecuritySensitive;\n+\n+public class HiveAzureConfig\n+{\n+    private String wasbStorageAccount;\n+    private String wasbAccessKey;\n+\n+    public String getWasbStorageAccount()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNDYzMg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366224632", "bodyText": "we should raise an exception when only one of them is present (maybe inHiveAzureConfig in some @PostConstruct method)", "author": "kokosing", "createdAt": "2020-01-14T09:17:26Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/PrestoAzureConfigurationInitializer.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import io.prestosql.plugin.hive.ConfigurationInitializer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import javax.inject.Inject;\n+\n+import static java.lang.String.format;\n+\n+public class PrestoAzureConfigurationInitializer\n+        implements ConfigurationInitializer\n+{\n+    private final String wasbAccessKey;\n+    private final String wasbStorageAccount;\n+\n+    @Inject\n+    public PrestoAzureConfigurationInitializer(HiveAzureConfig hiveAzureConfig)\n+    {\n+        this.wasbAccessKey = hiveAzureConfig.getWasbAccessKey();\n+        this.wasbStorageAccount = hiveAzureConfig.getWasbStorageAccount();\n+    }\n+\n+    @Override\n+    public void initializeConfiguration(Configuration config)\n+    {\n+        if (wasbAccessKey != null && wasbStorageAccount != null) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTEyNw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366225127", "bodyText": "how do you provide these in github actions? Is this test actually executed in github actions?", "author": "kokosing", "createdAt": "2020-01-14T09:18:35Z", "path": ".github/workflows/ci-tests.yml", "diffHunk": "@@ -42,6 +42,15 @@ jobs:\n             source presto-product-tests/conf/product-tests-${{ matrix.config }}.sh &&\n               presto-hive-hadoop2/bin/run_hive_s3_tests.sh\n           fi\n+      - name: Run Azure Wasb Tests\n+        env:\n+            WASB_CONTAINER: \"${WASB_TESTS_WASB_CONTAINER}\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMwMzQ0Mw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366303443", "bodyText": "tbd", "author": "findepi", "createdAt": "2020-01-14T12:10:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTIwMA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366225200", "bodyText": "use 4 spaces indentation", "author": "kokosing", "createdAt": "2020-01-14T09:18:46Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}/common.sh\"\n+\n+start_docker_containers\n+\n+# insert Azure credentials\n+exec_in_hadoop_master_container cp /etc/hadoop/conf/core-site.xml.wasb-template /etc/hadoop/conf/core-site.xml\n+exec_in_hadoop_master_container sed -i \\\n+  -e \"s|%WASB_ACCESS_KEY%|${WASB_ACCESS_KEY}|g\" \\", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTQxOQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366225419", "bodyText": "remove comment (it is wrong)", "author": "kokosing", "createdAt": "2020-01-14T09:19:16Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}/common.sh\"\n+\n+start_docker_containers\n+\n+# insert Azure credentials\n+exec_in_hadoop_master_container cp /etc/hadoop/conf/core-site.xml.wasb-template /etc/hadoop/conf/core-site.xml\n+exec_in_hadoop_master_container sed -i \\\n+  -e \"s|%WASB_ACCESS_KEY%|${WASB_ACCESS_KEY}|g\" \\\n+  -e \"s|%WASB_ACCOUNT%|${WASB_ACCOUNT}|g\" \\\n+  /etc/hadoop/conf/core-site.xml\n+\n+# create test table\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_v2/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /tmp/test_table.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"CREATE EXTERNAL TABLE presto_test_external_fs(t_bigint bigint) LOCATION '${table_path}'\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='1')\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header_and_footer/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header_and_footer.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header_and_footer(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='2', 'skip.footer.line.count'='2')\"\n+\n+stop_unnecessary_hadoop_services\n+\n+# run product tests", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTU2MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366225560", "bodyText": "I guess it is not needed.", "author": "kokosing", "createdAt": "2020-01-14T09:19:33Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}/common.sh\"\n+\n+start_docker_containers\n+\n+# insert Azure credentials\n+exec_in_hadoop_master_container cp /etc/hadoop/conf/core-site.xml.wasb-template /etc/hadoop/conf/core-site.xml\n+exec_in_hadoop_master_container sed -i \\\n+  -e \"s|%WASB_ACCESS_KEY%|${WASB_ACCESS_KEY}|g\" \\\n+  -e \"s|%WASB_ACCOUNT%|${WASB_ACCOUNT}|g\" \\\n+  /etc/hadoop/conf/core-site.xml\n+\n+# create test table\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_v2/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /tmp/test_table.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"CREATE EXTERNAL TABLE presto_test_external_fs(t_bigint bigint) LOCATION '${table_path}'\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='1')\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header_and_footer/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header_and_footer.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header_and_footer(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='2', 'skip.footer.line.count'='2')\"\n+\n+stop_unnecessary_hadoop_services", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNTg0Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366225847", "bodyText": "same comments", "author": "kokosing", "createdAt": "2020-01-14T09:20:15Z", "path": "presto-hive-hadoop2/bin/run_hive_adl_tests.sh", "diffHunk": "@@ -0,0 +1,61 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. ${BASH_SOURCE%/*}/common.sh\n+\n+start_docker_containers\n+\n+# insert Azure credentials\n+exec_in_hadoop_master_container cp /etc/hadoop/conf/core-site.xml.adl-template /etc/hadoop/conf/core-site.xml\n+exec_in_hadoop_master_container sed -i \\\n+  -e \"s|%ADL_CLIENT_ID%|${ADL_CLIENT_ID}|g\" \\\n+  -e \"s|%ADL_CREDENTIAL%|${ADL_CREDENTIAL}|g\" \\\n+  -e \"s|%ADL_REFRESH_URL%|${ADL_REFRESH_URL}|g\" \\\n+  /etc/hadoop/conf/core-site.xml\n+\n+# create test table\n+table_path=\"adl://${ADL_NAME}.azuredatalakestore.net/presto_test_external_fs_2/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /tmp/test_table.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"CREATE EXTERNAL TABLE presto_test_external_fs(t_bigint bigint) LOCATION '${table_path}'\"\n+\n+table_path=\"adl://${ADL_NAME}.azuredatalakestore.net/presto_test_external_fs_with_header/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='1')\"\n+\n+table_path=\"adl://${ADL_NAME}.azuredatalakestore.net/presto_test_external_fs_with_header_and_footer/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header_and_footer.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header_and_footer(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='2', 'skip.footer.line.count'='2')\"\n+\n+stop_unnecessary_hadoop_services\n+\n+# run product tests\n+pushd $PROJECT_ROOT\n+set +e\n+./mvnw -B -pl presto-hive-hadoop2 test -P test-hive-hadoop2-adl \\\n+  -DHADOOP_USER_NAME=hive \\\n+  -Dhive.hadoop2.metastoreHost=localhost \\\n+  -Dhive.hadoop2.metastorePort=9083 \\\n+  -Dhive.hadoop2.databaseName=default \\\n+  -Dhive.hadoop2.adl-name=${ADL_NAME} \\\n+  -Dhive.hadoop2.adl-client-id=${ADL_CLIENT_ID} \\\n+  -Dhive.hadoop2.adl-credential=${ADL_CREDENTIAL} \\\n+  -Dhive.hadoop2.adl-refresh-url=${ADL_REFRESH_URL}\n+EXIT_CODE=$?\n+set -e\n+popd\n+\n+cleanup_docker_containers", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNjA5MQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366226091", "bodyText": "does it belong to this commit?", "author": "kokosing", "createdAt": "2020-01-14T09:20:42Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemWasb.java", "diffHunk": "@@ -16,7 +16,6 @@\n import com.google.common.collect.ImmutableSet;\n import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n-import io.prestosql.plugin.hive.ConfigurationInitializer;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjIyNjUzNQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366226535", "bodyText": "maybe it should be squashed with some previous commits?", "author": "kokosing", "createdAt": "2020-01-14T09:21:36Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -44,6 +44,9 @@ pushd $PROJECT_ROOT\n set +e\n ./mvnw -B -pl presto-hive-hadoop2 test -P test-hive-hadoop2-wasb \\\n   -DHADOOP_USER_NAME=hive \\\n+  -Dhive.hadoop2.metastoreHost=localhost \\\n+  -Dhive.hadoop2.metastorePort=9083 \\\n+  -Dhive.hadoop2.databaseName=default \\", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMwNDcxNw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r366304717", "bodyText": "This doesn't source presto-product-tests/conf/product-tests-${{ matrix.config }}.sh and probably it doesn't need to.\nMaybe move out of hive-tests job to eg hive-tests-azure that's not matrix.\n@electrum opinion?", "author": "findepi", "createdAt": "2020-01-14T12:14:00Z", "path": ".github/workflows/ci-tests.yml", "diffHunk": "@@ -42,6 +42,54 @@ jobs:\n             source presto-product-tests/conf/product-tests-${{ matrix.config }}.sh &&\n               presto-hive-hadoop2/bin/run_hive_s3_tests.sh\n           fi\n+      - name: Run Azure Wasb Tests\n+        env:\n+            WASB_CONTAINER: \"${WASB_TESTS_WASB_CONTAINER}\"\n+            WASB_ACCOUNT: \"${WASB_TESTS_WASB_ACCOUNT}\"\n+            WASB_ACCESS_KEY: \"${WASB_TESTS_WASB_ACCESS_KEY}\"\n+        run: |\n+            if [ \"${WASB_CONTAINER}\" != \"\" ]; then\n+              presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2Mjc0MQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368162741", "bodyText": "i think this should be javax.validation. Let's\nhttps://github.com/prestosql/presto/blob/6d4272abaa2446ae849ab7c940adb1f20a4ab2a4/presto-main/src/main/java/io/prestosql/sql/analyzer/FeaturesConfig.java#L669-L673", "author": "findepi", "createdAt": "2020-01-17T22:29:18Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/HiveAzureConfig.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import io.airlift.configuration.Config;\n+import io.airlift.configuration.ConfigSecuritySensitive;\n+\n+import javax.annotation.PostConstruct;\n+\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n+\n+public class HiveAzureConfig\n+{\n+    private String wasbStorageAccount;\n+    private String wasbAccessKey;\n+\n+    @PostConstruct\n+    public void validate()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2MzU4MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368163580", "bodyText": "or do the check in PrestoAzureConfigurationInitializer ctor", "author": "findepi", "createdAt": "2020-01-17T22:32:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2Mjc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2MzQ0NQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368163445", "bodyText": "i'd wrap .addBinding().to(..).in(..) on one line, but i know @kokosing likes these cascades... no change requested", "author": "findepi", "createdAt": "2020-01-17T22:32:00Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/HiveAzureModule.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Scopes;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.prestosql.plugin.hive.ConfigurationInitializer;\n+\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class HiveAzureModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        newSetBinder(binder, ConfigurationInitializer.class)\n+                .addBinding()\n+                .to(PrestoAzureConfigurationInitializer.class)\n+                .in(Scopes.SINGLETON);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMTk0Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368311947", "bodyText": "The wrapping seems gratuitous to me, but a good compromise is\nnewSetBinder(binder, ConfigurationInitializer.class).addBinding()\n        .to(PrestoAzureConfigurationInitializer.class).in(Scopes.SINGLETON);", "author": "electrum", "createdAt": "2020-01-19T18:17:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2MzQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NDA3MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368164070", "bodyText": "unused?", "author": "findepi", "createdAt": "2020-01-17T22:34:36Z", "path": "pom.xml", "diffHunk": "@@ -440,6 +440,12 @@\n                 <version>3.2.0-5</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>org.apache.hadoop</groupId>\n+                <artifactId>hadoop-azure-datalake</artifactId>\n+                <version>3.2.0</version>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NDIyMA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368164220", "bodyText": "separate lines", "author": "findepi", "createdAt": "2020-01-17T22:35:15Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAdl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAdl\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String dataLakeName;\n+    private String clientId;\n+    private String credential;\n+    private String refreshUrl;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.adl-name\",\n+            \"hive.hadoop2.adl-client-id\",\n+            \"hive.hadoop2.adl-credential\",\n+            \"hive.hadoop2.adl-refresh-url\"})\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String dataLakeName, String clientId, String credential, String refreshUrl)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(dataLakeName), \"expected non empty dataLakeName\");\n+        checkArgument(!isNullOrEmpty(clientId), \"expected non empty clientId\");\n+        checkArgument(!isNullOrEmpty(credential), \"expected non empty credential\");\n+        checkArgument(!isNullOrEmpty(refreshUrl), \"expected non empty refreshUrl\");\n+\n+        this.dataLakeName = dataLakeName;\n+        this.clientId = clientId;\n+        this.credential = credential;\n+        this.refreshUrl = refreshUrl;\n+\n+        super.setup(host, port, databaseName, false, createHdfsConfiguration());\n+    }\n+\n+    private HdfsConfiguration createHdfsConfiguration()\n+    {\n+        ConfigurationInitializer azureConfig = new PrestoAzureConfigurationInitializer(new HiveAzureConfig()\n+                .setAdlClientId(clientId)\n+                .setAdlCredential(credential)\n+                .setAdlRefreshUrl(refreshUrl));\n+        return new HiveHdfsConfiguration(new HdfsConfigurationInitializer(new HdfsConfig(), ImmutableSet.of(azureConfig)), ImmutableSet.of());\n+    }\n+\n+    @Override\n+    protected Path getBasePath()\n+    {\n+        return new Path(format(\"adl://%s.azuredatalakestore.net/\", dataLakeName));\n+    }\n+\n+    @Override @Test", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NDU5Mw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368164593", "bodyText": "the message refers to adlAccessTokenProviderType setting which seems not to exist\nat the point of this commit, the AdlAccessTokenProviderType has just one option and is unused, so can be removed", "author": "findepi", "createdAt": "2020-01-17T22:36:34Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/HiveAzureConfig.java", "diffHunk": "@@ -36,6 +39,11 @@ public void validate()\n                     !isNullOrEmpty(wasbAccessKey) && !isNullOrEmpty(wasbStorageAccount),\n                     \"If one of wasbAccessKey, wasbStorageAccount is set, both must be set\");\n         }\n+        if (adlClientId != null || adlCredential != null || adlRefreshUrl != null) {\n+            checkState(\n+                    !isNullOrEmpty(adlClientId) && !isNullOrEmpty(adlCredential) && !isNullOrEmpty(adlRefreshUrl),\n+                    \"If one of adlClientId, adlCredential, adlRefreshUrl, all must be set for adlAccessTokenProviderType=CLIENT_CREDENTIAL\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NTAzMA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368165030", "bodyText": "always #!/usr/bin/env bash unless something is run in container, then #!/bin/bash", "author": "findepi", "createdAt": "2020-01-17T22:38:13Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_with_azure_tests.sh", "diffHunk": "@@ -0,0 +1,17 @@\n+#!/bin/bash", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NTMzMg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368165332", "bodyText": "is this script redundant over presto-hive-hadoop2/bin/run_hive_wasb_tests.sh?", "author": "findepi", "createdAt": "2020-01-17T22:39:12Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_with_azure_tests.sh", "diffHunk": "@@ -0,0 +1,17 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}\"/common.sh\n+\n+pushd $PROJECT_ROOT\n+\n+./mvnw -B -pl presto-hive-hadoop2 test -P test-hive-hadoop2-wasb \\", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NTYzNA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368165634", "bodyText": "what's the difference between this script and presto-hive-hadoop2/bin/run_hive_wasb_with_azure_tests.sh?", "author": "findepi", "createdAt": "2020-01-17T22:40:14Z", "path": "presto-hive-hadoop2/bin/run_hive_with_azure_tests.sh", "diffHunk": "@@ -0,0 +1,16 @@\n+#!/bin/bash\n+\n+set -euo pipefail -x\n+\n+. ${BASH_SOURCE%/*}/common.sh\n+\n+cd ${PROJECT_ROOT}\n+\n+./mvnw -B -pl presto-hive-hadoop2 test -P test-hive-hadoop2-hdinsight \\", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3ODYzMw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368178633", "bodyText": "This one just runs TestHiveClientHdinsight and run_hive_wasb_with_azure_tests runs TestHiveFileSystemWasb. The profile is defined in the pom", "author": "anusudarsan", "createdAt": "2020-01-17T23:36:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NTYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NTg3NA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368165874", "bodyText": "this property does not exist. can you try reenable the test?", "author": "findepi", "createdAt": "2020-01-17T22:41:11Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveClientHdinsight.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n+\n+public class TestHiveClientHdinsight\n+        extends AbstractTestHive\n+{\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.timeZone\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\",\n+    })\n+    @BeforeClass\n+    public void initialize(String host, int port, String databaseName, String timeZone, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(timeZone), \"Expected non empty timeZone\");\n+        checkArgument(!isNullOrEmpty(account), \"expected non empty account\");\n+        checkArgument(!isNullOrEmpty(accessKey), \"expected non empty accessKey\");\n+\n+        this.account = account;\n+        this.accessKey = accessKey;\n+\n+        setup(host, port, databaseName, timeZone);\n+    }\n+\n+    @Override\n+    protected HiveHdfsConfiguration createTestHdfsConfiguration()\n+    {\n+        ConfigurationInitializer wasbConfig = new PrestoAzureConfigurationInitializer(new HiveAzureConfig()\n+                .setWasbAccessKey(accessKey)\n+                .setWasbStorageAccount(account));\n+        return new HiveHdfsConfiguration(new HdfsConfigurationInitializer(new HdfsConfig(), ImmutableSet.of(wasbConfig)), ImmutableSet.of());\n+    }\n+\n+    @Override\n+    public void testBucketedTableDoubleFloat()\n+    {\n+        // requires `hive.empty-bucketed-partitions.enabled` to be set", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NjI4Ng==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368166286", "bodyText": "Since you also update io.prestosql.plugin.hive.HiveTestUtils#createTestHdfsEnvironment (IMO: good), why do you need createTestHdfsConfiguration here? Isn't HDFS_ENVIRONMENT const good enough?\n(also, HDFS_ENVIRONMENT is used elsewhere in the class, so we better consistently use one hdfs env everywhere)", "author": "findepi", "createdAt": "2020-01-17T22:42:44Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/AbstractTestHive.java", "diffHunk": "@@ -791,6 +798,18 @@ protected final void setup(String databaseName, HiveConfig hiveConfig, HiveMetas\n                 new GenericHiveRecordCursorProvider(hdfsEnvironment, hiveConfig));\n     }\n \n+    protected HdfsConfiguration createTestHdfsConfiguration()\n+    {\n+        return new HiveHdfsConfiguration(\n+                new HdfsConfigurationInitializer(\n+                        new HdfsConfig(),\n+                        ImmutableSet.of(\n+                                new PrestoS3ConfigurationInitializer(new HiveS3Config()),\n+                                new GoogleGcsConfigurationInitializer(new HiveGcsConfig()),\n+                                new PrestoAzureConfigurationInitializer(new HiveAzureConfig()))),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODIwMTMxMA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368201310", "bodyText": "I guess this was done to override the hdfsEnvironment in TestHiveClientHdinsight https://github.com/prestosql/presto/pull/2494/files#diff-801aff1af8a0e634a6ab2c26d6579de3R55", "author": "anusudarsan", "createdAt": "2020-01-18T02:51:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NjI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2Njg3Ng==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368166876", "bodyText": "\"hive.hadoop2.azure-wasb-access-key\",\\n   })", "author": "findepi", "createdAt": "2020-01-17T22:44:54Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAbfs.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.AbstractTestHive.Transaction;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import io.prestosql.spi.connector.ColumnMetadata;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKETED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKET_COUNT_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.EXTERNAL_LOCATION_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_FOOTER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_HEADER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SORTED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.STORAGE_FORMAT_PROPERTY;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAbfs\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"})", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NzIxNg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368167216", "bodyText": "nit: i was able to remove _v2 suffix in e364b6b\n(no change requested now)", "author": "findepi", "createdAt": "2020-01-17T22:46:12Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAbfs.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.AbstractTestHive.Transaction;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import io.prestosql.spi.connector.ColumnMetadata;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKETED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKET_COUNT_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.EXTERNAL_LOCATION_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_FOOTER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_HEADER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SORTED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.STORAGE_FORMAT_PROPERTY;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAbfs\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"})\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String container, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(container), \"expected non empty container\");\n+        checkArgument(!isNullOrEmpty(account), \"expected non empty account\");\n+        checkArgument(!isNullOrEmpty(accessKey), \"expected non empty accessKey\");\n+\n+        this.container = container;\n+        this.account = account;\n+        this.accessKey = accessKey;\n+\n+        super.setup(host, port, databaseName, false, createHdfsConfiguration());\n+    }\n+\n+    @Override\n+    protected void onSetupComplete()\n+    {\n+        ensureTableExists(table, \"presto_test_external_fs_v2\", ImmutableMap.of());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2NzI0Ng==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368167246", "bodyText": "two lines", "author": "findepi", "createdAt": "2020-01-17T22:46:21Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAbfs.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.AbstractTestHive.Transaction;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import io.prestosql.spi.connector.ColumnMetadata;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKETED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKET_COUNT_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.EXTERNAL_LOCATION_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_FOOTER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_HEADER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SORTED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.STORAGE_FORMAT_PROPERTY;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAbfs\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"})\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String container, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(container), \"expected non empty container\");\n+        checkArgument(!isNullOrEmpty(account), \"expected non empty account\");\n+        checkArgument(!isNullOrEmpty(accessKey), \"expected non empty accessKey\");\n+\n+        this.container = container;\n+        this.account = account;\n+        this.accessKey = accessKey;\n+\n+        super.setup(host, port, databaseName, false, createHdfsConfiguration());\n+    }\n+\n+    @Override\n+    protected void onSetupComplete()\n+    {\n+        ensureTableExists(table, \"presto_test_external_fs_v2\", ImmutableMap.of());\n+\n+        ensureTableExists(tableWithHeader, \"presto_test_external_fs_with_header\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 1));\n+        ensureTableExists(tableWithHeaderAndFooter, \"presto_test_external_fs_with_header_and_footer\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 2, SKIP_FOOTER_LINE_COUNT, 2));\n+    }\n+\n+    private void ensureTableExists(SchemaTableName table, String tableDirectoryName, Map<String, Object> tableProperties)\n+    {\n+        try (Transaction transaction = newTransaction()) {\n+            ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(\n+                    table,\n+                    ImmutableList.of(new ColumnMetadata(\"t_bigint\", BIGINT)),\n+                    ImmutableMap.<String, Object>builder()\n+                            .putAll(tableProperties)\n+                            .put(STORAGE_FORMAT_PROPERTY, HiveStorageFormat.TEXTFILE)\n+                            .put(EXTERNAL_LOCATION_PROPERTY, getBasePath().toString() + \"/\" + tableDirectoryName)\n+                            .put(BUCKET_COUNT_PROPERTY, 0)\n+                            .put(BUCKETED_BY_PROPERTY, ImmutableList.of())\n+                            .put(SORTED_BY_PROPERTY, ImmutableList.of())\n+                            .build());\n+            if (!transaction.getMetadata().listTables(newSession(), Optional.of(table.getSchemaName())).contains(table)) {\n+                transaction.getMetadata().createTable(newSession(), tableMetadata, false);\n+            }\n+            transaction.commit();\n+\n+            // Hack to work around the metastore not being configured for S3 or other FS.\n+            // The metastore tries to validate the location when creating the\n+            // table, which fails without explicit configuration for file system.\n+            // We work around that by using a dummy location when creating the\n+            // table and update it here to the correct location.\n+            metastoreClient.updateTableLocation(\n+                    database,\n+                    table.getTableName(),\n+                    getBasePath().toString() + \"/\" + tableDirectoryName);\n+        }\n+    }\n+\n+    private HdfsConfiguration createHdfsConfiguration()\n+    {\n+        ConfigurationInitializer azureConfig = new PrestoAzureConfigurationInitializer(new HiveAzureConfig()\n+                .setAbfsAccessKey(accessKey)\n+                .setAbfsStorageAccount(account));\n+        return new HiveHdfsConfiguration(new HdfsConfigurationInitializer(new HdfsConfig(), ImmutableSet.of(azureConfig)), ImmutableSet.of());\n+    }\n+\n+    @Override\n+    protected Path getBasePath()\n+    {\n+        return new Path(format(\"abfs://%s@%s.dfs.core.windows.net/\", container, account));\n+    }\n+\n+    @Override @Test", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0NjIyMA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368246220", "bodyText": "can we remove this enum?", "author": "findepi", "createdAt": "2020-01-18T20:15:33Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/AdlAccessTokenProviderType.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+public enum AdlAccessTokenProviderType", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzM2MQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313361", "bodyText": "This doesn't seem to be used anywhere", "author": "electrum", "createdAt": "2020-01-19T18:44:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0NjIyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0NjM1MQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368246351", "bodyText": "HDFS_ENVIRONMENT is also used in the first setup method (the method calls the this one).\nI'd suggest\n\nmove the new initialization of the hdfsEnvironment field to the first setup method\nuse hdfsEnvironment instead of HDFS_ENVIRONMENT everywhere", "author": "findepi", "createdAt": "2020-01-18T20:18:46Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/AbstractTestHive.java", "diffHunk": "@@ -726,7 +733,7 @@ protected final void setup(String databaseName, HiveConfig hiveConfig, HiveMetas\n \n         metastoreClient = hiveMetastore;\n         HivePartitionManager partitionManager = new HivePartitionManager(hiveConfig);\n-        hdfsEnvironment = HDFS_ENVIRONMENT;\n+        hdfsEnvironment = new HdfsEnvironment(createTestHdfsConfiguration(), new HdfsConfig(), new NoHdfsAuthentication());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMTg0Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368311847", "bodyText": "Remove this since it's already in HiveAzureModule", "author": "electrum", "createdAt": "2020-01-19T18:16:03Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveModule.java", "diffHunk": "@@ -75,6 +76,7 @@ public void configure(Binder binder)\n         binder.bind(DirectoryLister.class).to(CachingDirectoryLister.class).in(Scopes.SINGLETON);\n         configBinder(binder).bindConfig(HiveConfig.class);\n         configBinder(binder).bindConfig(HdfsConfig.class);\n+        configBinder(binder).bindConfig(HiveAzureConfig.class);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjA4Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312087", "bodyText": "This should be checkArgument since it is validating the argument of the method. Also, it might be clearer to split this up\ncheckArgument(\n        wasbAccessKey().isPresent() && !wasbAccessKey.get().isEmpty(),\n        \"wasb-storage-account is set, but wasb-access-key is not\");\ncheckArgument(\n        wasbStorageAccount.isPresent() && !wasbStorageAccount.get().isEmpty()\n        \"wasb-access-key is set, but wasb-storage-account is not\");", "author": "electrum", "createdAt": "2020-01-19T18:20:33Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/PrestoAzureConfigurationInitializer.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.azure;\n+\n+import io.prestosql.plugin.hive.ConfigurationInitializer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static java.lang.String.format;\n+\n+public class PrestoAzureConfigurationInitializer\n+        implements ConfigurationInitializer\n+{\n+    private final Optional<String> wasbAccessKey;\n+    private final Optional<String> wasbStorageAccount;\n+\n+    @Inject\n+    public PrestoAzureConfigurationInitializer(HiveAzureConfig hiveAzureConfig)\n+    {\n+        this.wasbAccessKey = hiveAzureConfig.getWasbAccessKey();\n+        this.wasbStorageAccount = hiveAzureConfig.getWasbStorageAccount();\n+        if (wasbAccessKey.isPresent() || wasbStorageAccount.isPresent()) {\n+            checkState(wasbAccessKey.isPresent() && !wasbAccessKey.get().isEmpty() &&", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjQ3OA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312478", "bodyText": "Rather than document the required variables in the commit message, let's list them here", "author": "electrum", "createdAt": "2020-01-19T18:28:19Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,57 @@\n+#!/usr/bin/env bash\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjU1NA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312554", "bodyText": "Do we need to restart the Hive metastore like we do for S3, to pick up the changes in core-site.xml?", "author": "electrum", "createdAt": "2020-01-19T18:29:39Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,57 @@\n+#!/usr/bin/env bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}/common.sh\"\n+\n+cleanup_docker_containers\n+start_docker_containers\n+\n+# insert Azure credentials\n+exec_in_hadoop_master_container cp /etc/hadoop/conf/core-site.xml.wasb-template /etc/hadoop/conf/core-site.xml\n+exec_in_hadoop_master_container sed -i \\\n+    -e \"s|%WASB_ACCESS_KEY%|${WASB_ACCESS_KEY}|g\" \\\n+    -e \"s|%WASB_ACCOUNT%|${WASB_ACCOUNT}|g\" \\\n+    /etc/hadoop/conf/core-site.xml\n+\n+# create test table\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_v2/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /tmp/test_table.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"CREATE EXTERNAL TABLE presto_test_external_fs(t_bigint bigint) LOCATION '${table_path}'\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='1')\"\n+\n+table_path=\"wasb://${WASB_CONTAINER}@${WASB_ACCOUNT}.blob.core.windows.net/presto_test_external_fs_with_header_and_footer/\"\n+exec_in_hadoop_master_container hadoop fs -mkdir -p \"${table_path}\"\n+exec_in_hadoop_master_container hadoop fs -copyFromLocal -f /docker/files/test_table_with_header_and_footer.csv{,.gz,.bz2,.lz4} \"${table_path}\"\n+exec_in_hadoop_master_container /usr/bin/hive -e \"\n+    CREATE EXTERNAL TABLE presto_test_external_fs_with_header_and_footer(t_bigint bigint)\n+    STORED AS TEXTFILE\n+    LOCATION '${table_path}'\n+    TBLPROPERTIES ('skip.header.line.count'='2', 'skip.footer.line.count'='2')\"\n+\n+stop_unnecessary_hadoop_services", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE1MzEwNw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r369153107", "bodyText": "ya good point, done", "author": "anusudarsan", "createdAt": "2020-01-21T17:56:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjY2Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312667", "bodyText": "Should we have a TODO about apply-site-xml-override.sh like in the S3 tests?", "author": "electrum", "createdAt": "2020-01-19T18:31:32Z", "path": "presto-hive-hadoop2/bin/run_hive_wasb_tests.sh", "diffHunk": "@@ -0,0 +1,57 @@\n+#!/usr/bin/env bash\n+\n+set -euo pipefail -x\n+\n+. \"${BASH_SOURCE%/*}/common.sh\"\n+\n+cleanup_docker_containers\n+start_docker_containers\n+\n+# insert Azure credentials", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjc2NQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312765", "bodyText": "Can we remove this and change the script to use /docker/files so that it's consistent with the S3 tests?", "author": "electrum", "createdAt": "2020-01-19T18:33:17Z", "path": "presto-hive-hadoop2/conf/docker-compose.yml", "diffHunk": "@@ -19,3 +19,4 @@ services:\n       - ../../presto-hive/src/test/sql:/docker/sql:ro\n       - ./files:/docker/files:ro\n       - ./files/tez-site.xml:/etc/tez/conf/tez-site.xml:ro\n+      - ./files/core-site.xml.wasb-template:/etc/hadoop/conf/core-site.xml.wasb-template:ro", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMjgzNg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368312836", "bodyText": "Nit: Is the azure- part redundant with wasb (since that's only for Azure)? We don't say aws-s3 just s3", "author": "electrum", "createdAt": "2020-01-19T18:34:47Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemWasb.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.lang.String.format;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemWasb\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzEyMg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313122", "bodyText": "Nit: lowercase \"expected\" to be consistent with the others", "author": "electrum", "createdAt": "2020-01-19T18:40:05Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemWasb.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.lang.String.format;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemWasb\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"\n+    })\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String container, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzE3Ng==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313176", "bodyText": "Same comments as wasb", "author": "electrum", "createdAt": "2020-01-19T18:40:58Z", "path": "presto-hive-hadoop2/bin/run_hive_adl_tests.sh", "diffHunk": "@@ -0,0 +1,60 @@\n+#!/usr/bin/env bash\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzM4OA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313388", "bodyText": "checkArgument", "author": "electrum", "createdAt": "2020-01-19T18:45:23Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/PrestoAzureConfigurationInitializer.java", "diffHunk": "@@ -39,6 +43,15 @@ public PrestoAzureConfigurationInitializer(HiveAzureConfig hiveAzureConfig)\n                             wasbStorageAccount.isPresent() && !wasbStorageAccount.get().isEmpty(),\n                     \"If one of wasbAccessKey, wasbStorageAccount is set, both must be set\");\n         }\n+        this.adlClientId = hiveAzureConfig.getAdlClientId();\n+        this.adlCredential = hiveAzureConfig.getAdlCredential();\n+        this.adlRefreshUrl = hiveAzureConfig.getAdlRefreshUrl();\n+        if (adlClientId.isPresent() || adlCredential.isPresent() || adlRefreshUrl.isPresent()) {\n+            checkState(adlClientId.isPresent() && !adlClientId.get().isEmpty() &&", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzYxMQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313611", "bodyText": "Let's call this TestHiveAzure (and rename related things like Maven profile name)\nThe Client part is legacy/old and Hdinsight is a brand name that is different than what we call it elsewhere in the code. Calling it Azure\u00a0makes it obvious what this is testing.", "author": "electrum", "createdAt": "2020-01-19T18:49:06Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveClientHdinsight.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n+\n+public class TestHiveClientHdinsight", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxMzc2NA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368313764", "bodyText": "Mentioning S3 here seems wrong. Also, I don't think we need this, since we run against our own metastore that we configured to support the file system. The code in AbstractTestHiveFileSystem is legacy from when we tested against a generic metastore. (and could probably be removed)", "author": "electrum", "createdAt": "2020-01-19T18:51:41Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAbfs.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.AbstractTestHive.Transaction;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import io.prestosql.spi.connector.ColumnMetadata;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKETED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKET_COUNT_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.EXTERNAL_LOCATION_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_FOOTER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_HEADER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SORTED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.STORAGE_FORMAT_PROPERTY;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAbfs\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"\n+    })\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String container, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(container), \"expected non empty container\");\n+        checkArgument(!isNullOrEmpty(account), \"expected non empty account\");\n+        checkArgument(!isNullOrEmpty(accessKey), \"expected non empty accessKey\");\n+\n+        this.container = container;\n+        this.account = account;\n+        this.accessKey = accessKey;\n+\n+        super.setup(host, port, databaseName, false, createHdfsConfiguration());\n+    }\n+\n+    @Override\n+    protected void onSetupComplete()\n+    {\n+        ensureTableExists(table, \"presto_test_external_fs_v2\", ImmutableMap.of());\n+\n+        ensureTableExists(tableWithHeader, \"presto_test_external_fs_with_header\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 1));\n+        ensureTableExists(tableWithHeaderAndFooter, \"presto_test_external_fs_with_header_and_footer\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 2, SKIP_FOOTER_LINE_COUNT, 2));\n+    }\n+\n+    private void ensureTableExists(SchemaTableName table, String tableDirectoryName, Map<String, Object> tableProperties)\n+    {\n+        try (Transaction transaction = newTransaction()) {\n+            ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(\n+                    table,\n+                    ImmutableList.of(new ColumnMetadata(\"t_bigint\", BIGINT)),\n+                    ImmutableMap.<String, Object>builder()\n+                            .putAll(tableProperties)\n+                            .put(STORAGE_FORMAT_PROPERTY, HiveStorageFormat.TEXTFILE)\n+                            .put(EXTERNAL_LOCATION_PROPERTY, getBasePath().toString() + \"/\" + tableDirectoryName)\n+                            .put(BUCKET_COUNT_PROPERTY, 0)\n+                            .put(BUCKETED_BY_PROPERTY, ImmutableList.of())\n+                            .put(SORTED_BY_PROPERTY, ImmutableList.of())\n+                            .build());\n+            if (!transaction.getMetadata().listTables(newSession(), Optional.of(table.getSchemaName())).contains(table)) {\n+                transaction.getMetadata().createTable(newSession(), tableMetadata, false);\n+            }\n+            transaction.commit();\n+\n+            // Hack to work around the metastore not being configured for S3 or other FS.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDczMg==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314732", "bodyText": "This test is long and I'd rather not fork it, since it's not obvious what is different. Let's add an instance check in the base method\n// rename foo.txt to foo.txt when foo.txt exists\nassertEquals(fs.rename(path, path), !(fs instanceof AzureBlobFileSystem));", "author": "electrum", "createdAt": "2020-01-19T19:07:58Z", "path": "presto-hive-hadoop2/src/test/java/io/prestosql/plugin/hive/TestHiveFileSystemAbfs.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.hive.AbstractTestHive.Transaction;\n+import io.prestosql.plugin.hive.azure.HiveAzureConfig;\n+import io.prestosql.plugin.hive.azure.PrestoAzureConfigurationInitializer;\n+import io.prestosql.spi.connector.ColumnMetadata;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Parameters;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.UUID;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKETED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.BUCKET_COUNT_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.EXTERNAL_LOCATION_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_FOOTER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SKIP_HEADER_LINE_COUNT;\n+import static io.prestosql.plugin.hive.HiveTableProperties.SORTED_BY_PROPERTY;\n+import static io.prestosql.plugin.hive.HiveTableProperties.STORAGE_FORMAT_PROPERTY;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+import static org.testng.util.Strings.isNullOrEmpty;\n+\n+public class TestHiveFileSystemAbfs\n+        extends AbstractTestHiveFileSystem\n+{\n+    private String container;\n+    private String account;\n+    private String accessKey;\n+\n+    @Parameters({\n+            \"hive.hadoop2.metastoreHost\",\n+            \"hive.hadoop2.metastorePort\",\n+            \"hive.hadoop2.databaseName\",\n+            \"hive.hadoop2.azure-wasb-container\",\n+            \"hive.hadoop2.azure-wasb-account\",\n+            \"hive.hadoop2.azure-wasb-access-key\"\n+    })\n+    @BeforeClass\n+    public void setup(String host, int port, String databaseName, String container, String account, String accessKey)\n+    {\n+        checkArgument(!isNullOrEmpty(host), \"expected non empty host\");\n+        checkArgument(!isNullOrEmpty(databaseName), \"Expected non empty databaseName\");\n+        checkArgument(!isNullOrEmpty(container), \"expected non empty container\");\n+        checkArgument(!isNullOrEmpty(account), \"expected non empty account\");\n+        checkArgument(!isNullOrEmpty(accessKey), \"expected non empty accessKey\");\n+\n+        this.container = container;\n+        this.account = account;\n+        this.accessKey = accessKey;\n+\n+        super.setup(host, port, databaseName, false, createHdfsConfiguration());\n+    }\n+\n+    @Override\n+    protected void onSetupComplete()\n+    {\n+        ensureTableExists(table, \"presto_test_external_fs_v2\", ImmutableMap.of());\n+\n+        ensureTableExists(tableWithHeader, \"presto_test_external_fs_with_header\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 1));\n+        ensureTableExists(tableWithHeaderAndFooter, \"presto_test_external_fs_with_header_and_footer\", ImmutableMap.of(SKIP_HEADER_LINE_COUNT, 2, SKIP_FOOTER_LINE_COUNT, 2));\n+    }\n+\n+    private void ensureTableExists(SchemaTableName table, String tableDirectoryName, Map<String, Object> tableProperties)\n+    {\n+        try (Transaction transaction = newTransaction()) {\n+            ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(\n+                    table,\n+                    ImmutableList.of(new ColumnMetadata(\"t_bigint\", BIGINT)),\n+                    ImmutableMap.<String, Object>builder()\n+                            .putAll(tableProperties)\n+                            .put(STORAGE_FORMAT_PROPERTY, HiveStorageFormat.TEXTFILE)\n+                            .put(EXTERNAL_LOCATION_PROPERTY, getBasePath().toString() + \"/\" + tableDirectoryName)\n+                            .put(BUCKET_COUNT_PROPERTY, 0)\n+                            .put(BUCKETED_BY_PROPERTY, ImmutableList.of())\n+                            .put(SORTED_BY_PROPERTY, ImmutableList.of())\n+                            .build());\n+            if (!transaction.getMetadata().listTables(newSession(), Optional.of(table.getSchemaName())).contains(table)) {\n+                transaction.getMetadata().createTable(newSession(), tableMetadata, false);\n+            }\n+            transaction.commit();\n+\n+            // Hack to work around the metastore not being configured for S3 or other FS.\n+            // The metastore tries to validate the location when creating the\n+            // table, which fails without explicit configuration for file system.\n+            // We work around that by using a dummy location when creating the\n+            // table and update it here to the correct location.\n+            metastoreClient.updateTableLocation(\n+                    database,\n+                    table.getTableName(),\n+                    getBasePath().toString() + \"/\" + tableDirectoryName);\n+        }\n+    }\n+\n+    private HdfsConfiguration createHdfsConfiguration()\n+    {\n+        ConfigurationInitializer azureConfig = new PrestoAzureConfigurationInitializer(new HiveAzureConfig()\n+                .setAbfsAccessKey(accessKey)\n+                .setAbfsStorageAccount(account));\n+        return new HiveHdfsConfiguration(new HdfsConfigurationInitializer(new HdfsConfig(), ImmutableSet.of(azureConfig)), ImmutableSet.of());\n+    }\n+\n+    @Override\n+    protected Path getBasePath()\n+    {\n+        return new Path(format(\"abfs://%s@%s.dfs.core.windows.net/\", container, account));\n+    }\n+\n+    @Override\n+    @Test\n+    public void testRename()\n+            throws Exception\n+    {\n+        Path basePath = new Path(getBasePath(), UUID.randomUUID().toString());\n+        FileSystem fs = hdfsEnvironment.getFileSystem(TESTING_CONTEXT, basePath);\n+        assertFalse(fs.exists(basePath));\n+\n+        // create file foo.txt\n+        Path path = new Path(basePath, \"foo.txt\");\n+        assertTrue(fs.createNewFile(path));\n+        assertTrue(fs.exists(path));\n+\n+        // rename foo.txt to bar.txt when bar does not exist\n+        Path newPath = new Path(basePath, \"bar.txt\");\n+        assertFalse(fs.exists(newPath));\n+        assertTrue(fs.rename(path, newPath));\n+        assertFalse(fs.exists(path));\n+        assertTrue(fs.exists(newPath));\n+\n+        // rename foo.txt to foo.txt when foo.txt does not exist\n+        assertFalse(fs.rename(path, path));\n+\n+        // create file foo.txt and rename to existing bar.txt\n+        assertTrue(fs.createNewFile(path));\n+        assertFalse(fs.rename(path, newPath));\n+\n+        // rename foo.txt to foo.txt when foo.txt exists\n+        // This returns true in AzureBlobFileSystem", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDc0Nw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314747", "bodyText": "checkArgument", "author": "electrum", "createdAt": "2020-01-19T19:08:12Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/PrestoAzureConfigurationInitializer.java", "diffHunk": "@@ -43,6 +46,15 @@ public PrestoAzureConfigurationInitializer(HiveAzureConfig hiveAzureConfig)\n                             wasbStorageAccount.isPresent() && !wasbStorageAccount.get().isEmpty(),\n                     \"If one of wasbAccessKey, wasbStorageAccount is set, both must be set\");\n         }\n+\n+        this.abfsAccessKey = hiveAzureConfig.getAbfsAccessKey();\n+        this.abfsStorageAccount = hiveAzureConfig.getAbfsStorageAccount();\n+        if (abfsAccessKey.isPresent() || abfsStorageAccount.isPresent()) {\n+            checkState(abfsAccessKey.isPresent() && !abfsAccessKey.get().isEmpty() &&", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDc2OQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314769", "bodyText": "If either abfsAccessKey or abfsStorageAccount is set, both must be set", "author": "electrum", "createdAt": "2020-01-19T19:08:44Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/azure/PrestoAzureConfigurationInitializer.java", "diffHunk": "@@ -43,6 +46,15 @@ public PrestoAzureConfigurationInitializer(HiveAzureConfig hiveAzureConfig)\n                             wasbStorageAccount.isPresent() && !wasbStorageAccount.get().isEmpty(),\n                     \"If one of wasbAccessKey, wasbStorageAccount is set, both must be set\");\n         }\n+\n+        this.abfsAccessKey = hiveAzureConfig.getAbfsAccessKey();\n+        this.abfsStorageAccount = hiveAzureConfig.getAbfsStorageAccount();\n+        if (abfsAccessKey.isPresent() || abfsStorageAccount.isPresent()) {\n+            checkState(abfsAccessKey.isPresent() && !abfsAccessKey.get().isEmpty() &&\n+                            abfsStorageAccount.isPresent() && !abfsStorageAccount.get().isEmpty(),\n+                    \"If one of abfsAccessKey, abfsStorageAccount is set, both must be set\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDgxMw==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314813", "bodyText": "Format like this so that it's clear this is intentionally empty\nprotected void onSetupComplete() {}", "author": "electrum", "createdAt": "2020-01-19T19:09:21Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/AbstractTestHiveFileSystem.java", "diffHunk": "@@ -153,6 +153,10 @@ public void tearDown()\n \n     protected abstract Path getBasePath();\n \n+    protected void onSetupComplete()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDk2MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314960", "bodyText": "This is already shaded in Hadoop and should not be necessary", "author": "electrum", "createdAt": "2020-01-19T19:11:41Z", "path": "presto-hive-hadoop2/pom.xml", "diffHunk": "@@ -65,6 +65,43 @@\n             <scope>provided</scope>\n         </dependency>\n \n+        <!-- For ADLS Gen2 in HDInsight -->\n+        <dependency>\n+            <groupId>org.codehaus.jackson</groupId>\n+            <artifactId>jackson-mapper-asl</artifactId>\n+            <version>1.9.13</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.squareup.okhttp</groupId>\n+            <artifactId>okhttp</artifactId>\n+            <version>2.7.5</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <!-- For ADLS Gen1 -->\n+        <dependency>\n+            <groupId>org.apache.commons</groupId>\n+            <artifactId>commons-lang3</artifactId>\n+            <version>3.4</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <!-- For ADLS Gen1 in HDInsight -->\n+        <dependency>\n+            <groupId>com.squareup.okio</groupId>\n+            <artifactId>okio</artifactId>\n+            <version>1.13.0</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-core</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNDk5MQ==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368314991", "bodyText": "This is already shaded in Hadoop and should not be necessary", "author": "electrum", "createdAt": "2020-01-19T19:12:06Z", "path": "presto-hive-hadoop2/pom.xml", "diffHunk": "@@ -65,6 +65,43 @@\n             <scope>provided</scope>\n         </dependency>\n \n+        <!-- For ADLS Gen2 in HDInsight -->\n+        <dependency>\n+            <groupId>org.codehaus.jackson</groupId>\n+            <artifactId>jackson-mapper-asl</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNTAwNA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368315004", "bodyText": "This is already shaded in Hadoop and should not be necessary", "author": "electrum", "createdAt": "2020-01-19T19:12:17Z", "path": "presto-hive-hadoop2/pom.xml", "diffHunk": "@@ -65,6 +65,43 @@\n             <scope>provided</scope>\n         </dependency>\n \n+        <!-- For ADLS Gen2 in HDInsight -->\n+        <dependency>\n+            <groupId>org.codehaus.jackson</groupId>\n+            <artifactId>jackson-mapper-asl</artifactId>\n+            <version>1.9.13</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.squareup.okhttp</groupId>\n+            <artifactId>okhttp</artifactId>\n+            <version>2.7.5</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <!-- For ADLS Gen1 -->\n+        <dependency>\n+            <groupId>org.apache.commons</groupId>\n+            <artifactId>commons-lang3</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODMxNTA0MA==", "url": "https://github.com/trinodb/trino/pull/2494#discussion_r368315040", "bodyText": "We should update the Hadoop shading to include this (and okio) rather than adding it here", "author": "electrum", "createdAt": "2020-01-19T19:12:53Z", "path": "presto-hive-hadoop2/pom.xml", "diffHunk": "@@ -65,6 +65,43 @@\n             <scope>provided</scope>\n         </dependency>\n \n+        <!-- For ADLS Gen2 in HDInsight -->\n+        <dependency>\n+            <groupId>org.codehaus.jackson</groupId>\n+            <artifactId>jackson-mapper-asl</artifactId>\n+            <version>1.9.13</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.squareup.okhttp</groupId>\n+            <artifactId>okhttp</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "f73a7b8e8b985dda5b2d166fd7cd010e52ccaf1e", "url": "https://github.com/trinodb/trino/commit/f73a7b8e8b985dda5b2d166fd7cd010e52ccaf1e", "message": "Add configuration variables for authenticating to Azure Blob", "committedDate": "2020-01-23T20:59:15Z", "type": "commit"}, {"oid": "d77932693af1b33364a02f7329d37c843b0a90e5", "url": "https://github.com/trinodb/trino/commit/d77932693af1b33364a02f7329d37c843b0a90e5", "message": "Setup TestHiveFileSystemWasb\n\nTo run bin/run_hive_wasb_tests.sh you need to first:\nexport WASB_CONTAINER=...\nexport WASB_ACCOUNT=...\nexport WASB_ACCESS_KEY=...", "committedDate": "2020-01-23T20:59:15Z", "type": "commit"}, {"oid": "758a18b9d999bff2190746ba3089236d7caa30f9", "url": "https://github.com/trinodb/trino/commit/758a18b9d999bff2190746ba3089236d7caa30f9", "message": "Add support for Azure ADL file system", "committedDate": "2020-01-23T20:59:15Z", "type": "commit"}, {"oid": "9b5632de1f4436f296172cdaa1d8d1ca012dbca6", "url": "https://github.com/trinodb/trino/commit/9b5632de1f4436f296172cdaa1d8d1ca012dbca6", "message": "Add tests for WASB with Azure usage", "committedDate": "2020-01-23T20:59:15Z", "type": "commit"}, {"oid": "d5b55b2120f2579268c7541164a87307ba66958e", "url": "https://github.com/trinodb/trino/commit/d5b55b2120f2579268c7541164a87307ba66958e", "message": "Add TestHiveClientHdinsight", "committedDate": "2020-01-23T20:59:16Z", "type": "commit"}, {"oid": "feefe2b404d34dc2562947b2f73e4e233546432c", "url": "https://github.com/trinodb/trino/commit/feefe2b404d34dc2562947b2f73e4e233546432c", "message": "Add support for Azure ALDS gen 2 aka abfs", "committedDate": "2020-01-23T20:59:16Z", "type": "commit"}, {"oid": "d375170bb773b43063f66c6669db7476d8427dfa", "url": "https://github.com/trinodb/trino/commit/d375170bb773b43063f66c6669db7476d8427dfa", "message": "Update hadoop shaded version", "committedDate": "2020-01-23T20:59:16Z", "type": "commit"}, {"oid": "d375170bb773b43063f66c6669db7476d8427dfa", "url": "https://github.com/trinodb/trino/commit/d375170bb773b43063f66c6669db7476d8427dfa", "message": "Update hadoop shaded version", "committedDate": "2020-01-23T20:59:16Z", "type": "forcePushed"}]}