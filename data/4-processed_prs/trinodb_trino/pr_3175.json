{"pr_number": 3175, "pr_title": "Optimize parquet gzip decompression", "pr_createdAt": "2020-03-20T13:59:43Z", "pr_url": "https://github.com/trinodb/trino/pull/3175", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTcyMjI0Nw==", "url": "https://github.com/trinodb/trino/pull/3175#discussion_r395722247", "bodyText": "I'd rename this variable to read, and maybe the other one to totalBytesRead to have a clear separation.", "author": "martint", "createdAt": "2020-03-20T15:41:15Z", "path": "presto-parquet/src/main/java/io/prestosql/parquet/ParquetCompressionUtils.java", "diffHunk": "@@ -89,15 +89,26 @@ private static Slice decompressGzip(Slice input, int uncompressedSize)\n             return EMPTY_SLICE;\n         }\n \n-        DynamicSliceOutput sliceOutput = new DynamicSliceOutput(uncompressedSize);\n         byte[] buffer = new byte[uncompressedSize];\n-        try (InputStream gzipInputStream = new GZIPInputStream(input.getInput(), GZIP_BUFFER_SIZE)) {\n-            int bytesRead;\n-            while ((bytesRead = gzipInputStream.read(buffer)) != -1) {\n-                sliceOutput.write(buffer, 0, bytesRead);\n+        int bytesRead = 0;\n+        boolean eos = false;\n+        try (GZIPInputStream gzipInputStream = new GZIPInputStream(input.getInput(), min(GZIP_BUFFER_SIZE, input.length()))) {\n+            int n;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTcyNDYxOA==", "url": "https://github.com/trinodb/trino/pull/3175#discussion_r395724618", "bodyText": "Have you considered using Guava's ByteStreams.read() instead of rolling you own? (it may be tricky due to the error handling below, but worth considering)", "author": "martint", "createdAt": "2020-03-20T15:44:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTcyMjI0Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "d8b431e8308dce0f19c62b3876b3fb26601cc8dc", "url": "https://github.com/trinodb/trino/commit/d8b431e8308dce0f19c62b3876b3fb26601cc8dc", "message": "Optimize parquet gzip decompression\n\nAvoids creating an intermediate buffer and copy from buffer to\nDynamicSliceOutput in parquet gzip decompression. Also avoids\nallocating the full 8k gzip input buffer size when the slice input\nis smaller. Finally, a validation check is added to verify the\nuncompressedSize is correct whereas previously the resulting slice\nwould contain garbage data at the end (likely zeroed memory).", "committedDate": "2020-03-20T16:44:42Z", "type": "commit"}, {"oid": "d8b431e8308dce0f19c62b3876b3fb26601cc8dc", "url": "https://github.com/trinodb/trino/commit/d8b431e8308dce0f19c62b3876b3fb26601cc8dc", "message": "Optimize parquet gzip decompression\n\nAvoids creating an intermediate buffer and copy from buffer to\nDynamicSliceOutput in parquet gzip decompression. Also avoids\nallocating the full 8k gzip input buffer size when the slice input\nis smaller. Finally, a validation check is added to verify the\nuncompressedSize is correct whereas previously the resulting slice\nwould contain garbage data at the end (likely zeroed memory).", "committedDate": "2020-03-20T16:44:42Z", "type": "forcePushed"}]}