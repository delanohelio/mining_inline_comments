{"pr_number": 5762, "pr_title": "kafka connector: optimize Kafka timestamp test case.", "pr_createdAt": "2020-10-31T03:27:25Z", "pr_url": "https://github.com/trinodb/trino/pull/5762", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjAwMA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466000", "bodyText": "Change commit message to\nOptimize Kafka timestamp pushdown test case", "author": "losipiuk", "createdAt": "2020-10-31T07:07:06Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -15,39 +15,46 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.util.concurrent.Futures;\n+import io.airlift.log.Logger;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjI5Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466292", "bodyText": "compute dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult) on a variable and use for both assertions", "author": "losipiuk", "createdAt": "2020-10-31T07:11:14Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjU1MA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466550", "bodyText": "static import format", "author": "losipiuk", "createdAt": "2020-10-31T07:14:35Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njc0Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466742", "bodyText": "Drop dump prefix from dump section headers.\nIt seems 5 is also not needed here.\nAlso I think it would be more readable if capture the result int List<String> and use Joiner.on(\"/n\").join(....) at the end.\nSome vertical whitespace would make this method a bit readable too.", "author": "losipiuk", "createdAt": "2020-10-31T07:17:16Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njc5MQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466791", "bodyText": "maybe buildDebugDumpString", "author": "losipiuk", "createdAt": "2020-10-31T07:18:25Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjgxNA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466814", "bodyText": "I think you can inline this one.", "author": "losipiuk", "createdAt": "2020-10-31T07:18:54Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njk2MA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466960", "bodyText": "I don't understand it. What does it mean?", "author": "losipiuk", "createdAt": "2020-10-31T07:20:41Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(String.format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        List<String> dumpMsgs = new ArrayList<>();\n         Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n         long lastTimeStamp = -1;\n+        // keep silence for start of multi-revoke", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEwNw==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467107", "bodyText": "name variable timestampTestMessageSignatures", "author": "losipiuk", "createdAt": "2020-10-31T07:23:01Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(String.format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        List<String> dumpMsgs = new ArrayList<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzM4OA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467388", "bodyText": "Also use ImmutableList.Builder for collection", "author": "losipiuk", "createdAt": "2020-10-31T07:26:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEyNQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467125", "bodyText": "testMessageSignatures", "author": "losipiuk", "createdAt": "2020-10-31T07:23:27Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzIxOQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467219", "bodyText": "long?", "author": "losipiuk", "createdAt": "2020-10-31T07:24:40Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzIzNA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467234", "bodyText": "I think this comments does not help. Remove maybe.", "author": "losipiuk", "createdAt": "2020-10-31T07:24:53Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzMxMg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467312", "bodyText": "Replace with single remainingRetries", "author": "losipiuk", "createdAt": "2020-10-31T07:25:57Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> dumpMessages)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.dumpMessages = dumpMessages;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getDumpMessages()\n+        {\n+            return dumpMessages;\n+        }\n+    }\n+\n+    private static class RetryAnalyzerForTimestampTest\n+            implements IRetryAnalyzer\n+    {\n+        int count = 1;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTkzOA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815938", "bodyText": "I do not see this change addressed", "author": "losipiuk", "createdAt": "2020-11-02T08:45:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzMxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzM0Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467342", "bodyText": "Rename to FixedCountRetryAnalyzer", "author": "losipiuk", "createdAt": "2020-10-31T07:26:21Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> dumpMessages)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.dumpMessages = dumpMessages;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getDumpMessages()\n+        {\n+            return dumpMessages;\n+        }\n+    }\n+\n+    private static class RetryAnalyzerForTimestampTest", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzQ5Ng==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467496", "bodyText": "If you are breaking parameter list then put each parameter in separate line please.", "author": "losipiuk", "createdAt": "2020-10-31T07:29:10Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzUyNg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467526", "bodyText": "inline", "author": "losipiuk", "createdAt": "2020-10-31T07:29:19Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMTk0MQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515811941", "bodyText": "drop dot from commit message\nWe roughly try to follow https://chris.beams.io/posts/git-commit/", "author": "losipiuk", "createdAt": "2020-11-02T08:38:43Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -13,41 +13,49 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableList;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMzc0Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515813742", "bodyText": "s/dumpMsg/debugDumptString", "author": "losipiuk", "createdAt": "2020-11-02T08:41:53Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMzg5Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515813892", "bodyText": "s/dempMsg/debugDumpString", "author": "losipiuk", "createdAt": "2020-11-02T08:42:09Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNDMyOA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515814328", "bodyText": "nit: this signature seems long enough to justify putting each argument in separate line", "author": "losipiuk", "createdAt": "2020-11-02T08:43:02Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTIzNg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815236", "bodyText": "drop \"dump \" just: \"SQL results:%s\"", "author": "losipiuk", "createdAt": "2020-11-02T08:44:38Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpSqlResult(queryResult.getResult()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTQ1Mw==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815453", "bodyText": "drop \"dump \" just \"Test SQL:%s\"", "author": "losipiuk", "createdAt": "2020-11-02T08:45:03Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNzM1Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515817352", "bodyText": "Please update this comment to be more descriptive here too", "author": "losipiuk", "createdAt": "2020-11-02T08:48:44Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -196,14 +262,18 @@ else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n                                 .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n                     }\n                     // Sleep for a while to ensure different timestamps for different messages..\n-                    Thread.sleep(20);\n+                    Thread.sleep(100);\n+                }\n+                else if (messageNum == TIMESTAMP_TEST_COUNT) {\n+                    // Keep silence to record other messages.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNzg5Mg==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515817892", "bodyText": "maybe just testMessageSignatures", "author": "losipiuk", "createdAt": "2020-11-02T08:49:47Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -187,7 +249,11 @@ private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQue\n                     RecordMetadata r = lastSendFuture.get();\n                     assertTrue(lastTimeStamp != r.timestamp());\n                     lastTimeStamp = r.timestamp();\n-                    if (messageNum == TIMESTAMP_TEST_START_INDEX) {\n+                    timestampTestMessageSignatures.add(format(\"timestamp:%s: partitionId:%s, offset:%s\", r.timestamp(), r.partition(), r.offset()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgyMDg5MQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515820891", "bodyText": "I suggested to inline this one. If you insist on keeping it separate please rename to  buildResultsDebugDumpString", "author": "losipiuk", "createdAt": "2020-11-02T08:55:23Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpSqlResult(queryResult.getResult()));\n+\n+        // dump data in kafka\n+        sql = format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic,\n+                recordMessage.getStartOffset(),\n+                recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpSqlResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpSqlResult(MaterializedResult rows)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0MzE1Nw==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516743157", "bodyText": "There is still dot at the end of commit message :)", "author": "losipiuk", "createdAt": "2020-11-03T15:15:50Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -13,41 +13,49 @@\n  */", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0MzcwOA==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516743708", "bodyText": "I suggested to rename this one to testMethodSignatures for brevity.", "author": "losipiuk", "createdAt": "2020-11-03T15:16:32Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,128 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String debugDumpString = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        debugDumpString = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String debugDumpString = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic,\n+                                        RecordMessage recordMessage,\n+                                        DistributedQueryRunner queryRunner,\n+                                        String sql,\n+                                        ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"test sql:%s\\n\", sql));\n+        sb.append(\"test sql result:\").append(\"\\n\").append(buildResultsDebugDumpString(queryResult.getResult()));\n+\n+        // dump data in kafka\n+        sql = format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic,\n+                recordMessage.getStartOffset(),\n+                recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"data check result:\").append(\"\\n\").append(buildResultsDebugDumpString(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String buildResultsDebugDumpString(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        ImmutableList.Builder<String> timestampTestMessageSignatures = ImmutableList.builder();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0NDA0NQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516744045", "bodyText": "I suggested to keep just one field remainingRetries here.", "author": "losipiuk", "createdAt": "2020-11-03T15:16:58Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +293,58 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        private final long startOffset;\n+        private final List<String> testMessageSignatures;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> testMessageSignatures)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.testMessageSignatures = testMessageSignatures;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getTestMessageSignatures()\n+        {\n+            return testMessageSignatures;\n+        }\n+    }\n+\n+    private static class FixedCountRetryAnalyzer\n+            implements IRetryAnalyzer\n+    {\n+        int count = 1;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0NDQ0MQ==", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516744441", "bodyText": "frout? What did you mean here?", "author": "losipiuk", "createdAt": "2020-11-03T15:17:31Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -196,14 +266,18 @@ else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n                                 .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n                     }\n                     // Sleep for a while to ensure different timestamps for different messages..\n-                    Thread.sleep(20);\n+                    Thread.sleep(100);\n+                }\n+                else if (messageNum == TIMESTAMP_TEST_COUNT) {\n+                    // Avoid back messages in test has impact frout messages.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5608e1e11fc0f47b1b9124a71b585af0a2855423", "url": "https://github.com/trinodb/trino/commit/5608e1e11fc0f47b1b9124a71b585af0a2855423", "message": "Optimize Kafka timestamp pushdown test case\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-11-06T06:28:24Z", "type": "commit"}, {"oid": "5608e1e11fc0f47b1b9124a71b585af0a2855423", "url": "https://github.com/trinodb/trino/commit/5608e1e11fc0f47b1b9124a71b585af0a2855423", "message": "Optimize Kafka timestamp pushdown test case\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>", "committedDate": "2020-11-06T06:28:24Z", "type": "forcePushed"}]}