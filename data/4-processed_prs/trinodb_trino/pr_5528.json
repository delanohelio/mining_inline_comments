{"pr_number": 5528, "pr_title": "Fix skipping in parquet int64 millis decoder", "pr_createdAt": "2020-10-12T15:06:04Z", "pr_url": "https://github.com/trinodb/trino/pull/5528", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQzNzkxOA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503437918", "bodyText": "its not clear what belongs to TestReader and what to TestParquetReader (same package)", "author": "findepi", "createdAt": "2020-10-12T17:31:54Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0MDM4MA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503440380", "bodyText": "Couldn't come up with a better name, and TestParquetReader is taken (it's the full blown test with all the combinations of readers and writers). This is a targeted test for one type. I could rename it to TestTimestampReader for now -- I'll have to revamp the TestParquetReader for the new parquet reader I'm working on, anyway, as the current one is too rigid to be able to implement the tests I need.", "author": "martint", "createdAt": "2020-10-12T17:37:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQzNzkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc1NzU2NA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503757564", "bodyText": "make it package-private?", "author": "sopel39", "createdAt": "2020-10-13T08:19:42Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetTester.java", "diffHunk": "@@ -569,7 +569,7 @@ private static FileFormat getFileFormat()\n         return OPTIMIZED ? FileFormat.PRESTO_PARQUET : FileFormat.HIVE_PARQUET;\n     }\n \n-    private static void writeParquetColumn(\n+    public static void writeParquetColumn(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA1OTAyMA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504059020", "bodyText": "It's now part of the public API of this class, so package private doesn't seem appropriate.", "author": "martint", "createdAt": "2020-10-13T15:42:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc1NzU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODA0OQ==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808049", "bodyText": "None of these is needed", "author": "findepi", "createdAt": "2020-10-13T09:34:20Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODUzOA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808538", "bodyText": "seems unrelated?", "author": "findepi", "createdAt": "2020-10-13T09:35:05Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTA2OA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809068", "bodyText": "assert firstPage.getPositionCount() > 0", "author": "findepi", "createdAt": "2020-10-13T09:35:48Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTU1Mg==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809552", "bodyText": "We should document what null return value means. \"This method is allowed to return null\" is not sufficient.", "author": "findepi", "createdAt": "2020-10-13T09:36:31Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NzA1Nw==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504067057", "bodyText": "Agreed, but unrelated to this PR", "author": "martint", "createdAt": "2020-10-13T15:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDIxNg==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810216", "bodyText": "add a message, othewise exception coming from here will be hard to understand", "author": "findepi", "createdAt": "2020-10-13T09:37:33Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDczMg==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810732", "bodyText": "Is negative value any special?\nis the range length (2000) special? Can it be eg 20?", "author": "findepi", "createdAt": "2020-10-13T09:38:18Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NjczMQ==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504066731", "bodyText": "No, nothing special about it, other than that it has to be large enough to produce multiple pages. I added some assertions to ensure the assumptions of the test hold.", "author": "martint", "createdAt": "2020-10-13T15:53:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDczMg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMDM1Nw==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504510357", "bodyText": "the shorter, intermediate version that you had\nassertTrue(firstPage.getPositionCount() > 0, \"Expected first page to have at least 1 row\");\n\nproduces equally informative exception message (assuming page.positionCount is non-negative)", "author": "findepi", "createdAt": "2020-10-14T08:50:52Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig());\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+\n+                assertThat(firstPage.getPositionCount())\n+                        .withFailMessage(\"Expected first page to have at least 1 row\")\n+                        .isGreaterThan(0);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTE1OA==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504511158", "bodyText": "TestTimestampColumnReader ?", "author": "findepi", "createdAt": "2020-10-14T08:52:06Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMjk5OQ==", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504512999", "bodyText": "or, if  you reserve that name for unit tests of that class, than maybe TestTimestamp?", "author": "findepi", "createdAt": "2020-10-14T08:54:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTE1OA=="}], "type": "inlineReview"}, {"oid": "87970a6636d483824e76c90e9c72605384890772", "url": "https://github.com/trinodb/trino/commit/87970a6636d483824e76c90e9c72605384890772", "message": "Fix skipping in parquet int64 millis decoder", "committedDate": "2020-10-14T16:10:18Z", "type": "commit"}, {"oid": "87970a6636d483824e76c90e9c72605384890772", "url": "https://github.com/trinodb/trino/commit/87970a6636d483824e76c90e9c72605384890772", "message": "Fix skipping in parquet int64 millis decoder", "committedDate": "2020-10-14T16:10:18Z", "type": "forcePushed"}]}