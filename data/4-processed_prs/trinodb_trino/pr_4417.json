{"pr_number": 4417, "pr_title": "Add Kafka raw encoder", "pr_createdAt": "2020-07-10T15:36:32Z", "pr_url": "https://github.com/trinodb/trino/pull/4417", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDI2OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452950269", "bodyText": "nit: possibly catch NumberFormatException and wrap in exception with clear message. It can throw if value provided does not fit in int.", "author": "losipiuk", "createdAt": "2020-07-10T16:33:18Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDY0Mw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452950643", "bodyText": "can we make it @VisibleForTesting and add unit test for it?", "author": "losipiuk", "createdAt": "2020-07-10T16:34:09Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY3ODkxNA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453678914", "bodyText": "I could test it without @VisibleForTesting annotation just by calling encoder factory create method on columns with different mappings", "author": "charlesjmorgan", "createdAt": "2020-07-13T14:11:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY4OTgxMA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453689810", "bodyText": "but that wouldn't actually test the method", "author": "charlesjmorgan", "createdAt": "2020-07-13T14:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY5NTI3MA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453695270", "bodyText": "Given logic complexity I would probably opt toward directly unit testing method. But if you can make it look nice and readable testing at wider scope. And still be able to excercise all corner cases, it should be fine too.", "author": "losipiuk", "createdAt": "2020-07-13T14:34:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDY0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452954164", "bodyText": "it does not seem to handle case where we are dealing with varchar and end is not explicitly specified.\nWould lenght be 1 then?", "author": "losipiuk", "createdAt": "2020-07-10T16:41:15Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk3MjAyOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452972029", "bodyText": "The way I had the encoders structured before didn't let me 'see' the value of the mapping for the next column so varchar types would need to specify the start and the end every time. Now I think I might be able to dynamically allocate space for varchar types by 'peaking' into the mapping for the next column and using the start index - 1 for the next column as the end index for the varchar column. I could also use VarcharType#getLength if there isn't a next column.", "author": "charlesjmorgan", "createdAt": "2020-07-10T17:17:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA0NjA3MA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453046070", "bodyText": "The documentation we have for the raw mapping states that when the end position is missing,\n\nWhen VARCHAR value is decoded all bytes from start position till the end of the message will be used.\n\nI have not checked the code.", "author": "aalbu", "createdAt": "2020-07-10T19:46:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2MjEyNw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453062127", "bodyText": "that's how the decoder works rn afaik", "author": "charlesjmorgan", "createdAt": "2020-07-10T20:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2ODI1MA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453068250", "bodyText": "So that means that a missing end value is legal only when the field is in the last position.", "author": "aalbu", "createdAt": "2020-07-10T20:43:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDI0NA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453204244", "bodyText": "Yes, I will have to figure out a way to allocate more space in the ByteBuffer if end is not specified for VARCHAR.", "author": "charlesjmorgan", "createdAt": "2020-07-11T15:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452954559", "bodyText": "this can throw value.getBytes() may return shorter array than valueLengths[currentColumnIndex]", "author": "losipiuk", "createdAt": "2020-07-10T16:42:06Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {\n+            checkArgument(!end.isPresent() || end.getAsInt() - start == length,\n+                    \"Bytes mapping for column '%s' does not match dataFormat '%s'; expected %s bytes but got %s\",\n+                    columnHandle.getName(),\n+                    length,\n+                    end.getAsInt() - start);\n+        }\n+\n+        return length;\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        buffer.put(new byte[valueLengths[currentColumnIndex]], currentBufferPosition, valueLengths[currentColumnIndex]);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put(currentBufferPosition, (byte) (value ? 1 : 0));\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        buffer.put(value.getBytes(StandardCharsets.UTF_8), currentBufferPosition, valueLengths[currentColumnIndex]);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk3ODQ3MQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452978471", "bodyText": "the best solution I can think of is create an intermediate ByteBuffer of the correct length for that column and then put the array from that into the actual ByteBuffer like this\nprotected void appendString(String value)\n{\n    ByteBuffer tempBuf = ByteBuffer.allocate(valueLengths[currentColumnIndex]);\n    tempBuf.put(value.getBytes(StandardCharsets.UTF_8), 0, valueLengths[currentColumnIndex]);\n    buffer.put(tempBuf.array(), currentBufferPosition, valueLengths[currentColumnIndex]);\n    currentBufferPosition += valueLengths[currentColumnIndex];\n}", "author": "charlesjmorgan", "createdAt": "2020-07-10T17:31:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA1MDQ4MQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453050481", "bodyText": "I recall from previous troubleshooting that the encoding for VARCHAR is kind of iffy.  Basically, this mapping represents a fixed length string.  Writing a shorter string will lead the consumer to read garbage, because we have no marker for the end of the string.  Also, when writing a longer string we truncate and end up with data loss - again not an ideal situation.\nI think we need to throw an exception if the lengths do not match and perhaps update the documentation to state the unpleasant reality.", "author": "aalbu", "createdAt": "2020-07-10T19:57:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDA1Mg==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453204052", "bodyText": "Yes for now I think that would be best. In the future it would be a good idea to rethink how the raw encoder maps values and define that better in the documentation.", "author": "charlesjmorgan", "createdAt": "2020-07-11T15:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ5MDE2OA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453490168", "bodyText": "Actually I meant that the code will throw if String value is shorter than valueLengths[currentColumnIndex]. It is still the case with updated code.\nbuffer.put(srcArray, offset, length) requires that there is at least length bytes in srcArray starting from offset.\nAlso I have just noticed that you pass currentBufferPosition source offset. This does feels wrong as the offset should be in source string space. And currentBufferPosition represents position in target buffer. You should pass 0 as offset i think.", "author": "losipiuk", "createdAt": "2020-07-13T08:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY3MTY3Ng==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453671676", "bodyText": "Actually I meant that the code will throw if String value is shorter than valueLengths[currentColumnIndex]\n\nIn this case I think that it is best if we throw an exception for now, otherwise the decoder will just read garbage.\n\nAlso I have just noticed that you pass currentBufferPosition source offset. This does feels wrong as the offset should be in source string space.\n\nI noticed this as well while working on this issue last week. I also realized that currentBufferPosition is a redundant variable because I can just use the built in position feature of the ByteBuffer to track the position and then reset it in toByteArray.", "author": "charlesjmorgan", "createdAt": "2020-07-13T14:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY4NDc4OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453684789", "bodyText": "Actually I meant that the code will throw if String value is shorter than valueLengths[currentColumnIndex]\n\n\n\nIn this case I think that it is best if we throw an exception for now, otherwise the decoder will just read garbage.\n\nYeah. I think I agree. Unless we want to pad with some whitespace. But still it should be a descriptive exception. Not IndexOutOfBounds... :)", "author": "losipiuk", "createdAt": "2020-07-13T14:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY5MTAyMg==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453691022", "bodyText": "what if it's IndexOutOfBoundsException with a descriptive message?", "author": "charlesjmorgan", "createdAt": "2020-07-13T14:29:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY5MzQyNA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453693424", "bodyText": "That should be fine :)", "author": "losipiuk", "createdAt": "2020-07-13T14:32:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzOTU1Mw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453039553", "bodyText": "This check is unnecessary, since it's covered by the one below.", "author": "aalbu", "createdAt": "2020-07-10T19:30:53Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc1MDg3NQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453750875", "bodyText": "Right, with the changes I just made I think that it is now necessary", "author": "charlesjmorgan", "createdAt": "2020-07-13T15:50:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzOTU1Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453828596", "bodyText": "This is over-complex for me :)\nI would suggest to simplify it by going doing parsing mulitpass:\n\npass1: go over all fields and parse mapping to tuple: Optionalnt start, OptionalInt end; just look at textual definition.\npass2: for each field if end == empty(), set it to start+field.length(); for varchar columns leave it as empty().\npass3: for each field verify that end-start matches field.length; fail otherwise\npass4; verify that no field mappings overlap each other; treat end==empty() as till the end of buffer; fail otherwise\n\nInstead having parseMapping just have a few loops over all column handles one by one, each doing simple thing. I have a suspicion it will be much easier to follow.", "author": "losipiuk", "createdAt": "2020-07-13T17:56:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(i, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(int index, FieldType fieldType)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0OTgwMw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453849803", "bodyText": "I'll try this out, already sounds much easier than what I was doing.", "author": "charlesjmorgan", "createdAt": "2020-07-13T18:32:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk0NTU4Mg==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453945582", "bodyText": "This still leaves the question of what to do with VARCHAR types that don't have an end specified. The way I see it we could,\n\nSet the length equal to 1 (byte)\nGuess the length based on the start mapping of the next column\nGuess the length based on the length of the VARCHAR column type instance\n\nOtherwise I don't know how many bytes to allocate to the ByteBuffer", "author": "charlesjmorgan", "createdAt": "2020-07-13T21:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE2NTczNQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454165735", "bodyText": "I think that if INSERT is to be supported for table we should require end to be specified for VARCHAR columns. Possibly allowing only exception where VARCHAR column maps to tail of message byte buffer; I am not sure if that is necessary though.\nI think this is fine that INSERT not necessarily works for all table definitions for which SELECT works (overlapping fields is a good example where we cannot make INSERT work).\nLet's add constraints and focus on on basic scenarios first (with well defined semantics). Also let's make sure that for those, INSERT is compatible with SELECT (we can do roundtrip). If we see that it would be useful, we can relax contraints later.", "author": "losipiuk", "createdAt": "2020-07-14T07:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDMzMjE4Mw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454332183", "bodyText": "Based on reading the docs and skimming through the code, it seems to me that a VARCHAR mapping without an end specified is valid only for the last column in the mapping.  For any other position, I think we need the end index and we need to ensure that the length of the value we are inserting matches the size of the mapping.\nI agree about the importance of round-trip tests, but we also need to make sure we conform to the spec we created originally, as up to this point, data has been created by non-Presto clients.", "author": "aalbu", "createdAt": "2020-07-14T12:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyOTA0OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453829049", "bodyText": "using valueLengths internally while building it based on result of this function is antipattern and make it much harder to follow.", "author": "losipiuk", "createdAt": "2020-07-13T17:57:20Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(i, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(int index, FieldType fieldType)\n+    {\n+        EncoderColumnHandle columnHandle = columnHandles.get(index);\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+\n+        int start = 0;\n+        for (int i = 0; i < index; i++) {\n+            start += valueLengths[i];", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NjAxOA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454986018", "bodyText": "This one non-final field is a glitch.\nCan we rename this class to ColumnMappingBuilder. And create separate ColumnMapping class with just getters and all fields final.", "author": "losipiuk", "createdAt": "2020-07-15T11:37:21Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzU0NA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454987544", "bodyText": "Parametrize with EncoderColumnHandler instead of index. Then you should be able to make this class static. Some helper function it calls will need to be made static too, but it should not be a problem.", "author": "losipiuk", "createdAt": "2020-07-15T11:40:24Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4OTQwOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454989409", "bodyText": "I would compute end to be set here and just call out to columnMapping.setEnd(end). Let's keep logic here and make ColumnMapping (or ColumnMappingBuilder if you follow my advice below) dumb.", "author": "losipiuk", "createdAt": "2020-07-15T11:44:12Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDI3OA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454990278", "bodyText": "make it else if { and drop one indentation level.", "author": "losipiuk", "createdAt": "2020-07-15T11:45:55Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExOTE0Ng==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455119146", "bodyText": "so the logic for this for loop should look like this?\nfor (ColumnMapping mapping : this.columnMappings) {\n    if (mapping.getEnd().isPresent()) {\n        continue;\n    }\n    else if (!isVarcharType(mapping.getType())) {\n        mapping.setEnd(mapping.getStart() + mapping.getFieldType().getSize());\n    }\n    else if (mapping.getName().equals(getLast(columnHandles).getName())) {\n        mapping.setEnd(mapping.getStart() + mapping.getFieldType().getSize());\n    }\n    else {\n        throw new IndexOutOfBoundsException(format(\n                \"No end mapping defined for column '%s' of type '%s'\",\n                mapping.getName(),\n                mapping.getType().getDisplayName()));\n    }\n}", "author": "charlesjmorgan", "createdAt": "2020-07-15T14:57:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE3NjE0MQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455176141", "bodyText": "You don't need the else after the continue.", "author": "aalbu", "createdAt": "2020-07-15T16:26:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDc4Mg==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454990782", "bodyText": "you can make it\n            if (columnMapping.getEnd().isPresent()) {\n                continue;\n            }\n\nto drop one indentation level", "author": "losipiuk", "createdAt": "2020-07-15T11:47:02Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MTgwMQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454991801", "bodyText": "combine conditions with &&", "author": "losipiuk", "createdAt": "2020-07-15T11:48:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MjE3OA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454992178", "bodyText": "name variable handle for brevity?", "author": "losipiuk", "createdAt": "2020-07-15T11:49:43Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MjMyMw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454992323", "bodyText": "Name variable mapping for brevity? (here and below)", "author": "losipiuk", "createdAt": "2020-07-15T11:50:00Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5NDkzMQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454994931", "bodyText": "May be personal taste but linear search hurts me.\nWhat about\nif (columnMapping.getName().equals(getLast(columnHandles).getName()))\n?", "author": "losipiuk", "createdAt": "2020-07-15T11:54:55Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA0MTMxOA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455041318", "bodyText": "Yeah that looks better to me", "author": "charlesjmorgan", "createdAt": "2020-07-15T13:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5NDkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5NjY2OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454996669", "bodyText": "This actually checks something stronger. We are not allowing gaps between fields too. I think this is fine though.", "author": "losipiuk", "createdAt": "2020-07-15T11:58:12Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5ODU0OA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454998548", "bodyText": "I do not believe you are calling this method ever with end non-set. Add precondition that end.isPresent() and drop if.", "author": "losipiuk", "createdAt": "2020-07-15T12:01:56Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwMDE3Nw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455000177", "bodyText": "I would initialize this.end with OptionalInt.empty() and drop else sth like:\n                    this.start = parseInt(mappingMatcher.group(1));\n                    this.end = OptionalInt.empty();\n                    if (mappingMatcher.group(2) != null) {\n...", "author": "losipiuk", "createdAt": "2020-07-15T12:05:06Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwMTg1Nw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455001857", "bodyText": "wrap with try\nMaybe extract method parseOffset() which does exception handling and call it like:\nparseOffset(mappingMatcher.group(1), \"start\", this.name)\nparseOffset(mappingMatcher.group(2), \"end\", this.name)", "author": "losipiuk", "createdAt": "2020-07-15T12:08:25Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNDY2NA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455004664", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n          \n          \n            \n                        buffer.put(valueBytes[i]);\n          \n          \n            \n                    }\n          \n          \n            \n                    buffer.put(valueBytes, 0, valueBytes.length)", "author": "losipiuk", "createdAt": "2020-07-15T12:13:45Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {\n+                return end.getAsInt() - start;\n+            }\n+            else {\n+                return fieldType.getSize();\n+            }\n+        }\n+\n+        public void setEnd()\n+        {\n+            this.end = OptionalInt.of(start + fieldType.getSize());\n+        }\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                        declaredFieldType.name(),\n+                        columnName,\n+                        columnType,\n+                        Joiner.on(\"/\").join(allowedFieldTypes)));\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length of message '%s' for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                columnHandles.get(currentColumnIndex),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put(valueBytes[i]);\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNTY4Ng==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455005686", "bodyText": "This is a strong constraint. Maybe allowing for shorter strings with some user-defined padding character would give better ux?\n@findepi @aalbu WDYT? (can be relaxed as a followup)", "author": "losipiuk", "createdAt": "2020-07-15T12:15:32Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {\n+                return end.getAsInt() - start;\n+            }\n+            else {\n+                return fieldType.getSize();\n+            }\n+        }\n+\n+        public void setEnd()\n+        {\n+            this.end = OptionalInt.of(start + fieldType.getSize());\n+        }\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                        declaredFieldType.name(),\n+                        columnName,\n+                        columnType,\n+                        Joiner.on(\"/\").join(allowedFieldTypes)));\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA0MDAyNQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455040025", "bodyText": "I didn't want to change the specifications for the raw format in this pr. I agree with you that padding shorter strings would be a good idea. The specifications for the raw format need to be rethought and clearly defined in the docs, figured it would be best to do that in a follow-up pr.", "author": "charlesjmorgan", "createdAt": "2020-07-15T13:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNTY4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5NDYwOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455194609", "bodyText": "I agree with @charlesjmorgan.  Without being explicit in the specification, it's probably best to require the users to pad in the query.", "author": "aalbu", "createdAt": "2020-07-15T16:56:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNTY4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNjc5OA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455006798", "bodyText": "I know that internally RoundTripTestCase now sends and receives two rows. But IMO it would be nicer if each row would be different, not same one repeated twice. Can we improve on that?", "author": "losipiuk", "createdAt": "2020-07-15T12:17:36Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -149,6 +149,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                         \"all_datatypes_csv\",\n                         ImmutableList.of(\"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n                         ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\")))\n+                .add(new RoundTripTestCase(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5OTM3NQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455199375", "bodyText": "A functional approach would make this shorter and more readable.", "author": "aalbu", "createdAt": "2020-07-15T17:03:45Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwMDgwMA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455200800", "bodyText": "else is unnecessary.", "author": "aalbu", "createdAt": "2020-07-15T17:06:09Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwNzEyOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455207129", "bodyText": "It doesn't look like the method throws NumberFormatException.  I also don't get the exception message.  But it doesn't look like you need to catch here.", "author": "aalbu", "createdAt": "2020-07-15T17:16:20Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    try {\n+                        this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                    }\n+                    catch (NumberFormatException e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxNjI4Mg==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455216282", "bodyText": "oh right, I moved the error handling to the method and forgot to remove it here", "author": "charlesjmorgan", "createdAt": "2020-07-15T17:29:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwNzEyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455213115", "bodyText": "This is another missing part in the spec, right?  It doesn't say how nulls should be encoded.", "author": "aalbu", "createdAt": "2020-07-15T17:25:25Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    try {\n+                        this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                    }\n+                    catch (NumberFormatException e) {\n+                        throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                    }\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse int '%s' for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;\n+        }\n+\n+        private static void checkFieldType(String columnName, Type columnType, FieldType fieldType)\n+        {\n+            if (columnType == BIGINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == INTEGER) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+            }\n+            else if (columnType == SMALLINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+            }\n+            else if (columnType == TINYINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+            else if (columnType == BOOLEAN) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == DOUBLE) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+            }\n+            else if (isVarcharType(columnType)) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+        }\n+\n+        private static void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+        {\n+            checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                    format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                            declaredFieldType.name(),\n+                            columnName,\n+                            columnType,\n+                            Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public int getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            return end - start;\n+        }\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIyMTAyOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455221029", "bodyText": "Correct. I could be interpreting this wrong but based on the code in the RawColumnDecoder it looks like the only most likely case in which a column can be null is if start and end values for the mapping are equal. Guess I should remove appendNullValue for now and we can figure that out in a future commit.\nedit I think that another case in which the value can be null is if the last column is VARCHAR and there is no value present for that column in the byte[] for RawColumnDecoder#decodeField", "author": "charlesjmorgan", "createdAt": "2020-07-15T17:33:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI0MjA1OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455242059", "bodyText": "Ugh the append null method is used when no key value is specified", "author": "charlesjmorgan", "createdAt": "2020-07-15T18:04:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYxMzY5MA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455613690", "bodyText": "when no key value is specified\n\nWhat do you mean by that. INSERT when only subset of columns are specified?\nCan we just disallow writing NULLs into table with Raw encoders? Which usecases does it break?\nIt seems that with mapping we now support for write we would not be able to write a value which would be interpreted as NULL by decoder anyway.", "author": "losipiuk", "createdAt": "2020-07-16T08:26:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTgxODAxNA==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455818014", "bodyText": "It wouldn't necessarily break anything, but you would have to insert values for every column when inserting. This includes the key column(s). It would definitely be best to disable writing null values until we can figure out a way to properly read/write them.", "author": "charlesjmorgan", "createdAt": "2020-07-16T14:12:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYyODY0NQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455628645", "bodyText": "can not hardcode it to 2 values, just generate the VALUES (..), (..), (...) for that many rows as present in input list?", "author": "losipiuk", "createdAt": "2020-07-16T08:50:59Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -125,10 +125,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n     {\n         assertUpdate(\"INSERT into write_test.\" + testCase.getTableName() +\n                 \" (\" + testCase.getFieldNames() + \")\" +\n-                \" VALUES (\" + testCase.getFieldValues() + \"), (\" + testCase.getFieldValues() + \")\", 2);\n+                \" VALUES (\" + testCase.getRowValues(0) + \"), (\" + testCase.getRowValues(1) + \")\", 2);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMTIzOQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455631239", "bodyText": "nit: could be done with stream...collect(toImmutableList)...", "author": "losipiuk", "createdAt": "2020-07-16T08:55:07Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMjQ5OQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455632499", "bodyText": "rename to checkMappingMatchesTypeSize and drop comment", "author": "losipiuk", "createdAt": "2020-07-16T08:57:07Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        return columnMappingsBuilder.build();\n+    }\n+\n+    // make sure the actual/expected lengths match\n+    private static void checkMappingLengths(ColumnMapping mapping)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzNjM5MQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455636391", "bodyText": "first clear() and then array(). I am supprised it works. I think we should just mark intention to clear here and do actual clearing on next append.", "author": "losipiuk", "createdAt": "2020-07-16T09:03:29Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        return columnMappingsBuilder.build();\n+    }\n+\n+    // make sure the actual/expected lengths match\n+    private static void checkMappingLengths(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                mapping.getEnd(),\n+                mapping.getName(),\n+                mapping.getStart()));\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final FieldType fieldType;\n+        private final int start;\n+        private final int end;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+\n+                if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse '%s' offset for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;\n+        }\n+\n+        private static void checkFieldType(String columnName, Type columnType, FieldType fieldType)\n+        {\n+            if (columnType == BIGINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == INTEGER) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+            }\n+            else if (columnType == SMALLINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+            }\n+            else if (columnType == TINYINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+            else if (columnType == BOOLEAN) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == DOUBLE) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+            }\n+            else if (isVarcharType(columnType)) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+        }\n+\n+        private static void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+        {\n+            checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                    format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                            declaredFieldType.name(),\n+                            columnName,\n+                            columnType,\n+                            Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public int getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            return end - start;\n+        }\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length '%s' of message '%s' for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                value,\n+                columnHandles.get(currentColumnIndex).getName(),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        buffer.put(valueBytes, 0, valueBytes.length);\n+    }\n+\n+    @Override\n+    protected void appendByteBuffer(ByteBuffer value)\n+    {\n+        byte[] valueBytes = value.array();\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length '%s' of message for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                columnHandles.get(currentColumnIndex).getName(),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        buffer.put(valueBytes, 0, valueBytes.length);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        resetColumnIndex(); // reset currentColumnIndex to prepare for next row\n+        buffer.clear(); // reset buffer position to prepare for next row", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc4NjY0Ng==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455786646", "bodyText": "clear() doesn't get rid of any of the data in the backing array, it only resets the position and limit", "author": "charlesjmorgan", "createdAt": "2020-07-16T13:30:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzNjM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTg2Mjk4Nw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455862987", "bodyText": "Yeah. You are right. I was expecting that this is just imlementation detail but actually it is documented behaviour. Can you make the comment by buffer.clear() more explicit so it is obvious that we know what we are doing, and avoid confustion of next generation of people reading this code :)", "author": "losipiuk", "createdAt": "2020-07-16T15:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzNjM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTg2NzU3NQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455867575", "bodyText": "yeah for sure", "author": "charlesjmorgan", "createdAt": "2020-07-16T15:18:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzNjM5MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE2ODkxNw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456168917", "bodyText": "You can inline this.", "author": "aalbu", "createdAt": "2020-07-17T01:32:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ0MTMxNQ==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456441315", "bodyText": "think I misunderstood what you meant by a functional approach originally hahah", "author": "charlesjmorgan", "createdAt": "2020-07-17T13:27:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE2ODkxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MDg2Mw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456170863", "bodyText": "checkMappingOffsets() is validating (in part) that the encoder itself is constructed properly.  The constructor of ColumnMapping is responsible for making sure that those objects are properly constructed.", "author": "aalbu", "createdAt": "2020-07-17T01:39:59Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());\n+    }\n+\n+    private static void checkMappingMatchesTypeSize(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MTgwMw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456171803", "bodyText": "Can you move this statement right after the if?", "author": "aalbu", "createdAt": "2020-07-17T01:43:40Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());\n+    }\n+\n+    private static void checkMappingMatchesTypeSize(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                mapping.getEnd(),\n+                mapping.getName(),\n+                mapping.getStart()));\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final FieldType fieldType;\n+        private final int start;\n+        private final int end;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+\n+                if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse '%s' offset for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI4MTA1Nw==", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456281057", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            rows[i] = \"(\" + rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \")) + \")\";\n          \n          \n            \n                            rows[i] = rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \", \"(\", \")\"));", "author": "losipiuk", "createdAt": "2020-07-17T07:55:44Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -144,62 +144,64 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                 .add(new RoundTripTestCase(\n                         \"all_datatypes_avro\",\n                         ImmutableList.of(\"f_bigint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n-                        ImmutableList.of(100000, 1000.001, true, \"'test'\")))\n+                        ImmutableList.of(\n+                                ImmutableList.of(100000, 1000.001, true, \"'test'\"),\n+                                ImmutableList.of(123456, 1234.123, false, \"'abcd'\"))))\n                 .add(new RoundTripTestCase(\n                         \"all_datatypes_csv\",\n                         ImmutableList.of(\"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n-                        ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\")))\n+                        ImmutableList.of(\n+                                ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\"),\n+                                ImmutableList.of(123456, 1234, 123, 12, 12345.123, false, \"'abcd'\"))))\n+                .add(new RoundTripTestCase(\n+                        \"all_datatypes_raw\",\n+                        ImmutableList.of(\"kafka_key\", \"f_varchar\", \"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\"),\n+                        ImmutableList.of(\n+                                ImmutableList.of(1, \"'test'\", 100000, 1000, 100, 10, 1000.001, true),\n+                                ImmutableList.of(1, \"'abcd'\", 123456, 1234, 123, 12, 12345.123, false))))\n                 .build();\n     }\n \n     protected static final class RoundTripTestCase\n     {\n         private final String tableName;\n         private final List<String> fieldNames;\n-        private final List<Object> fieldValues;\n-        private final int length;\n+        private final List<List<Object>> rowValues;\n+        private final int numRows;\n \n-        public RoundTripTestCase(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        public RoundTripTestCase(String tableName, List<String> fieldNames, List<List<Object>> rowValues)\n         {\n-            checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+            for (List<Object> row : rowValues) {\n+                checkArgument(fieldNames.size() == row.size(), \"sizes of fieldNames and rowValues are not equal\");\n+            }\n             this.tableName = requireNonNull(tableName, \"tableName is null\");\n             this.fieldNames = ImmutableList.copyOf(fieldNames);\n-            this.fieldValues = ImmutableList.copyOf(fieldValues);\n-            this.length = fieldNames.size();\n+            this.rowValues = ImmutableList.copyOf(rowValues);\n+            this.numRows = this.rowValues.size();\n         }\n \n         public String getTableName()\n         {\n             return tableName;\n         }\n \n-        private int getIndex(String fieldName)\n-        {\n-            return fieldNames.indexOf(fieldName);\n-        }\n-\n-        public String getFieldName(String fieldName)\n-        {\n-            int index = getIndex(fieldName);\n-            checkArgument(index >= 0 && index < length, \"index out of bounds\");\n-            return fieldNames.get(index);\n-        }\n-\n         public String getFieldNames()\n         {\n             return String.join(\", \", fieldNames);\n         }\n \n-        public Object getFieldValue(String fieldName)\n+        public String getRowValues()\n         {\n-            int index = getIndex(fieldName);\n-            checkArgument(index >= 0 && index < length, \"index out of bounds\");\n-            return fieldValues.get(index);\n+            String[] rows = new String[numRows];\n+            for (int i = 0; i < numRows; i++) {\n+                rows[i] = \"(\" + rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \")) + \")\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b9ffd9d861d5afedb490d637fefa944fc75e268a", "url": "https://github.com/trinodb/trino/commit/b9ffd9d861d5afedb490d637fefa944fc75e268a", "message": "Improve Kafka round trip test", "committedDate": "2020-07-17T14:24:24Z", "type": "commit"}, {"oid": "856d21c46c6280942b3397b44c4bb5db4e7de5e5", "url": "https://github.com/trinodb/trino/commit/856d21c46c6280942b3397b44c4bb5db4e7de5e5", "message": "Add Kafka raw encoder", "committedDate": "2020-07-17T14:24:27Z", "type": "commit"}, {"oid": "856d21c46c6280942b3397b44c4bb5db4e7de5e5", "url": "https://github.com/trinodb/trino/commit/856d21c46c6280942b3397b44c4bb5db4e7de5e5", "message": "Add Kafka raw encoder", "committedDate": "2020-07-17T14:24:27Z", "type": "forcePushed"}]}