{"pr_number": 2418, "pr_title": "Port streaming aggregation and assign unique id to work processor", "pr_createdAt": "2020-01-06T20:00:44Z", "pr_url": "https://github.com/trinodb/trino/pull/2418", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwNjExNw==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381606117", "bodyText": "This can be private", "author": "dain", "createdAt": "2020-02-19T23:25:46Z", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter\n+        implements AdapterWorkProcessorOperator\n+{\n+    public interface BasicAdapterWorkProcessorOperatorFactory\n+            extends WorkProcessorOperatorFactory\n+    {\n+        default WorkProcessorOperator createAdapterOperator(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return create(processorContext, sourcePages);\n+        }\n+\n+        BasicAdapterWorkProcessorOperatorFactory duplicate();\n+    }\n+\n+    public static OperatorFactory createAdapterOperatorFactory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+    {\n+        return WorkProcessorOperatorAdapter.createAdapterOperatorFactory(new Factory(operatorFactory));\n+    }\n+\n+    private static class Factory\n+            implements AdapterWorkProcessorOperatorFactory\n+    {\n+        final BasicAdapterWorkProcessorOperatorFactory operatorFactory;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwNjI2NQ==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381606265", "bodyText": "This can be private", "author": "dain", "createdAt": "2020-02-19T23:26:11Z", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter\n+        implements AdapterWorkProcessorOperator\n+{\n+    public interface BasicAdapterWorkProcessorOperatorFactory\n+            extends WorkProcessorOperatorFactory\n+    {\n+        default WorkProcessorOperator createAdapterOperator(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return create(processorContext, sourcePages);\n+        }\n+\n+        BasicAdapterWorkProcessorOperatorFactory duplicate();\n+    }\n+\n+    public static OperatorFactory createAdapterOperatorFactory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+    {\n+        return WorkProcessorOperatorAdapter.createAdapterOperatorFactory(new Factory(operatorFactory));\n+    }\n+\n+    private static class Factory\n+            implements AdapterWorkProcessorOperatorFactory\n+    {\n+        final BasicAdapterWorkProcessorOperatorFactory operatorFactory;\n+\n+        Factory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+        {\n+            this.operatorFactory = requireNonNull(operatorFactory, \"operatorFactory is null\");\n+        }\n+\n+        @Override\n+        public AdapterWorkProcessorOperatorFactory duplicate()\n+        {\n+            return new Factory(operatorFactory.duplicate());\n+        }\n+\n+        @Override\n+        public int getOperatorId()\n+        {\n+            return operatorFactory.getOperatorId();\n+        }\n+\n+        @Override\n+        public PlanNodeId getPlanNodeId()\n+        {\n+            return operatorFactory.getPlanNodeId();\n+        }\n+\n+        @Override\n+        public String getOperatorType()\n+        {\n+            return operatorFactory.getOperatorType();\n+        }\n+\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return operatorFactory.create(processorContext, sourcePages);\n+        }\n+\n+        @Override\n+        public AdapterWorkProcessorOperator createAdapterOperator(ProcessorContext processorContext)\n+        {\n+            return new BasicWorkProcessorOperatorAdapter(processorContext, operatorFactory);\n+        }\n+\n+        @Override\n+        public void lifespanFinished(Lifespan lifespan)\n+        {\n+            operatorFactory.lifespanFinished(lifespan);\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+            operatorFactory.close();\n+        }\n+    }\n+\n+    private final PageBuffer pageBuffer;\n+    private final WorkProcessorOperator operator;\n+\n+    public BasicWorkProcessorOperatorAdapter(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwODkzOA==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381608938", "bodyText": "It isn't clear when I would use this instead of AdapterWorkProcessorOperator.  The other direct implementations of AdapterWorkProcessorOperator use a buffer, but seem to have some slightly different code for things like needs input... maybe add a comment about the requirement (assumptions?) for using this class.", "author": "dain", "createdAt": "2020-02-19T23:34:04Z", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1Njg5Mw==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381956893", "bodyText": "added comments", "author": "sopel39", "createdAt": "2020-02-20T12:02:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwODkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMTE0OA==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381611148", "bodyText": "I think the invert of this condition might be more readable:\nif (outputPages.isEmpty()) {\n    return needsMoreData();\n}\nreturn ofResult(outputPages.removeFirst(), !finishing);", "author": "dain", "createdAt": "2020-02-19T23:41:10Z", "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "diffHunk": "@@ -64,234 +102,267 @@ public StreamingAggregationOperatorFactory(int operatorId, PlanNodeId planNodeId\n         }\n \n         @Override\n-        public Operator createOperator(DriverContext driverContext)\n+        public int getOperatorId()\n         {\n-            checkState(!closed, \"Factory is already closed\");\n-            OperatorContext operatorContext = driverContext.addOperatorContext(operatorId, planNodeId, StreamingAggregationOperator.class.getSimpleName());\n-            return new StreamingAggregationOperator(operatorContext, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return operatorId;\n         }\n \n         @Override\n-        public void noMoreOperators()\n+        public PlanNodeId getPlanNodeId()\n         {\n-            closed = true;\n+            return planNodeId;\n         }\n \n         @Override\n-        public OperatorFactory duplicate()\n+        public String getOperatorType()\n         {\n-            return new StreamingAggregationOperatorFactory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return StreamingAggregationOperator.class.getSimpleName();\n         }\n-    }\n \n-    private final OperatorContext operatorContext;\n-    private final LocalMemoryContext systemMemoryContext;\n-    private final LocalMemoryContext userMemoryContext;\n-    private final List<Type> groupByTypes;\n-    private final int[] groupByChannels;\n-    private final List<AccumulatorFactory> accumulatorFactories;\n-    private final Step step;\n-    private final PagesHashStrategy pagesHashStrategy;\n-\n-    private List<Aggregator> aggregates;\n-    private final PageBuilder pageBuilder;\n-    private final Deque<Page> outputPages = new LinkedList<>();\n-    private Page currentGroup;\n-    private boolean finishing;\n-\n-    public StreamingAggregationOperator(OperatorContext operatorContext, List<Type> sourceTypes, List<Type> groupByTypes, List<Integer> groupByChannels, Step step, List<AccumulatorFactory> accumulatorFactories, JoinCompiler joinCompiler)\n-    {\n-        this.operatorContext = requireNonNull(operatorContext, \"operatorContext is null\");\n-        this.systemMemoryContext = operatorContext.newLocalSystemMemoryContext(StreamingAggregationOperator.class.getSimpleName());\n-        this.userMemoryContext = operatorContext.localUserMemoryContext();\n-        this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n-        this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n-        this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n-        this.step = requireNonNull(step, \"step is null\");\n-\n-        this.aggregates = setupAggregates(step, accumulatorFactories);\n-        this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n-        requireNonNull(joinCompiler, \"joinCompiler is null\");\n-\n-        requireNonNull(sourceTypes, \"sourceTypes is null\");\n-        pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n-                .createPagesHashStrategy(\n-                        sourceTypes.stream()\n-                                .map(type -> ImmutableList.<Block>of())\n-                                .collect(toImmutableList()), OptionalInt.empty());\n-    }\n-\n-    private List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)\n-    {\n-        ImmutableList.Builder<Aggregator> builder = ImmutableList.builder();\n-        for (AccumulatorFactory factory : accumulatorFactories) {\n-            builder.add(new Aggregator(factory, step));\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            checkState(!closed, \"Factory is already closed\");\n+            return new StreamingAggregationOperator(processorContext, sourcePages, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n         }\n-        return builder.build();\n-    }\n \n-    private static List<Type> toTypes(List<Type> groupByTypes, List<Aggregator> aggregates)\n-    {\n-        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n-        builder.addAll(groupByTypes);\n-        aggregates.stream()\n-                .map(Aggregator::getType)\n-                .forEach(builder::add);\n-        return builder.build();\n-    }\n+        @Override\n+        public void close()\n+        {\n+            closed = true;\n+        }\n \n-    @Override\n-    public OperatorContext getOperatorContext()\n-    {\n-        return operatorContext;\n+        @Override\n+        public Factory duplicate()\n+        {\n+            return new Factory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+        }\n     }\n \n-    @Override\n-    public boolean needsInput()\n+    private final WorkProcessor<Page> pages;\n+\n+    private StreamingAggregationOperator(\n+            ProcessorContext processorContext,\n+            WorkProcessor<Page> sourcePages,\n+            List<Type> sourceTypes,\n+            List<Type> groupByTypes,\n+            List<Integer> groupByChannels,\n+            Step step,\n+            List<AccumulatorFactory> accumulatorFactories,\n+            JoinCompiler joinCompiler)\n     {\n-        return !finishing && outputPages.isEmpty();\n+        pages = sourcePages\n+                .transform(new StreamingAggregation(\n+                        processorContext,\n+                        sourceTypes,\n+                        groupByTypes,\n+                        groupByChannels,\n+                        step,\n+                        accumulatorFactories,\n+                        joinCompiler));\n     }\n \n     @Override\n-    public void addInput(Page page)\n+    public WorkProcessor<Page> getOutputPages()\n     {\n-        checkState(!finishing, \"Operator is already finishing\");\n-        requireNonNull(page, \"page is null\");\n-\n-        processInput(page);\n-        updateMemoryUsage();\n+        return pages;\n     }\n \n-    private void updateMemoryUsage()\n+    private static class StreamingAggregation\n+            implements Transformation<Page, Page>\n     {\n-        long memorySize = pageBuilder.getRetainedSizeInBytes();\n-        for (Page output : outputPages) {\n-            memorySize += output.getRetainedSizeInBytes();\n-        }\n-        for (Aggregator aggregator : aggregates) {\n-            memorySize += aggregator.getEstimatedSize();\n+        private final LocalMemoryContext systemMemoryContext;\n+        private final LocalMemoryContext userMemoryContext;\n+        private final List<Type> groupByTypes;\n+        private final int[] groupByChannels;\n+        private final List<AccumulatorFactory> accumulatorFactories;\n+        private final Step step;\n+        private final PagesHashStrategy pagesHashStrategy;\n+\n+        private List<Aggregator> aggregates;\n+        private final PageBuilder pageBuilder;\n+        private final Deque<Page> outputPages = new LinkedList<>();\n+        private Page currentGroup;\n+\n+        private StreamingAggregation(\n+                ProcessorContext processorContext,\n+                List<Type> sourceTypes,\n+                List<Type> groupByTypes,\n+                List<Integer> groupByChannels,\n+                Step step,\n+                List<AccumulatorFactory> accumulatorFactories,\n+                JoinCompiler joinCompiler)\n+        {\n+            requireNonNull(processorContext, \"processorContext is null\");\n+            this.systemMemoryContext = processorContext.getMemoryTrackingContext().localSystemMemoryContext();\n+            this.userMemoryContext = processorContext.getMemoryTrackingContext().localUserMemoryContext();\n+            this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n+            this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n+            this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n+            this.step = requireNonNull(step, \"step is null\");\n+\n+            this.aggregates = setupAggregates(step, accumulatorFactories);\n+            this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n+            requireNonNull(joinCompiler, \"joinCompiler is null\");\n+\n+            requireNonNull(sourceTypes, \"sourceTypes is null\");\n+            pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n+                    .createPagesHashStrategy(\n+                            sourceTypes.stream()\n+                                    .map(type -> ImmutableList.<Block>of())\n+                                    .collect(toImmutableList()), OptionalInt.empty());\n         }\n \n-        if (currentGroup != null) {\n-            memorySize += currentGroup.getRetainedSizeInBytes();\n-        }\n+        @Override\n+        public TransformationState<Page> process(@Nullable Page inputPage)\n+        {\n+            boolean finishing = inputPage == null;\n+\n+            if (finishing) {\n+                if (currentGroup != null) {\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                    currentGroup = null;\n+                }\n+\n+                if (!pageBuilder.isEmpty()) {\n+                    outputPages.add(pageBuilder.build());\n+                    pageBuilder.reset();\n+                }\n+\n+                if (outputPages.isEmpty()) {\n+                    return finished();\n+                }\n+            }\n+            else {\n+                processInput(inputPage);\n+                updateMemoryUsage();\n+            }\n \n-        if (step.isOutputPartial()) {\n-            systemMemoryContext.setBytes(memorySize);\n-        }\n-        else {\n-            userMemoryContext.setBytes(memorySize);\n-        }\n-    }\n+            if (!outputPages.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMjE0OQ==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381612149", "bodyText": "It is easier to review if you reorganize methods in a separate commit", "author": "dain", "createdAt": "2020-02-19T23:44:15Z", "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "diffHunk": "@@ -64,234 +102,267 @@ public StreamingAggregationOperatorFactory(int operatorId, PlanNodeId planNodeId\n         }\n \n         @Override\n-        public Operator createOperator(DriverContext driverContext)\n+        public int getOperatorId()\n         {\n-            checkState(!closed, \"Factory is already closed\");\n-            OperatorContext operatorContext = driverContext.addOperatorContext(operatorId, planNodeId, StreamingAggregationOperator.class.getSimpleName());\n-            return new StreamingAggregationOperator(operatorContext, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return operatorId;\n         }\n \n         @Override\n-        public void noMoreOperators()\n+        public PlanNodeId getPlanNodeId()\n         {\n-            closed = true;\n+            return planNodeId;\n         }\n \n         @Override\n-        public OperatorFactory duplicate()\n+        public String getOperatorType()\n         {\n-            return new StreamingAggregationOperatorFactory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return StreamingAggregationOperator.class.getSimpleName();\n         }\n-    }\n \n-    private final OperatorContext operatorContext;\n-    private final LocalMemoryContext systemMemoryContext;\n-    private final LocalMemoryContext userMemoryContext;\n-    private final List<Type> groupByTypes;\n-    private final int[] groupByChannels;\n-    private final List<AccumulatorFactory> accumulatorFactories;\n-    private final Step step;\n-    private final PagesHashStrategy pagesHashStrategy;\n-\n-    private List<Aggregator> aggregates;\n-    private final PageBuilder pageBuilder;\n-    private final Deque<Page> outputPages = new LinkedList<>();\n-    private Page currentGroup;\n-    private boolean finishing;\n-\n-    public StreamingAggregationOperator(OperatorContext operatorContext, List<Type> sourceTypes, List<Type> groupByTypes, List<Integer> groupByChannels, Step step, List<AccumulatorFactory> accumulatorFactories, JoinCompiler joinCompiler)\n-    {\n-        this.operatorContext = requireNonNull(operatorContext, \"operatorContext is null\");\n-        this.systemMemoryContext = operatorContext.newLocalSystemMemoryContext(StreamingAggregationOperator.class.getSimpleName());\n-        this.userMemoryContext = operatorContext.localUserMemoryContext();\n-        this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n-        this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n-        this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n-        this.step = requireNonNull(step, \"step is null\");\n-\n-        this.aggregates = setupAggregates(step, accumulatorFactories);\n-        this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n-        requireNonNull(joinCompiler, \"joinCompiler is null\");\n-\n-        requireNonNull(sourceTypes, \"sourceTypes is null\");\n-        pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n-                .createPagesHashStrategy(\n-                        sourceTypes.stream()\n-                                .map(type -> ImmutableList.<Block>of())\n-                                .collect(toImmutableList()), OptionalInt.empty());\n-    }\n-\n-    private List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)\n-    {\n-        ImmutableList.Builder<Aggregator> builder = ImmutableList.builder();\n-        for (AccumulatorFactory factory : accumulatorFactories) {\n-            builder.add(new Aggregator(factory, step));\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            checkState(!closed, \"Factory is already closed\");\n+            return new StreamingAggregationOperator(processorContext, sourcePages, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n         }\n-        return builder.build();\n-    }\n \n-    private static List<Type> toTypes(List<Type> groupByTypes, List<Aggregator> aggregates)\n-    {\n-        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n-        builder.addAll(groupByTypes);\n-        aggregates.stream()\n-                .map(Aggregator::getType)\n-                .forEach(builder::add);\n-        return builder.build();\n-    }\n+        @Override\n+        public void close()\n+        {\n+            closed = true;\n+        }\n \n-    @Override\n-    public OperatorContext getOperatorContext()\n-    {\n-        return operatorContext;\n+        @Override\n+        public Factory duplicate()\n+        {\n+            return new Factory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+        }\n     }\n \n-    @Override\n-    public boolean needsInput()\n+    private final WorkProcessor<Page> pages;\n+\n+    private StreamingAggregationOperator(\n+            ProcessorContext processorContext,\n+            WorkProcessor<Page> sourcePages,\n+            List<Type> sourceTypes,\n+            List<Type> groupByTypes,\n+            List<Integer> groupByChannels,\n+            Step step,\n+            List<AccumulatorFactory> accumulatorFactories,\n+            JoinCompiler joinCompiler)\n     {\n-        return !finishing && outputPages.isEmpty();\n+        pages = sourcePages\n+                .transform(new StreamingAggregation(\n+                        processorContext,\n+                        sourceTypes,\n+                        groupByTypes,\n+                        groupByChannels,\n+                        step,\n+                        accumulatorFactories,\n+                        joinCompiler));\n     }\n \n     @Override\n-    public void addInput(Page page)\n+    public WorkProcessor<Page> getOutputPages()\n     {\n-        checkState(!finishing, \"Operator is already finishing\");\n-        requireNonNull(page, \"page is null\");\n-\n-        processInput(page);\n-        updateMemoryUsage();\n+        return pages;\n     }\n \n-    private void updateMemoryUsage()\n+    private static class StreamingAggregation\n+            implements Transformation<Page, Page>\n     {\n-        long memorySize = pageBuilder.getRetainedSizeInBytes();\n-        for (Page output : outputPages) {\n-            memorySize += output.getRetainedSizeInBytes();\n-        }\n-        for (Aggregator aggregator : aggregates) {\n-            memorySize += aggregator.getEstimatedSize();\n+        private final LocalMemoryContext systemMemoryContext;\n+        private final LocalMemoryContext userMemoryContext;\n+        private final List<Type> groupByTypes;\n+        private final int[] groupByChannels;\n+        private final List<AccumulatorFactory> accumulatorFactories;\n+        private final Step step;\n+        private final PagesHashStrategy pagesHashStrategy;\n+\n+        private List<Aggregator> aggregates;\n+        private final PageBuilder pageBuilder;\n+        private final Deque<Page> outputPages = new LinkedList<>();\n+        private Page currentGroup;\n+\n+        private StreamingAggregation(\n+                ProcessorContext processorContext,\n+                List<Type> sourceTypes,\n+                List<Type> groupByTypes,\n+                List<Integer> groupByChannels,\n+                Step step,\n+                List<AccumulatorFactory> accumulatorFactories,\n+                JoinCompiler joinCompiler)\n+        {\n+            requireNonNull(processorContext, \"processorContext is null\");\n+            this.systemMemoryContext = processorContext.getMemoryTrackingContext().localSystemMemoryContext();\n+            this.userMemoryContext = processorContext.getMemoryTrackingContext().localUserMemoryContext();\n+            this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n+            this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n+            this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n+            this.step = requireNonNull(step, \"step is null\");\n+\n+            this.aggregates = setupAggregates(step, accumulatorFactories);\n+            this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n+            requireNonNull(joinCompiler, \"joinCompiler is null\");\n+\n+            requireNonNull(sourceTypes, \"sourceTypes is null\");\n+            pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n+                    .createPagesHashStrategy(\n+                            sourceTypes.stream()\n+                                    .map(type -> ImmutableList.<Block>of())\n+                                    .collect(toImmutableList()), OptionalInt.empty());\n         }\n \n-        if (currentGroup != null) {\n-            memorySize += currentGroup.getRetainedSizeInBytes();\n-        }\n+        @Override\n+        public TransformationState<Page> process(@Nullable Page inputPage)\n+        {\n+            boolean finishing = inputPage == null;\n+\n+            if (finishing) {\n+                if (currentGroup != null) {\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                    currentGroup = null;\n+                }\n+\n+                if (!pageBuilder.isEmpty()) {\n+                    outputPages.add(pageBuilder.build());\n+                    pageBuilder.reset();\n+                }\n+\n+                if (outputPages.isEmpty()) {\n+                    return finished();\n+                }\n+            }\n+            else {\n+                processInput(inputPage);\n+                updateMemoryUsage();\n+            }\n \n-        if (step.isOutputPartial()) {\n-            systemMemoryContext.setBytes(memorySize);\n-        }\n-        else {\n-            userMemoryContext.setBytes(memorySize);\n-        }\n-    }\n+            if (!outputPages.isEmpty()) {\n+                return ofResult(outputPages.removeFirst(), !finishing);\n+            }\n \n-    private void processInput(Page page)\n-    {\n-        requireNonNull(page, \"page is null\");\n+            return needsMoreData();\n+        }\n \n-        Page groupByPage = extractColumns(page, groupByChannels);\n-        if (currentGroup != null) {\n-            if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n-                // page starts with new group, so flush it\n-                evaluateAndFlushGroup(currentGroup, 0);\n+        private void updateMemoryUsage()\n+        {\n+            long memorySize = pageBuilder.getRetainedSizeInBytes();\n+            for (Page output : outputPages) {\n+                memorySize += output.getRetainedSizeInBytes();\n+            }\n+            for (Aggregator aggregator : aggregates) {\n+                memorySize += aggregator.getEstimatedSize();\n             }\n-            currentGroup = null;\n-        }\n \n-        int startPosition = 0;\n-        while (true) {\n-            // may be equal to page.getPositionCount() if the end is not found in this page\n-            int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n-            addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+            if (currentGroup != null) {\n+                memorySize += currentGroup.getRetainedSizeInBytes();\n+            }\n \n-            if (nextGroupStart < page.getPositionCount()) {\n-                // current group stops somewhere in the middle of the page, so flush it\n-                evaluateAndFlushGroup(page, startPosition);\n-                startPosition = nextGroupStart;\n+            if (step.isOutputPartial()) {\n+                systemMemoryContext.setBytes(memorySize);\n             }\n             else {\n-                currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n-                return;\n+                userMemoryContext.setBytes(memorySize);\n             }\n         }\n-    }\n \n-    private static Page extractColumns(Page page, int[] channels)\n-    {\n-        Block[] newBlocks = new Block[channels.length];\n-        for (int i = 0; i < channels.length; i++) {\n-            newBlocks[i] = page.getBlock(channels[i]);\n-        }\n-        return new Page(page.getPositionCount(), newBlocks);\n-    }\n+        private void processInput(Page page)\n+        {\n+            requireNonNull(page, \"page is null\");\n+\n+            Page groupByPage = extractColumns(page, groupByChannels);\n+            if (currentGroup != null) {\n+                if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n+                    // page starts with new group, so flush it\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                }\n+                currentGroup = null;\n+            }\n \n-    private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n-    {\n-        for (Aggregator aggregator : aggregates) {\n-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            int startPosition = 0;\n+            while (true) {\n+                // may be equal to page.getPositionCount() if the end is not found in this page\n+                int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n+                addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+\n+                if (nextGroupStart < page.getPositionCount()) {\n+                    // current group stops somewhere in the middle of the page, so flush it\n+                    evaluateAndFlushGroup(page, startPosition);\n+                    startPosition = nextGroupStart;\n+                }\n+                else {\n+                    currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n+                    return;\n+                }\n+            }\n         }\n-    }\n \n-    private void evaluateAndFlushGroup(Page page, int position)\n-    {\n-        pageBuilder.declarePosition();\n-        for (int i = 0; i < groupByTypes.size(); i++) {\n-            Block block = page.getBlock(groupByChannels[i]);\n-            Type type = groupByTypes.get(i);\n-            type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n-        }\n-        int offset = groupByTypes.size();\n-        for (int i = 0; i < aggregates.size(); i++) {\n-            aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n+        private static Page extractColumns(Page page, int[] channels)\n+        {\n+            Block[] newBlocks = new Block[channels.length];\n+            for (int i = 0; i < channels.length; i++) {\n+                newBlocks[i] = page.getBlock(channels[i]);\n+            }\n+            return new Page(page.getPositionCount(), newBlocks);\n         }\n \n-        if (pageBuilder.isFull()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n+        {\n+            for (Aggregator aggregator : aggregates) {\n+                aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            }\n         }\n \n-        aggregates = setupAggregates(step, accumulatorFactories);\n-    }\n-\n-    private int findNextGroupStart(int startPosition, Page page)\n-    {\n-        for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n-            if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n-                return i;\n+        private void evaluateAndFlushGroup(Page page, int position)\n+        {\n+            pageBuilder.declarePosition();\n+            for (int i = 0; i < groupByTypes.size(); i++) {\n+                Block block = page.getBlock(groupByChannels[i]);\n+                Type type = groupByTypes.get(i);\n+                type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n+            }\n+            int offset = groupByTypes.size();\n+            for (int i = 0; i < aggregates.size(); i++) {\n+                aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n             }\n-        }\n \n-        return page.getPositionCount();\n-    }\n+            if (pageBuilder.isFull()) {\n+                outputPages.add(pageBuilder.build());\n+                pageBuilder.reset();\n+            }\n \n-    @Override\n-    public Page getOutput()\n-    {\n-        if (!outputPages.isEmpty()) {\n-            return outputPages.removeFirst();\n+            aggregates = setupAggregates(step, accumulatorFactories);\n         }\n \n-        return null;\n-    }\n-\n-    @Override\n-    public void finish()\n-    {\n-        finishing = true;\n+        private int findNextGroupStart(int startPosition, Page page)\n+        {\n+            for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n+                if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n+                    return i;\n+                }\n+            }\n \n-        if (currentGroup != null) {\n-            evaluateAndFlushGroup(currentGroup, 0);\n-            currentGroup = null;\n+            return page.getPositionCount();\n         }\n \n-        if (!pageBuilder.isEmpty()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private static List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDAyNg==", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381950026", "bodyText": "Github has an option to ignore indents. Then reviewing changes like this is easier", "author": "sopel39", "createdAt": "2020-02-20T11:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMjE0OQ=="}], "type": "inlineReview"}, {"oid": "a53481ae9b84b041467f2ae7bc544340b8b7ab6a", "url": "https://github.com/trinodb/trino/commit/a53481ae9b84b041467f2ae7bc544340b8b7ab6a", "message": "Add task id to ProcessorContext", "committedDate": "2020-02-20T11:33:22Z", "type": "commit"}, {"oid": "a1731fb5f73e0dea0324afc23994597b5522bd81", "url": "https://github.com/trinodb/trino/commit/a1731fb5f73e0dea0324afc23994597b5522bd81", "message": "Make LookupJoinOperator constructor package private", "committedDate": "2020-02-20T11:33:22Z", "type": "commit"}, {"oid": "351dac5115f98e17cb5e22d8f2840cd816d7a94a", "url": "https://github.com/trinodb/trino/commit/351dac5115f98e17cb5e22d8f2840cd816d7a94a", "message": "Reorganize methods in LookupJoinOperatorFactory", "committedDate": "2020-02-20T11:33:22Z", "type": "commit"}, {"oid": "bb9cc0bf8e41f16e4af9cb25a3c53267f3ae2b5d", "url": "https://github.com/trinodb/trino/commit/bb9cc0bf8e41f16e4af9cb25a3c53267f3ae2b5d", "message": "Move private fields below public interfaces", "committedDate": "2020-02-20T11:33:22Z", "type": "commit"}, {"oid": "8a29d67f02202e5abc08545599cef1b31bc6e25d", "url": "https://github.com/trinodb/trino/commit/8a29d67f02202e5abc08545599cef1b31bc6e25d", "message": "Add WorkProcessorOperatorAdapter#createAdapterOperatorFactory\n\ncreateAdapterOperatorFactory method transforms\nAdapterWorkProcessorOperatorFactory into OperatorFactory.\nThis removes the need for operators to implement\ndual OperatorFactory and AdapterWorkProcessorOperatorFactory\noperator factories.", "committedDate": "2020-02-20T12:09:29Z", "type": "commit"}, {"oid": "7841fd32920457ff0e8c9829cf9ed3caf015d52a", "url": "https://github.com/trinodb/trino/commit/7841fd32920457ff0e8c9829cf9ed3caf015d52a", "message": "Provide default empty implementation for WorkProcessorOperator#close", "committedDate": "2020-02-20T12:09:30Z", "type": "commit"}, {"oid": "5fb71805e280a65f35e327127192bbf0fd72983b", "url": "https://github.com/trinodb/trino/commit/5fb71805e280a65f35e327127192bbf0fd72983b", "message": "Add BasicWorkProcessorOperatorAdapter\n\nBasicWorkProcessorOperatorAdapter removes input\npage buffering responsibility from work processor\noperators.", "committedDate": "2020-02-20T12:09:30Z", "type": "commit"}, {"oid": "dfc2f367e689701260eb4ae5b6e7ce3feb2a626c", "url": "https://github.com/trinodb/trino/commit/dfc2f367e689701260eb4ae5b6e7ce3feb2a626c", "message": "Port AssignUniqueIdOperator to work processor", "committedDate": "2020-02-20T12:09:30Z", "type": "commit"}, {"oid": "7a6e1f01b4bc78ee204ce8d0a06db8e7aea7dad8", "url": "https://github.com/trinodb/trino/commit/7a6e1f01b4bc78ee204ce8d0a06db8e7aea7dad8", "message": "Port StreamingAggregationOperator to work processor", "committedDate": "2020-02-20T12:09:31Z", "type": "commit"}, {"oid": "9121a3aecf8e4efadc6c6405ad4931d3297ff8fa", "url": "https://github.com/trinodb/trino/commit/9121a3aecf8e4efadc6c6405ad4931d3297ff8fa", "message": "Add javadoc description to WorkProcessorOperatorAdapter", "committedDate": "2020-02-20T12:09:38Z", "type": "commit"}, {"oid": "9121a3aecf8e4efadc6c6405ad4931d3297ff8fa", "url": "https://github.com/trinodb/trino/commit/9121a3aecf8e4efadc6c6405ad4931d3297ff8fa", "message": "Add javadoc description to WorkProcessorOperatorAdapter", "committedDate": "2020-02-20T12:09:38Z", "type": "forcePushed"}]}