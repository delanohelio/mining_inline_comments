{"pr_number": 5963, "pr_title": "Add pushdown smoke tests and refactor producer factory for Kafka connector", "pr_createdAt": "2020-11-14T20:41:00Z", "pr_url": "https://github.com/trinodb/trino/pull/5963", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI3NjkxNg==", "url": "https://github.com/trinodb/trino/pull/5963#discussion_r524276916", "bodyText": "I would prefer non-UTC timezone here. Or was it already discussed and we cannot for some reason?", "author": "losipiuk", "createdAt": "2020-11-16T13:45:01Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/kafka/TestKafkaWritesSmokeTest.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.tests.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.tempto.ProductTest;\n+import io.prestosql.tempto.Requirement;\n+import io.prestosql.tempto.RequirementsProvider;\n+import io.prestosql.tempto.Requires;\n+import io.prestosql.tempto.configuration.Configuration;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaTableDefinition;\n+import io.prestosql.tempto.fulfillment.table.kafka.ListKafkaDataSource;\n+import org.testng.annotations.Test;\n+\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+\n+import static io.prestosql.tempto.assertions.QueryAssert.Row.row;\n+import static io.prestosql.tempto.assertions.QueryAssert.assertThat;\n+import static io.prestosql.tempto.fulfillment.table.TableRequirements.immutableTable;\n+import static io.prestosql.tempto.query.QueryExecutor.query;\n+import static io.prestosql.tests.TestGroups.KAFKA;\n+import static io.prestosql.tests.TestGroups.PROFILE_SPECIFIC_TESTS;\n+import static java.lang.String.format;\n+\n+public class TestKafkaWritesSmokeTest\n+        extends ProductTest\n+{\n+    private static final String KAFKA_CATALOG = \"kafka\";\n+    private static final String SCHEMA_NAME = \"product_tests\";\n+\n+    private static final String SIMPLE_KEY_AND_VALUE_TABLE_NAME = \"write_simple_key_and_value\";\n+    private static final String SIMPLE_KEY_AND_VALUE_TOPIC_NAME = \"write_simple_key_and_value\";\n+\n+    // Kafka connector requires tables to be predefined in Presto configuration\n+    // the requirements here will be used to verify that table actually exists and to\n+    // create topics\n+\n+    private static class SimpleKeyAndValueTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + SIMPLE_KEY_AND_VALUE_TABLE_NAME,\n+                    SIMPLE_KEY_AND_VALUE_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(SimpleKeyAndValueTable.class)\n+    public void testInsertSimpleKeyAndValue()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 1, 'ania', 2), \" +\n+                        \"('piotr', 3, 'kasia', 4)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                SIMPLE_KEY_AND_VALUE_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(2);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                SIMPLE_KEY_AND_VALUE_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 1, \"ania\", 2),\n+                        row(\"piotr\", 3, \"kasia\", 4));\n+    }\n+\n+    private static final String ALL_DATATYPES_RAW_TABLE_NAME = \"write_all_datatypes_raw\";\n+    private static final String ALL_DATATYPES_RAW_TOPIC_NAME = \"write_all_datatypes_raw\";\n+\n+    private static class AllDataTypesRawTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_RAW_TABLE_NAME,\n+                    ALL_DATATYPES_RAW_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesRawTable.class)\n+    public void testInsertRawTable()\n+    {\n+        // TODO RawRowEncoder doesn't take mapping length into considertion while writing so a\n+        //  BIGINT with dataFormat = BYTE takes up 8 bytes during write (as opposed to 1 byte\n+        //  during read) and hence a buffer overflow is possible before we are able to reach\n+        //  the end of row.\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('piotr', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false), \" +\n+                        \"('hasan', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('kasia', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_RAW_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(4);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_RAW_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"piotr\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false),\n+                        row(\"hasan\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"kasia\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false));\n+    }\n+\n+    private static final String ALL_DATATYPES_CSV_TABLE_NAME = \"write_all_datatypes_csv\";\n+    private static final String ALL_DATATYPES_CSV_TOPIC_NAME = \"write_all_datatypes_csv\";\n+\n+    private static class AllDataTypesCsvTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_CSV_TABLE_NAME,\n+                    ALL_DATATYPES_CSV_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesCsvTable.class)\n+    public void testInsertCsvTable()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('stasio', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false), \" +\n+                        \"(null, null, null, null, null, null, null), \" +\n+                        \"('krzysio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, false), \" +\n+                        \"('kasia', 9223372036854775807, 2147483647, 32767, null, null, null)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_CSV_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(5);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_CSV_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"stasio\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false),\n+                        row(null, null, null, null, null, null, null),\n+                        row(\"krzysio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, false),\n+                        row(\"kasia\", 9223372036854775807L, 2147483647, 32767, null, null, null));\n+    }\n+\n+    private static final String ALL_DATATYPES_JSON_TABLE_NAME = \"write_all_datatypes_json\";\n+    private static final String ALL_DATATYPES_JSON_TOPIC_NAME = \"write_all_datatypes_json\";\n+\n+    private static class AllDataTypesJsonTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_JSON_TABLE_NAME,\n+                    ALL_DATATYPES_JSON_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesJsonTable.class)\n+    public void testInsertJsonTable()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES (\" +\n+                        \"'ala ma kota',\" +\n+                        \"9223372036854775807,\" +\n+                        \"2147483647,\" +\n+                        \"32767,\" +\n+                        \"127,\" +\n+                        \"1234567890.123456789,\" +\n+                        \"true,\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:16',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:17',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:18',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:19',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:20',\" +\n+                        \"DATE '2018-02-11',\" +\n+                        \"DATE '2018-02-13',\" +\n+                        \"TIME '13:15:16',\" +\n+                        \"TIME '13:15:17',\" +\n+                        \"TIME '13:15:18',\" +\n+                        \"TIME '13:15:20',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:18 UTC',\" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI4MTYwOQ==", "url": "https://github.com/trinodb/trino/pull/5963#discussion_r524281609", "bodyText": "I think we decided to skip that. If we want a followup please create an issue for that and add as //todo parameter.\nDrop whole comment otherwise.", "author": "losipiuk", "createdAt": "2020-11-16T13:51:52Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/kafka/TestKafkaPushdownSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.tests.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.tempto.ProductTest;\n+import io.prestosql.tempto.Requirement;\n+import io.prestosql.tempto.RequirementsProvider;\n+import io.prestosql.tempto.Requires;\n+import io.prestosql.tempto.configuration.Configuration;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaMessage;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaTableDefinition;\n+import io.prestosql.tempto.fulfillment.table.kafka.ListKafkaDataSource;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.LongStream;\n+\n+import static io.prestosql.tempto.assertions.QueryAssert.Row.row;\n+import static io.prestosql.tempto.assertions.QueryAssert.assertThat;\n+import static io.prestosql.tempto.fulfillment.table.TableRequirements.immutableTable;\n+import static io.prestosql.tempto.fulfillment.table.kafka.KafkaMessageContentsBuilder.contentsBuilder;\n+import static io.prestosql.tempto.query.QueryExecutor.query;\n+import static io.prestosql.tests.TestGroups.KAFKA;\n+import static io.prestosql.tests.TestGroups.PROFILE_SPECIFIC_TESTS;\n+import static java.lang.String.format;\n+\n+public class TestKafkaPushdownSmokeTest\n+        extends ProductTest\n+{\n+    private static final String KAFKA_CATALOG = \"kafka\";\n+    private static final String SCHEMA_NAME = \"product_tests\";\n+\n+    private static final long NUM_MESSAGES = 1000;\n+    private static final long TIMESTAMP_NUM_MESSAGES = 10;\n+\n+    private static final String PUSHDOWN_PARTITION_TABLE_NAME = \"pushdown_partition\";\n+    private static final String PUSHDOWN_PARTITION_TOPIC_NAME = \"pushdown_partition\";\n+\n+    private static final String PUSHDOWN_OFFSET_TABLE_NAME = \"pushdown_offset\";\n+    private static final String PUSHDOWN_OFFSET_TOPIC_NAME = \"pushdown_offset\";\n+\n+    private static final String PUSHDOWN_CREATE_TIME_TABLE_NAME = \"pushdown_create_time\";\n+    private static final String PUSHDOWN_CREATE_TIME_TOPIC_NAME = \"pushdown_create_time\";\n+\n+    // Kafka connector requires tables to be predefined in Presto configuration\n+    // the code here will be used to verify that table actually exists and to\n+    // create topics and insert test data\n+\n+    private static class PushdownPartitionTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            List<KafkaMessage> records = LongStream.rangeClosed(1, NUM_MESSAGES)\n+                    .boxed()\n+                    .map(i -> new KafkaMessage(\n+                            // only two possible keys to ensure each partition has NUM_MESSAGES/2 messages\n+                            contentsBuilder().appendUTF8(format(\"%s\", i % 2)).build(),\n+                            contentsBuilder().appendUTF8(format(\"%s\", i)).build()))\n+                    .collect(Collectors.toList());\n+\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_PARTITION_TABLE_NAME,\n+                    PUSHDOWN_PARTITION_TOPIC_NAME,\n+                    new ListKafkaDataSource(records),\n+                    2,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownPartitionTable.class)\n+    public void testPartitionPushdown()\n+    {\n+        // TODO Assert from the query stats that only NUM_MESSAGES / 2 records were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_id = 1\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_PARTITION_TABLE_NAME)))\n+                .hasAnyRows();\n+    }\n+\n+    private static class PushdownOffsetTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            List<KafkaMessage> records = LongStream.rangeClosed(1, NUM_MESSAGES)\n+                    .boxed()\n+                    .map(i -> new KafkaMessage(\n+                            // only two possible keys to ensure each partition has NUM_MESSAGES/2 messages\n+                            contentsBuilder().appendUTF8(format(\"%s\", i % 2)).build(),\n+                            contentsBuilder().appendUTF8(format(\"%s\", i)).build()))\n+                    .collect(Collectors.toList());\n+\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_OFFSET_TABLE_NAME,\n+                    PUSHDOWN_OFFSET_TOPIC_NAME,\n+                    new ListKafkaDataSource(records),\n+                    2,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownOffsetTable.class)\n+    public void testOffsetPushdown()\n+    {\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset BETWEEN 6 AND 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 8 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset > 5 AND _partition_offset < 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(8));\n+\n+        // TODO Assert from the query stats that only 12 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset >= 5 AND _partition_offset <= 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(12));\n+\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset >= 5 AND _partition_offset < 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset > 5 AND _partition_offset <= 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 2 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset = 5\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(2));\n+    }\n+\n+    private static class PushdownCreateTimeTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_CREATE_TIME_TABLE_NAME,\n+                    PUSHDOWN_CREATE_TIME_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownCreateTimeTable.class)\n+    public void testCreateTimePushdown()\n+            throws InterruptedException\n+    {\n+        // Ensure a spread of at-least TIMESTAMP_NUM_MESSAGES * 100 milliseconds\n+        for (int i = 1; i <= TIMESTAMP_NUM_MESSAGES; i++) {\n+            query(format(\"INSERT INTO %s.%s.%s (bigint_key, bigint_value) VALUES (%s, %s)\",\n+                    KAFKA_CATALOG, SCHEMA_NAME, PUSHDOWN_CREATE_TIME_TABLE_NAME, i, i));\n+            Thread.sleep(100);\n+        }\n+\n+        long startKey = 4;\n+        long endKey = 6;\n+        List<List<?>> rows = query(format(\n+                \"SELECT CAST(_timestamp AS VARCHAR) FROM %s.%s.%s WHERE bigint_key IN (\" + startKey + \", \" + endKey + \") ORDER BY bigint_key\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_CREATE_TIME_TABLE_NAME))\n+                .rows();\n+        String startTime = (String) rows.get(0).get(0);\n+        String endTime = (String) rows.get(1).get(0);\n+\n+        // TODO Assert from the query stats that only TIMESTAMP_NUM_MESSAGES - startKey messages were read because upper bound is not pushed down", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMxMjc3MA==", "url": "https://github.com/trinodb/trino/pull/5963#discussion_r524312770", "bodyText": "Will drop this comment. I too don't see any possible way to fulfill this TODO.", "author": "hashhar", "createdAt": "2020-11-16T14:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI4MTYwOQ=="}], "type": "inlineReview"}, {"oid": "ff4a2e65a536da812f960069f4d9f5a42fbe19f5", "url": "https://github.com/trinodb/trino/commit/ff4a2e65a536da812f960069f4d9f5a42fbe19f5", "message": "Make naming regarding Kafka read tests more explicit", "committedDate": "2020-11-21T20:42:25Z", "type": "commit"}, {"oid": "001a4f2333e2112285618f8aea36395b62e312da", "url": "https://github.com/trinodb/trino/commit/001a4f2333e2112285618f8aea36395b62e312da", "message": "Fix Kafka Avro encoder to use mappings\n\nFixes #5791.", "committedDate": "2020-11-21T20:42:25Z", "type": "commit"}, {"oid": "c41498b51391b8f17367b6eb6e69081185349a90", "url": "https://github.com/trinodb/trino/commit/c41498b51391b8f17367b6eb6e69081185349a90", "message": "Fix Kafka CSV, JSON and RAW encoders to use mappings\n\nSee https://github.com/prestosql/presto/issues/5791.", "committedDate": "2020-11-21T20:42:25Z", "type": "commit"}, {"oid": "ce275e9b4da376e4dd9b8fafe0ae808940cc6226", "url": "https://github.com/trinodb/trino/commit/ce275e9b4da376e4dd9b8fafe0ae808940cc6226", "message": "Add Kafka pushdown smoke tests", "committedDate": "2020-11-21T20:47:08Z", "type": "commit"}, {"oid": "4b4a8ba5306fde220b505c269942f022df8ee9ba", "url": "https://github.com/trinodb/trino/commit/4b4a8ba5306fde220b505c269942f022df8ee9ba", "message": "Refactor Kafka Producer factory", "committedDate": "2020-11-21T20:47:08Z", "type": "commit"}, {"oid": "0d7a1a69e02b2ff1818b87fa438db50c236e073f", "url": "https://github.com/trinodb/trino/commit/0d7a1a69e02b2ff1818b87fa438db50c236e073f", "message": "Refactor Kafka Admin factory", "committedDate": "2020-11-21T20:47:08Z", "type": "commit"}]}