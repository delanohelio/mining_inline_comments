{"pr_number": 2536, "pr_title": "Support Reading Encrypted S3 Objects of Unknown Size", "pr_createdAt": "2020-01-17T06:56:38Z", "pr_url": "https://github.com/trinodb/trino/pull/2536", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAyMDIyNg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368020226", "bodyText": "This seems incoherent with the handling of empty estimatedFileSize", "author": "pettyjamesm", "createdAt": "2020-01-17T16:20:54Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rcfile/RcFilePageSourceFactory.java", "diffHunk": "@@ -132,26 +155,32 @@ else if (deserializerClassName.equals(ColumnarSerDe.class.getName())) {\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, splitError(e, path, start, length), e);\n         }\n \n+        length = min(dataSource.getSize() - start, length);\n+        // Split may be empty now that the correct file size is known\n+        if (length <= 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI5MTI5Mw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371291293", "bodyText": "I didn't follow that.  Can you be more specific?", "author": "dain", "createdAt": "2020-01-27T15:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAyMDIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQyNTk3OQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371425979", "bodyText": "On line 128 an empty estimated file size is handled by throwing an exception, but after resolving the actual size we\u2019re just returning an empty page source.\nMaybe both situations should just return empty page sources?", "author": "pettyjamesm", "createdAt": "2020-01-27T19:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAyMDIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjYxNTU4OA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r372615588", "bodyText": "Ah now I understand.  They should do the same thing.  The existing code declares an empty RCFile as error, so I think we should maintain that for this commit.", "author": "dain", "createdAt": "2020-01-29T20:33:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAyMDIyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxMzMyNA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368613324", "bodyText": "Looks like this would also have to handle handle the case where split start + length exceeds the discovered file size as well as the rarer case where start >= discovered file size.", "author": "pettyjamesm", "createdAt": "2020-01-20T15:44:05Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -191,7 +191,9 @@ private static OrcPageSource createOrcPageSource(\n         AggregatedMemoryContext systemMemoryUsage = newSimpleAggregatedMemoryContext();\n         try {\n             OrcReader reader = new OrcReader(orcDataSource, options);\n-\n+            if (reader.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI4OTgzMg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371289832", "bodyText": "ORC uses a similar system to parquet.  The split region is basically a selection for ORC stripes.  Only stripes that start within the split region are considered part of the split, so if the split region is off the end of the file, there will be no blocks in that region.", "author": "dain", "createdAt": "2020-01-27T15:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxMzMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTM0MjkzMg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r375342932", "bodyText": "Got it, makes sense that the check isn't necessary for ORC and Parquet then.", "author": "pettyjamesm", "createdAt": "2020-02-05T15:54:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxMzMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjA5OA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368616098", "bodyText": "This needs to handle start + length > discovered fileSize and start >= discovered file size as well.", "author": "pettyjamesm", "createdAt": "2020-01-20T15:49:49Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -167,10 +167,11 @@ private static ParquetPageSource createParquetPageSource(\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(user, path, configuration);\n             FSDataInputStream inputStream = hdfsEnvironment.doAs(user, () -> fileSystem.open(path));\n-            ParquetMetadata parquetMetadata = MetadataReader.readFooter(inputStream, path, fileSize);\n+            dataSource = new HdfsParquetDataSource(new ParquetDataSourceId(path.toString()), estimatedFileSize, inputStream, stats, options);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI4NjY3Mw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371286673", "bodyText": "I believe the split region (i.e., start and length) is handled below when footerBlocks.  Basically, the code filters the block as declared in the footer to only those that start withing the split region, and only those blocks are processed.  If the split region is off the end of the file it shouldn't matter because no actual blocks can appear in that region.", "author": "dain", "createdAt": "2020-01-27T14:55:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjY5NA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368616694", "bodyText": "This is an exception because actually the getFileStatus call here will actually be the exact file size now. If this code changes to avoid the status call, then it'll need to handle start >= actual file size and start + length > actual file size just like the other page sources.", "author": "pettyjamesm", "createdAt": "2020-01-20T15:51:00Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSourceProvider.java", "diffHunk": "@@ -158,10 +159,10 @@ private static ConnectorPageSource createParquetPageSource(\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(user, path, configuration);\n             FileStatus fileStatus = fileSystem.getFileStatus(path);\n-            long fileSize = fileStatus.getLen();\n+            long estimatedFileSize = fileStatus.getLen();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI4Nzc4Mw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371287783", "bodyText": "I believe the same reasoning about split region applies to Iceberg.", "author": "dain", "createdAt": "2020-01-27T14:57:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA2MTE4NQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r369061185", "bodyText": "Should probably record bytes read and timing like readInternal does.", "author": "pettyjamesm", "createdAt": "2020-01-21T15:16:23Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/HdfsOrcDataSource.java", "diffHunk": "@@ -54,6 +56,15 @@ public void close()\n         inputStream.close();\n     }\n \n+    @Override\n+    public Slice readTail(int length)\n+            throws IOException\n+    {\n+        //  Handle potentially imprecise file lengths by reading the footer", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI4Nzk2OQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r371287969", "bodyText": "Yep.  Missed it in the parquet code also.", "author": "dain", "createdAt": "2020-01-27T14:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA2MTE4NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwMzA1Nw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481403057", "bodyText": "There is a new method\n.orElseThrow();", "author": "electrum", "createdAt": "2020-09-01T20:11:39Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestOrcPageSourceMemoryTracking.java", "diffHunk": "@@ -497,14 +496,14 @@ public ConnectorPageSource newPageSource(FileFormatDataSourceStats stats, Connec\n                     schema,\n                     TupleDomain.all(),\n                     columns,\n-                    partitonName,\n+                    partitionName,\n                     partitionKeys,\n                     TYPE_MANAGER,\n                     TableToPartitionMapping.empty(),\n                     Optional.empty(),\n                     false,\n                     Optional.empty())\n-                    .get();\n+                    .orElseThrow(RuntimeException::new);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwNDY4Nw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481404687", "bodyText": "\"data source\"", "author": "electrum", "createdAt": "2020-09-01T20:14:57Z", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcDataSource.java", "diffHunk": "@@ -31,6 +31,12 @@\n \n     long getSize();\n \n+    /**\n+     * Gets the memory size of this datasource.  This only includes memory", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwNTM3Mw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481405373", "bodyText": "Should this be OrcDataSource.class.getSimpleName() to match other usages?", "author": "electrum", "createdAt": "2020-09-01T20:16:03Z", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcRecordReader.java", "diffHunk": "@@ -205,6 +207,8 @@ public OrcRecordReader(\n \n         orcDataSource = wrapWithCacheIfTinyStripes(orcDataSource, this.stripes, options.getMaxMergeDistance(), options.getTinyStripeThreshold());\n         this.orcDataSource = orcDataSource;\n+        this.orcDataSourceMemoryUsage = systemMemoryUsage.newLocalMemoryContext(\"orcDataSource\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzODEwNg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481438106", "bodyText": "Should these be in the previous commit Add memory tests for both cached and uncached ORC?", "author": "electrum", "createdAt": "2020-09-01T21:19:42Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestOrcPageSourceMemoryTracking.java", "diffHunk": "@@ -194,7 +194,13 @@ private void testPageSource(boolean useCache)\n         FileFormatDataSourceStats stats = new FileFormatDataSourceStats();\n         ConnectorPageSource pageSource = testPreparer.newPageSource(stats, useCache ? CACHED_SESSION : UNCACHED_SESSION);\n \n-        assertEquals(pageSource.getSystemMemoryUsage(), 0);\n+        if (useCache) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTc3NTQ5Mw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481775493", "bodyText": "I have no idea \ud83e\udd37", "author": "dain", "createdAt": "2020-09-02T06:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzODEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzOTIyMQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481439221", "bodyText": "toIntExact should not be needed here", "author": "electrum", "createdAt": "2020-09-01T21:22:03Z", "path": "presto-orc/src/main/java/io/prestosql/orc/MemoryOrcDataSource.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.orc;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.stream.MemoryOrcDataReader;\n+import io.prestosql.orc.stream.OrcDataReader;\n+\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+\n+public class MemoryOrcDataSource\n+        implements OrcDataSource\n+{\n+    private final OrcDataSourceId id;\n+    private final Slice data;\n+    private long readBytes;\n+\n+    public MemoryOrcDataSource(OrcDataSourceId id, Slice data)\n+    {\n+        this.id = requireNonNull(id, \"id is null\");\n+        this.data = requireNonNull(data, \"data is null\");\n+    }\n+\n+    @Override\n+    public OrcDataSourceId getId()\n+    {\n+        return id;\n+    }\n+\n+    @Override\n+    public long getReadBytes()\n+    {\n+        return readBytes;\n+    }\n+\n+    @Override\n+    public long getReadTimeNanos()\n+    {\n+        return 0;\n+    }\n+\n+    @Override\n+    public final long getSize()\n+    {\n+        return data.length();\n+    }\n+\n+    @Override\n+    public long getRetainedSize()\n+    {\n+        return data.getRetainedSize();\n+    }\n+\n+    @Override\n+    public final Slice readFully(long position, int length)\n+    {\n+        readBytes += length;\n+        return data.slice(toIntExact(position), length);\n+    }\n+\n+    @Override\n+    public final <K> Map<K, OrcDataReader> readFully(Map<K, DiskRange> diskRanges)\n+    {\n+        requireNonNull(diskRanges, \"diskRanges is null\");\n+\n+        if (diskRanges.isEmpty()) {\n+            return ImmutableMap.of();\n+        }\n+\n+        ImmutableMap.Builder<K, OrcDataReader> slices = ImmutableMap.builder();\n+        for (Entry<K, DiskRange> entry : diskRanges.entrySet()) {\n+            DiskRange diskRange = entry.getValue();\n+            Slice slice = readFully(toIntExact(diskRange.getOffset()), diskRange.getLength());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0MTc4Ng==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481441786", "bodyText": "Static import", "author": "electrum", "createdAt": "2020-09-01T21:27:08Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcDeleteDeltaPageSourceFactory.java", "diffHunk": "@@ -43,9 +46,9 @@ public OrcDeleteDeltaPageSourceFactory(\n         this.stats = requireNonNull(stats, \"stats is null\");\n     }\n \n-    public OrcDeleteDeltaPageSource createPageSource(Path path, long fileSize)\n+    public Optional<ConnectorPageSource> createPageSource(Path path, long fileSize)\n     {\n-        return new OrcDeleteDeltaPageSource(\n+        return OrcDeleteDeltaPageSource.createOrcDeleteDeltaPageSource(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0MjYyNg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481442626", "bodyText": "Static import", "author": "electrum", "createdAt": "2020-09-01T21:28:51Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcDeleteDeltaPageSource.java", "diffHunk": "@@ -96,26 +97,11 @@ public OrcDeleteDeltaPageSource(\n         }\n \n         try {\n-            OrcReader reader = new OrcReader(orcDataSource, options);\n-\n-            verifyAcidSchema(reader, path);\n-            Map<String, OrcColumn> acidColumns = uniqueIndex(\n-                    reader.getRootColumn().getNestedColumns(),\n-                    orcColumn -> orcColumn.getColumnName().toLowerCase(ENGLISH));\n-            List<OrcColumn> rowIdColumns = ImmutableList.of(\n-                    acidColumns.get(ACID_COLUMN_ORIGINAL_TRANSACTION.toLowerCase(ENGLISH)),\n-                    acidColumns.get(ACID_COLUMN_ROW_ID.toLowerCase(ENGLISH)));\n-\n-            recordReader = reader.createRecordReader(\n-                    rowIdColumns,\n-                    ImmutableList.of(BIGINT, BIGINT),\n-                    OrcPredicate.TRUE,\n-                    0,\n-                    fileSize,\n-                    UTC,\n-                    systemMemoryContext,\n-                    MAX_BATCH_SIZE,\n-                    exception -> handleException(orcDataSource.getId(), exception));\n+            Optional<OrcReader> orcReader = OrcReader.createOrcReader(orcDataSource, options);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NDAyOQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481444029", "bodyText": "MISSING_DATA is used for HDFS BlockMissingException. This is like a corrupt file or something. How about\n.orElseThrow(() -> new PrestoException(ICEBERG_BAD_DATA, \"ORC file is zero length\"));", "author": "electrum", "createdAt": "2020-09-01T21:32:00Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSourceProvider.java", "diffHunk": "@@ -254,7 +254,8 @@ private static ConnectorPageSource createOrcPageSource(\n                     inputStream,\n                     stats);\n \n-            OrcReader reader = new OrcReader(orcDataSource, options);\n+            OrcReader reader = OrcReader.createOrcReader(orcDataSource, options)\n+                    .orElseThrow(() -> new PrestoException(ICEBERG_MISSING_DATA, \"File is empty\"));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NDQ2OA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481444468", "bodyText": "Move this next to the other read method", "author": "electrum", "createdAt": "2020-09-01T21:32:59Z", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcDataSource.java", "diffHunk": "@@ -29,7 +29,10 @@\n \n     long getReadTimeNanos();\n \n-    long getSize();\n+    long getEstimatedSize();\n+\n+    Slice readTail(int length)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NTkyMQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481445921", "bodyText": "Why the change? Is seems the caller still handles IOException", "author": "electrum", "createdAt": "2020-09-01T21:36:19Z", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcReader.java", "diffHunk": "@@ -374,10 +387,15 @@ else if (orcType.getOrcTypeKind() == OrcTypeKind.UNION) {\n      * Does the file start with the ORC magic bytes?\n      */\n     private static boolean isValidHeaderMagic(OrcDataSource source)\n-            throws IOException\n     {\n-        Slice headerMagic = source.readFully(0, MAGIC.length());\n-        return MAGIC.equals(headerMagic);\n+        try {\n+            Slice headerMagic = source.readFully(0, MAGIC.length());\n+            return MAGIC.equals(headerMagic);\n+        }\n+        catch (IOException e) {\n+            // assume file is an ORC file", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTc1NDQ4MA==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481754480", "bodyText": "This is only used in one place, and this the behavior that is desired there.  I just inlined this so it is easier to understand.", "author": "dain", "createdAt": "2020-09-02T05:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NTkyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NzcyMg==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481447722", "bodyText": "This isn't called if the read throws. I think we can wrap this in try-with-resources", "author": "electrum", "createdAt": "2020-09-01T21:40:27Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rcfile/RcFilePageSourceFactory.java", "diffHunk": "@@ -134,10 +138,17 @@ else if (deserializerClassName.equals(ColumnarSerDe.class.getName())) {\n                 .map(ReaderProjections::getReaderColumns)\n                 .orElse(columns);\n \n-        FSDataInputStream inputStream;\n+        RcFileDataSource dataSource;\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(session.getUser(), path, configuration);\n-            inputStream = hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.open(path));\n+            FSDataInputStream inputStream = hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.open(path));\n+            dataSource = new HdfsRcFileDataSource(path.toString(), inputStream, estimatedFileSize, stats);\n+            if (estimatedFileSize < BUFFER_SIZE.toBytes()) {\n+                byte[] buffer = new byte[toIntExact(estimatedFileSize)];\n+                dataSource.readFully(0, buffer, 0, buffer.length);\n+                dataSource.close();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0OTUwNw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481449507", "bodyText": "checkArgument(\"fileSize is negative: %s\", fileSize);\ncheckArgument(tailSlice.length() <= fileSize, \"length (%s) is greater than fileSize (%s)\", tailSlice.length(), fileSize);", "author": "electrum", "createdAt": "2020-09-01T21:44:42Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MDA3MQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481450071", "bodyText": "checkArgument(length >= 0, \"length is negative: %s\", length);\ncheckArgument(length <= MAXIMUM_READ_LENGTH, \"length (%s) exceeds maximum (%s)\", length, MAXIMUM_READ_LENGTH);", "author": "electrum", "createdAt": "2020-09-01T21:45:52Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MDYwNQ==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481450605", "bodyText": "Should this be < 0 to be consistent with the above?", "author": "electrum", "createdAt": "2020-09-01T21:47:04Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);\n+        long readSize = min(paddedFileSize, (length + MAX_SUPPORTED_PADDING_BYTES));\n+        long position = paddedFileSize - readSize;\n+        // Actual read will be 1 byte larger to ensure we encounter an EOF where expected\n+        byte[] buffer = new byte[toIntExact(readSize + 1)];\n+        int bytesRead = 0;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            while (bytesRead < buffer.length) {\n+                int n = inputStream.read(buffer, bytesRead, buffer.length - bytesRead);\n+                if (n < 0) {\n+                    break;\n+                }\n+                bytesRead += n;\n+            }\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+        if (bytesRead > readSize) {\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        return new FSDataInputStreamTail(position + bytesRead, Slices.wrappedBuffer(buffer, max(0, bytesRead - length), min(bytesRead, length)));\n+    }\n+\n+    public static long readTailForFileSize(String path, long paddedFileSize, FSDataInputStream inputStream)\n+            throws IOException\n+    {\n+        long position = max(paddedFileSize - MAX_SUPPORTED_PADDING_BYTES, 0);\n+        long maxEOFAt = paddedFileSize + 1;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            int c;\n+            while (position < maxEOFAt) {\n+                c = inputStream.read();\n+                if (c == -1) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MjUxNw==", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481452517", "bodyText": "This should throw IOException since it is used by PrestoS3FileSystem:\nthrow new IOException(format(\"Incorrect file size (%s) for file (end of stream not reached): %s\", reportedSize, path));", "author": "electrum", "createdAt": "2020-09-01T21:51:35Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);\n+        long readSize = min(paddedFileSize, (length + MAX_SUPPORTED_PADDING_BYTES));\n+        long position = paddedFileSize - readSize;\n+        // Actual read will be 1 byte larger to ensure we encounter an EOF where expected\n+        byte[] buffer = new byte[toIntExact(readSize + 1)];\n+        int bytesRead = 0;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            while (bytesRead < buffer.length) {\n+                int n = inputStream.read(buffer, bytesRead, buffer.length - bytesRead);\n+                if (n < 0) {\n+                    break;\n+                }\n+                bytesRead += n;\n+            }\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+        if (bytesRead > readSize) {\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        return new FSDataInputStreamTail(position + bytesRead, Slices.wrappedBuffer(buffer, max(0, bytesRead - length), min(bytesRead, length)));\n+    }\n+\n+    public static long readTailForFileSize(String path, long paddedFileSize, FSDataInputStream inputStream)\n+            throws IOException\n+    {\n+        long position = max(paddedFileSize - MAX_SUPPORTED_PADDING_BYTES, 0);\n+        long maxEOFAt = paddedFileSize + 1;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            int c;\n+            while (position < maxEOFAt) {\n+                c = inputStream.read();\n+                if (c == -1) {\n+                    return position;\n+                }\n+                position++;\n+            }\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+    }\n+\n+    private static PrestoException rejectInvalidFileSize(String path, long reportedSize)\n+    {\n+        throw new PrestoException(HIVE_FILESYSTEM_ERROR, format(\"Incorrect fileSize %s for file %s, end of stream not reached\", reportedSize, path));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "19bfa154cf450f295071f03e5557ed00883f47e2", "url": "https://github.com/trinodb/trino/commit/19bfa154cf450f295071f03e5557ed00883f47e2", "message": "Rename Hive fileSize to estimatedFileSize\n\nEncrypted S3 files may be padded, so reported size may not refect actual size.", "committedDate": "2020-09-11T18:02:00Z", "type": "commit"}, {"oid": "f5985243582a78c7f19f6e876962c6387627d7f3", "url": "https://github.com/trinodb/trino/commit/f5985243582a78c7f19f6e876962c6387627d7f3", "message": "Cleanup warnings in TestOrcPageSourceMemoryTracking", "committedDate": "2020-09-11T18:02:02Z", "type": "commit"}, {"oid": "b0c04f157ebae5940c35a7e4185b5b8e25454777", "url": "https://github.com/trinodb/trino/commit/b0c04f157ebae5940c35a7e4185b5b8e25454777", "message": "Add memory tests for both cached and uncached ORC", "committedDate": "2020-09-11T18:02:03Z", "type": "commit"}, {"oid": "377014336f4241832fe964946651115f12f492c7", "url": "https://github.com/trinodb/trino/commit/377014336f4241832fe964946651115f12f492c7", "message": "Add getRetainedSize to OrcDataSource", "committedDate": "2020-09-11T18:02:04Z", "type": "commit"}, {"oid": "cf94d20a5d5940e1aaaf2493334af3be18894975", "url": "https://github.com/trinodb/trino/commit/cf94d20a5d5940e1aaaf2493334af3be18894975", "message": "Add MemoryOrcDataSource", "committedDate": "2020-09-11T18:02:05Z", "type": "commit"}, {"oid": "288de65d048387a1498d11bcd063a618e1502c0a", "url": "https://github.com/trinodb/trino/commit/288de65d048387a1498d11bcd063a618e1502c0a", "message": "Add readTail to OrcDataSource", "committedDate": "2020-09-11T18:02:06Z", "type": "commit"}, {"oid": "cdb3c9c25bcd5e7dbd600f5291086420f87591d5", "url": "https://github.com/trinodb/trino/commit/cdb3c9c25bcd5e7dbd600f5291086420f87591d5", "message": "Cleanup ParquetPageSourceFactory", "committedDate": "2020-09-11T18:02:07Z", "type": "commit"}, {"oid": "c2bb601599344477dbbd47cb159db895ef6ef5cf", "url": "https://github.com/trinodb/trino/commit/c2bb601599344477dbbd47cb159db895ef6ef5cf", "message": "Remove unused method from ParquetDataSource", "committedDate": "2020-09-11T18:02:08Z", "type": "commit"}, {"oid": "3e4eb185a86f26b22de3ef971f19faacf0511f49", "url": "https://github.com/trinodb/trino/commit/3e4eb185a86f26b22de3ef971f19faacf0511f49", "message": "Change ParquetDataSource readFully to return Slice", "committedDate": "2020-09-11T18:02:10Z", "type": "commit"}, {"oid": "26a3035b41a5406a9351a0c265b147ea872ee7a7", "url": "https://github.com/trinodb/trino/commit/26a3035b41a5406a9351a0c265b147ea872ee7a7", "message": "Change Parquet MetadataReader to use ParquetDataSource", "committedDate": "2020-09-11T18:02:11Z", "type": "commit"}, {"oid": "6031c52166c42ece7023be73ac84620f9e98bb94", "url": "https://github.com/trinodb/trino/commit/6031c52166c42ece7023be73ac84620f9e98bb94", "message": "Add readTail to ParquetDataSource", "committedDate": "2020-09-11T18:02:12Z", "type": "commit"}, {"oid": "f2ef167bba4dc1757eea909a2e1d648f00d17094", "url": "https://github.com/trinodb/trino/commit/f2ef167bba4dc1757eea909a2e1d648f00d17094", "message": "Fully buffer small RC files", "committedDate": "2020-09-11T18:02:13Z", "type": "commit"}, {"oid": "a5bed0427f26764044bfea94409f9a74f179e330", "url": "https://github.com/trinodb/trino/commit/a5bed0427f26764044bfea94409f9a74f179e330", "message": "Introduce FSDataInputStreamTail reads\n\nAdds support for pre-reading the tail section of FSDataInputStream to\ndetect and handle file sizes that are actually smaller than initially\nreported by some small padding factor (eg: client side encrypted S3\nobjects where the last block must be read to determine the decrypted\nlength).\n\nThese pre-reads are then used to optimize Parquet and ORC metadata\nsection reads which can re-use the result. RCFile PageSources don't\nbenefit from pre-reading the tail of the file, and therefore must\nenable file size is calculated directly.", "committedDate": "2020-09-11T18:02:14Z", "type": "commit"}, {"oid": "6796dad70168ec79994eb2d00fd3e03c59975ba8", "url": "https://github.com/trinodb/trino/commit/6796dad70168ec79994eb2d00fd3e03c59975ba8", "message": "Support CSE-KMS S3 object file size detection via tail read", "committedDate": "2020-09-11T18:02:15Z", "type": "commit"}, {"oid": "6796dad70168ec79994eb2d00fd3e03c59975ba8", "url": "https://github.com/trinodb/trino/commit/6796dad70168ec79994eb2d00fd3e03c59975ba8", "message": "Support CSE-KMS S3 object file size detection via tail read", "committedDate": "2020-09-11T18:02:15Z", "type": "forcePushed"}]}