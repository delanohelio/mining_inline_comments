{"pr_number": 5695, "pr_title": "Add support for variable precision timestamps in Hive array, map, and struct", "pr_createdAt": "2020-10-26T17:57:51Z", "pr_url": "https://github.com/trinodb/trino/pull/5695", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI1MjU2MQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r512252561", "bodyText": "This check was already in the class, but when does it come up? SerDeUtils is only used to read structural types in GenericHiveRecordCursor, and that class always assumes that the object can be read by the associated inspector, as in the else block a few lines down.", "author": "jirassimok", "createdAt": "2020-10-26T20:37:28Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -312,11 +321,22 @@ private static long formatDateAsLong(Object object, DateObjectInspector inspecto\n         return inspector.getPrimitiveJavaObject(object).toEpochDay();\n     }\n \n-    private static long formatTimestampAsLong(Object object, TimestampObjectInspector inspector)\n+    private static DecodedTimestamp formatTimestamp(TimestampType type, Object object, TimestampObjectInspector inspector)\n     {\n+        long seconds;\n+        int nanos;\n+\n         if (object instanceof TimestampWritable) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzNTcxNg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r513835716", "bodyText": "I prefer the SQL comma style from the earlier commits because it makes it easier to avoid missing commas. But it also looks weird, so here's a commit I can squash if other people don't like it.\n(The \\ns are there for similar reasons, to make it easier to see when there is or isn't whitespace between the lines.)", "author": "jirassimok", "createdAt": "2020-10-29T00:14:41Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -443,11 +443,11 @@ public void testStructTimestamps(StorageFormat format)\n \n         onPresto().executeQuery(format(\n                 \"CREATE TABLE %s (\\n\"\n-                        + \"id INTEGER\\n\"\n-                        + \",arr ARRAY(TIMESTAMP)\\n\"\n-                        + \",map MAP(TIMESTAMP, TIMESTAMP)\\n\"\n-                        + \",row ROW(col TIMESTAMP)\\n\"\n-                        + \",nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"\n+                        + \"id INTEGER,\\n\"\n+                        + \"arr ARRAY(TIMESTAMP),\\n\"\n+                        + \"map MAP(TIMESTAMP, TIMESTAMP),\\n\"\n+                        + \"row ROW(col TIMESTAMP),\\n\"\n+                        + \"nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r522407909", "bodyText": "This is concerning, and I don't know what caused it.", "author": "jirassimok", "createdAt": "2020-11-12T20:31:21Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -453,22 +453,23 @@ public void testStructTimestamps(StorageFormat format)\n                 tableName,\n                 format.getStoragePropertiesAsSql()));\n \n-        onHive().executeQuery(format(\n-                \"INSERT INTO %s %s\",\n-                tableName,\n-                TIMESTAMPS_FROM_HIVE.stream()\n-                        .map(entry -> format(\n-                                \"SELECT\\n\"\n-                                        + \"%s\\n\"\n-                                        + \",arr(%2$s)\\n\"\n-                                        + \",map(%2$s, %2$s)\\n\"\n-                                        + \",named_struct('col', %2$s)\\n\"\n-                                        + \",array(map(%2$s, named_struct('col', array(%2$s))))\\n\"\n-                                        // some hive versions don't allow insert from bare select\n-                                        + \"FROM (SELECT 1) t\\n\",\n-                                entry.getId(),\n-                                format(\"TIMESTAMP '%s'\", entry.getWriteValue())))\n-                        .collect(Collectors.joining(\" UNION ALL \"))));\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjg5MzAyOQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r526893029", "bodyText": "It is concerning. Did you try to reproduce the problem outside of this PR?", "author": "findepi", "createdAt": "2020-11-19T13:38:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDM0NzA4OQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r534347089", "bodyText": "I haven't tried to reproduce it outside the PR, but it doesn't seem to depend on anything from the PR (I removed the timestamps and arrays/maps/structs from the test, and it still happened). I'll try to get a working example.", "author": "jirassimok", "createdAt": "2020-12-02T17:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2Mjg4NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r538662884", "bodyText": "It seems like even a simple example works.\nhive> CREATE TABLE test (col integer);\nhive> INSERT INTO test SELECT 1 UNION ALL SELECT 2;\nhive> SELECT count(*) FROM test;\n2\n... (log out of hive, in to Presto)\nprestosql> SELECT count(*) FROM hive.default.test;\n0", "author": "jirassimok", "createdAt": "2020-12-08T17:54:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NzcxMg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540867712", "bodyText": "Is the data visible in Hive, but not in Presto?\nor, is it just a bug in INSERT + UNION ALL in (some version(s) of) Hive?", "author": "findepi", "createdAt": "2020-12-11T11:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA1OTQ4NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r541059485", "bodyText": "Correct. Hive has no problem with the data that I've seen.", "author": "jirassimok", "createdAt": "2020-12-11T16:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIzMTkzMQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547231931", "bodyText": "This sounds like a bug (in Presto) then.\nCan you please file an issue (and link it here)?", "author": "findepi", "createdAt": "2020-12-22T11:45:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDY5Mzg2NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r550693864", "bodyText": "Filed as #6485.", "author": "jirassimok", "createdAt": "2020-12-31T21:30:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjAyMzI0MQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r552023241", "bodyText": "Please link to #6485 in the comment too", "author": "findepi", "createdAt": "2021-01-05T15:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNzkwOQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5MjUzNA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531092534", "bodyText": "inline", "author": "findepi", "createdAt": "2020-11-26T15:12:41Z", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestBackgroundHiveSplitLoader.java", "diffHunk": "@@ -123,6 +123,7 @@\n public class TestBackgroundHiveSplitLoader\n {\n     private static final int BUCKET_COUNT = 2;\n+    private static final HiveTimestampPrecision TIMESTAMP_PRECISION = HiveTimestampPrecision.MILLISECONDS;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2MTQ4NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r533561485", "bodyText": "Inlined as DEFAULT_PRECISION.", "author": "jirassimok", "createdAt": "2020-12-01T16:45:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5MjUzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5MzkxOA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531093918", "bodyText": "\"Use the passed type to serialize in SerDeUtils\" commit looks good. Can you split it out to a separate PR to reduce scope of this PR?", "author": "findepi", "createdAt": "2020-11-26T15:15:03Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -114,25 +104,25 @@ private static void serializePrimitive(Type type, BlockBuilder builder, Object o\n \n         switch (inspector.getPrimitiveCategory()) {\n             case BOOLEAN:\n-                BooleanType.BOOLEAN.writeBoolean(builder, ((BooleanObjectInspector) inspector).get(object));\n+                type.writeBoolean(builder, ((BooleanObjectInspector) inspector).get(object));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg1NzkxOQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r533857919", "bodyText": "Done.", "author": "jirassimok", "createdAt": "2020-12-02T02:41:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5MzkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5NDkyOA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531094928", "bodyText": "keep this line directly below this elaborate comment adorning it", "author": "findepi", "createdAt": "2020-11-26T15:16:41Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -258,26 +256,26 @@ private static TypeSignature getTypeSignature(TypeInfo typeInfo)\n                 }\n                 ImmutableList.Builder<TypeSignatureParameter> typeSignatureBuilder = ImmutableList.builder();\n                 for (int i = 0; i < structFieldTypeInfos.size(); i++) {\n-                    TypeSignature typeSignature = getTypeSignature(structFieldTypeInfos.get(i));\n                     // Lower case the struct field names.\n                     // Otherwise, Presto will refuse to write to columns whose struct type has field names containing upper case characters.\n                     // Users can't work around this by casting in their queries because Presto parser always lower case types.\n                     // TODO: This is a hack. Presto engine should be able to handle identifiers in a case insensitive way where necessary.\n-                    String rowFieldName = structFieldNames.get(i).toLowerCase(Locale.US);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5NjYwMQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531096601", "bodyText": "\"Simplify HiveType.getTypeSignature with factory methods\" also looks good % one comment.\nYou can split it out together with \"Use the passed type to serialize in SerDeUtils\"", "author": "findepi", "createdAt": "2020-11-26T15:19:38Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -238,17 +240,13 @@ private static TypeSignature getTypeSignature(TypeInfo typeInfo)\n                 return primitiveType.getTypeSignature();\n             case MAP:\n                 MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;\n-                TypeSignature keyType = getTypeSignature(mapTypeInfo.getMapKeyTypeInfo());\n-                TypeSignature valueType = getTypeSignature(mapTypeInfo.getMapValueTypeInfo());\n-                return new TypeSignature(\n-                        StandardTypes.MAP,\n-                        ImmutableList.of(TypeSignatureParameter.typeParameter(keyType), TypeSignatureParameter.typeParameter(valueType)));\n+                return mapType(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5NzYwMw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531097603", "bodyText": "Streams.zip does not necessarily improve readability, because i need to think what's the behavior when zipped streams have different lengths. I'd suggest dropping this change.", "author": "findepi", "createdAt": "2020-11-26T15:21:19Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -254,17 +256,15 @@ private static TypeSignature getTypeSignature(TypeInfo typeInfo)\n                 if (fieldTypes.size() != fieldNames.size()) {\n                     throw new PrestoException(HiveErrorCode.HIVE_INVALID_METADATA, format(\"Invalid Hive struct type: %s\", typeInfo));\n                 }\n-                ImmutableList.Builder<TypeSignatureParameter> typeSignatureBuilder = ImmutableList.builder();\n-                for (int i = 0; i < fieldTypes.size(); i++) {\n-                    // Lower case the struct field names.\n-                    // Otherwise, Presto will refuse to write to columns whose struct type has field names containing upper case characters.\n-                    // Users can't work around this by casting in their queries because Presto parser always lower case types.\n-                    // TODO: This is a hack. Presto engine should be able to handle identifiers in a case insensitive way where necessary.\n-                    typeSignatureBuilder.add(namedField(\n-                            fieldNames.get(i).toLowerCase(Locale.US),\n-                            getTypeSignature(fieldTypes.get(i))));\n-                }\n-                return rowType(typeSignatureBuilder.build());\n+                // We lower case the struct field names.\n+                // Otherwise, Presto will refuse to write to columns whose struct type has field names containing upper case characters.\n+                // Users can't work around this by casting in their queries because Presto parser always lower case types.\n+                // TODO: This is a hack. Presto engine should be able to handle identifiers in a case insensitive way where necessary.\n+                return rowType(Streams.zip(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU0ODg0NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r533548844", "bodyText": "In this case, the lines above guarantee that the streams are the same size (because the two lists represents pairs of values). (This might be clearer with the comments above moved to the next line.)\n(I actually find the indexed for-loop slightly less readable, because there's a moment of confusion before I parse why it's indexed instead of a for-each loop.)", "author": "jirassimok", "createdAt": "2020-12-01T16:28:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5NzYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg1ODcyMw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r533858723", "bodyText": "This is now part of #6175, and I've copied over the comments.", "author": "jirassimok", "createdAt": "2020-12-02T02:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5NzYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5ODA0MQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r531098041", "bodyText": "the name of the class is \"translator\", so \"translate\" is not a bad name, is it?", "author": "findepi", "createdAt": "2020-11-26T15:22:04Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/HiveTypeTranslator.java", "diffHunk": "@@ -64,7 +64,7 @@\n {\n     private HiveTypeTranslator() {}\n \n-    public static TypeInfo translate(Type type)\n+    public static TypeInfo toTypeInfo(Type type)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzUwNjY1Mg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r533506652", "bodyText": "I renamed that because I added other translation methods to the class.", "author": "jirassimok", "createdAt": "2020-12-01T15:34:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTA5ODA0MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0MzAzMQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540843031", "bodyText": "Annotate the method @Nullable", "author": "findepi", "createdAt": "2020-12-11T10:23:20Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/HiveTypeTranslator.java", "diffHunk": "@@ -149,4 +173,90 @@ public static TypeInfo toTypeInfo(Type type)\n         }\n         throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Hive type: %s\", type));\n     }\n+\n+    public static TypeSignature toTypeSignature(TypeInfo typeInfo)\n+    {\n+        switch (typeInfo.getCategory()) {\n+            case PRIMITIVE:\n+                Type primitiveType = fromPrimitiveType((PrimitiveTypeInfo) typeInfo);\n+                if (primitiveType == null) {\n+                    break;\n+                }\n+                return primitiveType.getTypeSignature();\n+            case MAP:\n+                MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;\n+                return mapType(\n+                        toTypeSignature(mapTypeInfo.getMapKeyTypeInfo()),\n+                        toTypeSignature(mapTypeInfo.getMapValueTypeInfo()));\n+            case LIST:\n+                ListTypeInfo listTypeInfo = (ListTypeInfo) typeInfo;\n+                TypeSignature elementType = toTypeSignature(listTypeInfo.getListElementTypeInfo());\n+                return arrayType(typeParameter(elementType));\n+            case STRUCT:\n+                StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;\n+                List<TypeInfo> fieldTypes = structTypeInfo.getAllStructFieldTypeInfos();\n+                List<String> fieldNames = structTypeInfo.getAllStructFieldNames();\n+                if (fieldTypes.size() != fieldNames.size()) {\n+                    throw new PrestoException(HiveErrorCode.HIVE_INVALID_METADATA, format(\"Invalid Hive struct type: %s\", typeInfo));\n+                }\n+                return rowType(Streams.zip(\n+                        // We lower case the struct field names.\n+                        // Otherwise, Presto will refuse to write to columns whose struct type has field names containing upper case characters.\n+                        // Users can't work around this by casting in their queries because Presto parser always lower case types.\n+                        // TODO: This is a hack. Presto engine should be able to handle identifiers in a case insensitive way where necessary.\n+                        fieldNames.stream().map(s -> s.toLowerCase(Locale.US)),\n+                        fieldTypes.stream().map(HiveTypeTranslator::toTypeSignature),\n+                        TypeSignatureParameter::namedField)\n+                        .collect(Collectors.toList()));\n+            case UNION:\n+                // Use a row type to represent a union type in Hive for reading\n+                UnionTypeInfo unionTypeInfo = (UnionTypeInfo) typeInfo;\n+                List<TypeInfo> unionObjectTypes = unionTypeInfo.getAllUnionObjectTypeInfos();\n+                ImmutableList.Builder<TypeSignatureParameter> typeSignatures = ImmutableList.builder();\n+                typeSignatures.add(namedField(\"tag\", TINYINT.getTypeSignature()));\n+                for (int i = 0; i < unionObjectTypes.size(); i++) {\n+                    TypeInfo unionObjectType = unionObjectTypes.get(i);\n+                    typeSignatures.add(namedField(\"field\" + i, toTypeSignature(unionObjectType)));\n+                }\n+                return rowType(typeSignatures.build());\n+        }\n+        throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported Hive type: %s\", typeInfo));\n+    }\n+\n+    public static Type fromPrimitiveType(PrimitiveTypeInfo typeInfo)\n+    {\n+        switch (typeInfo.getPrimitiveCategory()) {\n+            case BOOLEAN:\n+                return BOOLEAN;\n+            case BYTE:\n+                return TINYINT;\n+            case SHORT:\n+                return SMALLINT;\n+            case INT:\n+                return INTEGER;\n+            case LONG:\n+                return BIGINT;\n+            case FLOAT:\n+                return REAL;\n+            case DOUBLE:\n+                return DOUBLE;\n+            case STRING:\n+                return createUnboundedVarcharType();\n+            case VARCHAR:\n+                return createVarcharType(((VarcharTypeInfo) typeInfo).getLength());\n+            case CHAR:\n+                return createCharType(((CharTypeInfo) typeInfo).getLength());\n+            case DATE:\n+                return DATE;\n+            case TIMESTAMP:\n+                return TIMESTAMP_MILLIS;\n+            case BINARY:\n+                return VARBINARY;\n+            case DECIMAL:\n+                DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfo;\n+                return createDecimalType(decimalTypeInfo.precision(), decimalTypeInfo.scale());\n+            default:\n+                return null;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NjI0Ng==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540846246", "bodyText": "you can replace the Map with an ordinary switch (keep it simple), which will allow you to add default case throwing cleanly in case of a new, unsupported HiveTimestampPrecision.", "author": "findepi", "createdAt": "2020-12-11T10:28:29Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -675,18 +554,42 @@ public String toString()\n         }\n     }\n \n+    /**\n+     * Create {@code TimestampAndPrecision} with specified write precision and rounded fractional seconds.\n+     * @param precision Precision for writing value.\n+     * @param writeValue The literal value to write.\n+     * @param milliFraction Expected fraction when reading with millisecond precision.\n+     * @param microFraction Expected fraction when reading with microsecond precision.\n+     * @param nanoFraction Expected fraction when reading with nanosecond precision.\n+     */\n+    private static TimestampAndPrecision timestampAndPrecision(\n+            HiveTimestampPrecision precision,\n+            String writeValue,\n+            String milliFraction,\n+            String microFraction,\n+            String nanoFraction)\n+    {\n+        // Remove fractional second from write value\n+        String writeValueNoFraction = writeValue.split(\"\\\\.\")[0];\n+        Map<HiveTimestampPrecision, String> readValues = Map.of(\n+                MILLISECONDS, writeValueNoFraction + \".\" + milliFraction,\n+                MICROSECONDS, writeValueNoFraction + \".\" + microFraction,\n+                NANOSECONDS, writeValueNoFraction + \".\" + nanoFraction);\n+        return new TimestampAndPrecision(precision, readValues.get(precision), readValues);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTM1NTcyMg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r541355722", "bodyText": "The Map is used as data in TimestampAndPrecision, but I can check if the precision is unknown and throw an exception. This is checked in TimestampAndPrecision.getReadValue as of 9fe7d4e.", "author": "jirassimok", "createdAt": "2020-12-11T22:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NjI0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NzI2Nw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540847267", "bodyText": "This is not correct, but we do not have such a test case yet.\nWe should have test cases:\n\nwhen written value has .9995 second fraction and read back will milli precision comes as +1 second\nwhen written value has .9999995 second fraction and read back will micros precision comes as +1 second\nwhen written value has .999999999 second fraction", "author": "findepi", "createdAt": "2020-12-11T10:30:14Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -675,18 +554,42 @@ public String toString()\n         }\n     }\n \n+    /**\n+     * Create {@code TimestampAndPrecision} with specified write precision and rounded fractional seconds.\n+     * @param precision Precision for writing value.\n+     * @param writeValue The literal value to write.\n+     * @param milliFraction Expected fraction when reading with millisecond precision.\n+     * @param microFraction Expected fraction when reading with microsecond precision.\n+     * @param nanoFraction Expected fraction when reading with nanosecond precision.\n+     */\n+    private static TimestampAndPrecision timestampAndPrecision(\n+            HiveTimestampPrecision precision,\n+            String writeValue,\n+            String milliFraction,\n+            String microFraction,\n+            String nanoFraction)\n+    {\n+        // Remove fractional second from write value\n+        String writeValueNoFraction = writeValue.split(\"\\\\.\")[0];\n+        Map<HiveTimestampPrecision, String> readValues = Map.of(\n+                MILLISECONDS, writeValueNoFraction + \".\" + milliFraction,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTM4MjE3NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r541382175", "bodyText": "Hm. Maybe I should separate the common prefix and the write value?\nNo, that stops being easy to read, which defeats the point of the factory.\nThen the current cases would look like the first line, and a case with rounding might look like the second:\ntimestampAndPrecision(NANOS, \"2020-01-02 12:34:56.\", \"111111999\", \"111\", \"111112\", \"111111999\"),\ntimestampAndPrecision(MICROS, \"2020-01-02 12:34:\", \"39.999991999\", \"40.000\", \"39.999992\", \"39.999991999\"),\n\nI think I'll remove the string processing from the factory, but leave the separate arguments. This is much more useful than the current factory, and it's easier to see how the values relate than the original version where the values weren't lined up.\ntimestampAndPrecision(writeValue, writePrecision, milliReadValue, microReadValue, nanoReadValue);\n// (First two arguments switched to keep the write value separate from the read value.)\n\ntimestampAndPrecision(\n    \"2020-01-02 12:34:56.111111999\",\n    NANOSECONDS,\n    \"2020-01-02 12:34:56.111\",\n    \"2020-01-02 12:34:56.111112\",\n    \"2020-01-02 12:34:56.111111999\"),\n\ntimestampAndPrecision(\n    \"2020-01-02 12:23:39.111111999\",\n    NANOSECONDS,\n    \"2020-01-02 12:34:40.000\",\n    \"2020-01-02 12:34:39.111112\",\n    \"2020-01-02 12:34:39.111111999\")", "author": "jirassimok", "createdAt": "2020-12-11T22:49:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NzI2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDI3MTgwMA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r544271800", "bodyText": "I like this change.\nMind that the changes being discussed here, mainly change from using eg milliFraction to milliReadValue went in  Allow custom timestamp precision in Hive structural types.  They should be part of Clean up TestHiveStorageFormats commit. (Actually, in Clean up TestHiveStorageFormats the invocations are correct, but the timestampAndPrecision impl is wrong)", "author": "findepi", "createdAt": "2020-12-16T12:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NzI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0OTU1OA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540849558", "bodyText": "My intellij automatically reformats this line into multiline form.", "author": "findepi", "createdAt": "2020-12-11T10:33:52Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -93,11 +93,19 @@ public TypeInfo getTypeInfo()\n         return typeInfo;\n     }\n \n+    /** @deprecated Prefer {@link #getTypeSignature(HiveTimestampPrecision)}. */", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MjE3NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540862175", "bodyText": "instead of passing \"unchecked\" DEFAULT_PRECISION, we should pass Optional<HiveTimestampPrecision>\n\nthere are places where hive.timestamp-precision doesn't matter --  a bunch of places dealing with bucketing columns, and timestamp cannot be bucketed on in Presto\n\nit will be hard to find motivation to fix those cases, we will be tempted to use the now-deprecated methods\n\n\npassing Optional.empty will allow us to signal \"timestamp precision cannot matter here\"; and will also allow us to validate this in fromPrimitiveType(PrimitiveTypeInfo typeInfo, below\n\nProbably the Optional-taking method should be a public overload, so that we can call it directly in those places where we deal with bucketing columns. Currently we call the deprecated overload there, which looks not good, if we don't have plans to \"fix\" it.", "author": "findepi", "createdAt": "2020-12-11T10:54:57Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -93,11 +93,19 @@ public TypeInfo getTypeInfo()\n         return typeInfo;\n     }\n \n+    /** @deprecated Prefer {@link #getTypeSignature(HiveTimestampPrecision)}. */\n+    @Deprecated\n     public TypeSignature getTypeSignature()\n     {\n-        return toTypeSignature(typeInfo);\n+        return getTypeSignature(DEFAULT_PRECISION);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjU1NTUxMA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542555510", "bodyText": "I think renaming the method to something like getTypeWithUnknownTimePrecision might be a better idea if we don't plan to fix it, because it makes the meaning more clear.\nIf TypeManager were local to Hive, it would be a better idea to include the timestamp precision there, which could eliminate all but one use of getTypeSignature().", "author": "jirassimok", "createdAt": "2020-12-14T17:11:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MjE3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDI2MjMwMw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r544262303", "bodyText": "I think renaming the method to something like getTypeWithUnknownTimePrecision might be a better idea if we don't plan to fix it, because it makes the meaning more clear.\n\nSounds good!\n(nit: TimePrecision -> TimestampPrecision)", "author": "findepi", "createdAt": "2020-12-16T12:33:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MjE3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyNjg5Mw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547226893", "bodyText": "let's address this as a followup", "author": "findepi", "createdAt": "2020-12-22T11:33:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MjE3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NTM0Ng==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540865346", "bodyText": "epochSecond", "author": "findepi", "createdAt": "2020-12-11T11:00:16Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -312,11 +321,22 @@ private static long formatDateAsLong(Object object, DateObjectInspector inspecto\n         return inspector.getPrimitiveJavaObject(object).toEpochDay();\n     }\n \n-    private static long formatTimestampAsLong(Object object, TimestampObjectInspector inspector)\n+    private static DecodedTimestamp formatTimestamp(TimestampType type, Object object, TimestampObjectInspector inspector)\n     {\n+        long seconds;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NTUyOQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540865529", "bodyText": "nanosOfSecond", "author": "findepi", "createdAt": "2020-12-11T11:00:35Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -312,11 +321,22 @@ private static long formatDateAsLong(Object object, DateObjectInspector inspecto\n         return inspector.getPrimitiveJavaObject(object).toEpochDay();\n     }\n \n-    private static long formatTimestampAsLong(Object object, TimestampObjectInspector inspector)\n+    private static DecodedTimestamp formatTimestamp(TimestampType type, Object object, TimestampObjectInspector inspector)\n     {\n+        long seconds;\n+        int nanos;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NjQ3NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540866475", "bodyText": "rounding here may move you to next second (ie may return NANOS_PER_SECOND)\nthis needs to be handled here before invoking the DecodedTimestamp ctor", "author": "findepi", "createdAt": "2020-12-11T11:02:10Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -312,11 +321,22 @@ private static long formatDateAsLong(Object object, DateObjectInspector inspecto\n         return inspector.getPrimitiveJavaObject(object).toEpochDay();\n     }\n \n-    private static long formatTimestampAsLong(Object object, TimestampObjectInspector inspector)\n+    private static DecodedTimestamp formatTimestamp(TimestampType type, Object object, TimestampObjectInspector inspector)\n     {\n+        long seconds;\n+        int nanos;\n+\n         if (object instanceof TimestampWritable) {\n-            return ((TimestampWritable) object).getTimestamp().getTime() * MICROSECONDS_PER_MILLISECOND;\n+            LocalDateTime datetime = ((TimestampWritable) object).getTimestamp().toLocalDateTime();\n+            seconds = datetime.toEpochSecond(UTC);\n+            nanos = datetime.getNano();\n+        }\n+        else {\n+            Timestamp timestamp = inspector.getPrimitiveJavaObject(object);\n+            seconds = timestamp.toEpochSecond();\n+            nanos = timestamp.getNanos();\n         }\n-        return inspector.getPrimitiveJavaObject(object).toEpochMilli() * MICROSECONDS_PER_MILLISECOND;\n+\n+        return new DecodedTimestamp(seconds, (int) round(nanos, 9 - type.getPrecision()));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjU0MDA4NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542540084", "bodyText": "I've put it here, though that check should probably also be in the PrestoTimestampEncoders, or maybe DecodedTimestamp should have a separate constructor for that.", "author": "jirassimok", "createdAt": "2020-12-14T16:51:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NjQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NjczNA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540866734", "bodyText": "I don't understand this comment", "author": "findepi", "createdAt": "2020-12-11T11:02:32Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -68,6 +71,32 @@\n     @Named(\"databases.presto.admin_role_enabled\")\n     private boolean adminRoleEnabled;\n \n+    // Declare these statically so they have the same IDs in every test", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUzNjc2NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542536764", "bodyText": "I'll just remove it, it's misleading anyway.", "author": "jirassimok", "createdAt": "2020-12-14T16:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2NjczNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2ODA2NQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540868065", "bodyText": "bans -> does not support", "author": "findepi", "createdAt": "2020-12-11T11:04:51Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -430,6 +434,83 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)\n+    public void testStructTimestamps(StorageFormat format)\n+            throws SQLException\n+    {\n+        setAdminRole(onPresto().getConnection());\n+        ensureDummyExists();\n+\n+        String tableName = format(\"test_struct_timestamp_precision_%s_%s\", format.getName().toLowerCase(Locale.ENGLISH), randomTableSuffix());\n+\n+        onPresto().executeQuery(format(\n+                \"CREATE TABLE %s (\\n\"\n+                        + \"id INTEGER\\n\"\n+                        + \",arr ARRAY(TIMESTAMP)\\n\"\n+                        + \",map MAP(TIMESTAMP, TIMESTAMP)\\n\"\n+                        + \",row ROW(col TIMESTAMP)\\n\"\n+                        + \",nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"\n+                        + \") WITH (%s)\",\n+                tableName,\n+                format.getStoragePropertiesAsSql()));\n+\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto\n+        for (TimestampAndPrecision entry : TIMESTAMPS_FROM_HIVE) {\n+            onHive().executeQuery(format(\n+                    \"INSERT INTO %1$s\\n\"\n+                            // insert with SELECT because hive bans array/map/struct functions in VALUES", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2ODgzNQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540868835", "bodyText": "i like the convention of , at the line start because it allows be to add new line without modifying the previous line, resulting in cleaner diff\nthis is not a convention in the project though, so i'd recommend putting the comma at line's end\nalso, \\n is redundant, and reads worse than just a space", "author": "findepi", "createdAt": "2020-12-11T11:06:14Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -430,6 +434,83 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)\n+    public void testStructTimestamps(StorageFormat format)\n+            throws SQLException\n+    {\n+        setAdminRole(onPresto().getConnection());\n+        ensureDummyExists();\n+\n+        String tableName = format(\"test_struct_timestamp_precision_%s_%s\", format.getName().toLowerCase(Locale.ENGLISH), randomTableSuffix());\n+\n+        onPresto().executeQuery(format(\n+                \"CREATE TABLE %s (\\n\"\n+                        + \"id INTEGER\\n\"\n+                        + \",arr ARRAY(TIMESTAMP)\\n\"\n+                        + \",map MAP(TIMESTAMP, TIMESTAMP)\\n\"\n+                        + \",row ROW(col TIMESTAMP)\\n\"\n+                        + \",nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"\n+                        + \") WITH (%s)\",\n+                tableName,\n+                format.getStoragePropertiesAsSql()));\n+\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto\n+        for (TimestampAndPrecision entry : TIMESTAMPS_FROM_HIVE) {\n+            onHive().executeQuery(format(\n+                    \"INSERT INTO %1$s\\n\"\n+                            // insert with SELECT because hive bans array/map/struct functions in VALUES\n+                            + \"SELECT\\n\"\n+                            + \"%3$s\\n\"\n+                            + \",array(%2$s)\\n\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUwOTAzNQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542509035", "bodyText": "I like including \\n because IntelliJ highlights it, so it makes it more obvious when I'm missing whitespace; it's annoying when a test fails because I accidentally wrote \"SELECTid,valueFROM sometable (and the failure messages aren't usually super helpful). (IntelliJ also includes the newlines if you use \"edit SQL fragment\" and use multiple lines.)\nBut as with the commas, it doesn't match the style elsewhere in the file, so I'll just indent everything with spaces instead (though at least one other test (TestHiveIntegrationSmokeTest) does use newlines in its SQL).", "author": "jirassimok", "createdAt": "2020-12-14T16:11:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2ODgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2OTk3OQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540869979", "bodyText": "That's hard to read. Why not just read the value and check it is exactly as expected?\nSee also #6297", "author": "findepi", "createdAt": "2020-12-11T11:08:08Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -430,6 +434,83 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)\n+    public void testStructTimestamps(StorageFormat format)\n+            throws SQLException\n+    {\n+        setAdminRole(onPresto().getConnection());\n+        ensureDummyExists();\n+\n+        String tableName = format(\"test_struct_timestamp_precision_%s_%s\", format.getName().toLowerCase(Locale.ENGLISH), randomTableSuffix());\n+\n+        onPresto().executeQuery(format(\n+                \"CREATE TABLE %s (\\n\"\n+                        + \"id INTEGER\\n\"\n+                        + \",arr ARRAY(TIMESTAMP)\\n\"\n+                        + \",map MAP(TIMESTAMP, TIMESTAMP)\\n\"\n+                        + \",row ROW(col TIMESTAMP)\\n\"\n+                        + \",nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"\n+                        + \") WITH (%s)\",\n+                tableName,\n+                format.getStoragePropertiesAsSql()));\n+\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto\n+        for (TimestampAndPrecision entry : TIMESTAMPS_FROM_HIVE) {\n+            onHive().executeQuery(format(\n+                    \"INSERT INTO %1$s\\n\"\n+                            // insert with SELECT because hive bans array/map/struct functions in VALUES\n+                            + \"SELECT\\n\"\n+                            + \"%3$s\\n\"\n+                            + \",array(%2$s)\\n\"\n+                            + \",map(%2$s, %2$s)\\n\"\n+                            + \",named_struct('col', %2$s)\\n\"\n+                            + \",array(map(%2$s, named_struct('col', array(%2$s))))\\n\"\n+                            // some hive versions don't allow INSERT from SELECT without FROM\n+                            + \"FROM dummy\\n\",\n+                    tableName,\n+                    format(\"TIMESTAMP '%s'\", entry.getWriteValue()),\n+                    entry.getId()));\n+        }\n+\n+        for (HiveTimestampPrecision precision : HiveTimestampPrecision.values()) {\n+            setSessionProperty(onPresto().getConnection(), \"hive.timestamp_precision\", precision.name());\n+            assertThat(onPresto()\n+                    .executeQuery(format(\n+                            \"SELECT\\n\"\n+                                    + \"id\\n\"\n+                                    + \",arr[1]\\n\"\n+                                    + \",map_entries(map)[1][1]\\n\" // key\n+                                    + \",map_entries(map)[1][2]\\n\" // value\n+                                    + \",row.col\\n\"\n+                                    + \",map_entries(nested[1])[1][1]\\n\" // key\n+                                    + \",map_entries(nested[1])[1][2].col[1]\\n\" // value\n+                                    + \"FROM %s\\n\"\n+                                    + \"ORDER BY id\\n\",\n+                            tableName)))\n+                    .as(\"timestamp containers on %s\", format.getName().toLowerCase(Locale.ENGLISH))\n+                    .containsExactly(TIMESTAMPS_FROM_HIVE.stream()\n+                            .sorted(comparingInt(TimestampAndPrecision::getId))\n+                            .map(e -> new Row(Lists.asList(\n+                                    e.getId(),\n+                                    nCopies(6, Timestamp.valueOf(e.getReadValue(precision))).toArray())))\n+                            .collect(toList()));\n+\n+            for (TimestampAndPrecision entry : TIMESTAMPS_FROM_HIVE) {\n+                if (entry.isRounded(precision)) {\n+                    // Check rounding in the array column\n+                    assertThat(onPresto()\n+                            .executeQuery(format(\n+                                    \"SELECT id FROM %2$s WHERE id = %3$s AND TIMESTAMP '%4$s' %1$c arr[1]\",\n+                                    entry.isRoundedUp(precision) ? '<' : '>',", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjU0NzU4Mg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542547582", "bodyText": "I originally had based this on runTimestampQueries. Now that the rounding checks have been removed there, I've removed this as well.", "author": "jirassimok", "createdAt": "2020-12-14T17:00:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2OTk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3MDMxMA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540870310", "bodyText": "Is it related to authentication?", "author": "findepi", "createdAt": "2020-12-11T11:08:45Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -465,6 +546,21 @@ private void setAdminRole(Connection connection)\n         }\n     }\n \n+    /**\n+     * Ensures that a view named \"dummy\" with exactly one row exists in the default schema.\n+     */\n+    // These tests run on versions of Hive (1.0.0 on CDH 5) that don't fully support SELECT without FROM\n+    // (particularly when using certain forms of authentication).", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUzMDQ2MQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542530461", "bodyText": "The original issue was that Hive tries to check permissions on the internal dummy table when you use a bare SELECT, and fails. I'm not actually sure if it was related to some types of authentication or if it shows up with any authentication at all.\n(It looks like bug slipped through the gaps in Hive's development process. The ticket for this bug in version 1.1 was closed because it was resolved in another ticket, but that one doesn't list 1.1.)\n(I'll just remove that line of the comment; it doesn't really matter.)", "author": "jirassimok", "createdAt": "2020-12-14T16:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3MDMxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3MDUwNQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540870505", "bodyText": "i think CDH 5 uses Hive 1.1.x, not 1.0.0 ?", "author": "findepi", "createdAt": "2020-12-11T11:09:03Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -465,6 +546,21 @@ private void setAdminRole(Connection connection)\n         }\n     }\n \n+    /**\n+     * Ensures that a view named \"dummy\" with exactly one row exists in the default schema.\n+     */\n+    // These tests run on versions of Hive (1.0.0 on CDH 5) that don't fully support SELECT without FROM", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3MTQxMQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540871411", "bodyText": "Let's not implicate views here. Let's just have single-row table.\nI see you're concerned someone will add a second row to the table and thus it won't be single-row table anymore, but that would be rogue and tests would fail telling us about that. thus i wouldn't be concerned about that.", "author": "findepi", "createdAt": "2020-12-11T11:10:33Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -465,6 +546,21 @@ private void setAdminRole(Connection connection)\n         }\n     }\n \n+    /**\n+     * Ensures that a view named \"dummy\" with exactly one row exists in the default schema.\n+     */\n+    // These tests run on versions of Hive (1.0.0 on CDH 5) that don't fully support SELECT without FROM\n+    // (particularly when using certain forms of authentication).\n+    private void ensureDummyExists()\n+            throws SQLException\n+    {\n+        onHive().executeQuery(\"CREATE TABLE IF NOT EXISTS `_dummy_table_` (`_dummy_column_` varchar(1))\");\n+        onHive().executeQuery(\"INSERT INTO `_dummy_table_` VALUES ('x')\");\n+        onHive().executeQuery(\n+                \"CREATE VIEW IF NOT EXISTS dummy (`_dummy_column_`)\"\n+                        + \"AS SELECT 'x' FROM `_dummy_table_` LIMIT 1\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUzMTUzNQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542531535", "bodyText": "Already done. I just re-create the table each time.", "author": "jirassimok", "createdAt": "2020-12-14T16:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3MTQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3ODQ1Mw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540878453", "bodyText": "The JDBC representation may or may not play tricks on us.\nWould be nice to also (first) validate the typeof(the_array) and CAST(the_array AS varchar).\nSee also #6297. I use this technique in some other upcoming (unrelated) PR as well.", "author": "findepi", "createdAt": "2020-12-11T11:23:25Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -430,6 +434,83 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)\n+    public void testStructTimestamps(StorageFormat format)\n+            throws SQLException\n+    {\n+        setAdminRole(onPresto().getConnection());\n+        ensureDummyExists();\n+\n+        String tableName = format(\"test_struct_timestamp_precision_%s_%s\", format.getName().toLowerCase(Locale.ENGLISH), randomTableSuffix());\n+\n+        onPresto().executeQuery(format(\n+                \"CREATE TABLE %s (\\n\"\n+                        + \"id INTEGER\\n\"\n+                        + \",arr ARRAY(TIMESTAMP)\\n\"\n+                        + \",map MAP(TIMESTAMP, TIMESTAMP)\\n\"\n+                        + \",row ROW(col TIMESTAMP)\\n\"\n+                        + \",nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\\n\"\n+                        + \") WITH (%s)\",\n+                tableName,\n+                format.getStoragePropertiesAsSql()));\n+\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto\n+        for (TimestampAndPrecision entry : TIMESTAMPS_FROM_HIVE) {\n+            onHive().executeQuery(format(\n+                    \"INSERT INTO %1$s\\n\"\n+                            // insert with SELECT because hive bans array/map/struct functions in VALUES\n+                            + \"SELECT\\n\"\n+                            + \"%3$s\\n\"\n+                            + \",array(%2$s)\\n\"\n+                            + \",map(%2$s, %2$s)\\n\"\n+                            + \",named_struct('col', %2$s)\\n\"\n+                            + \",array(map(%2$s, named_struct('col', array(%2$s))))\\n\"\n+                            // some hive versions don't allow INSERT from SELECT without FROM\n+                            + \"FROM dummy\\n\",\n+                    tableName,\n+                    format(\"TIMESTAMP '%s'\", entry.getWriteValue()),\n+                    entry.getId()));\n+        }\n+\n+        for (HiveTimestampPrecision precision : HiveTimestampPrecision.values()) {\n+            setSessionProperty(onPresto().getConnection(), \"hive.timestamp_precision\", precision.name());\n+            assertThat(onPresto()\n+                    .executeQuery(format(\n+                            \"SELECT\\n\"\n+                                    + \"id\\n\"\n+                                    + \",arr[1]\\n\"\n+                                    + \",map_entries(map)[1][1]\\n\" // key\n+                                    + \",map_entries(map)[1][2]\\n\" // value\n+                                    + \",row.col\\n\"\n+                                    + \",map_entries(nested[1])[1][1]\\n\" // key\n+                                    + \",map_entries(nested[1])[1][2].col[1]\\n\" // value\n+                                    + \"FROM %s\\n\"\n+                                    + \"ORDER BY id\\n\",\n+                            tableName)))\n+                    .as(\"timestamp containers on %s\", format.getName().toLowerCase(Locale.ENGLISH))\n+                    .containsExactly(TIMESTAMPS_FROM_HIVE.stream()\n+                            .sorted(comparingInt(TimestampAndPrecision::getId))\n+                            .map(e -> new Row(Lists.asList(\n+                                    e.getId(),\n+                                    nCopies(6, Timestamp.valueOf(e.getReadValue(precision))).toArray())))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDkyNjI3Mw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r540926273", "bodyText": "some other upcoming (unrelated) PR as well.\n\nsee #6300, the Verify bound parameters on the server side commit", "author": "findepi", "createdAt": "2020-12-11T12:53:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3ODQ1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjkwNDAzOA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r542904038", "bodyText": "The type check works fine, but trying to cast these values to varchar like in runTimestampQueries causes a Presto compiler error. I don't think this is the kind of bug I was supposed to catch with that test, but it's what I found.\nSET SESSION hive.timestamp_precision = 'NANOSECONDS';\nCREATE TABLE test (col ARRAY(TIMESTAMP));\nSELECT CAST(col[1] AS VARCHAR) FROM test;\n\nQuery 20201214_224143_00038_sipnt failed: Compiler failed\n\nThis is the root cause on the server:\nCaused by: java.lang.VerifyError: Bad type on operand stack\nException Details:\n  Location:\n    io/prestosql/$gen/CursorProcessor_20201214_34.project_0(ConnectorSession;RecordCursor;BlockBuilder;) @75: invokedynamic\n  Reason:\n    Type 'java/lang/Object' (current frame, stack[1]) is not assignable to 'io/prestosql/spi/type/LongTimestamp'\n\nHere is a more-complete version of the stack trace\nio.prestosql.spi.PrestoException: Compiler failed\n\tat io.prestosql.sql.planner.LocalExecutionPlanner$Visitor.visitScanFilterAndProject(LocalExecutionPlanner.java:1386)\n\tat io.prestosql.sql.planner.LocalExecutionPlanner$Visitor.visitProject(LocalExecutionPlanner.java:1265)\n\tat io.prestosql.sql.planner.LocalExecutionPlanner$Visitor.visitProject(LocalExecutionPlanner.java:744)\n\tat io.prestosql.sql.planner.plan.ProjectNode.accept(ProjectNode.java:82)\n\tat io.prestosql.sql.planner.LocalExecutionPlanner.plan(LocalExecutionPlanner.java:484)\n\tat io.prestosql.sql.planner.LocalExecutionPlanner.plan(LocalExecutionPlanner.java:406)\n\tat io.prestosql.execution.SqlTaskExecutionFactory.create(SqlTaskExecutionFactory.java:84)\n\tat io.prestosql.execution.SqlTask.updateTask(SqlTask.java:418)\n\tat io.prestosql.execution.SqlTaskManager.updateTask(SqlTaskManager.java:395)\n\tat io.prestosql.server.TaskResource.createOrUpdateTask(TaskResource.java:132)\n\tat jdk.internal.reflect.GeneratedMethodAccessor748.invoke(Unknown Source)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)\n        [...] (lots of Jersey and Jetty internals with a few Airtlift lines)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: java.lang.VerifyError: Bad type on operand stack\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3951)\n\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)\n\tat com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)\n\tat io.prestosql.sql.gen.ExpressionCompiler.compileCursorProcessor(ExpressionCompiler.java:83)\n\tat io.prestosql.sql.planner.LocalExecutionPlanner$Visitor.visitScanFilterAndProject(LocalExecutionPlanner.java:1349)\n\t... 71 more\nCaused by: java.lang.RuntimeException: java.lang.VerifyError: Bad type on operand stack\n\tat io.airlift.bytecode.ClassGenerator.defineClasses(ClassGenerator.java:181)\n\tat io.airlift.bytecode.ClassGenerator.defineClass(ClassGenerator.java:117)\n\tat io.prestosql.util.CompilerUtils.defineClass(CompilerUtils.java:63)\n\tat io.prestosql.util.CompilerUtils.defineClass(CompilerUtils.java:57)\n\tat io.prestosql.sql.gen.ExpressionCompiler.compileProcessor(ExpressionCompiler.java:167)\n\tat io.prestosql.sql.gen.ExpressionCompiler.compile(ExpressionCompiler.java:134)\n\tat io.prestosql.sql.gen.ExpressionCompiler.lambda$new$0(ExpressionCompiler.java:70)\n\tat com.google.common.cache.CacheLoader$FunctionToCacheLoader.load(CacheLoader.java:165)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n\t... 77 more\nCaused by: java.lang.VerifyError: Bad type on operand stack\nException Details:\n  Location:\n    io/prestosql/$gen/CursorProcessor_20201214_224144_34.project_0(Lio/prestosql/spi/connector/ConnectorSession;Lio/prestosql/spi/connector/RecordCursor;Lio/prestosql/spi/block/BlockBuilder;)V @75: invokedynamic\n  Reason:\n    Type 'java/lang/Object' (current frame, stack[1]) is not assignable to 'io/prestosql/spi/type/LongTimestamp'\n  Current Frame:\n    bci: @75\n    flags: { }\n    locals: { 'io/prestosql/$gen/CursorProcessor_20201214_224144_34', 'io/prestosql/spi/connector/ConnectorSession', 'io/prestosql/spi/connector/RecordCursor', 'io/prestosql/spi/block/BlockBuilder', integer }\n    stack: { 'io/prestosql/spi/block/BlockBuilder', 'java/lang/Object' }\n  Bytecode:\n    0000000: 0336 042d 2c03 b900 4402 0099 000a 0436\n    0000010: 0401 a700 0d2c 03b9 0048 0200 c000 4a15\n    0000020: 0499 0008 5701 a700 1b0a 1504 9900 0958\n    0000030: 5701 a700 0fba 0057 0000 59c7 0006 0436\n    0000040: 0415 0499 0008 5701 a700 08ba 005d 0000\n    0000050: 1504 9900 0d57 b900 6301 0057 a700 153a\n    0000060: 053a 06ba 0069 0000 1906 1905 b900 6f03\n    0000070: 00b1                                   \n  Stackmap Table:\n    full_frame(@21,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95]})\n    full_frame(@31,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#74]})\n    full_frame(@41,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#74]})\n    full_frame(@53,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#74],Long})\n    full_frame(@65,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#4]})\n    full_frame(@75,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#4]})\n    full_frame(@80,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#118]})\n    full_frame(@95,{Object[#2],Object[#64],Object[#23],Object[#95],Integer},{Object[#95],Object[#118]})\n    same_frame(@113)\n\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:398)\n\tat com.google.common.reflect.Reflection.initialize(Reflection.java:65)\n\tat io.airlift.bytecode.ClassGenerator.defineClasses(ClassGenerator.java:177)\n\t... 88 more", "author": "jirassimok", "createdAt": "2020-12-14T22:58:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3ODQ1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzQwNjExOA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r543406118", "bodyText": "See #4824 for previous cause.", "author": "hashhar", "createdAt": "2020-12-15T14:43:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3ODQ1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzYzNjMzNA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r543636334", "bodyText": "Opened as issue #6350.", "author": "jirassimok", "createdAt": "2020-12-15T19:43:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3ODQ1Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDI2ODc3NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r544268774", "bodyText": "The cmt msg says:\n\n\nRename two methods in TestHiveStorageFormats\n\nsetupTimestampData to setupTimestampTable\n\n\n\n\nI guess it would need an update", "author": "findepi", "createdAt": "2020-12-16T12:44:13Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -338,107 +346,93 @@ public void testTimestampCreatedFromHive(StorageFormat storageFormat)\n             throws Exception\n     {\n         String tableName = \"test_timestamp_\" + storageFormat.getName().toLowerCase(Locale.ENGLISH);\n-        setupTimestampData(tableName, storageFormat);\n+        createSimpleTimestampTable(tableName, storageFormat);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDI3MjQ1OQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r544272459", "bodyText": "The getPrecision could return the precision and let the (only) caller apply name() on their own.", "author": "findepi", "createdAt": "2020-12-16T12:50:01Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -693,22 +705,22 @@ public int getId()\n \n         public String getPrecision()\n         {\n-            return precision;\n+            return precision.name();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyMzY4Nw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547223687", "bodyText": "Let's add // TODO https://github.com/prestosql/presto/issues/6350\nAs a temporary workaround, let's use cast to JSON:\npresto:default> SELECT CAST(col AS json) FROM test;\n              _col0\n---------------------------------\n [\"2020-12-22 12:19:45.7786160\"]\n\nand then\npresto:default> SELECT regexp_replace(json_format(CAST(col AS json)), '^\\[\"|\"\\]$') FROM test;\n            _col0\n-----------------------------\n 2020-12-22 12:19:45.7786160", "author": "findepi", "createdAt": "2020-12-22T11:26:17Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -600,30 +600,30 @@ public void testStructTimestamps(StorageFormat format)\n                             format(\"row(col %s)\", type),\n                             format(\"array(map(%1$s, row(col array(%1$s))))\", type)));\n \n-            // Check the values as varchar\n-            String query = format(\n-                    \"SELECT\"\n-                            + \"   id,\"\n-                            + \"   CAST(arr[1] AS VARCHAR),\"\n-                            + \"   CAST(map_entries(map)[1][1] AS VARCHAR),\" // key\n-                            + \"   CAST(map_entries(map)[1][2] AS VARCHAR),\" // value\n-                            + \"   CAST(row.col AS VARCHAR),\"\n-                            + \"   CAST(map_entries(nested[1])[1][1] AS VARCHAR),\" // key\n-                            + \"   CAST(map_entries(nested[1])[1][2].col[1] AS VARCHAR)\" // value\n-                            + \" FROM %s\"\n-                            + \" ORDER BY id\",\n-                    tableName);\n-            System.out.println(query);\n-            onPresto().executeQuery(query);\n-            assertThat(onPresto()\n-                    .executeQuery(query))\n-                    .as(\"timestamp containers on %s\", format.getName().toLowerCase(Locale.ENGLISH))\n-                    .containsExactly(TIMESTAMPS_FROM_HIVE.stream()\n-                            .sorted(comparingInt(TimestampAndPrecision::getId))\n-                            .map(e -> new Row(Lists.asList(\n-                                    e.getId(),\n-                                    nCopies(6, e.getReadValue(precision)).toArray())))\n-                            .collect(toList()));\n+            // // Check the values as varchar\n+            // String query = format(\n+            //         \"SELECT\"\n+            //                 + \"   id,\"\n+            //                 + \"   CAST(arr[1] AS VARCHAR),\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyNTY0NA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547225644", "bodyText": "do not abbreviate prec", "author": "findepi", "createdAt": "2020-12-22T11:30:33Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -67,6 +67,138 @@\n     @Named(\"databases.presto.admin_role_enabled\")\n     private boolean adminRoleEnabled;\n \n+    private static final List<TimestampAndPrecision> TIMESTAMPS_FROM_HIVE;\n+\n+    static {\n+        // Tests set from 1960 to 1968 are duplicated below as tests in the 2020s\n+        List<TimestampAndPrecision> preEpoch = List.of(\n+                // write precision is not relevant here, as Hive always uses nanos\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111\", // millis, no rounding\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111000\",\n+                        \"1967-01-02 12:01:00.111000000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1114\", // hundreds of micros, rounds down in millis\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111400\",\n+                        \"1967-01-02 12:01:00.111400000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1115\", // hundreds of micros, rounds up in millis (smallest)\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.112\",\n+                        \"1967-01-02 12:01:00.111500\",\n+                        \"1967-01-02 12:01:00.111500000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111499\", // micros, rounds down (largest)\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111499\",\n+                        \"1967-01-02 12:01:00.111499000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1113334\", // hundreds of nanos, rounds down\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111333\",\n+                        \"1967-01-02 12:01:00.111333400\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1110019\", // hundreds of nanos, rounds down in millis and up in micros\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111002\",\n+                        \"1967-01-02 12:01:00.111001900\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111901001\", // nanos, rounds up in millis and down in micros\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.112\",\n+                        \"1967-01-02 12:01:00.111901\",\n+                        \"1967-01-02 12:01:00.111901001\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 23:59:59.999999999\", // nanos, rounds up to next day\n+                        NANOSECONDS,\n+                        \"1967-01-03 00:00:00.000\",\n+                        \"1967-01-03 00:00:00.000000\",\n+                        \"1967-01-02 23:59:59.999999999\"),\n+                timestampAndPrecision(\n+                        \"1967-12-31 23:59:59.999999499\", // nanos, rounds micros down (largest), rounds millis up to next year\n+                        NANOSECONDS,\n+                        \"1968-01-01 00:00:00.000\",\n+                        \"1967-12-31 23:59:59.999999\",\n+                        \"1967-12-31 23:59:59.999999499\"));\n+\n+        // Make post-epoch tests\n+        Stream<TimestampAndPrecision> postEpoch = preEpoch.stream()\n+                // Exclude years outside of 1960-1968\n+                .filter(timePrec -> !timePrec.getWriteValue().matches(\"^196[^9]\"))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyNjExNg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547226116", "bodyText": "This makes it not so explicit about what's actually tested.\nLet's enumerate the test cases explicitly, without generating them on the fly.", "author": "findepi", "createdAt": "2020-12-22T11:31:45Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -67,6 +67,138 @@\n     @Named(\"databases.presto.admin_role_enabled\")\n     private boolean adminRoleEnabled;\n \n+    private static final List<TimestampAndPrecision> TIMESTAMPS_FROM_HIVE;\n+\n+    static {\n+        // Tests set from 1960 to 1968 are duplicated below as tests in the 2020s\n+        List<TimestampAndPrecision> preEpoch = List.of(\n+                // write precision is not relevant here, as Hive always uses nanos\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111\", // millis, no rounding\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111000\",\n+                        \"1967-01-02 12:01:00.111000000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1114\", // hundreds of micros, rounds down in millis\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111400\",\n+                        \"1967-01-02 12:01:00.111400000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1115\", // hundreds of micros, rounds up in millis (smallest)\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.112\",\n+                        \"1967-01-02 12:01:00.111500\",\n+                        \"1967-01-02 12:01:00.111500000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111499\", // micros, rounds down (largest)\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111499\",\n+                        \"1967-01-02 12:01:00.111499000\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1113334\", // hundreds of nanos, rounds down\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111333\",\n+                        \"1967-01-02 12:01:00.111333400\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.1110019\", // hundreds of nanos, rounds down in millis and up in micros\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.111\",\n+                        \"1967-01-02 12:01:00.111002\",\n+                        \"1967-01-02 12:01:00.111001900\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 12:01:00.111901001\", // nanos, rounds up in millis and down in micros\n+                        NANOSECONDS,\n+                        \"1967-01-02 12:01:00.112\",\n+                        \"1967-01-02 12:01:00.111901\",\n+                        \"1967-01-02 12:01:00.111901001\"),\n+                timestampAndPrecision(\n+                        \"1967-01-02 23:59:59.999999999\", // nanos, rounds up to next day\n+                        NANOSECONDS,\n+                        \"1967-01-03 00:00:00.000\",\n+                        \"1967-01-03 00:00:00.000000\",\n+                        \"1967-01-02 23:59:59.999999999\"),\n+                timestampAndPrecision(\n+                        \"1967-12-31 23:59:59.999999499\", // nanos, rounds micros down (largest), rounds millis up to next year\n+                        NANOSECONDS,\n+                        \"1968-01-01 00:00:00.000\",\n+                        \"1967-12-31 23:59:59.999999\",\n+                        \"1967-12-31 23:59:59.999999499\"));\n+\n+        // Make post-epoch tests\n+        Stream<TimestampAndPrecision> postEpoch = preEpoch.stream()\n+                // Exclude years outside of 1960-1968\n+                .filter(timePrec -> !timePrec.getWriteValue().matches(\"^196[^9]\"))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDMyMDQyOA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r550320428", "bodyText": "I find that having the fully-expanded test cases makes them harder to keep track of because there are so many covering similar cases.\nBut we probably don't need all of the test cases duplicated pre/post-epoch, so I'll remove the post-epoch duplication and add fewer, more explicit tests instead.", "author": "jirassimok", "createdAt": "2020-12-30T20:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyNjExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyNzI2MA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547227260", "bodyText": "public static Type fromPrimitiveType(PrimitiveTypeInfo typeInfo) should be deprecated now, right?", "author": "findepi", "createdAt": "2020-12-22T11:34:20Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/HiveTypeTranslator.java", "diffHunk": "@@ -227,6 +229,12 @@ public static TypeSignature toTypeSignature(TypeInfo typeInfo)\n \n     @Nullable\n     public static Type fromPrimitiveType(PrimitiveTypeInfo typeInfo)\n+    {\n+        return fromPrimitiveType(typeInfo, DEFAULT_PRECISION);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyODgzNQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547228835", "bodyText": "This is probably a change in behavior, but probably a correct one.\nThe old code\n((TimestampWritable) object).getTimestamp().getTime() * MICROSECONDS_PER_MILLISECOND\n\ninspector.getPrimitiveJavaObject(object).toEpochMilli() * MICROSECONDS_PER_MILLISECOND;\n\nwas probably truncating.\nAm i correct?\nDoes it apply to timestamp outside of structural types as well?\nDo we have a test for this?", "author": "findepi", "createdAt": "2020-12-22T11:37:47Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -312,11 +320,28 @@ private static long formatDateAsLong(Object object, DateObjectInspector inspecto\n         return inspector.getPrimitiveJavaObject(object).toEpochDay();\n     }\n \n-    private static long formatTimestampAsLong(Object object, TimestampObjectInspector inspector)\n+    private static DecodedTimestamp formatTimestamp(TimestampType type, Object object, TimestampObjectInspector inspector)\n     {\n+        long epochSecond;\n+        int nanoOfSecond;\n+\n         if (object instanceof TimestampWritable) {\n-            return ((TimestampWritable) object).getTimestamp().getTime() * MICROSECONDS_PER_MILLISECOND;\n+            TimestampWritable timestamp = (TimestampWritable) object;\n+            epochSecond = timestamp.getSeconds();\n+            nanoOfSecond = timestamp.getNanos();\n         }\n-        return inspector.getPrimitiveJavaObject(object).toEpochMilli() * MICROSECONDS_PER_MILLISECOND;\n+        else {\n+            Timestamp timestamp = inspector.getPrimitiveJavaObject(object);\n+            epochSecond = timestamp.toEpochSecond();\n+            nanoOfSecond = timestamp.getNanos();\n+        }\n+\n+        nanoOfSecond = (int) round(nanoOfSecond, 9 - type.getPrecision());\n+        if (nanoOfSecond == NANOSECONDS_PER_SECOND) { // round nanos up to seconds\n+            epochSecond += 1;\n+            nanoOfSecond = 0;\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDMzODQ0Mg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r550338442", "bodyText": "Yes, the original version did truncate.\n\nDoes it apply to timestamp outside of structural types as well?\n\nNo; the only public API in SerdeUtils is getBlockObject1, which is only used in GenericHiveRecordCursor.parseObjectColumn, and only when type.getJavaType() == Block.class.\n\nDo we have a test for this?\n\nWe did not previously have a  test for this (the only tests I can find I in five minutes for arrays in Hive is in TestHiveIntegrationSmokeTest). The tests added in this PR do cover it, though (because they test all available precisions).\nWe should probably have a class like TestHiveTypes. Should I open an issue for that?\n\n1 serializeObject is also public, but it looks like it might be missing @VisibleForTesting. It is only used in SerDeUtils and tests, and its overload is @VisibleForTesting.", "author": "jirassimok", "createdAt": "2020-12-30T21:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyODgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIyOTMwNg==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547229306", "bodyText": "move the TIMESTAMP case back here, after DATE case.", "author": "findepi", "createdAt": "2020-12-22T11:38:51Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/SerDeUtils.java", "diffHunk": "@@ -137,12 +143,14 @@ private static void serializePrimitive(Type type, BlockBuilder builder, Object o\n             case DATE:\n                 type.writeLong(builder, formatDateAsLong(object, (DateObjectInspector) inspector));\n                 return;\n-            case TIMESTAMP:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIzMTQzNA==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547231434", "bodyText": "Pre-existing, but would be good to have storageFormatsWithNanosecondPrecision be defined in terms of storageFormats + exclusions, eg\n@DataProvider\npublic static Object[][] storageFormatsWithNanosecondPrecision()\n{\n    return Stream.of(storageFormats())\n            .map(array -> (StorageFormat) getOnlyElement(Arrays.asList(array)))\n            .filter(storageFormat -> !storageFormat.getName().equals(\"AVRO\"))\n            .map(storageFormat -> new Object[] {storageFormat})\n            .toArray(Object[][]::new);\n}", "author": "findepi", "createdAt": "2020-12-22T11:44:02Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -537,6 +540,117 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIzMzU3Nw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r547233577", "bodyText": "How much time does the test take?\nHow many times do we run the test on CI?\nstorage formats are run in multiple configurations -- for good reason. Since HDFS access patterns vary between formats, we run these tests for all supported HDFS authentication/impersonation configurations. This one, however, doesn't need to be run for all of them. Thus --and especially if it takes significant amount of time -- we should change its group definiion. Perhaps, it would be enough to a) remove storage_format group and b) add a comment why the test is not in that group.\n(this applies to existing test for nonstructural timestamp as well)", "author": "findepi", "createdAt": "2020-12-22T11:49:22Z", "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -537,6 +540,117 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI1MjM4MQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r550252381", "bodyText": "This test doesn't always have the same behavior for different storage formats because of time zone storage differences.", "author": "jirassimok", "createdAt": "2020-12-30T16:29:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzIzMzU3Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "6003c6eeefbc32cfb7e615c9f36c99aa2653efd1", "url": "https://github.com/trinodb/trino/commit/6003c6eeefbc32cfb7e615c9f36c99aa2653efd1", "message": "Improve timestamp test cases used in Hive storage format tests\n\n- Update timestamp values\n  - Use values with visually-clearer rounding behavior (like \"1113339\"\n    instead of \"1234567\").\n    - This also includes rounding up from 5 and down from 499.\n  - Add timestamp that round up to the next day/year\n  - Add comments indicating each test's purpose\n\n- Move test data out of timestamp tests\n\n  The TimestampAndPrecision data container uses a static,\n  auto-incrementing ID. Making the data static ensures that the IDs\n  will be the same for each storage format.", "committedDate": "2021-01-05T17:10:12Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "732d42237b47e3e9f94082ba65afd8b997046449", "url": "https://github.com/trinodb/trino/commit/732d42237b47e3e9f94082ba65afd8b997046449", "message": "Allow custom timestamp precision in Hive structural types\n\nAlso, mark methods not taking timestamp precision as deprecated:\n- HiveTypeTranslator.fromPrimitiveType(PrimitiveTypeInfo)\n- HiveType.getTypeSignature()", "committedDate": "2021-01-05T17:21:46Z", "type": "commit"}, {"oid": "426273bdc8585d9b5cd743c9bfe1f6dc9c97330a", "url": "https://github.com/trinodb/trino/commit/426273bdc8585d9b5cd743c9bfe1f6dc9c97330a", "message": "Fix type of TimestampAndPrecision.getPrecision", "committedDate": "2021-01-05T17:21:53Z", "type": "commit"}, {"oid": "4aff1cc9a4aabaeb485be89d9c30dd1a5e0e16d8", "url": "https://github.com/trinodb/trino/commit/4aff1cc9a4aabaeb485be89d9c30dd1a5e0e16d8", "message": "Improve data providers in TestHiveStorageFormats\n\n- Generate list of storage formats with nanosecond precision by\n  filtering the list of all tested storage formats.\n- Use explicit data provider return types (to facilitate the above)\n- Use data provider method names instead of assigning custom names", "committedDate": "2021-01-05T17:21:53Z", "type": "commit"}, {"oid": "4aff1cc9a4aabaeb485be89d9c30dd1a5e0e16d8", "url": "https://github.com/trinodb/trino/commit/4aff1cc9a4aabaeb485be89d9c30dd1a5e0e16d8", "message": "Improve data providers in TestHiveStorageFormats\n\n- Generate list of storage formats with nanosecond precision by\n  filtering the list of all tested storage formats.\n- Use explicit data provider return types (to facilitate the above)\n- Use data provider method names instead of assigning custom names", "committedDate": "2021-01-05T17:21:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMwOTk1Mw==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r552309953", "bodyText": "Can you do this with VALUES?", "author": "electrum", "createdAt": "2021-01-06T01:18:09Z", "path": "testing/trino-product-tests/src/main/java/io/trino/tests/hive/TestHiveStorageFormats.java", "diffHunk": "@@ -543,6 +546,114 @@ private void runTimestampQueries(String tableName, List<TimestampAndPrecision> d\n         onPresto().executeQuery(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test(dataProvider = \"storageFormatsWithNanosecondPrecision\", groups = STORAGE_FORMATS)\n+    public void testStructTimestamps(StorageFormat format)\n+            throws SQLException\n+    {\n+        setAdminRole(onPresto().getConnection());\n+        ensureDummyExists();\n+\n+        String tableName = format(\"test_struct_timestamp_precision_%s_%s\", format.getName().toLowerCase(Locale.ENGLISH), randomTableSuffix());\n+\n+        onPresto().executeQuery(format(\n+                \"CREATE TABLE %s (\"\n+                        + \"   id INTEGER,\"\n+                        + \"   arr ARRAY(TIMESTAMP),\"\n+                        + \"   map MAP(TIMESTAMP, TIMESTAMP),\"\n+                        + \"   row ROW(col TIMESTAMP),\"\n+                        + \"   nested ARRAY(MAP(TIMESTAMP, ROW(col ARRAY(TIMESTAMP))))\"\n+                        + \") WITH (%s)\",\n+                tableName,\n+                format.getStoragePropertiesAsSql()));\n+\n+        // Insert in a loop because inserting with UNION ALL sometimes makes values invisible to Presto", "originalCommit": "732d42237b47e3e9f94082ba65afd8b997046449", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg1NTQ1OQ==", "url": "https://github.com/trinodb/trino/pull/5695#discussion_r552855459", "bodyText": "No, because Hive doesn't allow functions like array and named_struct in VALUES.\n\n\nHive does not support literals for complex types (array, map, struct, union), so it is not possible to use them in INSERT INTO...VALUES clauses. This means that the user cannot insert data into a complex datatype column using the INSERT INTO...VALUES clause.", "author": "jirassimok", "createdAt": "2021-01-06T17:49:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMwOTk1Mw=="}], "type": "inlineReview"}]}