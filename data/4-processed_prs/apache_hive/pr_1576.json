{"pr_number": 1576, "pr_title": "HIVE-24266", "pr_createdAt": "2020-10-13T13:00:27Z", "pr_url": "https://github.com/apache/hive/pull/1576", "timeline": [{"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253", "url": "https://github.com/apache/hive/commit/66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253", "message": "HIVE-24266 - initial commit\n\nChange-Id: I54f414af6b81f3180217f70ac7ac03d2c376324b", "committedDate": "2020-10-13T12:53:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk0NTYwOQ==", "url": "https://github.com/apache/hive/pull/1576#discussion_r503945609", "bodyText": "nit: space", "author": "pvary", "createdAt": "2020-10-13T13:21:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {", "originalCommit": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA==", "url": "https://github.com/apache/hive/pull/1576#discussion_r503950040", "bodyText": "Why is this capped with blockSize?\nIsn't there a case where we are at the end of the block but still writing?\nSplitLength might be this and it is easier to understad:\nlogicalLen - blockOffset", "author": "pvary", "createdAt": "2020-10-13T13:26:55Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {\n                 //don't create splits for anything past logical EOF\n-                continue;\n+                //map is ordered, thus any possible entry in the iteration after this is bound to be > logicalLen\n+                break;\n               }\n-              OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), fileKey, entry.getKey(),\n-                entry.getValue().getLength(), entry.getValue().getHosts(), null, isOriginal, true,\n-                deltas, -1, logicalLen, dir, offsetAndBucket);\n+              long splitLength = blockLength;\n+\n+              long blockEndOvershoot = (blockOffset + blockLength) - logicalLen;\n+              if (blockEndOvershoot > 0) {\n+                // if logicalLen is placed within a block, we should make (this last) split out of the part of this block\n+                // -> we should read less than block end\n+                splitLength -= blockEndOvershoot;\n+              } else if (blockOffsets.lastKey() == blockOffset && blockEndOvershoot < 0) {\n+                // This is the last block but it ends before logicalLen\n+                // This can happen with HDFS if hflush was called and blocks are not persisted to disk yet, but content\n+                // is otherwise available for readers, as DNs have these buffers in memory at this time.\n+                // -> we should read more than (persisted) block end, but surely not more than the whole block\n+                if (fileStatus instanceof HdfsLocatedFileStatus) {\n+                  HdfsLocatedFileStatus hdfsFileStatus = (HdfsLocatedFileStatus)fileStatus;\n+                  if (hdfsFileStatus.getLocatedBlocks().isUnderConstruction()) {\n+                    // blockEndOvershoot is negative here...\n+                    splitLength = Math.min(splitLength - blockEndOvershoot, hdfsFileStatus.getBlockSize());", "originalCommit": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk3MDk2MA==", "url": "https://github.com/apache/hive/pull/1576#discussion_r503970960", "bodyText": "hdfsFileStatus.blockSize() is not the block length, but the configured (max) block size (e.g. 256MB) - that's how big a block can be max, and that's why we shouldn't read past that in this split", "author": "szlta", "createdAt": "2020-10-13T13:53:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUzNDkxMg==", "url": "https://github.com/apache/hive/pull/1576#discussion_r504534912", "bodyText": "Maybe this is just a theoretical problem, but if the blockOffset + hdfsFileStatus.getBlockSize() is greater in the last block than the logicalLen, then we should throw an exception, and then we do not need the min here", "author": "pvary", "createdAt": "2020-10-14T09:28:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU0OTAzNQ==", "url": "https://github.com/apache/hive/pull/1576#discussion_r504549035", "bodyText": "okay, that makes sense", "author": "szlta", "createdAt": "2020-10-14T09:51:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}], "type": "inlineReview"}, {"oid": "82d734c1f92e71b4ebdb391d7fc227c4b2344ce2", "url": "https://github.com/apache/hive/commit/82d734c1f92e71b4ebdb391d7fc227c4b2344ce2", "message": "Addressing review comments\n\nChange-Id: I5ba9f0f98d694b99b4a62ee9a322177a2a1c914d", "committedDate": "2020-10-14T09:50:31Z", "type": "commit"}]}