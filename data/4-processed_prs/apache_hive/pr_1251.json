{"pr_number": 1251, "pr_title": "HIVE-23840: Use LLAP to get orc metadata", "pr_createdAt": "2020-07-14T09:48:45Z", "pr_url": "https://github.com/apache/hive/pull/1251", "timeline": [{"oid": "67a71b4a9953a3e897342bf7c1de39ba93bdfbfd", "url": "https://github.com/apache/hive/commit/67a71b4a9953a3e897342bf7c1de39ba93bdfbfd", "message": "HIVE-23840: Use LLAP to get orc metadata", "committedDate": "2020-07-14T09:07:10Z", "type": "commit"}, {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "url": "https://github.com/apache/hive/commit/c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "message": "Addendum", "committedDate": "2020-07-14T10:06:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454390429", "bodyText": "We could spare the deserialization of MapWork from JobConf here, if we pass the MapWork instance already present in LlapRecordReader to VectorizedOrcAcidRowBatchReader ctor. (Downside is that in turn we would need to adjust the other ctor's of VectorizedOrcAcidRowBatchReader too)", "author": "szlta", "createdAt": "2020-07-14T14:18:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -232,6 +250,17 @@ private VectorizedOrcAcidRowBatchReader(JobConf conf, OrcSplit orcSplit, Reporte\n \n     this.syntheticProps = orcSplit.getSyntheticAcidProps();\n \n+    if (LlapHiveUtils.isLlapMode(conf) && LlapProxy.isDaemon()\n+            && HiveConf.getBoolVar(conf, ConfVars.LLAP_TRACK_CACHE_USAGE))\n+    {\n+      MapWork mapWork = LlapHiveUtils.findMapWork(conf);", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjcyNw==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602727", "bodyText": "Good idea, done!", "author": "pvary", "createdAt": "2020-07-14T19:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454393621", "bodyText": "Initialized to true on purpose for now? If not, I don't see it getting set to false.", "author": "szlta", "createdAt": "2020-07-14T14:22:30Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -129,6 +137,16 @@\n    */\n   private SearchArgument deleteEventSarg = null;\n \n+  /**\n+   * Cachetag associated with the Split\n+   */\n+  private final CacheTag cacheTag;\n+\n+  /**\n+   * Skip using Llap IO cache for checking delete_delta files if the configuration is not correct\n+   */\n+  private static boolean skipLlapCache = true;", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjkwNA==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602904", "bodyText": "That was a mistake. Corrected, and initialized as false", "author": "pvary", "createdAt": "2020-07-14T19:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454403919", "bodyText": "nit: comment could be more verbose, like: Reader can be reused if it was created before: only for non-LLAP cache cases, otherwise we need to create it here", "author": "szlta", "createdAt": "2020-07-14T14:35:46Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1562,20 +1580,31 @@ public int compareTo(CompressedOwid other) {\n       try {\n         final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n         if (deleteDeltaDirs.length > 0) {\n+          FileSystem fs = orcSplit.getPath().getFileSystem(conf);\n+          AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n+              AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            FileSystem fs = deleteDeltaDir.getFileSystem(conf);\n+            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+              continue;\n+            }\n             Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n                 new OrcRawRecordMerger.Options().isCompacting(false), null);\n             for (Path deleteDeltaFile : deleteDeltaFiles) {\n               try {\n-                /**\n-                 * todo: we have OrcSplit.orcTail so we should be able to get stats from there\n-                 */\n-                Reader deleteDeltaReader = OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                if (deleteDeltaReader.getNumberOfRows() <= 0) {\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n                   continue; // just a safe check to ensure that we are not reading empty delete files.\n                 }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Create the reader if we got the OrcTail from cache", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMzA0Mg==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454603042", "bodyText": "Added more comment", "author": "pvary", "createdAt": "2020-07-14T19:48:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ=="}], "type": "inlineReview"}, {"oid": "57c575ffe2cec83e44008f65574cce4b6711b0e2", "url": "https://github.com/apache/hive/commit/57c575ffe2cec83e44008f65574cce4b6711b0e2", "message": "Addressing Adam's comments, and test failures", "committedDate": "2020-07-14T19:33:35Z", "type": "commit"}, {"oid": "328b2477910ec0a3e45355e6716338d2eace6143", "url": "https://github.com/apache/hive/commit/328b2477910ec0a3e45355e6716338d2eace6143", "message": "Fixed test failures - prepared the stuff for multi statement transactions.\nAlso fix some minor formatting/niceity issues", "committedDate": "2020-07-15T09:45:15Z", "type": "commit"}]}