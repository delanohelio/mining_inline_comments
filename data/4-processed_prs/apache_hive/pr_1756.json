{"pr_number": 1756, "pr_title": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "pr_createdAt": "2020-12-08T23:23:36Z", "pr_url": "https://github.com/apache/hive/pull/1756", "timeline": [{"oid": "34d4f16342262c2f1141a701735a002538f00992", "url": "https://github.com/apache/hive/commit/34d4f16342262c2f1141a701735a002538f00992", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T02:54:50Z", "type": "forcePushed"}, {"oid": "b704f8292717d18d0b6d95a2345ae41e552cbfb6", "url": "https://github.com/apache/hive/commit/b704f8292717d18d0b6d95a2345ae41e552cbfb6", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T09:58:51Z", "type": "forcePushed"}, {"oid": "45375271e127db5186799ed4798ac8fc4225e785", "url": "https://github.com/apache/hive/commit/45375271e127db5186799ed4798ac8fc4225e785", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T17:41:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTY5Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791696", "bodyText": "No need to define this here.  Just use JDK StandardCharsets.", "author": "belugabehr", "createdAt": "2021-01-01T18:09:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDA1NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804054", "bodyText": "Fixed, didn't know about that.", "author": "miklosgergely", "createdAt": "2021-01-01T20:31:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTY5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTgxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791817", "bodyText": "I personally hate NULL values.  Can you get rid of this check (exclude == null) and simply call this method with Collections.emptySet() ?", "author": "belugabehr", "createdAt": "2021-01-01T18:11:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTUxMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805513", "bodyText": "Totally agree, fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTgxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTg5Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791896", "bodyText": "Please add JavaDoc here and also use JDK7+ ability of not needing to explicitly define the Type on the right hand side in several places, for example:\nList<String> realProps = new ArrayList<>();", "author": "belugabehr", "createdAt": "2021-01-01T18:12:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTQ1OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805459", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:50:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTg5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTk0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791945", "bodyText": "Can now just use Java String#join method instead of something third party.", "author": "belugabehr", "createdAt": "2021-01-01T18:12:56Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDY1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804652", "bodyText": "Done", "author": "miklosgergely", "createdAt": "2021-01-01T20:39:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjAxNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792016", "bodyText": "Use StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:13:33Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDAyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804026", "bodyText": "Fixed, thanks.", "author": "miklosgergely", "createdAt": "2021-01-01T20:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjQ3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792472", "bodyText": "new String[0]\nhttps://docs.oracle.com/javase/8/docs/api/java/util/Collection.html#toArray-T:A-", "author": "belugabehr", "createdAt": "2021-01-01T18:18:17Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzkwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923905", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-02T21:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjU4MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792581", "bodyText": "Math#min", "author": "belugabehr", "createdAt": "2021-01-01T18:19:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDcyOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924729", "bodyText": "Math#max - fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792617", "bodyText": "Arrays.asList", "author": "belugabehr", "createdAt": "2021-01-01T18:20:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDg0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924846", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:49:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792633", "bodyText": "new String[0]", "author": "belugabehr", "createdAt": "2021-01-01T18:20:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));\n+    }\n+\n+    public String renderTable(boolean isOutputPadded) {\n+      StringBuilder stringBuilder = new StringBuilder();\n+      for (List<String> row : table) {\n+        formatOutput(row.toArray(new String[] {}), stringBuilder, isOutputPadded, isOutputPadded);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDU3Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924576", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:46:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjg5MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792891", "bodyText": "There's got to be a better way of doing this...\nList#addAll or something other than 1-by-1 iteration.", "author": "belugabehr", "createdAt": "2021-01-01T18:24:07Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));\n+    }\n+\n+    public String renderTable(boolean isOutputPadded) {\n+      StringBuilder stringBuilder = new StringBuilder();\n+      for (List<String> row : table) {\n+        formatOutput(row.toArray(new String[] {}), stringBuilder, isOutputPadded, isOutputPadded);\n+      }\n+      return stringBuilder.toString();\n+    }\n+\n+    public void transpose() {\n+      if (table.size() == 0) {\n+        return;\n+      }\n+      List<List<String>> newTable = new ArrayList<List<String>>();\n+      for (int i = 0; i < table.get(0).size(); i++) {\n+        newTable.add(new ArrayList<>());\n+      }\n+      for (List<String> sourceRow : table) {\n+        if (newTable.size() != sourceRow.size()) {\n+          throw new RuntimeException(\"invalid table size\");\n+        }\n+        for (int i = 0; i < sourceRow.size(); i++) {\n+          newTable.get(i).add(sourceRow.get(i));\n+        }\n+      }\n+      table = newTable;\n+    }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyODczNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550928737", "bodyText": "I don't think it is possible in any other way. Transposing a table must be done like this in essence.", "author": "miklosgergely", "createdAt": "2021-01-02T22:37:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjg5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792946", "bodyText": "StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:24:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.database.desc;\n+\n+import org.apache.commons.collections.MapUtils;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.PrincipalType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Formats DESC DATABASES results.\n+ */\n+abstract class DescDatabaseFormatter {\n+  static DescDatabaseFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonDescDatabaseFormatter();\n+    } else {\n+      return new TextDescDatabaseFormatter();\n+    }\n+  }\n+\n+  abstract void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+      String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+      throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonDescDatabaseFormatter extends DescDatabaseFormatter {\n+    @Override\n+    void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+        String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+        throws HiveException {\n+      MapBuilder builder = MapBuilder.create()\n+          .put(\"database\", database)\n+          .put(\"comment\", comment)\n+          .put(\"location\", location);\n+      if (managedLocation != null) {\n+        builder.put(\"managedLocation\", managedLocation);\n+      }\n+      if (ownerName != null) {\n+        builder.put(\"owner\", ownerName);\n+      }\n+      if (ownerType != null) {\n+        builder.put(\"ownerType\", ownerType.name());\n+      }\n+      if (MapUtils.isNotEmpty(params)) {\n+        builder.put(\"params\", params);\n+      }\n+      ShowUtils.asJson(out, builder.build());\n+    }\n+  }\n+\n+  static class TextDescDatabaseFormatter extends DescDatabaseFormatter {\n+    @Override\n+    void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+        String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+        throws HiveException {\n+      try {\n+        out.write(database.getBytes(\"UTF-8\"));\n+        out.write(Utilities.tabCode);\n+        if (comment != null) {\n+          out.write(HiveStringUtils.escapeJava(comment).getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (location != null) {\n+          out.write(location.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (managedLocation != null) {\n+          out.write(managedLocation.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (ownerName != null) {\n+          out.write(ownerName.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (ownerType != null) {\n+          out.write(ownerType.name().getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (MapUtils.isNotEmpty(params)) {\n+          out.write(params.toString().getBytes(\"UTF-8\"));\n+        }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzczOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923738", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk2NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792965", "bodyText": "StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:25:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.database.show;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW DATABASES results.\n+ */\n+abstract class ShowDatabasesFormatter {\n+  static ShowDatabasesFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowDatabasesFormatter();\n+    } else {\n+      return new TextShowDatabasesFormatter();\n+    }\n+  }\n+\n+  abstract void showDatabases(DataOutputStream out, List<String> databases) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowDatabasesFormatter extends ShowDatabasesFormatter {\n+    @Override\n+    void showDatabases(DataOutputStream out, List<String> databases) throws HiveException {\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"databases\", databases).build());\n+    }\n+  }\n+\n+  static class TextShowDatabasesFormatter extends ShowDatabasesFormatter {\n+    @Override\n+    void showDatabases(DataOutputStream out, List<String> databases) throws HiveException {\n+      try {\n+        for (String database : databases) {\n+          out.write(database.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzcyMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923722", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:36:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793072", "bodyText": "Remove this formatting change.  Little value and adds to this already large review.", "author": "belugabehr", "createdAt": "2021-01-01T18:26:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java", "diffHunk": "@@ -32,7 +32,8 @@\n   private static final long serialVersionUID = 1L;\n \n   public static final String SCHEMA =\n-      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n+      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,\" +\n+      \"errormessage#\" +", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTU1OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925559", "bodyText": "There is a 120 limit in the Hive checkstyle, and I'm trying to make all DDL codes checkstyle violation free. This patch is about making Show kind commands cleaner, that is why it is here.", "author": "miklosgergely", "createdAt": "2021-01-02T21:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzMDYyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550930626", "bodyText": "Added @Formatter:off - @Formatter:on instead", "author": "miklosgergely", "createdAt": "2021-01-02T23:01:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzIxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793217", "bodyText": "... =new ArrayList<>(columns.size());", "author": "belugabehr", "createdAt": "2021-01-01T18:27:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats DESC TABLE results to json format.\n+ */\n+public class JsonDescTableFormatter extends DescTableFormatter {\n+  private static final String COLUMN_NAME = \"name\";\n+  private static final String COLUMN_TYPE = \"type\";\n+  private static final String COLUMN_COMMENT = \"comment\";\n+  private static final String COLUMN_MIN = \"min\";\n+  private static final String COLUMN_MAX = \"max\";\n+  private static final String COLUMN_NUM_NULLS = \"numNulls\";\n+  private static final String COLUMN_NUM_TRUES = \"numTrues\";\n+  private static final String COLUMN_NUM_FALSES = \"numFalses\";\n+  private static final String COLUMN_DISTINCT_COUNT = \"distinctCount\";\n+  private static final String COLUMN_AVG_LENGTH = \"avgColLen\";\n+  private static final String COLUMN_MAX_LENGTH = \"maxColLen\";\n+\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"columns\", createColumnsInfo(columns, columnStats));\n+\n+    if (isExtended) {\n+      addExtendedInfo(table, partition, builder);\n+    }\n+\n+    ShowUtils.asJson(out, builder.build());\n+  }\n+\n+  public static List<Map<String, Object>> createColumnsInfo(List<FieldSchema> columns,\n+      List<ColumnStatisticsObj> columnStatisticsList) {\n+    List<Map<String, Object>> columnsInfo = new ArrayList<>();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDUyMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924522", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzIxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793374", "bodyText": "new String[0]", "author": "belugabehr", "createdAt": "2021-01-01T18:29:14Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzk3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923972", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793378", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:29:34Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzY2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923661", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzQ2OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793468", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:30:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzY0MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923640", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzQ2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzUyOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793528", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:31:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzYyNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923625", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzYxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793617", "bodyText": "tableInfo.append(LINE_DELIM).apppend('#').......append(LINE_DELIM);", "author": "belugabehr", "createdAt": "2021-01-01T18:33:19Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTQwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929405", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:46:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793707", "bodyText": ".append().append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:33:46Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTUxNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929515", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcyMQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793721", "bodyText": ".append().append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:33:52Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTUyNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929524", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzc1MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793751", "bodyText": "(CollectionUtils.isNotEmpty(skewedColValues) (just like immediately below)", "author": "belugabehr", "createdAt": "2021-01-01T18:34:23Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDAyOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924029", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzc1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzg2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793861", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:35:37Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTU0Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929543", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzg2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkxOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793918", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:36:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTU4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929585", "bodyText": "Fixed..", "author": "miklosgergely", "createdAt": "2021-01-02T22:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793933", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:36:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTYxMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929612", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:48:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDM1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794352", "bodyText": "Should be able to replace these methods easily with Lambdas...\nlist.stream().sorted(...).collect(Collectors.toList());", "author": "belugabehr", "createdAt": "2021-01-01T18:40:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTMzOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925339", "bodyText": "Fixed, nice catch!", "author": "miklosgergely", "createdAt": "2021-01-02T21:56:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDM1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDQ5Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794497", "bodyText": "intern of a constant values is probably useless.  Please remove.", "author": "belugabehr", "createdAt": "2021-01-01T18:41:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTQyMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925420", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:57:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDU0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794545", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:42:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTcyOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929728", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:50:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDU0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYxMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794610", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:21Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA1Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924057", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794633", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:34Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    }\n+  }\n+\n+  private void addExtendedConstraintData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      out.write((\"Constraints\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+        out.write(table.getPrimaryKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+        out.write(table.getForeignKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+        out.write(table.getUniqueKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+        out.write(table.getNotNullConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+        out.write(table.getDefaultConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+        out.write(table.getCheckConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+    }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA2MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924060", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDY2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794661", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:49Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    }\n+  }\n+\n+  private void addExtendedConstraintData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      out.write((\"Constraints\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+        out.write(table.getPrimaryKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+        out.write(table.getForeignKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+        out.write(table.getUniqueKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+        out.write(table.getNotNullConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+        out.write(table.getDefaultConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+        out.write(table.getCheckConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+    }\n+  }\n+\n+  private void addExtendedStorageData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getStorageHandlerInfo() != null) {\n+      out.write((\"StorageHandlerInfo\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.newLineCode);\n+      out.write(table.getStorageHandlerInfo().formatAsText().getBytes(\"UTF-8\"));\n+      out.write(Utilities.newLineCode);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA2OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924068", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDczOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794739", "bodyText": "Collections.emptyList()", "author": "belugabehr", "createdAt": "2021-01-01T18:44:56Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter.JsonDescTableFormatter;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to json format.\n+ */\n+public class JsonShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    List<Map<String, Object>> tableData = new ArrayList<>();\n+    try {\n+      for (Table table : tables) {\n+        tableData.add(makeOneTableStatus(table, db, conf, partition));\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+    ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tableData).build());\n+  }\n+\n+  private Map<String, Object> makeOneTableStatus(Table table, Hive db, HiveConf conf, Partition partition)\n+      throws HiveException, IOException {\n+    StorageInfo storageInfo = getStorageInfo(table, partition);\n+\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"tableName\", table.getTableName());\n+    builder.put(\"ownerType\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\");\n+    builder.put(\"owner\", table.getOwner());\n+    builder.put(\"location\", storageInfo.location);\n+    builder.put(\"inputFormat\", storageInfo.inputFormatClass);\n+    builder.put(\"outputFormat\", storageInfo.outputFormatClass);\n+    builder.put(\"columns\", JsonDescTableFormatter.createColumnsInfo(table.getCols(), new ArrayList<>()));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDQzNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924436", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:45:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDc0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794745", "bodyText": "Collections.emptyList()", "author": "belugabehr", "createdAt": "2021-01-01T18:45:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter.JsonDescTableFormatter;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to json format.\n+ */\n+public class JsonShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    List<Map<String, Object>> tableData = new ArrayList<>();\n+    try {\n+      for (Table table : tables) {\n+        tableData.add(makeOneTableStatus(table, db, conf, partition));\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+    ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tableData).build());\n+  }\n+\n+  private Map<String, Object> makeOneTableStatus(Table table, Hive db, HiveConf conf, Partition partition)\n+      throws HiveException, IOException {\n+    StorageInfo storageInfo = getStorageInfo(table, partition);\n+\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"tableName\", table.getTableName());\n+    builder.put(\"ownerType\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\");\n+    builder.put(\"owner\", table.getOwner());\n+    builder.put(\"location\", storageInfo.location);\n+    builder.put(\"inputFormat\", storageInfo.inputFormatClass);\n+    builder.put(\"outputFormat\", storageInfo.outputFormatClass);\n+    builder.put(\"columns\", JsonDescTableFormatter.createColumnsInfo(table.getCols(), new ArrayList<>()));\n+\n+    builder.put(\"partitioned\", table.isPartitioned());\n+    if (table.isPartitioned()) {\n+      builder.put(\"partitionColumns\", JsonDescTableFormatter.createColumnsInfo(table.getPartCols(), new ArrayList<>()));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDMzMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924330", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDc0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDg1OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794858", "bodyText": "\"Cannot access File System. File System status will be unknown.", "author": "belugabehr", "createdAt": "2021-01-01T18:46:00Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDM3OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924379", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:45:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDkzNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794937", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:46:43Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDIwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924209", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:43:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk1MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794951", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:46:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924109", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk5OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794998", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:11Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEyMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924123", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795009", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEzOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924138", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAyNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795027", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:22Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {\n+        fileData.minFileSize = fileLength;\n+      }\n+\n+      if (entryStatus.getAccessTime() > fileData.lastAccessTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDE0Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924142", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAzNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795035", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:27Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {\n+        fileData.minFileSize = fileLength;\n+      }\n+\n+      if (entryStatus.getAccessTime() > fileData.lastAccessTime) {\n+        fileData.lastAccessTime = entryStatus.getAccessTime();\n+      }\n+      if (entryStatus.getModificationTime() > fileData.lastUpdateTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTY5NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805694", "bodyText": "Nice catch, fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:54:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTA3OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795078", "bodyText": "StandardCharsets\nAlso,...\nout.write(\"owner:\");\nout.write(tabler.getOwner());\n```\n\nNo need to concat strings here.", "author": "belugabehr", "createdAt": "2021-01-01T18:48:12Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to text format.\n+ */\n+public class TextShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    try {\n+      for (Table table : tables) {\n+        writeBasicInfo(out, table);\n+        writeStorageInfo(out, partition, table);\n+        writeColumnsInfo(out, table);\n+        writeFileSystemInfo(out, db, conf, partition, table);\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void writeBasicInfo(DataOutputStream out, Table table) throws IOException, UnsupportedEncodingException {\n+    out.write((\"tableName:\" + table.getTableName()).getBytes(\"UTF-8\"));\n+    out.write(Utilities.newLineCode);\n+    out.write((\"owner:\" + table.getOwner()).getBytes(\"UTF-8\"));\n+    out.write(Utilities.newLineCode);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDI0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804246", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:34:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTA3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTIwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795207", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:49:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.tables;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLES results.\n+ */\n+public abstract class ShowTablesFormatter {\n+  public static ShowTablesFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTablesFormatter();\n+    } else {\n+      return new TextShowTablesFormatter();\n+    }\n+  }\n+\n+  public abstract void showTables(DataOutputStream out, List<String> tables) throws HiveException;\n+\n+  abstract void showTablesExtended(DataOutputStream out, List<Table> tables) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowTablesFormatter extends ShowTablesFormatter {\n+    @Override\n+    public void showTables(DataOutputStream out, List<String> tables) throws HiveException {\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tables).build());\n+    }\n+\n+    @Override\n+    void showTablesExtended(DataOutputStream out, List<Table> tables) throws HiveException {\n+      if (tables.isEmpty()) {\n+        return;\n+      }\n+\n+      List<Map<String, Object>> tableDataList = new ArrayList<>();\n+      for (Table table : tables) {\n+        Map<String, Object> tableData = ImmutableMap.of(\n+            \"Table Name\", table.getTableName(),\n+            \"Table Type\", table.getTableType().toString());\n+        tableDataList.add(tableData);\n+      }\n+\n+      ShowUtils.asJson(out, ImmutableMap.of(\"tables\", tableDataList));\n+    }\n+  }\n+\n+  static class TextShowTablesFormatter extends ShowTablesFormatter {\n+    @Override\n+    public void showTables(DataOutputStream out, List<String> tables) throws HiveException {\n+      Iterator<String> iterTbls = tables.iterator();\n+\n+      try {\n+        while (iterTbls.hasNext()) {\n+          // create a row per table name\n+          out.write(iterTbls.next().getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDQ0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804441", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-01T20:36:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTI0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795241", "bodyText": "JDK String.split", "author": "belugabehr", "createdAt": "2021-01-01T18:50:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.partition.show;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.common.FileUtils;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW PARTITIONS results.\n+ */\n+abstract class ShowPartitionsFormatter {\n+  static ShowPartitionsFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowPartitionsFormatter();\n+    } else {\n+      return new TextShowPartitionsFormatter();\n+    }\n+  }\n+\n+  abstract void showTablePartitions(DataOutputStream out, List<String> partitions) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowPartitionsFormatter extends ShowPartitionsFormatter {\n+    @Override\n+    void showTablePartitions(DataOutputStream out, List<String> partitions) throws HiveException {\n+      List<Map<String, Object>> partitionData = new ArrayList<>(partitions.size());\n+      for (String partition : partitions) {\n+        partitionData.add(makeOneTablePartition(partition));\n+      }\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"partitions\", partitionData).build());\n+    }\n+\n+    // TODO: This seems like a very wrong implementation.\n+    private Map<String, Object> makeOneTablePartition(String partition) {\n+      List<Map<String, Object>> result = new ArrayList<>();\n+\n+      List<String> names = new ArrayList<String>();\n+      for (String part : StringUtils.split(partition, \"/\")) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDM1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804352", "bodyText": "Done", "author": "miklosgergely", "createdAt": "2021-01-01T20:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTI0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTM2MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795360", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:52:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.workloadmanagement.resourceplan.show.formatter;\n+\n+import org.apache.hadoop.hive.metastore.api.WMFullResourcePlan;\n+import org.apache.hadoop.hive.metastore.api.WMResourcePlan;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW RESOURCE PLAN(S) results to text format.\n+ */\n+class TextShowResourcePlanFormatter extends ShowResourcePlanFormatter {\n+  @Override\n+  public void showResourcePlans(DataOutputStream out, List<WMResourcePlan> resourcePlans) throws HiveException {\n+    try {\n+      for (WMResourcePlan plan : resourcePlans) {\n+        out.write(plan.getName().getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        out.write(plan.getStatus().name().getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        String queryParallelism = plan.isSetQueryParallelism() ? Integer.toString(plan.getQueryParallelism()) : \"null\";\n+        out.write(queryParallelism.getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        String defaultPoolPath = plan.isSetDefaultPoolPath() ? plan.getDefaultPoolPath() : \"null\";\n+        out.write(defaultPoolPath.getBytes(ShowUtils.UTF_8));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDUyMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804520", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-01T20:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTM2MA=="}], "type": "inlineReview"}, {"oid": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "url": "https://github.com/apache/hive/commit/68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-01T21:07:15Z", "type": "forcePushed"}, {"oid": "4e3d3432aa90f33c2c2f7600ded88b992c866cb8", "url": "https://github.com/apache/hive/commit/4e3d3432aa90f33c2c2f7600ded88b992c866cb8", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-01T23:39:59Z", "type": "forcePushed"}, {"oid": "4162a391704b6519a4bfed0074fe14b19c5a9099", "url": "https://github.com/apache/hive/commit/4162a391704b6519a4bfed0074fe14b19c5a9099", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T14:00:09Z", "type": "forcePushed"}, {"oid": "dd51df4b8e215b16c573ffc0896004b1737a4f8f", "url": "https://github.com/apache/hive/commit/dd51df4b8e215b16c573ffc0896004b1737a4f8f", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T16:41:46Z", "type": "forcePushed"}, {"oid": "636c0af4548c0da4b23644b16dabfef97a04f1ef", "url": "https://github.com/apache/hive/commit/636c0af4548c0da4b23644b16dabfef97a04f1ef", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T22:02:18Z", "type": "forcePushed"}, {"oid": "9a75f17ed23d20705d6702fb7e6cf81f91c09349", "url": "https://github.com/apache/hive/commit/9a75f17ed23d20705d6702fb7e6cf81f91c09349", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T22:50:32Z", "type": "forcePushed"}, {"oid": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "url": "https://github.com/apache/hive/commit/6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T23:01:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTEwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552889107", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:28:45Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java", "diffHunk": "@@ -20,25 +20,17 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.HashMap;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTA3NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905074", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTc1OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552889758", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:29:54Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java", "diffHunk": "@@ -68,7 +68,8 @@ public void analyzeInternal(ASTNode root) throws SemanticException {\n     }\n \n     Table table = getTable(tableName);\n-    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf, false);\n+    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf,\n+        false);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTIxNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905214", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTc1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDE0MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890140", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.\nI think this is probably an artifact of your IDE.", "author": "belugabehr", "createdAt": "2021-01-06T18:30:41Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java", "diffHunk": "@@ -31,9 +31,11 @@\n public class ShowCompactionsDesc implements DDLDesc, Serializable {\n   private static final long serialVersionUID = 1L;\n \n+  // @formatter:off\n   public static final String SCHEMA =\n       \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n       \"string:string:string:string:string:string:string:string:string:string:string:string:string\";\n+  // @formatter:on", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTEzNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905134", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDE0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDI1Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890253", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:30:55Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java", "diffHunk": "@@ -35,7 +35,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.constraint.add.AlterTableAddConstraintOperation;\n-import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTMyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905326", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDM5MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890390", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:31:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java", "diffHunk": "@@ -79,7 +79,8 @@ protected void analyzeCommand(TableName tableName, Map<String, String> partition\n       throw new SemanticException(ErrorMsg.NOT_RECOGNIZED_CONSTRAINT.getMsg(constraintNode.getToken().getText()));\n     }\n \n-    Constraints constraints = new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);\n+    Constraints constraints =\n+        new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTI5Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905297", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDM5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDgwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890805", "bodyText": "I believe these changes are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:31:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java", "diffHunk": "@@ -83,5 +82,4 @@ public Long getWriteId() {\n   public boolean mayNeedWriteId() {\n     return true;\n   }\n-", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTUwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891509", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java", "diffHunk": "@@ -23,9 +23,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n-import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n-import org.apache.hadoop.hive.ql.lockmgr.LockException;\n-import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTM4Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905387", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTUwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTYxMQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891611", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:36Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java", "diffHunk": "@@ -26,9 +26,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n-import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n-import org.apache.hadoop.hive.ql.lockmgr.LockException;\n-import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTcxMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905713", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTYxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTg0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891841", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java", "diffHunk": "@@ -234,7 +233,7 @@ public void setWriteId(long writeId) {\n \n   @Override\n   public String getFullTableName() {\n-    return AcidUtils.getFullTableName(dbName,tableName);\n+    return AcidUtils.getFullTableName(dbName, tableName);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTQ0Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905442", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTkxOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891919", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:11Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java", "diffHunk": "@@ -26,7 +26,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.AlterTableUtils;\n-import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjAwMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892003", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:22Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java", "diffHunk": "@@ -20,7 +20,6 @@\n \n import java.net.URI;\n import java.net.URISyntaxException;\n-import java.util.ArrayList;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTQ4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905485", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjA4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892085", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java", "diffHunk": "@@ -84,7 +84,7 @@ private void validateCreateView(AlterViewAsDesc desc, SemanticAnalyzer analyzer)\n \n     if (oldView == null) {\n       String viewNotExistErrorMsg = \"The following view does not exist: \" + desc.getViewName();\n-      throw new SemanticException( ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));\n+      throw new SemanticException(ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTUyNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905524", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjY2Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892663", "bodyText": "All changes (there are several) in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:35:42Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java", "diffHunk": "@@ -17,51 +17,53 @@\n  */\n package org.apache.hadoop.hive.ql.metadata.formatting;\n \n-import java.util.HashMap;\n import java.util.LinkedHashMap;\n import java.util.Map;\n \n /**\n  * Helper class to build Maps consumed by the JSON formatter.  Only\n  * add non-null entries to the Map.\n  */\n-public class MapBuilder {\n-    private Map<String, Object> map = new LinkedHashMap<String, Object>();\n+public final class MapBuilder {\n+  private Map<String, Object> map = new LinkedHashMap<String, Object>();\n \n-    private MapBuilder() {}\n+  private MapBuilder() {\n+  }\n \n-    public static MapBuilder create() {\n-        return new MapBuilder();\n-    }\n+  public static MapBuilder create() {\n+    return new MapBuilder();\n+  }\n \n-    public MapBuilder put(String name, Object val) {\n-        if (val != null)\n-            map.put(name, val);\n-        return this;\n+  public MapBuilder put(String name, Object val) {\n+    if (val != null) {\n+      map.put(name, val);\n     }\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, boolean val) {\n-        map.put(name, Boolean.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, boolean val) {\n+    map.put(name, Boolean.valueOf(val));\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, int val) {\n-        map.put(name, Integer.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, int val) {\n+    map.put(name, Integer.valueOf(val));\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, long val) {\n-        map.put(name, Long.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, long val) {\n+    map.put(name, Long.valueOf(val));\n+    return this;\n+  }\n \n-    public <T> MapBuilder put(String name, T val, boolean use) {\n-        if (use)\n-            put(name, val);\n-        return this;\n+  public <T> MapBuilder put(String name, T val, boolean use) {\n+    if (use) {\n+      put(name, val);\n     }\n+    return this;\n+  }\n \n-    public Map<String, Object> build() {\n-        return map;\n-    }\n+  public Map<String, Object> build() {\n+    return map;\n+  }", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTU2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905561", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjY2Mw=="}], "type": "inlineReview"}, {"oid": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "url": "https://github.com/apache/hive/commit/0dbd8020ff5779ede7ab743e2df2b1da89fee455", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-06T18:59:34Z", "type": "forcePushed"}, {"oid": "c75da22d29c78c81dbb0aed961261ec2affda89e", "url": "https://github.com/apache/hive/commit/c75da22d29c78c81dbb0aed961261ec2affda89e", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-08T17:15:37Z", "type": "commit"}, {"oid": "c75da22d29c78c81dbb0aed961261ec2affda89e", "url": "https://github.com/apache/hive/commit/c75da22d29c78c81dbb0aed961261ec2affda89e", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-08T17:15:37Z", "type": "forcePushed"}]}