{"pr_number": 1142, "pr_title": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely, reviewed by Peter Vary)", "pr_createdAt": "2020-06-17T21:58:28Z", "pr_url": "https://github.com/apache/hive/pull/1142", "timeline": [{"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "url": "https://github.com/apache/hive/commit/f558e2488632ed4adfe7a6c193b3ae4570a58718", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)", "committedDate": "2020-06-18T08:57:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMDgyNA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442100824", "bodyText": "What about driverTxnHandler.acquireLocksIfNeeded, and pushing the logic to the TxnHandler?", "author": "pvary", "createdAt": "2020-06-18T09:40:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -565,9 +368,9 @@ public void lockAndRespond() throws CommandProcessorException {\n           \"No previously compiled query for driver - queryId=\" + driverContext.getQueryState().getQueryId());\n     }\n \n-    if (requiresLock()) {", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5ODAxNQ==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442498015", "bodyText": "I agree, I'll modify it.", "author": "miklosgergely", "createdAt": "2020-06-18T20:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMDgyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTU4OA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442505588", "bodyText": "I agree, I'll modify this.", "author": "miklosgergely", "createdAt": "2020-06-18T21:12:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMDgyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMTUzNA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442101534", "bodyText": "What about removing Driver.releaseLocksAndCommitOrRollback altogether, and call driverTxnHandler.releaseLocksAndCommitOrRollback(commit, txnManager) immediately?", "author": "pvary", "createdAt": "2020-06-18T09:41:14Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -308,110 +255,6 @@ public FetchTask getFetchTask() {\n     return driverContext.getFetchTask();\n   }\n \n-  /**\n-   * Acquire read and write locks needed by the statement. The list of objects to be locked are\n-   * obtained from the inputs and outputs populated by the compiler.  Locking strategy depends on\n-   * HiveTxnManager and HiveLockManager configured\n-   *\n-   * This method also records the list of valid transactions.  This must be done after any\n-   * transactions have been opened.\n-   * @throws CommandProcessorException\n-   **/\n-  private void acquireLocks() throws CommandProcessorException {\n-    PerfLogger perfLogger = SessionState.getPerfLogger();\n-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n-\n-    if(!driverContext.getTxnManager().isTxnOpen() && driverContext.getTxnManager().supportsAcid()) {\n-      /*non acid txn managers don't support txns but fwd lock requests to lock managers\n-        acid txn manager requires all locks to be associated with a txn so if we\n-        end up here w/o an open txn it's because we are processing something like \"use <database>\n-        which by definition needs no locks*/\n-      return;\n-    }\n-    try {\n-      String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-\n-      // Set the table write id in all of the acid file sinks\n-      if (!driverContext.getPlan().getAcidSinks().isEmpty()) {\n-        List<FileSinkDesc> acidSinks = new ArrayList<>(driverContext.getPlan().getAcidSinks());\n-        //sorting makes tests easier to write since file names and ROW__IDs depend on statementId\n-        //so this makes (file name -> data) mapping stable\n-        acidSinks.sort((FileSinkDesc fsd1, FileSinkDesc fsd2) ->\n-          fsd1.getDirName().compareTo(fsd2.getDirName()));\n-        for (FileSinkDesc desc : acidSinks) {\n-          TableDesc tableInfo = desc.getTableInfo();\n-          final TableName tn = HiveTableName.ofNullable(tableInfo.getTableName());\n-          long writeId = driverContext.getTxnManager().getTableWriteId(tn.getDb(), tn.getTable());\n-          desc.setTableWriteId(writeId);\n-\n-          /**\n-           * it's possible to have > 1 FileSink writing to the same table/partition\n-           * e.g. Merge stmt, multi-insert stmt when mixing DP and SP writes\n-           * Insert ... Select ... Union All Select ... using\n-           * {@link org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}\n-           */\n-          desc.setStatementId(driverContext.getTxnManager().getStmtIdAndIncrement());\n-          String unionAllSubdir = \"/\" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX;\n-          if(desc.getInsertOverwrite() && desc.getDirName().toString().contains(unionAllSubdir) &&\n-              desc.isFullAcidTable()) {\n-            throw new UnsupportedOperationException(\"QueryId=\" + driverContext.getPlan().getQueryId() +\n-                \" is not supported due to OVERWRITE and UNION ALL.  Please use truncate + insert\");\n-          }\n-        }\n-      }\n-\n-      if (driverContext.getPlan().getAcidAnalyzeTable() != null) {\n-        // Allocate write ID for the table being analyzed.\n-        Table t = driverContext.getPlan().getAcidAnalyzeTable().getTable();\n-        driverContext.getTxnManager().getTableWriteId(t.getDbName(), t.getTableName());\n-      }\n-\n-\n-      DDLDescWithWriteId acidDdlDesc = driverContext.getPlan().getAcidDdlDesc();\n-      boolean hasAcidDdl = acidDdlDesc != null && acidDdlDesc.mayNeedWriteId();\n-      if (hasAcidDdl) {\n-        String fqTableName = acidDdlDesc.getFullTableName();\n-        final TableName tn = HiveTableName.ofNullableWithNoDefault(fqTableName);\n-        long writeId = driverContext.getTxnManager().getTableWriteId(tn.getDb(), tn.getTable());\n-        acidDdlDesc.setWriteId(writeId);\n-      }\n-\n-      /*It's imperative that {@code acquireLocks()} is called for all commands so that\n-      HiveTxnManager can transition its state machine correctly*/\n-      driverContext.getTxnManager().acquireLocks(driverContext.getPlan(), context, userFromUGI, driverState);\n-      final List<HiveLock> locks = context.getHiveLocks();\n-      LOG.info(\"Operation {} obtained {} locks\", driverContext.getPlan().getOperation(),\n-          ((locks == null) ? 0 : locks.size()));\n-      // This check is for controlling the correctness of the current state\n-      if (driverContext.getTxnManager().recordSnapshot(driverContext.getPlan()) &&\n-          !driverContext.isValidTxnListsGenerated()) {\n-        throw new IllegalStateException(\n-            \"Need to record valid WriteID list but there is no valid TxnID list (\" +\n-                JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()) +\n-                \", queryId:\" + driverContext.getPlan().getQueryId() + \")\");\n-      }\n-\n-      if (driverContext.getPlan().hasAcidResourcesInQuery() || hasAcidDdl) {\n-        validTxnManager.recordValidWriteIds();\n-      }\n-\n-    } catch (Exception e) {\n-      String errorMessage;\n-      if (driverState.isDestroyed() || driverState.isAborted() || driverState.isClosed()) {\n-        errorMessage = String.format(\"Ignore lock acquisition related exception in terminal state (%s): %s\",\n-            driverState.toString(), e.getMessage());\n-        CONSOLE.printInfo(errorMessage);\n-      } else {\n-        errorMessage = String.format(\"FAILED: Error in acquiring locks: %s\", e.getMessage());\n-        CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n-      }\n-      throw DriverUtils.createProcessorException(driverContext, 10, errorMessage, ErrorMsg.findSQLState(e.getMessage()),\n-          e);\n-    } finally {\n-      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n-    }\n-  }\n-\n   public void releaseLocksAndCommitOrRollback(boolean commit) throws LockException {\n     releaseLocksAndCommitOrRollback(commit, driverContext.getTxnManager());\n   }", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwODI5Nw==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442508297", "bodyText": "I'm planning to remove both releaseLocksAndCommitOrRollback functions from Driver, but unfortunately they are both used by some external callers. In a future patch I'll try to remove those. I've removed all the references to them within the Driver class for now.", "author": "miklosgergely", "createdAt": "2020-06-18T21:18:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMTUzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjcwMTExMw==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442701113", "bodyText": "Sounds good", "author": "pvary", "createdAt": "2020-06-19T08:16:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMTUzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTA5Mg==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105092", "bodyText": "Do we need this magic? DriverTxnHandler already has a Context.\nIf I understand correctly this is just clearing the context at hand, should be the responsibility of the DriverTxnHandler.... or even better it can have a DriverTxnHandlerContext...", "author": "pvary", "createdAt": "2020-06-18T09:47:40Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -988,7 +741,7 @@ private void releaseContext() {\n         }\n         context.clear(deleteResultDir);\n         if (context.getHiveLocks() != null) {\n-          hiveLocks.addAll(context.getHiveLocks());\n+          driverTxnHandler.addHiveLocksFromContext();", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTIzNQ==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442515235", "bodyText": "The main problem here is that we have some HiveLocks in the hiveLocks variable (previously in Driver, after this patch in DriverTxnHandler), and also some HiveLocks in the Context. The two should be unified I believe, probably in Context. I think it's a good goal, but I'd rather not put it into this patch, as this is only moving code around. Let's do this in another jira, do you agree?", "author": "miklosgergely", "createdAt": "2020-06-18T21:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjcwMTc2Mw==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442701763", "bodyText": "Ok", "author": "pvary", "createdAt": "2020-06-19T08:18:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTU1NA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105554", "bodyText": "Again, maybe just a driverTxnHandler.release?", "author": "pvary", "createdAt": "2020-06-18T09:48:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -1058,15 +811,15 @@ private int closeInProcess(boolean destroyed) {\n     releaseResStream();\n     releaseContext();\n     if (destroyed) {\n-      if (!hiveLocks.isEmpty()) {\n+      if (!driverTxnHandler.hasHiveLock()) {", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTE4OQ==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442505189", "bodyText": "I agree, I'll modify this.", "author": "miklosgergely", "createdAt": "2020-06-18T21:11:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTU1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTY1Mw==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442505653", "bodyText": "I agree, I'll modify this.", "author": "miklosgergely", "createdAt": "2020-06-18T21:12:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTc5MQ==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105791", "bodyText": "See comment above: DriverTxnHanlder.destroy?", "author": "pvary", "createdAt": "2020-06-18T09:48:53Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -1112,15 +865,15 @@ public void destroy() {\n     boolean isTxnOpen = driverContext != null\n         && driverContext.getTxnManager() != null\n         && driverContext.getTxnManager().isTxnOpen();\n-    if (!hiveLocks.isEmpty() || isTxnOpen) {\n+    if (!driverTxnHandler.hasHiveLock() || isTxnOpen) {", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTE0MA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442505140", "bodyText": "I agree, I'll modify this.", "author": "miklosgergely", "createdAt": "2020-06-18T21:11:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTY4OA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442505688", "bodyText": "I agree, I'll modify this.", "author": "miklosgergely", "createdAt": "2020-06-18T21:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNjYyMA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442106620", "bodyText": "I would prefer keeping the locks local to DriverTxnManager, and its' own context(?), and not storing in the global context?", "author": "pvary", "createdAt": "2020-06-18T09:50:23Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.hive.common.JavaUtils;\n+import org.apache.hadoop.hive.common.TableName;\n+import org.apache.hadoop.hive.common.ValidTxnList;\n+import org.apache.hadoop.hive.common.ValidTxnWriteIdList;\n+import org.apache.hadoop.hive.conf.Constants;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.ql.ddl.DDLDesc.DDLDescWithWriteId;\n+import org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator;\n+import org.apache.hadoop.hive.ql.exec.ConditionalTask;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveLock;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n+import org.apache.hadoop.hive.ql.lockmgr.LockException;\n+import org.apache.hadoop.hive.ql.log.PerfLogger;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.HiveTableName;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.HiveOperation;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hive.common.util.ShutdownHookManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * \n+ */\n+public class DriverTxnHandler {\n+  private static final String CLASS_NAME = Driver.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  private static final LogHelper CONSOLE = new LogHelper(LOG);\n+  private static final int SHUTDOWN_HOOK_PRIORITY = 0;\n+\n+  private final DriverContext driverContext;\n+  private final DriverState driverState;\n+  private final ValidTxnManager validTxnManager;\n+\n+  private final List<HiveLock> hiveLocks = new ArrayList<HiveLock>();\n+\n+  private Context context;\n+\n+  public DriverTxnHandler(DriverContext driverContext, DriverState driverState, ValidTxnManager validTxnManager) {\n+    this.driverContext = driverContext;\n+    this.driverState = driverState;\n+    this.validTxnManager = validTxnManager;\n+  }\n+\n+  public void createTxnManager() throws CommandProcessorException {\n+    try {\n+      // Initialize the transaction manager.  This must be done before analyze is called.\n+      HiveTxnManager queryTxnManager = (driverContext.getInitTxnManager() != null) ?\n+          driverContext.getInitTxnManager() : SessionState.get().initTxnMgr(driverContext.getConf());\n+\n+      if (queryTxnManager instanceof Configurable) {\n+        ((Configurable) queryTxnManager).setConf(driverContext.getConf());\n+      }\n+      driverContext.setTxnManager(queryTxnManager);\n+      driverContext.getQueryState().setTxnManager(queryTxnManager);\n+\n+      // In case when user Ctrl-C twice to kill Hive CLI JVM, we want to release locks\n+      // if compile is being called multiple times, clear the old shutdownhook\n+      ShutdownHookManager.removeShutdownHook(driverContext.getShutdownRunner());\n+      Runnable shutdownRunner = new Runnable() {\n+        @Override\n+        public void run() {\n+          try {\n+            releaseLocksAndCommitOrRollback(false, driverContext.getTxnManager());\n+          } catch (LockException e) {\n+            LOG.warn(\"Exception when releasing locks in ShutdownHook for Driver: \" +\n+                e.getMessage());\n+          }\n+        }\n+      };\n+      ShutdownHookManager.addShutdownHook(shutdownRunner, SHUTDOWN_HOOK_PRIORITY);\n+      driverContext.setShutdownRunner(shutdownRunner);\n+    } catch (LockException e) {\n+      ErrorMsg error = ErrorMsg.getErrorMsg(e.getMessage());\n+      String errorMessage = \"FAILED: \" + e.getClass().getSimpleName() + \" [Error \"  + error.getErrorCode()  + \"]:\";\n+\n+      CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n+      throw DriverUtils.createProcessorException(driverContext, error.getErrorCode(), errorMessage, error.getSQLState(),\n+          e);\n+    }\n+  }\n+\n+  public void setContext(Context context) {\n+    this.context = context;\n+  }\n+\n+  public boolean requiresLock() {\n+    if (!DriverUtils.checkConcurrency(driverContext)) {\n+      LOG.info(\"Concurrency mode is disabled, not creating a lock manager\");\n+      return false;\n+    }\n+\n+    // Lock operations themselves don't require the lock.\n+    if (isExplicitLockOperation()) {\n+      return false;\n+    }\n+\n+    if (!HiveConf.getBoolVar(driverContext.getConf(), ConfVars.HIVE_LOCK_MAPRED_ONLY)) {\n+      return true;\n+    }\n+\n+    if (driverContext.getConf().get(Constants.HIVE_QUERY_EXCLUSIVE_LOCK) != null) {\n+      return true;\n+    }\n+\n+    Queue<Task<?>> tasks = new LinkedList<Task<?>>();\n+    tasks.addAll(driverContext.getPlan().getRootTasks());\n+    while (tasks.peek() != null) {\n+      Task<?> task = tasks.remove();\n+      if (task.requireLock()) {\n+        return true;\n+      }\n+\n+      if (task instanceof ConditionalTask) {\n+        tasks.addAll(((ConditionalTask)task).getListTasks());\n+      }\n+\n+      if (task.getChildTasks() != null) {\n+        tasks.addAll(task.getChildTasks());\n+      }\n+      // does not add back up task here, because back up task should be the same type of the original task.\n+    }\n+\n+    return false;\n+  }\n+\n+  private boolean isExplicitLockOperation() {\n+    HiveOperation currentOpt = driverContext.getPlan().getOperation();\n+    if (currentOpt != null) {\n+      switch (currentOpt) {\n+      case LOCKDB:\n+      case UNLOCKDB:\n+      case LOCKTABLE:\n+      case UNLOCKTABLE:\n+        return true;\n+      default:\n+        return false;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Acquire read and write locks needed by the statement. The list of objects to be locked are obtained from the inputs\n+   * and outputs populated by the compiler. Locking strategy depends on HiveTxnManager and HiveLockManager configured.\n+   *\n+   * This method also records the list of valid transactions. This must be done after any transactions have been opened.\n+   */\n+  public void acquireLocks() throws CommandProcessorException {\n+    PerfLogger perfLogger = SessionState.getPerfLogger();\n+    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n+\n+    if (!driverContext.getTxnManager().isTxnOpen() && driverContext.getTxnManager().supportsAcid()) {\n+      /* non acid txn managers don't support txns but fwd lock requests to lock managers\n+         acid txn manager requires all locks to be associated with a txn so if we end up here w/o an open txn\n+         it's because we are processing something like \"use <database> which by definition needs no locks */\n+      return;\n+    }\n+\n+    try {\n+      setWriteIdForAcidFileSinks();\n+      allocateWriteIdForAcidAnalyzeTable();\n+      boolean hasAcidDdl = setWriteIdForAcidDdl();\n+      acquireLocksInternal();\n+\n+      if (driverContext.getPlan().hasAcidResourcesInQuery() || hasAcidDdl) {\n+        validTxnManager.recordValidWriteIds();\n+      }\n+    } catch (Exception e) {\n+      String errorMessage;\n+      if (driverState.isDestroyed() || driverState.isAborted() || driverState.isClosed()) {\n+        errorMessage = String.format(\"Ignore lock acquisition related exception in terminal state (%s): %s\",\n+            driverState.toString(), e.getMessage());\n+        CONSOLE.printInfo(errorMessage);\n+      } else {\n+        errorMessage = String.format(\"FAILED: Error in acquiring locks: %s\", e.getMessage());\n+        CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n+      }\n+      throw DriverUtils.createProcessorException(driverContext, 10, errorMessage, ErrorMsg.findSQLState(e.getMessage()),\n+          e);\n+    } finally {\n+      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n+    }\n+  }\n+\n+  private void setWriteIdForAcidFileSinks() throws SemanticException, LockException {\n+    if (!driverContext.getPlan().getAcidSinks().isEmpty()) {\n+      List<FileSinkDesc> acidSinks = new ArrayList<>(driverContext.getPlan().getAcidSinks());\n+      //sorting makes tests easier to write since file names and ROW__IDs depend on statementId\n+      //so this makes (file name -> data) mapping stable\n+      acidSinks.sort((FileSinkDesc fsd1, FileSinkDesc fsd2) -> fsd1.getDirName().compareTo(fsd2.getDirName()));\n+      for (FileSinkDesc acidSink : acidSinks) {\n+        TableDesc tableInfo = acidSink.getTableInfo();\n+        TableName tableName = HiveTableName.of(tableInfo.getTableName());\n+        long writeId = driverContext.getTxnManager().getTableWriteId(tableName.getDb(), tableName.getTable());\n+        acidSink.setTableWriteId(writeId);\n+\n+        /**\n+         * it's possible to have > 1 FileSink writing to the same table/partition\n+         * e.g. Merge stmt, multi-insert stmt when mixing DP and SP writes\n+         * Insert ... Select ... Union All Select ... using\n+         * {@link org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}\n+         */\n+        acidSink.setStatementId(driverContext.getTxnManager().getStmtIdAndIncrement());\n+        String unionAllSubdir = \"/\" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX;\n+        if (acidSink.getInsertOverwrite() && acidSink.getDirName().toString().contains(unionAllSubdir) &&\n+            acidSink.isFullAcidTable()) {\n+          throw new UnsupportedOperationException(\"QueryId=\" + driverContext.getPlan().getQueryId() +\n+              \" is not supported due to OVERWRITE and UNION ALL.  Please use truncate + insert\");\n+        }\n+      }\n+    }\n+  }\n+\n+  private void allocateWriteIdForAcidAnalyzeTable() throws LockException {\n+    if (driverContext.getPlan().getAcidAnalyzeTable() != null) {\n+      Table table = driverContext.getPlan().getAcidAnalyzeTable().getTable();\n+      driverContext.getTxnManager().getTableWriteId(table.getDbName(), table.getTableName());\n+    }\n+  }\n+\n+  private boolean setWriteIdForAcidDdl() throws SemanticException, LockException {\n+    DDLDescWithWriteId acidDdlDesc = driverContext.getPlan().getAcidDdlDesc();\n+    boolean hasAcidDdl = acidDdlDesc != null && acidDdlDesc.mayNeedWriteId();\n+    if (hasAcidDdl) {\n+      String fqTableName = acidDdlDesc.getFullTableName();\n+      TableName tableName = HiveTableName.of(fqTableName);\n+      long writeId = driverContext.getTxnManager().getTableWriteId(tableName.getDb(), tableName.getTable());\n+      acidDdlDesc.setWriteId(writeId);\n+    }\n+    return hasAcidDdl;\n+  }\n+\n+  private void acquireLocksInternal() throws CommandProcessorException, LockException {\n+    /* It's imperative that {@code acquireLocks()} is called for all commands so that\n+       HiveTxnManager can transition its state machine correctly */\n+    String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+    driverContext.getTxnManager().acquireLocks(driverContext.getPlan(), context, userFromUGI, driverState);\n+    List<HiveLock> locks = context.getHiveLocks();\n+    LOG.info(\"Operation {} obtained {} locks\", driverContext.getPlan().getOperation(),\n+        ((locks == null) ? 0 : locks.size()));\n+    // This check is for controlling the correctness of the current state\n+    if (driverContext.getTxnManager().recordSnapshot(driverContext.getPlan()) &&\n+        !driverContext.isValidTxnListsGenerated()) {\n+      throw new IllegalStateException(\"Need to record valid WriteID list but there is no valid TxnID list (\" +\n+          JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()) +\n+          \", queryId: \" + driverContext.getPlan().getQueryId() + \")\");\n+    }\n+  }\n+\n+  public void addHiveLocksFromContext() {\n+    hiveLocks.addAll(context.getHiveLocks());\n+  }\n+\n+  public boolean hasHiveLock() {\n+    return hiveLocks.isEmpty();\n+  }", "originalCommit": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNjYwNA==", "url": "https://github.com/apache/hive/pull/1142#discussion_r442516604", "bodyText": "I agree, that HiveLocks should be handled together. Currently MoveTask also needs to access them, as it wants to remove somme locks, so to have the hiveLocks only locally in DriverTxnHandler should somehow solve that. I suggest to solve this issue in a separate jira, as this one is only for moving code to a cleaner structure.", "author": "miklosgergely", "createdAt": "2020-06-18T21:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNjYyMA=="}], "type": "inlineReview"}, {"oid": "96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "url": "https://github.com/apache/hive/commit/96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)", "committedDate": "2020-06-18T21:39:10Z", "type": "forcePushed"}, {"oid": "75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "url": "https://github.com/apache/hive/commit/75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)", "committedDate": "2020-06-30T20:50:14Z", "type": "commit"}, {"oid": "75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "url": "https://github.com/apache/hive/commit/75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)", "committedDate": "2020-06-30T20:50:14Z", "type": "forcePushed"}]}