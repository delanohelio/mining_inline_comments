{"pr_number": 1750, "pr_title": "HIVE-24388: Enhance swo optimizations to merge EventOperators", "pr_createdAt": "2020-12-07T13:10:15Z", "pr_url": "https://github.com/apache/hive/pull/1750", "timeline": [{"oid": "7f3d10b974e9d58112911f1202ea10350867efe9", "url": "https://github.com/apache/hive/commit/7f3d10b974e9d58112911f1202ea10350867efe9", "message": "add test\n\n(cherry picked from commit fbfe28081700d701ea5e99463d3761bdae259d15)", "committedDate": "2020-11-16T13:18:08Z", "type": "commit"}, {"oid": "37e75a40232e3ff6cf4db1f9b274c4c0751c04f4", "url": "https://github.com/apache/hive/commit/37e75a40232e3ff6cf4db1f9b274c4c0751c04f4", "message": "add debug; q.out", "committedDate": "2020-11-16T15:04:01Z", "type": "commit"}, {"oid": "ebfcba96ca147156c9c3abc8fde4f5bbad1587cf", "url": "https://github.com/apache/hive/commit/ebfcba96ca147156c9c3abc8fde4f5bbad1587cf", "message": "en", "committedDate": "2020-11-16T16:02:22Z", "type": "commit"}, {"oid": "644f16fbada79af45ddecf66ef6987972afab26c", "url": "https://github.com/apache/hive/commit/644f16fbada79af45ddecf66ef6987972afab26c", "message": "accept", "committedDate": "2020-11-17T14:53:41Z", "type": "commit"}, {"oid": "751ad448d1a1891dedb75b073e3a20b3e5a1d717", "url": "https://github.com/apache/hive/commit/751ad448d1a1891dedb75b073e3a20b3e5a1d717", "message": "Merge remote-tracking branch 'apache/master' into HIVE-24388-event-merge", "committedDate": "2020-11-19T11:40:17Z", "type": "commit"}, {"oid": "5576d77bfae7f1c6b9222f2de865c4aa2be7ff91", "url": "https://github.com/apache/hive/commit/5576d77bfae7f1c6b9222f2de865c4aa2be7ff91", "message": "(X) HIVE-23965: Improve plan regression tests using TPCDS30TB metastore dump and custom configs\n\ncommit 7ad4c9d6af0e840213ad5da41f1b42ff3e83bfeb\nAuthor: Stamatis Zampetakis <zabetak@gmail.com>\nDate:   Mon Jun 15 21:09:14 2020 +0200\n\n    HIVE-23965: Improve plan regression tests using TPCDS30TB metastore dump and custom configs\n\n    1. Add new perf driver, TestTezTPCDS30TBCliDriver, relying on a dockerized metastore.\n    2. Use Dockerized postgres metastore with TPC-DS 30TB dump\n    3. Remove old drivers (with and without constraints), related classes\n    (e.g., MetastoreDumpUtility), and resources.\n\n    After discussion in the JIRA we decided that keeping the old drivers\n    is most likely useless.\n\n    4. Use Hive config properties obtained and curated from real-life usages\n    5. Allow AbstractCliConfig to override metastore DB type\n    6. Rework CorePerfCliDriver to allow pre-initialized metastores\n\n    Remove system property settings in the initialization of the driver and\n    leave in the configuration to set it up if needed. This is necessary to\n    be able to use the driver with a preinitialised metastore.\n\n    Remove redundant logs in System.err. Logging and throwing an exception\n    is an anti-pattern.\n\n    Replace assertions with exceptions and improve the messages.\n\n    7. Upgrade postgres JDBC driver to version 42.2.14 to be compatible\n    with the docker image used\n    8. Update path to new postgres version in TestBeelineArgParsing\n    9. Display plans using hive.explain.user set to false which seems\n    to be preferrable for performing a diff and checking regressions\n\n    10. Disable queries 14 (HIVE-24167), 30 (HIVE-23964)\n    11. Re-enable CBO plan tests for queries 44, 45, 67, 70, 86\n\n    The queries were disabled as part of HIVE-20718. They were supposed to\n    be fixed in Calcite 1.18.0 and currently Hive is in 1.21.0 so it is not\n    surprising that they pass.\n\n    12. Add missing queries: cbo_query41, cbo_query62, query62\n\ncommit 4ee53365d94f127131332f3e552c8cac3ce9938e\nAuthor: Stamatis Zampetakis <zabetak@gmail.com>\nDate:   Tue Nov 17 11:13:22 2020 +0100\n\n    HIVE-24395: Intermittent failures to initialize dockerized Postgres metastore in tests\n\n    Ensure the Postgres init process is complete the database port is open\n    and accepting connections before declaring the container ready.\n\ncommit a2815c2bf6152e7fd35612450f370fe5a5b10b8d\nAuthor: Stamatis Zampetakis <zabetak@gmail.com>\nDate:   Tue Jun 16 22:17:10 2020 +0200\n\n    HIVE-23742: Remove unintentional execution of TPC-DS query39 in qtests", "committedDate": "2020-11-19T11:41:05Z", "type": "commit"}, {"oid": "e100138eb48108d853ec0b30cb9a0dcd6cf1231f", "url": "https://github.com/apache/hive/commit/e100138eb48108d853ec0b30cb9a0dcd6cf1231f", "message": "Revert \"(X) HIVE-23965: Improve plan regression tests using TPCDS30TB metastore dump and custom configs\"\n\nThis reverts commit 5576d77bfae7f1c6b9222f2de865c4aa2be7ff91.", "committedDate": "2020-12-04T13:11:34Z", "type": "commit"}, {"oid": "7df746bf5055a89a7fec678340340f02d312f00b", "url": "https://github.com/apache/hive/commit/7df746bf5055a89a7fec678340340f02d312f00b", "message": "Merge remote-tracking branch 'apache/master' into HIVE-24388-event-merge", "committedDate": "2020-12-04T13:11:38Z", "type": "commit"}, {"oid": "cb66915b6a89b8a77f610062022374c078c6eeb2", "url": "https://github.com/apache/hive/commit/cb66915b6a89b8a77f610062022374c078c6eeb2", "message": "accept qouts", "committedDate": "2020-12-04T15:30:00Z", "type": "commit"}, {"oid": "80fa406b1a330bbbc297974128d0300bc0572475", "url": "https://github.com/apache/hive/commit/80fa406b1a330bbbc297974128d0300bc0572475", "message": "dont traverse event operators during valiudation", "committedDate": "2020-12-07T09:34:41Z", "type": "commit"}, {"oid": "3d74bb58db59b58f89a70a35aba0001ecd1cd10f", "url": "https://github.com/apache/hive/commit/3d74bb58db59b58f89a70a35aba0001ecd1cd10f", "message": "doesnt count eventop in parent/child relationg; optree vill filter out inbvalid cases", "committedDate": "2020-12-07T10:03:02Z", "type": "commit"}, {"oid": "9d0dd60f225aaeec8852d34c5bb8fe89d30a35d6", "url": "https://github.com/apache/hive/commit/9d0dd60f225aaeec8852d34c5bb8fe89d30a35d6", "message": "Revert \"doesnt count eventop in parent/child relationg; optree vill filter out inbvalid cases\"\n\nThis reverts commit 3d74bb58db59b58f89a70a35aba0001ecd1cd10f.", "committedDate": "2020-12-07T10:13:19Z", "type": "commit"}, {"oid": "bfaba51305e4f533f2571d58b8579ad9aeac3873", "url": "https://github.com/apache/hive/commit/bfaba51305e4f533f2571d58b8579ad9aeac3873", "message": "add mayRemoveInputs", "committedDate": "2020-12-07T11:18:28Z", "type": "commit"}, {"oid": "f74c516cad84c89310b4f1be1c04e28405089e7a", "url": "https://github.com/apache/hive/commit/f74c516cad84c89310b4f1be1c04e28405089e7a", "message": "Revert \"Revert \"doesnt count eventop in parent/child relationg; optree vill filter out inbvalid cases\"\"\n\nThis reverts commit 9d0dd60f225aaeec8852d34c5bb8fe89d30a35d6.", "committedDate": "2020-12-07T11:19:50Z", "type": "commit"}, {"oid": "2966843a31d895bd9a407e33cec90b0221516069", "url": "https://github.com/apache/hive/commit/2966843a31d895bd9a407e33cec90b0221516069", "message": "Revert \"dont traverse event operators during valiudation\"\n\nThis reverts commit 80fa406b1a330bbbc297974128d0300bc0572475.", "committedDate": "2020-12-07T11:20:00Z", "type": "commit"}, {"oid": "ea312679dbb3abe915c86a9bc74e666f461941ff", "url": "https://github.com/apache/hive/commit/ea312679dbb3abe915c86a9bc74e666f461941ff", "message": "re-patch/etc;", "committedDate": "2020-12-07T11:59:34Z", "type": "commit"}, {"oid": "eda0089e21f5f81833b4f020cfebe72774aa3c83", "url": "https://github.com/apache/hive/commit/eda0089e21f5f81833b4f020cfebe72774aa3c83", "message": "accept q2 - removed parallel edge!", "committedDate": "2020-12-07T13:09:31Z", "type": "commit"}, {"oid": "472b7e62535f72c5d56db812914a951dca57431d", "url": "https://github.com/apache/hive/commit/472b7e62535f72c5d56db812914a951dca57431d", "message": "Merge remote-tracking branch 'apache/master' into HIVE-24388-event-merge", "committedDate": "2020-12-07T15:04:30Z", "type": "commit"}, {"oid": "e6c44d152f729cb4d3ae4697e20ce7ffb2a4d3d9", "url": "https://github.com/apache/hive/commit/e6c44d152f729cb4d3ae4697e20ce7ffb2a4d3d9", "message": "fixup q75", "committedDate": "2020-12-07T15:45:20Z", "type": "commit"}, {"oid": "fd8cc61dc2b047585392cdf2aa2a829c11cc3caf", "url": "https://github.com/apache/hive/commit/fd8cc61dc2b047585392cdf2aa2a829c11cc3caf", "message": "remove file writes", "committedDate": "2020-12-07T15:46:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY5NzM1MA==", "url": "https://github.com/apache/hive/pull/1750#discussion_r539697350", "bodyText": "Needed?", "author": "jcamachor", "createdAt": "2020-12-09T22:30:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.optimizer;\n \n+import java.io.File;", "originalCommit": "fd8cc61dc2b047585392cdf2aa2a829c11cc3caf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExODIwNQ==", "url": "https://github.com/apache/hive/pull/1750#discussion_r540118205", "bodyText": "yeah...I've some stuff which write out File-s for now...and I just remove it at the end of preparing the patch....\nit gives easier insights into what happened in the SWO.\nI would really like to build it into the system and expose it on the HS2 web interface - it would be very usefull...will get to that as well :)\nbut first I would like to finish this set of patches :)", "author": "kgyrtkirk", "createdAt": "2020-12-10T12:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY5NzM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTcxNTkyMw==", "url": "https://github.com/apache/hive/pull/1750#discussion_r539715923", "bodyText": "Could merging these TS operators be effectively worse?\nFor instance, in this specific mock query, no partition will be pruned for the x1_store_sales table, while before partition pruning was kicking in. Thus, in this case, you are scanning the same data whether you have one or two TS operators (two partitions), however after merging the TS, the size of the data you are shuffling for the join doubles (data in both partitions twice)? Is that analysis correct?\nOff the top of my head, this could be beneficial if i) both TS only select a small subset of the partitions in the table, or ii) overlapping in the partition list for those two different TS is greater than a certain threshold. Should we work in that direction, i.e., introduce some config parameters for this?\n@rbalamohan , what is your take? It would be helpful to have a second opinion.", "author": "jcamachor", "createdAt": "2020-12-09T23:08:16Z", "path": "ql/src/test/results/clientpositive/llap/swo_event_merge.q.out", "diffHunk": "@@ -0,0 +1,291 @@\n+PREHOOK: query: drop table if exists x1_store_sales\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists x1_store_sales\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: drop table if exists x1_date_dim\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists x1_date_dim\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: drop table if exists x1_item\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists x1_item\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table x1_store_sales \n+(\n+\tss_item_sk\tint\n+)\n+partitioned by (ss_sold_date_sk int)\n+stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@x1_store_sales\n+POSTHOOK: query: create table x1_store_sales \n+(\n+\tss_item_sk\tint\n+)\n+partitioned by (ss_sold_date_sk int)\n+stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@x1_store_sales\n+PREHOOK: query: create table x1_date_dim\n+(\n+\td_date_sk\tint,\n+\td_month_seq\tint,\n+\td_year\t\tint,\n+\td_moy\t\tint\n+)\n+stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@x1_date_dim\n+POSTHOOK: query: create table x1_date_dim\n+(\n+\td_date_sk\tint,\n+\td_month_seq\tint,\n+\td_year\t\tint,\n+\td_moy\t\tint\n+)\n+stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@x1_date_dim\n+PREHOOK: query: insert into x1_date_dim values\t(1,1,2000,2),\n+\t\t\t\t(2,2,2001,2)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@x1_date_dim\n+POSTHOOK: query: insert into x1_date_dim values\t(1,1,2000,2),\n+\t\t\t\t(2,2,2001,2)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@x1_date_dim\n+POSTHOOK: Lineage: x1_date_dim.d_date_sk SCRIPT []\n+POSTHOOK: Lineage: x1_date_dim.d_month_seq SCRIPT []\n+POSTHOOK: Lineage: x1_date_dim.d_moy SCRIPT []\n+POSTHOOK: Lineage: x1_date_dim.d_year SCRIPT []\n+PREHOOK: query: insert into x1_store_sales partition (ss_sold_date_sk=1) values (1)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@x1_store_sales@ss_sold_date_sk=1\n+POSTHOOK: query: insert into x1_store_sales partition (ss_sold_date_sk=1) values (1)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@x1_store_sales@ss_sold_date_sk=1\n+POSTHOOK: Lineage: x1_store_sales PARTITION(ss_sold_date_sk=1).ss_item_sk SCRIPT []\n+PREHOOK: query: insert into x1_store_sales partition (ss_sold_date_sk=2) values (2)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@x1_store_sales@ss_sold_date_sk=2\n+POSTHOOK: query: insert into x1_store_sales partition (ss_sold_date_sk=2) values (2)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@x1_store_sales@ss_sold_date_sk=2\n+POSTHOOK: Lineage: x1_store_sales PARTITION(ss_sold_date_sk=2).ss_item_sk SCRIPT []\n+PREHOOK: query: alter table x1_store_sales partition (ss_sold_date_sk=1) update statistics set(\n+'numRows'='123456',\n+'rawDataSize'='1234567')\n+PREHOOK: type: ALTERTABLE_UPDATEPARTSTATS\n+PREHOOK: Input: default@x1_store_sales\n+PREHOOK: Output: default@x1_store_sales@ss_sold_date_sk=1\n+POSTHOOK: query: alter table x1_store_sales partition (ss_sold_date_sk=1) update statistics set(\n+'numRows'='123456',\n+'rawDataSize'='1234567')\n+POSTHOOK: type: ALTERTABLE_UPDATEPARTSTATS\n+POSTHOOK: Input: default@x1_store_sales\n+POSTHOOK: Input: default@x1_store_sales@ss_sold_date_sk=1\n+POSTHOOK: Output: default@x1_store_sales@ss_sold_date_sk=1\n+PREHOOK: query: alter table x1_date_dim update statistics set(\n+'numRows'='56',\n+'rawDataSize'='81449')\n+PREHOOK: type: ALTERTABLE_UPDATETABLESTATS\n+PREHOOK: Input: default@x1_date_dim\n+PREHOOK: Output: default@x1_date_dim\n+POSTHOOK: query: alter table x1_date_dim update statistics set(\n+'numRows'='56',\n+'rawDataSize'='81449')\n+POSTHOOK: type: ALTERTABLE_UPDATETABLESTATS\n+POSTHOOK: Input: default@x1_date_dim\n+POSTHOOK: Output: default@x1_date_dim\n+PREHOOK: query: explain \n+select   count(*) cnt\n+ from\n+     x1_store_sales s\n+     ,x1_date_dim d\n+ where  \n+\t1=1\n+\tand s.ss_sold_date_sk = d.d_date_sk\n+\tand d.d_year=2000\n+union\n+select   s.ss_item_sk*d_date_sk\n+ from\n+     x1_store_sales s\n+     ,x1_date_dim d\n+ where  \n+\t1=1\n+\tand s.ss_sold_date_sk = d.d_date_sk\n+\tand d.d_year=2001\n+\tgroup by s.ss_item_sk*d_date_sk\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@x1_date_dim\n+PREHOOK: Input: default@x1_store_sales\n+PREHOOK: Input: default@x1_store_sales@ss_sold_date_sk=1\n+PREHOOK: Input: default@x1_store_sales@ss_sold_date_sk=2\n+#### A masked pattern was here ####\n+POSTHOOK: query: explain \n+select   count(*) cnt\n+ from\n+     x1_store_sales s\n+     ,x1_date_dim d\n+ where  \n+\t1=1\n+\tand s.ss_sold_date_sk = d.d_date_sk\n+\tand d.d_year=2000\n+union\n+select   s.ss_item_sk*d_date_sk\n+ from\n+     x1_store_sales s\n+     ,x1_date_dim d\n+ where  \n+\t1=1\n+\tand s.ss_sold_date_sk = d.d_date_sk\n+\tand d.d_year=2001\n+\tgroup by s.ss_item_sk*d_date_sk\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@x1_date_dim\n+POSTHOOK: Input: default@x1_store_sales\n+POSTHOOK: Input: default@x1_store_sales@ss_sold_date_sk=1\n+POSTHOOK: Input: default@x1_store_sales@ss_sold_date_sk=2\n+#### A masked pattern was here ####\n+Plan optimized by CBO.\n+\n+Vertex dependency in root stage\n+Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 8 (SIMPLE_EDGE)\n+Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE), Union 4 (CONTAINS)\n+Reducer 5 <- Union 4 (SIMPLE_EDGE)\n+Reducer 6 <- Map 1 (SIMPLE_EDGE), Map 8 (SIMPLE_EDGE)\n+Reducer 7 <- Reducer 6 (SIMPLE_EDGE), Union 4 (CONTAINS)\n+\n+Stage-0\n+  Fetch Operator\n+    limit:-1\n+    Stage-1\n+      Reducer 5 vectorized, llap\n+      File Output Operator [FS_89]\n+        Group By Operator [GBY_88] (rows=1 width=8)\n+          Output:[\"_col0\"],keys:KEY._col0\n+        <-Union 4 [SIMPLE_EDGE]\n+          <-Reducer 3 [CONTAINS] vectorized, llap\n+            Reduce Output Operator [RS_87]\n+              PartitionCols:_col0\n+              Group By Operator [GBY_86] (rows=1 width=8)\n+                Output:[\"_col0\"],keys:_col0\n+                Group By Operator [GBY_85] (rows=1 width=8)\n+                  Output:[\"_col0\"],aggregations:[\"count(VALUE._col0)\"]\n+                <-Reducer 2 [CUSTOM_SIMPLE_EDGE] llap\n+                  PARTITION_ONLY_SHUFFLE [RS_11]\n+                    Group By Operator [GBY_10] (rows=1 width=8)\n+                      Output:[\"_col0\"],aggregations:[\"count()\"]\n+                      Merge Join Operator [MERGEJOIN_51] (rows=1728398 width=8)\n+                        Conds:RS_71._col0=RS_77._col0(Inner)\n+                      <-Map 1 [SIMPLE_EDGE] vectorized, llap\n+                        SHUFFLE [RS_71]\n+                          PartitionCols:_col0\n+                          Select Operator [SEL_69] (rows=123457 width=4)\n+                            Output:[\"_col0\"]\n+                            Filter Operator [FIL_68]\n+                              predicate:ss_sold_date_sk is not null\n+                              TableScan [TS_0] (rows=123457 width=14)\n+                                default@x1_store_sales,s,Tbl:COMPLETE,Col:COMPLETE,Output:[\"ss_item_sk\"]", "originalCommit": "fd8cc61dc2b047585392cdf2aa2a829c11cc3caf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDExNjc2NA==", "url": "https://github.com/apache/hive/pull/1750#discussion_r540116764", "bodyText": "I think right now we don't have a sanity check filter right before the TS to limit reduce shuffled data size to the previous amount; but reducing the number of scans is beneficial - IIRC in the q23 query the scanned partitions were the same; so the benefit was real.\nIf we make available these things the same way as we have the SJ filters - then we could for sure avoid shuffling more data.\nNote: I think it would enable some further opportunities if we would change the SJ data transmission method from RS to EVENTOP - that way we shouldn't have to worry about parallel edges anymore...", "author": "kgyrtkirk", "createdAt": "2020-12-10T12:08:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTcxNTkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMzU0Ng==", "url": "https://github.com/apache/hive/pull/1750#discussion_r540303546", "bodyText": "@jcamachor I've added a config knob to control event operator merge", "author": "kgyrtkirk", "createdAt": "2020-12-10T16:19:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTcxNTkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU1NzYwMA==", "url": "https://github.com/apache/hive/pull/1750#discussion_r540557600", "bodyText": "however after merging the TS, the size of the data you are shuffling for the join doubles\n\n\nYes, this is possible with merging. +1 on adding an option for enabling it.\nThere is additional option to reduce number of group by operator evaluations (e.g Q65 computes group by operator twice) which can help in reducing runtime.", "author": "rbalamohan", "createdAt": "2020-12-10T22:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTcxNTkyMw=="}], "type": "inlineReview"}, {"oid": "eb1d52d71f71628bb0d7e338c889c679e662c8c6", "url": "https://github.com/apache/hive/commit/eb1d52d71f71628bb0d7e338c889c679e662c8c6", "message": "remove import", "committedDate": "2020-12-10T16:20:35Z", "type": "commit"}, {"oid": "1af5bb83b5749295eb1ecfe9843338d30dc80ac7", "url": "https://github.com/apache/hive/commit/1af5bb83b5749295eb1ecfe9843338d30dc80ac7", "message": "add config knob", "committedDate": "2020-12-10T16:24:49Z", "type": "commit"}]}