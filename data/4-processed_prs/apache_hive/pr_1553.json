{"pr_number": 1553, "pr_title": "HIVE-24231: Enhance shared work optimizer to merge scans with filters on both sides", "pr_createdAt": "2020-10-05T16:48:47Z", "pr_url": "https://github.com/apache/hive/pull/1553", "timeline": [{"oid": "af0863b1318e6e38866ec420e4116eb6bdd4b9ec", "url": "https://github.com/apache/hive/commit/af0863b1318e6e38866ec420e4116eb6bdd4b9ec", "message": "existing work", "committedDate": "2020-10-05T14:36:08Z", "type": "commit"}, {"oid": "92ce37b832749aa6c813dafad9774da89a4ecdac", "url": "https://github.com/apache/hive/commit/92ce37b832749aa6c813dafad9774da89a4ecdac", "message": "add semi;", "committedDate": "2020-10-05T14:36:46Z", "type": "commit"}, {"oid": "3d5607da61ddf1943e4b0a9a031e467a6ce430c4", "url": "https://github.com/apache/hive/commit/3d5607da61ddf1943e4b0a9a031e467a6ce430c4", "message": "smi", "committedDate": "2020-10-05T14:40:14Z", "type": "commit"}, {"oid": "59c3d1f1641964969e3d18b6071264b038298931", "url": "https://github.com/apache/hive/commit/59c3d1f1641964969e3d18b6071264b038298931", "message": "x", "committedDate": "2020-10-05T16:05:44Z", "type": "commit"}, {"oid": "758c900cf823d9106ec6248d0aa587fa0fac3f49", "url": "https://github.com/apache/hive/commit/758c900cf823d9106ec6248d0aa587fa0fac3f49", "message": "remove exception", "committedDate": "2020-10-05T16:42:29Z", "type": "commit"}, {"oid": "1ee6e173f55c851517290960ff185c5375952b25", "url": "https://github.com/apache/hive/commit/1ee6e173f55c851517290960ff185c5375952b25", "message": "update tpcds", "committedDate": "2020-10-05T16:45:17Z", "type": "commit"}, {"oid": "d67efb6868282cbbb4c8e397b996ab2e35c390e1", "url": "https://github.com/apache/hive/commit/d67efb6868282cbbb4c8e397b996ab2e35c390e1", "message": "remove typo", "committedDate": "2020-10-05T16:49:56Z", "type": "commit"}, {"oid": "38470efc8211bc572ea2e73f23270528d2b50205", "url": "https://github.com/apache/hive/commit/38470efc8211bc572ea2e73f23270528d2b50205", "message": "update", "committedDate": "2020-10-06T07:03:57Z", "type": "commit"}, {"oid": "4bb84bf202b9918d40d4dd1abd901b8cb1c7b5c1", "url": "https://github.com/apache/hive/commit/4bb84bf202b9918d40d4dd1abd901b8cb1c7b5c1", "message": "disable dppu", "committedDate": "2020-10-06T07:10:04Z", "type": "commit"}, {"oid": "afd8a66a92cbd18ef99992112ac031d619f6485b", "url": "https://github.com/apache/hive/commit/afd8a66a92cbd18ef99992112ac031d619f6485b", "message": "add comment", "committedDate": "2020-10-06T07:20:04Z", "type": "commit"}, {"oid": "6c54baaabaefae4192b062caeb008540c9e5e37d", "url": "https://github.com/apache/hive/commit/6c54baaabaefae4192b062caeb008540c9e5e37d", "message": "add comment+fix", "committedDate": "2020-10-06T07:43:25Z", "type": "commit"}, {"oid": "e889d4b8db5ab1941025f43620f535cd164ae6c9", "url": "https://github.com/apache/hive/commit/e889d4b8db5ab1941025f43620f535cd164ae6c9", "message": "update mergeablecheck", "committedDate": "2020-10-06T08:00:23Z", "type": "commit"}, {"oid": "4a87c2ff86976e9f024b902b4f7d5a8054e3c576", "url": "https://github.com/apache/hive/commit/4a87c2ff86976e9f024b902b4f7d5a8054e3c576", "message": "q.out updates", "committedDate": "2020-10-06T08:28:24Z", "type": "commit"}, {"oid": "7d1584d89f5e606941b781bcdffe54873995003c", "url": "https://github.com/apache/hive/commit/7d1584d89f5e606941b781bcdffe54873995003c", "message": "update", "committedDate": "2020-10-06T08:38:49Z", "type": "commit"}, {"oid": "b43cb827ba2a0b2973e27e87422e62eb8678c724", "url": "https://github.com/apache/hive/commit/b43cb827ba2a0b2973e27e87422e62eb8678c724", "message": "en-x", "committedDate": "2020-10-06T12:55:00Z", "type": "commit"}, {"oid": "261fdcc93a6b6e15cdf96daaf8726d0636f7b725", "url": "https://github.com/apache/hive/commit/261fdcc93a6b6e15cdf96daaf8726d0636f7b725", "message": "Revert \"en-x\"\n\nThis reverts commit b43cb827ba2a0b2973e27e87422e62eb8678c724.", "committedDate": "2020-10-06T13:15:56Z", "type": "commit"}, {"oid": "c35896fb5ebaefcb742d257462f0685bab44cbc9", "url": "https://github.com/apache/hive/commit/c35896fb5ebaefcb742d257462f0685bab44cbc9", "message": "qqqqqqqqqqqqqqqqRevert \"Revert \"en-x\"\"\n\nThis reverts commit 261fdcc93a6b6e15cdf96daaf8726d0636f7b725.", "committedDate": "2020-10-06T13:21:42Z", "type": "commit"}, {"oid": "34c4423bdd30a1220f5c0bf9def94eefc70b847c", "url": "https://github.com/apache/hive/commit/34c4423bdd30a1220f5c0bf9def94eefc70b847c", "message": "accept q.out; small updates", "committedDate": "2020-10-06T15:48:04Z", "type": "commit"}, {"oid": "0320b9a2da55856ad9071d28725705798586cc0f", "url": "https://github.com/apache/hive/commit/0320b9a2da55856ad9071d28725705798586cc0f", "message": "accept dyn-op", "committedDate": "2020-10-07T08:06:56Z", "type": "commit"}, {"oid": "8c41061f78c969a9d33f058e56d7269f64f45d5a", "url": "https://github.com/apache/hive/commit/8c41061f78c969a9d33f058e56d7269f64f45d5a", "message": "update messages", "committedDate": "2020-10-07T08:25:55Z", "type": "commit"}, {"oid": "8d33c6ebc65ad04e7d4316a4d3c000007de8eef4", "url": "https://github.com/apache/hive/commit/8d33c6ebc65ad04e7d4316a4d3c000007de8eef4", "message": "updates", "committedDate": "2020-10-07T08:37:55Z", "type": "commit"}, {"oid": "e5b14a1c445d4618f3a3415b9c145eb561e6144d", "url": "https://github.com/apache/hive/commit/e5b14a1c445d4618f3a3415b9c145eb561e6144d", "message": "downstreammerge", "committedDate": "2020-10-07T09:06:23Z", "type": "commit"}, {"oid": "dda3523ff6342bc8452afd9bf002774edd25f805", "url": "https://github.com/apache/hive/commit/dda3523ff6342bc8452afd9bf002774edd25f805", "message": "fixes to downstream merge", "committedDate": "2020-10-07T11:18:57Z", "type": "commit"}, {"oid": "6f4faeaa9c2fa9604116bfaf6ac83d702ff10dbd", "url": "https://github.com/apache/hive/commit/6f4faeaa9c2fa9604116bfaf6ac83d702ff10dbd", "message": "remove fixme", "committedDate": "2020-10-07T11:38:31Z", "type": "commit"}, {"oid": "9b18ff94f42916b87c6441ebbcb98a5b9baa713e", "url": "https://github.com/apache/hive/commit/9b18ff94f42916b87c6441ebbcb98a5b9baa713e", "message": "fix npe", "committedDate": "2020-10-07T12:55:06Z", "type": "commit"}, {"oid": "ad629eb1e8ee19f949f66d577b3999895521abeb", "url": "https://github.com/apache/hive/commit/ad629eb1e8ee19f949f66d577b3999895521abeb", "message": "remove downstream-merge", "committedDate": "2020-10-07T12:56:35Z", "type": "commit"}, {"oid": "d1a62b4bc4c12ddb79332d669eae885457dc6792", "url": "https://github.com/apache/hive/commit/d1a62b4bc4c12ddb79332d669eae885457dc6792", "message": "accepted q.out changes", "committedDate": "2020-10-07T13:25:41Z", "type": "commit"}, {"oid": "610b7892c03fdd24a9e4c16f3998e6a1b7d4ed8b", "url": "https://github.com/apache/hive/commit/610b7892c03fdd24a9e4c16f3998e6a1b7d4ed8b", "message": "fix bug", "committedDate": "2020-10-07T14:29:18Z", "type": "commit"}, {"oid": "94608ec4b45105b752c0c9c9ed8766489060a60d", "url": "https://github.com/apache/hive/commit/94608ec4b45105b752c0c9c9ed8766489060a60d", "message": "back to 0 diff", "committedDate": "2020-10-07T15:04:06Z", "type": "commit"}, {"oid": "d4f41a311a592f0398413f9b29bfc6458022f88b", "url": "https://github.com/apache/hive/commit/d4f41a311a592f0398413f9b29bfc6458022f88b", "message": "fix", "committedDate": "2020-10-07T15:04:16Z", "type": "commit"}, {"oid": "06cfe04f9fcc2f3412c68fe7e062b930898b4bfc", "url": "https://github.com/apache/hive/commit/06cfe04f9fcc2f3412c68fe7e062b930898b4bfc", "message": "accept q.outs", "committedDate": "2020-10-07T15:15:49Z", "type": "commit"}, {"oid": "8d7b4f50611e6743a64e4b9d369f3f92e608b84b", "url": "https://github.com/apache/hive/commit/8d7b4f50611e6743a64e4b9d369f3f92e608b84b", "message": "accept syubquery_in", "committedDate": "2020-10-07T15:16:08Z", "type": "commit"}, {"oid": "15e5eff1e8bd9d97d16ce6e2a98b0c611611eb7b", "url": "https://github.com/apache/hive/commit/15e5eff1e8bd9d97d16ce6e2a98b0c611611eb7b", "message": "Revert \"remove downstream-merge\"\n\nThis reverts commit ad629eb1e8ee19f949f66d577b3999895521abeb.", "committedDate": "2020-10-07T15:16:22Z", "type": "commit"}, {"oid": "a97ac5117b5795c1a00805b8de81b0db76b9f887", "url": "https://github.com/apache/hive/commit/a97ac5117b5795c1a00805b8de81b0db76b9f887", "message": "Revert \"Revert \"remove downstream-merge\"\"\n\nThis reverts commit 15e5eff1e8bd9d97d16ce6e2a98b0c611611eb7b.", "committedDate": "2020-10-07T19:53:04Z", "type": "commit"}, {"oid": "e1021490b12c4289ee03b61ff85680d22d7d9843", "url": "https://github.com/apache/hive/commit/e1021490b12c4289ee03b61ff85680d22d7d9843", "message": "q.out updates", "committedDate": "2020-10-07T20:16:12Z", "type": "commit"}, {"oid": "aaccbdc062291001850b5d824565930b498d67c3", "url": "https://github.com/apache/hive/commit/aaccbdc062291001850b5d824565930b498d67c3", "message": "fix union", "committedDate": "2020-10-07T20:55:04Z", "type": "commit"}, {"oid": "9838961e57ebe20765ad7a046fc4283ec7b7f1f4", "url": "https://github.com/apache/hive/commit/9838961e57ebe20765ad7a046fc4283ec7b7f1f4", "message": "x", "committedDate": "2020-10-07T21:03:29Z", "type": "commit"}, {"oid": "90607e86627c57fe41e0db0f9f9d5f7764d87c71", "url": "https://github.com/apache/hive/commit/90607e86627c57fe41e0db0f9f9d5f7764d87c71", "message": "q5 back", "committedDate": "2020-10-07T21:08:36Z", "type": "commit"}, {"oid": "5656c7dc20fa1f904374b44b30270f7987f5b1de", "url": "https://github.com/apache/hive/commit/5656c7dc20fa1f904374b44b30270f7987f5b1de", "message": "changes relating to removal of downstream-merge", "committedDate": "2020-10-08T07:34:37Z", "type": "commit"}, {"oid": "199fee4b3bc338e0d5c153a9f8063c47f9bd9474", "url": "https://github.com/apache/hive/commit/199fee4b3bc338e0d5c153a9f8063c47f9bd9474", "message": "fix recompilation in SWO", "committedDate": "2020-10-08T07:37:54Z", "type": "commit"}, {"oid": "831c708df50f3cf433b35fe2c19d0e41f330d197", "url": "https://github.com/apache/hive/commit/831c708df50f3cf433b35fe2c19d0e41f330d197", "message": "Merge remote-tracking branch 'apache/master' into HIVE-swo-dppunion", "committedDate": "2020-10-08T08:16:42Z", "type": "commit"}, {"oid": "18678dc0b15bd11592eccf5929f8d0dabf25160d", "url": "https://github.com/apache/hive/commit/18678dc0b15bd11592eccf5929f8d0dabf25160d", "message": "remove logger changes", "committedDate": "2020-10-08T08:17:48Z", "type": "commit"}, {"oid": "f672130cc66044edbcb5718bb5ec7fdfb4368329", "url": "https://github.com/apache/hive/commit/f672130cc66044edbcb5718bb5ec7fdfb4368329", "message": "remove ws change", "committedDate": "2020-10-08T08:18:23Z", "type": "commit"}, {"oid": "b0d9aef86a0aaca6acc1e3d0f68a126b0eb8c6a7", "url": "https://github.com/apache/hive/commit/b0d9aef86a0aaca6acc1e3d0f68a126b0eb8c6a7", "message": "remove non-finished test", "committedDate": "2020-10-08T08:24:30Z", "type": "commit"}, {"oid": "e9bb99199899fc87048e680a04c086169782b2eb", "url": "https://github.com/apache/hive/commit/e9bb99199899fc87048e680a04c086169782b2eb", "message": "cleanup", "committedDate": "2020-10-08T09:42:03Z", "type": "commit"}, {"oid": "92f2951bf50a65abe655e952c22520abbd14caa4", "url": "https://github.com/apache/hive/commit/92f2951bf50a65abe655e952c22520abbd14caa4", "message": "archive testresult-xmls", "committedDate": "2020-10-08T20:18:52Z", "type": "commit"}, {"oid": "c210b8e48e5e3e593315764c7a946b22502e1ddc", "url": "https://github.com/apache/hive/commit/c210b8e48e5e3e593315764c7a946b22502e1ddc", "message": "archive results for each split", "committedDate": "2020-10-09T07:27:12Z", "type": "commit"}, {"oid": "6abc1ac36fcdb9dcaab91539fb55ffa8bccb4444", "url": "https://github.com/apache/hive/commit/6abc1ac36fcdb9dcaab91539fb55ffa8bccb4444", "message": "create test-results.tgz archive", "committedDate": "2020-10-09T08:14:23Z", "type": "commit"}, {"oid": "b4b9ce11e6b591ff702c43b847a2780da96afe9c", "url": "https://github.com/apache/hive/commit/b4b9ce11e6b591ff702c43b847a2780da96afe9c", "message": "fix typo", "committedDate": "2020-10-09T12:18:21Z", "type": "commit"}, {"oid": "31ddb97bf412a2679489a5a0d45d335d2708005c", "url": "https://github.com/apache/hive/commit/31ddb97bf412a2679489a5a0d45d335d2708005c", "message": "use surefire M5", "committedDate": "2020-10-09T12:34:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjU0Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502816547", "bodyText": "It seems this is unrelated to this patch? It may be better to split into multiple JIRAs/PRs.", "author": "jcamachor", "createdAt": "2020-10-10T18:10:36Z", "path": "Jenkinsfile", "diffHunk": "@@ -174,6 +174,17 @@ def loadWS() {\n     tar -xf archive.tar'''\n }\n \n+def saveFile(name) {", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgxNjcwOQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502816709", "bodyText": "Same as above, not sure if it would belong exactly to same PR, but it would be better to have different JIRAs/PRs for this different issues.", "author": "jcamachor", "createdAt": "2020-10-10T18:12:14Z", "path": "pom.xml", "diffHunk": "@@ -104,7 +104,7 @@\n     <maven.checkstyle.plugin.version>2.17</maven.checkstyle.plugin.version>\n     <maven.build-helper.plugin.version>1.12</maven.build-helper.plugin.version>\n     <maven.eclipse.plugin.version>2.10</maven.eclipse.plugin.version>\n-    <maven.surefire.plugin.version>3.0.0-M4</maven.surefire.plugin.version>\n+    <maven.surefire.plugin.version>3.0.0-M5</maven.surefire.plugin.version>", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTEzNg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502941136", "bodyText": "Can we add a comment about each of these modes?", "author": "jcamachor", "createdAt": "2020-10-11T17:09:00Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -284,6 +294,41 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {\n     return pctx;\n   }\n \n+  public enum Mode {\n+    RemoveSemijoin, SubtreeMerge, DPPUnion,", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU3MA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502941570", "bodyText": "Does this need to be swapped with the one below? It seems that if HIVE_SHARED_WORK_MERGE_TS_SCHEMA is true, we should use the SchemaAwareSharedWorkOptimizer (or maybe the name is a bit misleading)?", "author": "jcamachor", "createdAt": "2020-10-11T17:12:44Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -159,9 +158,15 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {\n     // Gather information about the DPP table scans and store it in the cache\n     gatherDPPTableScanOps(pctx, optimizerCache);\n \n+    BaseSharedWorkOptimizer swo;\n+    if (pctx.getConf().getBoolVar(ConfVars.HIVE_SHARED_WORK_MERGE_TS_SCHEMA)) {\n+      swo = new BaseSharedWorkOptimizer();", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE2OTUzNw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503169537", "bodyText": "SchemaAwareSharedWorkOptimizer is the more strict version\n\n  \n    \n      hive/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java\n    \n    \n         Line 591\n      in\n      78d42f0\n    \n    \n    \n    \n\n        \n          \n              * More strict implementation of shared work optimizer. \n        \n    \n  \n\n\nI just wanted to enable schema merge for all the optimizations - so I've moved it here", "author": "kgyrtkirk", "createdAt": "2020-10-12T09:39:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTU3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MTk4MA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502941980", "bodyText": "Comment about this class. It seems it holds a TS, the filter expression that does not include the SJ, and the SJ filters?", "author": "jcamachor", "createdAt": "2020-10-11T17:16:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -284,6 +294,41 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {\n     return pctx;\n   }\n \n+  public enum Mode {\n+    RemoveSemijoin, SubtreeMerge, DPPUnion,\n+  }\n+\n+  static class SharedWorkModel {", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MjA2Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502942067", "bodyText": "typo -> areMergeableDppUninon", "author": "jcamachor", "createdAt": "2020-10-11T17:17:15Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -338,8 +385,29 @@ public boolean sharedWorkOptimization(ParseContext pctx, SharedWorkOptimizerCach\n               // about the part of the tree that can be merged. We need to regenerate the\n               // cache because semijoin operators have been removed\n               sr = extractSharedOptimizationInfoForRoot(\n-                  pctx, optimizerCache, retainableTsOp, discardableTsOp);\n-            } else {\n+                  pctx, optimizerCache, retainableTsOp, discardableTsOp, true);\n+            } else if (mode == Mode.DPPUnion) {\n+              boolean mergeable = areMergeable(pctx, retainableTsOp, discardableTsOp);\n+              if (!mergeable) {\n+                LOG.debug(\"{} and {} cannot be merged\", retainableTsOp, discardableTsOp);\n+                continue;\n+              }\n+              boolean validMerge =\n+                  areMergeableDppUninon(pctx, optimizerCache, retainableTsOp, discardableTsOp);", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MjQxMQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502942411", "bodyText": "Is this a TODO for this patch or follow-up work?", "author": "jcamachor", "createdAt": "2020-10-11T17:20:49Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -386,125 +456,81 @@ public boolean sharedWorkOptimization(ParseContext pctx, SharedWorkOptimizerCach\n               LOG.debug(\"Merging subtree starting at {} into subtree starting at {}\",\n                   discardableTsOp, retainableTsOp);\n             } else {\n-              ExprNodeDesc newRetainableTsFilterExpr = null;\n-              List<ExprNodeDesc> semijoinExprNodes = new ArrayList<>();\n-              if (retainableTsOp.getConf().getFilterExpr() != null) {\n-                // Gather SJ expressions and normal expressions\n-                List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n-                splitExpressions(retainableTsOp.getConf().getFilterExpr(),\n-                    allExprNodesExceptSemijoin, semijoinExprNodes);\n-                // Create new expressions\n-                if (allExprNodesExceptSemijoin.size() > 1) {\n-                  newRetainableTsFilterExpr = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), allExprNodesExceptSemijoin);\n-                } else if (allExprNodesExceptSemijoin.size() > 0 &&\n-                    allExprNodesExceptSemijoin.get(0) instanceof ExprNodeGenericFuncDesc) {\n-                  newRetainableTsFilterExpr = allExprNodesExceptSemijoin.get(0);\n-                }\n-                // Push filter on top of children for retainable\n-                pushFilterToTopOfTableScan(optimizerCache, retainableTsOp);\n+\n+              if (sr.discardableOps.size() > 1) {\n+                throw new RuntimeException(\"we can't discard more in this path\");\n               }\n-              ExprNodeDesc newDiscardableTsFilterExpr = null;\n-              if (discardableTsOp.getConf().getFilterExpr() != null) {\n-                // If there is a single discardable operator, it is a TableScanOperator\n-                // and it means that we will merge filter expressions for it. Thus, we\n-                // might need to remove DPP predicates before doing that\n-                List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n-                splitExpressions(discardableTsOp.getConf().getFilterExpr(),\n-                    allExprNodesExceptSemijoin, new ArrayList<>());\n-                // Create new expressions\n-                if (allExprNodesExceptSemijoin.size() > 1) {\n-                  newDiscardableTsFilterExpr = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), allExprNodesExceptSemijoin);\n-                } else if (allExprNodesExceptSemijoin.size() > 0 &&\n-                    allExprNodesExceptSemijoin.get(0) instanceof ExprNodeGenericFuncDesc) {\n-                  newDiscardableTsFilterExpr = allExprNodesExceptSemijoin.get(0);\n-                }\n-                // Remove and add semijoin filter from expressions\n-                replaceSemijoinExpressions(discardableTsOp, semijoinExprNodes);\n-                // Push filter on top of children for discardable\n-                pushFilterToTopOfTableScan(optimizerCache, discardableTsOp);\n+\n+              SharedWorkModel modelR = new SharedWorkModel(retainableTsOp);\n+              SharedWorkModel modelD = new SharedWorkModel(discardableTsOp);\n+\n+              // Push filter on top of children for retainable\n+              pushFilterToTopOfTableScan(optimizerCache, retainableTsOp);\n+\n+              if (mode == Mode.RemoveSemijoin || mode == Mode.SubtreeMerge) {\n+                // FIXME: I think idea here is to clear the discardable's semijoin filter", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwNjc5OA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503806798", "bodyText": "I made this note - because it was not obvious to me what's happening here..I've rephrased it to be easier to understand", "author": "kgyrtkirk", "createdAt": "2020-10-13T09:32:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MjQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MjU5OQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502942599", "bodyText": "Is this a TODO for this patch or follow-up work? Also there is a typo (assymetric)", "author": "jcamachor", "createdAt": "2020-10-11T17:22:20Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -326,6 +372,7 @@ public boolean sharedWorkOptimization(ParseContext pctx, SharedWorkOptimizerCach\n                 LOG.debug(\"{} and {} cannot be merged\", retainableTsOp, discardableTsOp);\n                 continue;\n               }\n+              // FIXME: I think this optimization is assymetric; but the check is symmetric", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTAzOQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503809039", "bodyText": "this could be done as a cleanup - however I've already concluded that because of the table ordering the problematic case will actually never happen; so we are safe\nI've removed the FIXME", "author": "kgyrtkirk", "createdAt": "2020-10-13T09:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MjU5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MzM5NA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502943394", "bodyText": "Can you leave a comment explaining how we could hit this error? What should have happened? Somehow we have something in retainable/discardable that should be equivalent but it is not?", "author": "jcamachor", "createdAt": "2020-10-11T17:28:58Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -386,125 +456,81 @@ public boolean sharedWorkOptimization(ParseContext pctx, SharedWorkOptimizerCach\n               LOG.debug(\"Merging subtree starting at {} into subtree starting at {}\",\n                   discardableTsOp, retainableTsOp);\n             } else {\n-              ExprNodeDesc newRetainableTsFilterExpr = null;\n-              List<ExprNodeDesc> semijoinExprNodes = new ArrayList<>();\n-              if (retainableTsOp.getConf().getFilterExpr() != null) {\n-                // Gather SJ expressions and normal expressions\n-                List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n-                splitExpressions(retainableTsOp.getConf().getFilterExpr(),\n-                    allExprNodesExceptSemijoin, semijoinExprNodes);\n-                // Create new expressions\n-                if (allExprNodesExceptSemijoin.size() > 1) {\n-                  newRetainableTsFilterExpr = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), allExprNodesExceptSemijoin);\n-                } else if (allExprNodesExceptSemijoin.size() > 0 &&\n-                    allExprNodesExceptSemijoin.get(0) instanceof ExprNodeGenericFuncDesc) {\n-                  newRetainableTsFilterExpr = allExprNodesExceptSemijoin.get(0);\n-                }\n-                // Push filter on top of children for retainable\n-                pushFilterToTopOfTableScan(optimizerCache, retainableTsOp);\n+\n+              if (sr.discardableOps.size() > 1) {\n+                throw new RuntimeException(\"we can't discard more in this path\");", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMjEzMg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503812132", "bodyText": "there could be a few things which could go south here - one is that pushing filters out from the discardable ts will most likely not work as desired.\nI feel tempted to remove this multi operator matching stuff in HIVE-24241 - because that approach is much simpler; more separated from merging of the operators.", "author": "kgyrtkirk", "createdAt": "2020-10-13T09:40:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MzM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MzQ5Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502943497", "bodyText": "it seems you can use conjunction method here too.", "author": "jcamachor", "createdAt": "2020-10-11T17:29:50Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -284,6 +294,41 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {\n     return pctx;\n   }\n \n+  public enum Mode {\n+    RemoveSemijoin, SubtreeMerge, DPPUnion,\n+  }\n+\n+  static class SharedWorkModel {\n+\n+    private TableScanOperator ts;\n+    private ExprNodeDesc normalFilterExpr;\n+    private List<ExprNodeDesc> semijoinExprNodes = new ArrayList<>();\n+\n+    public SharedWorkModel(TableScanOperator ts) throws UDFArgumentException {\n+      this.ts = ts;\n+      TableScanOperator retainableTsOp = ts;\n+      if (retainableTsOp.getConf().getFilterExpr() != null) {\n+        // Gather SJ expressions and normal expressions\n+        List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n+        splitExpressions(retainableTsOp.getConf().getFilterExpr(),\n+            allExprNodesExceptSemijoin, semijoinExprNodes);\n+        // Create new expressions\n+        if (allExprNodesExceptSemijoin.size() > 1) {\n+          normalFilterExpr = ExprNodeGenericFuncDesc.newInstance(", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MzYxMQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502943611", "bodyText": "Probably this comment should go within the if clause now.", "author": "jcamachor", "createdAt": "2020-10-11T17:30:53Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -386,125 +456,81 @@ public boolean sharedWorkOptimization(ParseContext pctx, SharedWorkOptimizerCach\n               LOG.debug(\"Merging subtree starting at {} into subtree starting at {}\",\n                   discardableTsOp, retainableTsOp);\n             } else {\n-              ExprNodeDesc newRetainableTsFilterExpr = null;\n-              List<ExprNodeDesc> semijoinExprNodes = new ArrayList<>();\n-              if (retainableTsOp.getConf().getFilterExpr() != null) {\n-                // Gather SJ expressions and normal expressions\n-                List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n-                splitExpressions(retainableTsOp.getConf().getFilterExpr(),\n-                    allExprNodesExceptSemijoin, semijoinExprNodes);\n-                // Create new expressions\n-                if (allExprNodesExceptSemijoin.size() > 1) {\n-                  newRetainableTsFilterExpr = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), allExprNodesExceptSemijoin);\n-                } else if (allExprNodesExceptSemijoin.size() > 0 &&\n-                    allExprNodesExceptSemijoin.get(0) instanceof ExprNodeGenericFuncDesc) {\n-                  newRetainableTsFilterExpr = allExprNodesExceptSemijoin.get(0);\n-                }\n-                // Push filter on top of children for retainable\n-                pushFilterToTopOfTableScan(optimizerCache, retainableTsOp);\n+\n+              if (sr.discardableOps.size() > 1) {\n+                throw new RuntimeException(\"we can't discard more in this path\");\n               }\n-              ExprNodeDesc newDiscardableTsFilterExpr = null;\n-              if (discardableTsOp.getConf().getFilterExpr() != null) {\n-                // If there is a single discardable operator, it is a TableScanOperator\n-                // and it means that we will merge filter expressions for it. Thus, we\n-                // might need to remove DPP predicates before doing that\n-                List<ExprNodeDesc> allExprNodesExceptSemijoin = new ArrayList<>();\n-                splitExpressions(discardableTsOp.getConf().getFilterExpr(),\n-                    allExprNodesExceptSemijoin, new ArrayList<>());\n-                // Create new expressions\n-                if (allExprNodesExceptSemijoin.size() > 1) {\n-                  newDiscardableTsFilterExpr = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), allExprNodesExceptSemijoin);\n-                } else if (allExprNodesExceptSemijoin.size() > 0 &&\n-                    allExprNodesExceptSemijoin.get(0) instanceof ExprNodeGenericFuncDesc) {\n-                  newDiscardableTsFilterExpr = allExprNodesExceptSemijoin.get(0);\n-                }\n-                // Remove and add semijoin filter from expressions\n-                replaceSemijoinExpressions(discardableTsOp, semijoinExprNodes);\n-                // Push filter on top of children for discardable\n-                pushFilterToTopOfTableScan(optimizerCache, discardableTsOp);\n+\n+              SharedWorkModel modelR = new SharedWorkModel(retainableTsOp);\n+              SharedWorkModel modelD = new SharedWorkModel(discardableTsOp);\n+\n+              // Push filter on top of children for retainable\n+              pushFilterToTopOfTableScan(optimizerCache, retainableTsOp);\n+\n+              if (mode == Mode.RemoveSemijoin || mode == Mode.SubtreeMerge) {\n+                // FIXME: I think idea here is to clear the discardable's semijoin filter\n+                // - by using the retainable's (which should be empty in case of this mode)\n+                replaceSemijoinExpressions(discardableTsOp, modelR.getSemiJoinFilter());\n               }\n+              // Push filter on top of children for discardable\n+              pushFilterToTopOfTableScan(optimizerCache, discardableTsOp);\n+\n               // Obtain filter for shared TS operator\n-              ExprNodeGenericFuncDesc exprNode = null;\n-              if (newRetainableTsFilterExpr != null && newDiscardableTsFilterExpr != null) {\n-                // Combine\n-                exprNode = (ExprNodeGenericFuncDesc) newRetainableTsFilterExpr;\n-                if (!exprNode.isSame(newDiscardableTsFilterExpr)) {\n-                  // We merge filters from previous scan by ORing with filters from current scan\n-                  if (exprNode.getGenericUDF() instanceof GenericUDFOPOr) {\n-                    List<ExprNodeDesc> newChildren = new ArrayList<>(exprNode.getChildren().size() + 1);\n-                    for (ExprNodeDesc childExprNode : exprNode.getChildren()) {\n-                      if (childExprNode.isSame(newDiscardableTsFilterExpr)) {\n-                        // We do not need to do anything, it is in the OR expression\n-                        break;\n-                      }\n-                      newChildren.add(childExprNode);\n-                    }\n-                    if (exprNode.getChildren().size() == newChildren.size()) {\n-                      newChildren.add(newDiscardableTsFilterExpr);\n-                      exprNode = ExprNodeGenericFuncDesc.newInstance(\n-                          new GenericUDFOPOr(),\n-                          newChildren);\n-                    }\n-                  } else {\n-                    exprNode = ExprNodeGenericFuncDesc.newInstance(\n-                        new GenericUDFOPOr(),\n-                        Arrays.asList(exprNode, newDiscardableTsFilterExpr));\n-                  }\n-                }\n+              ExprNodeDesc exprNode = null;\n+              if (modelR.normalFilterExpr != null && modelD.normalFilterExpr != null) {\n+                exprNode = disjunction(modelR.normalFilterExpr, modelD.normalFilterExpr);\n               }\n-              // Create expression node that will be used for the retainable table scan\n-              if (!semijoinExprNodes.isEmpty()) {\n-                if (exprNode != null) {\n-                  semijoinExprNodes.add(0, exprNode);\n-                }\n-                if (semijoinExprNodes.size() > 1) {\n-                  exprNode = ExprNodeGenericFuncDesc.newInstance(\n-                      new GenericUDFOPAnd(), semijoinExprNodes);\n-                } else {\n-                  exprNode = (ExprNodeGenericFuncDesc) semijoinExprNodes.get(0);\n-                }\n+              List<ExprNodeDesc> semiJoinExpr = null;\n+              if (mode == Mode.DPPUnion) {\n+                assert modelR.semijoinExprNodes != null;\n+                assert modelD.semijoinExprNodes != null;\n+                ExprNodeDesc disjunction = disjunction(conjunction(modelR.semijoinExprNodes), conjunction(modelD.semijoinExprNodes));\n+                semiJoinExpr = disjunction == null ? null : Lists.newArrayList(disjunction);\n+              } else {\n+                semiJoinExpr = modelR.semijoinExprNodes;\n               }\n+\n+              // Create expression node that will be used for the retainable table scan\n+              exprNode = conjunction(semiJoinExpr, exprNode);\n               // Replace filter\n-              retainableTsOp.getConf().setFilterExpr(exprNode);\n+              retainableTsOp.getConf().setFilterExpr((ExprNodeGenericFuncDesc) exprNode);\n               // Replace table scan operator\n               List<Operator<? extends OperatorDesc>> allChildren =\n                   Lists.newArrayList(discardableTsOp.getChildOperators());\n               for (Operator<? extends OperatorDesc> op : allChildren) {\n-                discardableTsOp.getChildOperators().remove(op);\n                 op.replaceParent(discardableTsOp, retainableTsOp);\n                 retainableTsOp.getChildOperators().add(op);\n               }\n+              discardableTsOp.getChildOperators().clear();\n \n               LOG.debug(\"Merging {} into {}\", discardableTsOp, retainableTsOp);\n             }\n \n             // First we remove the input operators of the expression that", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgyMTE2OQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503821169", "bodyText": "this small block can be done with a function(adoptChildren) from the next patch; I've not yet noticed that I can use it here as well - I've added the method and called it", "author": "kgyrtkirk", "createdAt": "2020-10-13T09:54:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0MzYxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzg0Ng==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502943846", "bodyText": "Any reason to set this to false here?", "author": "jcamachor", "createdAt": "2020-10-11T17:33:05Z", "path": "ql/src/test/queries/clientpositive/explainuser_1.q", "diffHunk": "@@ -9,6 +9,7 @@\n --! qt:dataset:cbo_t1\n set hive.vectorized.execution.enabled=false;\n set hive.strict.checks.bucketing=false;\n+set hive.optimize.shared.work.dppunion=false;", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgyMzQ0Mw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503823443", "bodyText": "I didn't wanted the new feature to do further twists with the q.out of existing \"directed\" tests\nI've removed these set calls from the q files", "author": "kgyrtkirk", "createdAt": "2020-10-13T09:58:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0Mzk4Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502943987", "bodyText": "It seems there is a few of these tests for which the new mode is disabled. Can we enable it? These kind of complicated subqueries where we have multiple repetitions of same tables is where it can become pretty useful.", "author": "jcamachor", "createdAt": "2020-10-11T17:34:28Z", "path": "ql/src/test/queries/clientpositive/subquery_ALL.q", "diffHunk": "@@ -1,4 +1,5 @@\n --! qt:dataset:part\n+set hive.optimize.shared.work.dppunion=false;", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDIwMw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502944203", "bodyText": "Why do we end up with an additional reducer in this plan?", "author": "jcamachor", "createdAt": "2020-10-11T17:36:41Z", "path": "ql/src/test/results/clientpositive/llap/cbo_SortUnionTransposeRule.q.out", "diffHunk": "@@ -1006,6 +1027,22 @@ STAGE PLANS:\n                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Reducer 5 \n+            Execution mode: vectorized, llap", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgyNjU1Ng==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503826556", "bodyText": "This is caused by enabling the schema merge for all the optimizations. Apparently the greedy operator chain matching logic worked slightly better when it was run w/o ts-merge first and only then executed to also consider merging the schema.\nIn this patch I will only introduce the new optimization and do the generalization in either in the \"downstream merge\" patch or completely separately; it will worth because it will enable the RemoveSemiJoin mode to consider merging the ts schema", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDk3Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502944977", "bodyText": "It seems we are introducing a redundant filter in the new plan. Note that before it was only present in TS operator.", "author": "jcamachor", "createdAt": "2020-10-11T17:44:38Z", "path": "ql/src/test/results/clientpositive/llap/dynamic_partition_pruning.q.out", "diffHunk": "@@ -4277,21 +4277,37 @@ STAGE PLANS:\n                   alias: srcpart\n                   filterExpr: ds is not null (type: boolean)\n                   Statistics: Num rows: 2000 Data size: 389248 Basic stats: COMPLETE Column stats: COMPLETE\n-                  Group By Operator\n-                    keys: ds (type: string)\n-                    minReductionHashAggr: 0.99\n-                    mode: hash\n-                    outputColumnNames: _col0\n-                    Statistics: Num rows: 2 Data size: 368 Basic stats: COMPLETE Column stats: COMPLETE\n-                    Reduce Output Operator\n-                      key expressions: _col0 (type: string)\n-                      null sort order: z\n-                      sort order: +\n-                      Map-reduce partition columns: _col0 (type: string)\n+                  Filter Operator\n+                    predicate: ds is not null (type: boolean)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzNDgwMw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503834803", "bodyText": "I see 2 filter operators doing the same in this plan - which will be merged by the \"downstream merge\" patch.\nHowever; to my best knowledge the TS filterExpr should only be considered \"best-effort\" because the reader may decide to not filter by some parts of the expr.\nIn this plan we should probably have removed 2 extra srcpart scans by the SubTree logic - that should be investigated - I've opened HIVE-24268", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:17:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NDk3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NjEyNQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502946125", "bodyText": "In this case, it seems we are still missing this merging opportunity (it may be for a different reason).", "author": "jcamachor", "createdAt": "2020-10-11T17:55:59Z", "path": "ql/src/test/results/clientpositive/llap/join_parse.q.out", "diffHunk": "@@ -499,34 +499,41 @@ STAGE PLANS:\n                         sort order: +\n                         Map-reduce partition columns: _col0 (type: string)\n                         Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: (value is not null and key is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n                       Reduce Output Operator\n                         key expressions: _col0 (type: string)\n                         null sort order: z\n                         sort order: +\n                         Map-reduce partition columns: _col0 (type: string)\n-                        Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE\n+                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n+                        value expressions: _col1 (type: string)\n             Execution mode: vectorized, llap\n             LLAP IO: all inputs\n         Map 6 \n             Map Operator Tree:\n                 TableScan\n-                  alias: src1\n-                  filterExpr: (value is not null and key is not null) (type: boolean)\n-                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n+                  alias: src2", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzNTUzNg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503835536", "bodyText": "difference is gone - no more ts merging for now; will get back to it later", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:18:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NjEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NjIzOQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502946239", "bodyText": "There is not new reutilization opportunities, but there is additional filter expressions. Expected?", "author": "jcamachor", "createdAt": "2020-10-11T17:57:09Z", "path": "ql/src/test/results/clientpositive/llap/keep_uniform.q.out", "diffHunk": "@@ -516,6 +516,21 @@ STAGE PLANS:\n                             valueColumns: 15:int\n                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                         value expressions: _col0 (type: int)\n+                  Filter Operator", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NjM1MA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502946350", "bodyText": "Looking at this filter expression, I think we could call foldExpr when the new expression is created in SWO, in order to simplify these filters.", "author": "jcamachor", "createdAt": "2020-10-11T17:58:26Z", "path": "ql/src/test/results/clientpositive/llap/ppd_repeated_alias.q.out", "diffHunk": "@@ -348,14 +348,14 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Edges:\n         Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE)\n-        Reducer 3 <- Map 4 (XPROD_EDGE), Reducer 2 (XPROD_EDGE)\n+        Reducer 3 <- Map 1 (XPROD_EDGE), Reducer 2 (XPROD_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n             Map Operator Tree:\n                 TableScan\n                   alias: c\n-                  filterExpr: foo is not null (type: boolean)\n+                  filterExpr: (foo is not null or (foo = 1)) (type: boolean)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzNzc1Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503837757", "bodyText": "this difference is gone - no more ts merging for now; will get back to it later\nyes; we might want to run simplification on these - I've opened HIVE-24269 to add that", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NjM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NzI4Nw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502947287", "bodyText": "Missed opportunity (maybe not very relevant but wanted to point out in case there is some oversight).", "author": "jcamachor", "createdAt": "2020-10-11T18:07:06Z", "path": "ql/src/test/results/clientpositive/llap/sharedworkresidual.q.out", "diffHunk": "@@ -143,6 +143,10 @@ STAGE PLANS:\n                         sort order: \n                         Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE\n                         value expressions: _col0 (type: string)\n+                    Select Operator", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzODE2OQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503838169", "bodyText": "this difference is gone - no more ts merging for now; will get back to it later", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:22:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NzI4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NzMzMA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502947330", "bodyText": "ConstantPropagateProcFactory.foldExpr", "author": "jcamachor", "createdAt": "2020-10-11T18:07:28Z", "path": "ql/src/test/results/clientpositive/llap/special_character_in_tabnames_1.q.out", "diffHunk": "@@ -1986,18 +1986,18 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:\n-        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 6 (SIMPLE_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)\n         Reducer 3 <- Reducer 2 (SIMPLE_EDGE)\n-        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)\n         Reducer 5 <- Reducer 4 (SIMPLE_EDGE)\n-        Reducer 7 <- Map 6 (SIMPLE_EDGE)\n+        Reducer 6 <- Map 1 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n             Map Operator Tree:\n                 TableScan\n                   alias: b\n-                  filterExpr: key is not null (type: boolean)\n+                  filterExpr: (key is not null or (key > '9')) (type: boolean)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzODUyMg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503838522", "bodyText": "this difference is gone - no more ts merging for now; will get back to it later", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:23:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0NzMzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODAwOA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502948008", "bodyText": "It seems in this case optimization led to an additional stage.", "author": "jcamachor", "createdAt": "2020-10-11T18:13:43Z", "path": "ql/src/test/results/clientpositive/llap/special_character_in_tabnames_quotes_1.q.out", "diffHunk": "@@ -2896,10 +2896,11 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:\n-        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Reducer 5 (CUSTOM_SIMPLE_EDGE)\n+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Reducer 6 (CUSTOM_SIMPLE_EDGE)\n         Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n         Reducer 4 <- Reducer 3 (SIMPLE_EDGE)\n         Reducer 5 <- Map 1 (CUSTOM_SIMPLE_EDGE)\n+        Reducer 6 <- Map 1 (CUSTOM_SIMPLE_EDGE)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODU0Mg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502948542", "bodyText": "Additional filter (though in this case new mode is disabled?)", "author": "jcamachor", "createdAt": "2020-10-11T18:19:07Z", "path": "ql/src/test/results/clientpositive/llap/subquery_in.q.out", "diffHunk": "@@ -4355,6 +4355,9 @@ STAGE PLANS:\n                         sort order: +\n                         Map-reduce partition columns: _col0 (type: string)\n                         Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgzODY5NQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503838695", "bodyText": "this difference is gone - no more ts merging for now; will get back to it later", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODYwNg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502948606", "bodyText": "It seems we have an additional stage here (though in this case, new mode is disabled).", "author": "jcamachor", "createdAt": "2020-10-11T18:19:42Z", "path": "ql/src/test/results/clientpositive/llap/subquery_in.q.out", "diffHunk": "@@ -5078,9 +5087,10 @@ STAGE PLANS:\n       Edges:\n         Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n         Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)\n-        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 7 (SIMPLE_EDGE)\n         Reducer 5 <- Map 1 (SIMPLE_EDGE)\n         Reducer 6 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 7 <- Map 1 (SIMPLE_EDGE)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzg0MDM2Mg==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503840362", "bodyText": "yes; I was too brave to enable ts merging for every existing op... :)", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:26:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0ODY4OA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502948688", "bodyText": "Same as with other subquery tests, it seems in these cases reutilization is less.", "author": "jcamachor", "createdAt": "2020-10-11T18:20:22Z", "path": "ql/src/test/results/clientpositive/llap/subquery_multi.q.out", "diffHunk": "@@ -3040,11 +3037,12 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:\n-        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 8 (SIMPLE_EDGE)\n         Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n         Reducer 4 <- Map 1 (SIMPLE_EDGE), Reducer 6 (SIMPLE_EDGE)\n-        Reducer 5 <- Reducer 4 (XPROD_EDGE), Reducer 6 (XPROD_EDGE)\n+        Reducer 5 <- Reducer 4 (XPROD_EDGE), Reducer 7 (XPROD_EDGE)\n         Reducer 6 <- Map 1 (CUSTOM_SIMPLE_EDGE)\n+        Reducer 7 <- Map 1 (CUSTOM_SIMPLE_EDGE)", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk0OTg0NA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502949844", "bodyText": "It seems in this case, we are missing a reutilization opportunity.", "author": "jcamachor", "createdAt": "2020-10-11T18:31:09Z", "path": "ql/src/test/results/clientpositive/perf/tez/constraints/mv_query44.q.out", "diffHunk": "@@ -125,102 +129,143 @@ Stage-0\n                 Top N Key Operator [TNK_99] (rows=6951 width=218)\n                   keys:_col1,top n:100\n                   Merge Join Operator [MERGEJOIN_116] (rows=6951 width=218)\n-                    Conds:RS_66._col2=RS_146._col0(Inner),Output:[\"_col1\",\"_col5\",\"_col7\"]\n-                  <-Map 11 [SIMPLE_EDGE] vectorized\n-                    SHUFFLE [RS_146]\n+                    Conds:RS_66._col2=RS_163._col0(Inner),Output:[\"_col1\",\"_col5\",\"_col7\"]\n+                  <-Map 14 [SIMPLE_EDGE] vectorized\n+                    SHUFFLE [RS_163]\n                       PartitionCols:_col0\n-                      Select Operator [SEL_144] (rows=462000 width=111)\n+                      Select Operator [SEL_161] (rows=462000 width=111)\n                         Output:[\"_col0\",\"_col1\"]\n                         TableScan [TS_56] (rows=462000 width=111)\n                           default@item,i1,Tbl:COMPLETE,Col:COMPLETE,Output:[\"i_item_sk\",\"i_product_name\"]\n                   <-Reducer 6 [SIMPLE_EDGE]\n                     SHUFFLE [RS_66]\n                       PartitionCols:_col2\n                       Merge Join Operator [MERGEJOIN_115] (rows=6951 width=115)\n-                        Conds:RS_63._col0=RS_145._col0(Inner),Output:[\"_col1\",\"_col2\",\"_col5\"]\n-                      <-Map 11 [SIMPLE_EDGE] vectorized\n-                        SHUFFLE [RS_145]\n+                        Conds:RS_63._col0=RS_162._col0(Inner),Output:[\"_col1\",\"_col2\",\"_col5\"]\n+                      <-Map 14 [SIMPLE_EDGE] vectorized\n+                        SHUFFLE [RS_162]\n                           PartitionCols:_col0\n-                           Please refer to the previous Select Operator [SEL_144]\n+                           Please refer to the previous Select Operator [SEL_161]\n                       <-Reducer 5 [SIMPLE_EDGE]\n                         SHUFFLE [RS_63]\n                           PartitionCols:_col0\n                           Merge Join Operator [MERGEJOIN_114] (rows=6951 width=12)\n-                            Conds:RS_138._col1=RS_143._col1(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n-                          <-Reducer 4 [SIMPLE_EDGE] vectorized\n-                            SHUFFLE [RS_138]\n+                            Conds:RS_146._col1=RS_160._col1(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n+                          <-Reducer 12 [SIMPLE_EDGE] vectorized\n+                            SHUFFLE [RS_160]\n                               PartitionCols:_col1\n-                              Select Operator [SEL_137] (rows=6951 width=8)\n+                              Select Operator [SEL_159] (rows=6951 width=8)\n                                 Output:[\"_col0\",\"_col1\"]\n-                                Filter Operator [FIL_136] (rows=6951 width=116)\n+                                Filter Operator [FIL_158] (rows=6951 width=116)\n                                   predicate:(rank_window_0 < 11)\n-                                  PTF Operator [PTF_135] (rows=20854 width=116)\n-                                    Function definitions:[{},{\"name:\":\"windowingtablefunction\",\"order by:\":\"_col1 ASC NULLS LAST\",\"partition by:\":\"0\"}]\n-                                    Select Operator [SEL_134] (rows=20854 width=116)\n+                                  PTF Operator [PTF_157] (rows=20854 width=116)\n+                                    Function definitions:[{},{\"name:\":\"windowingtablefunction\",\"order by:\":\"_col1 DESC NULLS FIRST\",\"partition by:\":\"0\"}]\n+                                    Select Operator [SEL_156] (rows=20854 width=116)\n                                       Output:[\"_col0\",\"_col1\"]\n-                                    <-Reducer 3 [SIMPLE_EDGE]\n-                                      SHUFFLE [RS_21]\n+                                    <-Reducer 11 [SIMPLE_EDGE]\n+                                      SHUFFLE [RS_49]\n                                         PartitionCols:0\n-                                        Top N Key Operator [TNK_100] (rows=20854 width=228)\n+                                        Top N Key Operator [TNK_101] (rows=20854 width=228)\n                                           keys:_col1,top n:11\n-                                          Filter Operator [FIL_20] (rows=20854 width=228)\n+                                          Filter Operator [FIL_48] (rows=20854 width=228)\n                                             predicate:(_col1 > (0.9 * _col2))\n-                                            Merge Join Operator [MERGEJOIN_112] (rows=62562 width=228)\n+                                            Merge Join Operator [MERGEJOIN_113] (rows=62562 width=228)\n                                               Conds:(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n                                             <-Reducer 10 [CUSTOM_SIMPLE_EDGE] vectorized\n-                                              PARTITION_ONLY_SHUFFLE [RS_133]\n-                                                Select Operator [SEL_132] (rows=1 width=112)\n+                                              PARTITION_ONLY_SHUFFLE [RS_150]\n+                                                Select Operator [SEL_149] (rows=62562 width=116)\n+                                                  Output:[\"_col0\",\"_col1\"]\n+                                                  Filter Operator [FIL_148] (rows=62562 width=124)\n+                                                    predicate:CAST( (_col1 / _col2) AS decimal(11,6)) is not null\n+                                                    Group By Operator [GBY_147] (rows=62562 width=124)\n+                                                      Output:[\"_col0\",\"_col1\",\"_col2\"],aggregations:[\"sum(VALUE._col0)\",\"count(VALUE._col1)\"],keys:KEY._col0\n+                                                    <-Map 1 [SIMPLE_EDGE] vectorized\n+                                                      SHUFFLE [RS_131]\n+                                                        PartitionCols:_col0\n+                                                        Group By Operator [GBY_127] (rows=3199976 width=124)\n+                                                          Output:[\"_col0\",\"_col1\",\"_col2\"],aggregations:[\"sum(ss_net_profit)\",\"count(ss_net_profit)\"],keys:ss_item_sk\n+                                                          Select Operator [SEL_123] (rows=6399952 width=114)\n+                                                            Output:[\"ss_item_sk\",\"ss_net_profit\"]\n+                                                            Filter Operator [FIL_119] (rows=6399952 width=114)\n+                                                              predicate:(ss_store_sk = 410)\n+                                                              TableScan [TS_0] (rows=575995635 width=114)\n+                                                                default@store_sales,ss1,Tbl:COMPLETE,Col:COMPLETE,Output:[\"ss_item_sk\",\"ss_store_sk\",\"ss_net_profit\",\"ss_hdemo_sk\"]", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MDcyOA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502950728", "bodyText": "This seems to be another missing opportunity.\n(Btw, I am pointing out the missing optimizations... But there are certainly some improvements too! It is just that we should understand why we are regressing in these cases, if the missed opportunity can lead to a significant performance hit, and in that case, if there is anything that can be done within the scope of this patch, or otherwise what we should be doing in a follow-up to fix it)", "author": "jcamachor", "createdAt": "2020-10-11T18:38:55Z", "path": "ql/src/test/results/clientpositive/perf/tez/constraints/query2.q.out", "diffHunk": "@@ -128,46 +128,104 @@ Plan optimized by CBO.\n \n Vertex dependency in root stage\n Map 1 <- Union 2 (CONTAINS)\n-Map 9 <- Union 2 (CONTAINS)\n-Reducer 3 <- Map 10 (SIMPLE_EDGE), Union 2 (SIMPLE_EDGE)\n+Map 13 <- Union 14 (CONTAINS)\n+Map 15 <- Union 14 (CONTAINS)\n+Map 8 <- Union 2 (CONTAINS)\n+Reducer 10 <- Map 9 (SIMPLE_EDGE), Union 14 (SIMPLE_EDGE)\n+Reducer 11 <- Reducer 10 (SIMPLE_EDGE)\n+Reducer 12 <- Map 9 (SIMPLE_EDGE), Reducer 11 (SIMPLE_EDGE)\n+Reducer 3 <- Map 9 (SIMPLE_EDGE), Union 2 (SIMPLE_EDGE)\n Reducer 4 <- Reducer 3 (SIMPLE_EDGE)\n-Reducer 5 <- Map 10 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)\n-Reducer 6 <- Reducer 5 (SIMPLE_EDGE), Reducer 8 (SIMPLE_EDGE)\n+Reducer 5 <- Map 9 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)\n+Reducer 6 <- Reducer 12 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n Reducer 7 <- Reducer 6 (SIMPLE_EDGE)\n-Reducer 8 <- Map 10 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)\n \n Stage-0\n   Fetch Operator\n     limit:-1\n     Stage-1\n       Reducer 7 vectorized\n-      File Output Operator [FS_173]\n-        Select Operator [SEL_172] (rows=12881 width=788)\n+      File Output Operator [FS_187]\n+        Select Operator [SEL_186] (rows=12881 width=788)\n           Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"]\n         <-Reducer 6 [SIMPLE_EDGE]\n           SHUFFLE [RS_57]\n             Select Operator [SEL_56] (rows=12881 width=788)\n               Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"]\n               Merge Join Operator [MERGEJOIN_146] (rows=12881 width=1572)\n                 Conds:RS_53.(_col0 - 53)=RS_54._col0(Inner),Output:[\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\",\"_col9\",\"_col10\",\"_col11\",\"_col12\",\"_col13\",\"_col14\",\"_col15\",\"_col16\"]\n+              <-Reducer 12 [SIMPLE_EDGE]\n+                SHUFFLE [RS_54]\n+                  PartitionCols:_col0\n+                  Merge Join Operator [MERGEJOIN_145] (rows=652 width=788)\n+                    Conds:RS_185._col0=RS_181._col0(Inner),Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"]\n+                  <-Map 9 [SIMPLE_EDGE] vectorized\n+                    SHUFFLE [RS_181]\n+                      PartitionCols:_col0\n+                      Select Operator [SEL_177] (rows=652 width=4)\n+                        Output:[\"_col0\"]\n+                        Filter Operator [FIL_173] (rows=652 width=8)\n+                          predicate:((d_year = 2001) and d_week_seq is not null)\n+                          TableScan [TS_8] (rows=73049 width=99)\n+                            default@date_dim,date_dim,Tbl:COMPLETE,Col:COMPLETE,Output:[\"d_date_sk\",\"d_week_seq\",\"d_day_name\",\"d_year\"]\n+                  <-Reducer 11 [SIMPLE_EDGE] vectorized\n+                    SHUFFLE [RS_185]\n+                      PartitionCols:_col0\n+                      Group By Operator [GBY_184] (rows=13152 width=788)\n+                        Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"],aggregations:[\"sum(VALUE._col0)\",\"sum(VALUE._col1)\",\"sum(VALUE._col2)\",\"sum(VALUE._col3)\",\"sum(VALUE._col4)\",\"sum(VALUE._col5)\",\"sum(VALUE._col6)\"],keys:KEY._col0\n+                      <-Reducer 10 [SIMPLE_EDGE]\n+                        SHUFFLE [RS_40]\n+                          PartitionCols:_col0\n+                          Group By Operator [GBY_39] (rows=3182784 width=788)\n+                            Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"],aggregations:[\"sum(_col1)\",\"sum(_col2)\",\"sum(_col3)\",\"sum(_col4)\",\"sum(_col5)\",\"sum(_col6)\",\"sum(_col7)\"],keys:_col0\n+                            Select Operator [SEL_37] (rows=430516591 width=143)\n+                              Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\"]\n+                              Merge Join Operator [MERGEJOIN_144] (rows=430516591 width=143)\n+                                Conds:Union 14._col0=RS_180._col0(Inner),Output:[\"_col1\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\",\"_col8\",\"_col9\",\"_col10\"]\n+                              <-Map 9 [SIMPLE_EDGE] vectorized\n+                                SHUFFLE [RS_180]\n+                                  PartitionCols:_col0\n+                                  Select Operator [SEL_176] (rows=73049 width=36)\n+                                    Output:[\"_col0\",\"_col1\",\"_col2\",\"_col3\",\"_col4\",\"_col5\",\"_col6\",\"_col7\",\"_col8\"]\n+                                    Filter Operator [FIL_172] (rows=73049 width=99)\n+                                      predicate:d_week_seq is not null\n+                                       Please refer to the previous TableScan [TS_8]\n+                              <-Union 14 [SIMPLE_EDGE]\n+                                <-Map 13 [CONTAINS] vectorized\n+                                  Reduce Output Operator [RS_193]\n+                                    PartitionCols:_col0\n+                                    Select Operator [SEL_192] (rows=143966864 width=115)\n+                                      Output:[\"_col0\",\"_col1\"]\n+                                      Filter Operator [FIL_191] (rows=143966864 width=115)\n+                                        predicate:ws_sold_date_sk is not null\n+                                        TableScan [TS_157] (rows=144002668 width=115)\n+                                          default@web_sales,web_sales,Tbl:COMPLETE,Col:COMPLETE,Output:[\"ws_sold_date_sk\",\"ws_ext_sales_price\"]\n+                                <-Map 15 [CONTAINS] vectorized\n+                                  Reduce Output Operator [RS_196]\n+                                    PartitionCols:_col0\n+                                    Select Operator [SEL_195] (rows=286549727 width=115)\n+                                      Output:[\"_col0\",\"_col1\"]\n+                                      Filter Operator [FIL_194] (rows=286549727 width=115)\n+                                        predicate:cs_sold_date_sk is not null\n+                                        TableScan [TS_162] (rows=287989836 width=115)\n+                                          default@catalog_sales,catalog_sales,Tbl:COMPLETE,Col:COMPLETE,Output:[\"cs_sold_date_sk\",\"cs_ext_sales_price\"]", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzg0MzIyNw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503843227", "bodyText": "we should understand why we are regressing in these cases\n\nabsolutely;  it's important to not loose existing stuff!...and in fact the downstream patch was not fixing all these regressions; so I took the approach of removing ts merge from this patch - then I'll add downstream merge/etc changes next - and after that we'll see what when ts-merge is enabled for all optimizations.", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MDcyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTk0MA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r502951940", "bodyText": "This is similar to mv_query44.", "author": "jcamachor", "createdAt": "2020-10-11T18:51:04Z", "path": "ql/src/test/results/clientpositive/perf/tez/constraints/query44.q.out", "diffHunk": "@@ -103,102 +107,143 @@ Stage-0\n                 Top N Key Operator [TNK_99] (rows=6951 width=218)\n                   keys:_col1,top n:100\n                   Merge Join Operator [MERGEJOIN_116] (rows=6951 width=218)\n-                    Conds:RS_66._col2=RS_146._col0(Inner),Output:[\"_col1\",\"_col5\",\"_col7\"]\n-                  <-Map 11 [SIMPLE_EDGE] vectorized\n-                    SHUFFLE [RS_146]\n+                    Conds:RS_66._col2=RS_163._col0(Inner),Output:[\"_col1\",\"_col5\",\"_col7\"]\n+                  <-Map 14 [SIMPLE_EDGE] vectorized\n+                    SHUFFLE [RS_163]\n                       PartitionCols:_col0\n-                      Select Operator [SEL_144] (rows=462000 width=111)\n+                      Select Operator [SEL_161] (rows=462000 width=111)\n                         Output:[\"_col0\",\"_col1\"]\n                         TableScan [TS_56] (rows=462000 width=111)\n                           default@item,i1,Tbl:COMPLETE,Col:COMPLETE,Output:[\"i_item_sk\",\"i_product_name\"]\n                   <-Reducer 6 [SIMPLE_EDGE]\n                     SHUFFLE [RS_66]\n                       PartitionCols:_col2\n                       Merge Join Operator [MERGEJOIN_115] (rows=6951 width=115)\n-                        Conds:RS_63._col0=RS_145._col0(Inner),Output:[\"_col1\",\"_col2\",\"_col5\"]\n-                      <-Map 11 [SIMPLE_EDGE] vectorized\n-                        SHUFFLE [RS_145]\n+                        Conds:RS_63._col0=RS_162._col0(Inner),Output:[\"_col1\",\"_col2\",\"_col5\"]\n+                      <-Map 14 [SIMPLE_EDGE] vectorized\n+                        SHUFFLE [RS_162]\n                           PartitionCols:_col0\n-                           Please refer to the previous Select Operator [SEL_144]\n+                           Please refer to the previous Select Operator [SEL_161]\n                       <-Reducer 5 [SIMPLE_EDGE]\n                         SHUFFLE [RS_63]\n                           PartitionCols:_col0\n                           Merge Join Operator [MERGEJOIN_114] (rows=6951 width=12)\n-                            Conds:RS_138._col1=RS_143._col1(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n-                          <-Reducer 4 [SIMPLE_EDGE] vectorized\n-                            SHUFFLE [RS_138]\n+                            Conds:RS_146._col1=RS_160._col1(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n+                          <-Reducer 12 [SIMPLE_EDGE] vectorized\n+                            SHUFFLE [RS_160]\n                               PartitionCols:_col1\n-                              Select Operator [SEL_137] (rows=6951 width=8)\n+                              Select Operator [SEL_159] (rows=6951 width=8)\n                                 Output:[\"_col0\",\"_col1\"]\n-                                Filter Operator [FIL_136] (rows=6951 width=116)\n+                                Filter Operator [FIL_158] (rows=6951 width=116)\n                                   predicate:(rank_window_0 < 11)\n-                                  PTF Operator [PTF_135] (rows=20854 width=116)\n-                                    Function definitions:[{},{\"name:\":\"windowingtablefunction\",\"order by:\":\"_col1 ASC NULLS LAST\",\"partition by:\":\"0\"}]\n-                                    Select Operator [SEL_134] (rows=20854 width=116)\n+                                  PTF Operator [PTF_157] (rows=20854 width=116)\n+                                    Function definitions:[{},{\"name:\":\"windowingtablefunction\",\"order by:\":\"_col1 DESC NULLS FIRST\",\"partition by:\":\"0\"}]\n+                                    Select Operator [SEL_156] (rows=20854 width=116)\n                                       Output:[\"_col0\",\"_col1\"]\n-                                    <-Reducer 3 [SIMPLE_EDGE]\n-                                      SHUFFLE [RS_21]\n+                                    <-Reducer 11 [SIMPLE_EDGE]\n+                                      SHUFFLE [RS_49]\n                                         PartitionCols:0\n-                                        Top N Key Operator [TNK_100] (rows=20854 width=228)\n+                                        Top N Key Operator [TNK_101] (rows=20854 width=228)\n                                           keys:_col1,top n:11\n-                                          Filter Operator [FIL_20] (rows=20854 width=228)\n+                                          Filter Operator [FIL_48] (rows=20854 width=228)\n                                             predicate:(_col1 > (0.9 * _col2))\n-                                            Merge Join Operator [MERGEJOIN_112] (rows=62562 width=228)\n+                                            Merge Join Operator [MERGEJOIN_113] (rows=62562 width=228)\n                                               Conds:(Inner),Output:[\"_col0\",\"_col1\",\"_col2\"]\n                                             <-Reducer 10 [CUSTOM_SIMPLE_EDGE] vectorized\n-                                              PARTITION_ONLY_SHUFFLE [RS_133]\n-                                                Select Operator [SEL_132] (rows=1 width=112)\n+                                              PARTITION_ONLY_SHUFFLE [RS_150]\n+                                                Select Operator [SEL_149] (rows=62562 width=116)\n+                                                  Output:[\"_col0\",\"_col1\"]\n+                                                  Filter Operator [FIL_148] (rows=62562 width=124)\n+                                                    predicate:CAST( (_col1 / _col2) AS decimal(11,6)) is not null\n+                                                    Group By Operator [GBY_147] (rows=62562 width=124)\n+                                                      Output:[\"_col0\",\"_col1\",\"_col2\"],aggregations:[\"sum(VALUE._col0)\",\"count(VALUE._col1)\"],keys:KEY._col0\n+                                                    <-Map 1 [SIMPLE_EDGE] vectorized\n+                                                      SHUFFLE [RS_131]\n+                                                        PartitionCols:_col0\n+                                                        Group By Operator [GBY_127] (rows=3199976 width=124)\n+                                                          Output:[\"_col0\",\"_col1\",\"_col2\"],aggregations:[\"sum(ss_net_profit)\",\"count(ss_net_profit)\"],keys:ss_item_sk\n+                                                          Select Operator [SEL_123] (rows=6399952 width=114)\n+                                                            Output:[\"ss_item_sk\",\"ss_net_profit\"]\n+                                                            Filter Operator [FIL_119] (rows=6399952 width=114)\n+                                                              predicate:(ss_store_sk = 410)\n+                                                              TableScan [TS_0] (rows=575995635 width=114)\n+                                                                default@store_sales,ss1,Tbl:COMPLETE,Col:COMPLETE,Output:[\"ss_item_sk\",\"ss_store_sk\",\"ss_net_profit\",\"ss_hdemo_sk\"]", "originalCommit": "31ddb97bf412a2679489a5a0d45d335d2708005c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzg0MDYxMA==", "url": "https://github.com/apache/hive/pull/1553#discussion_r503840610", "bodyText": "gone back to master state in latest update", "author": "kgyrtkirk", "createdAt": "2020-10-13T10:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk1MTk0MA=="}], "type": "inlineReview"}, {"oid": "053281c5e09a49caed5f00f494a15a77e4934fa5", "url": "https://github.com/apache/hive/commit/053281c5e09a49caed5f00f494a15a77e4934fa5", "message": "Merge remote-tracking branch 'apache/master' into HIVE-swo-dppunion", "committedDate": "2020-10-12T09:48:52Z", "type": "commit"}, {"oid": "520598f64691f0193be270b81d74a90eb9b8bcd7", "url": "https://github.com/apache/hive/commit/520598f64691f0193be270b81d74a90eb9b8bcd7", "message": "upgrade to M5 in other parts", "committedDate": "2020-10-12T10:31:34Z", "type": "commit"}, {"oid": "35fccf29e537fbb2c31d493ec9d9cacb3927897e", "url": "https://github.com/apache/hive/commit/35fccf29e537fbb2c31d493ec9d9cacb3927897e", "message": "remove HIVE-24624 related stuff", "committedDate": "2020-10-12T12:34:35Z", "type": "commit"}, {"oid": "84685724bc2c874e1d53640e8d4dccdd2d2605f8", "url": "https://github.com/apache/hive/commit/84685724bc2c874e1d53640e8d4dccdd2d2605f8", "message": "remove feature disable from subq tests", "committedDate": "2020-10-12T12:34:51Z", "type": "commit"}, {"oid": "b336b842e36a363d96ed8ec33d09d122fbe3ce6b", "url": "https://github.com/apache/hive/commit/b336b842e36a363d96ed8ec33d09d122fbe3ce6b", "message": "remove set from explain", "committedDate": "2020-10-12T12:54:20Z", "type": "commit"}, {"oid": "9e07afdd178f5abf75a985609f05ca3dab50c53b", "url": "https://github.com/apache/hive/commit/9e07afdd178f5abf75a985609f05ca3dab50c53b", "message": "update qfiles; go back to simpler changes", "committedDate": "2020-10-12T13:45:31Z", "type": "commit"}, {"oid": "e775b74ce740ab162a7164b7bad6652fd3c25d34", "url": "https://github.com/apache/hive/commit/e775b74ce740ab162a7164b7bad6652fd3c25d34", "message": "tests back to master state", "committedDate": "2020-10-13T08:24:02Z", "type": "commit"}, {"oid": "a54fc6a506dbb6ef603cac61fe0a04c3835fc990", "url": "https://github.com/apache/hive/commit/a54fc6a506dbb6ef603cac61fe0a04c3835fc990", "message": "cleanup", "committedDate": "2020-10-13T08:34:47Z", "type": "commit"}, {"oid": "c3087ae830ab55683b3ea06d6b664c6c8f15b1dd", "url": "https://github.com/apache/hive/commit/c3087ae830ab55683b3ea06d6b664c6c8f15b1dd", "message": "removal of ()", "committedDate": "2020-10-13T08:43:42Z", "type": "commit"}, {"oid": "70e4ac2e0f910ce1fd33984e183cd68e696abf1a", "url": "https://github.com/apache/hive/commit/70e4ac2e0f910ce1fd33984e183cd68e696abf1a", "message": "removal of ()", "committedDate": "2020-10-13T08:44:10Z", "type": "commit"}, {"oid": "acec785a26c159a5534afbd3875c99bb51b5232c", "url": "https://github.com/apache/hive/commit/acec785a26c159a5534afbd3875c99bb51b5232c", "message": "cleanup; move functions; address review comments", "committedDate": "2020-10-13T09:48:58Z", "type": "commit"}, {"oid": "7fa1eef1491570d93fb231561ab245f4504053ef", "url": "https://github.com/apache/hive/commit/7fa1eef1491570d93fb231561ab245f4504053ef", "message": "use adoptchildren", "committedDate": "2020-10-13T09:55:15Z", "type": "commit"}, {"oid": "c1e35d3e5c0bccd20ce51b223ec6aca189378c64", "url": "https://github.com/apache/hive/commit/c1e35d3e5c0bccd20ce51b223ec6aca189378c64", "message": "add doc to mode", "committedDate": "2020-10-13T10:36:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3NTM5Mw==", "url": "https://github.com/apache/hive/pull/1553#discussion_r504075393", "bodyText": "these 2 filter operators will be merged by the 'downstream merge'  patch", "author": "kgyrtkirk", "createdAt": "2020-10-13T16:05:15Z", "path": "ql/src/test/results/clientpositive/llap/vectorized_dynamic_partition_pruning.q.out", "diffHunk": "@@ -4816,18 +4816,34 @@ STAGE PLANS:\n                   alias: srcpart\n                   filterExpr: ds is not null (type: boolean)\n                   Statistics: Num rows: 2000 Data size: 389248 Basic stats: COMPLETE Column stats: COMPLETE\n-                  Group By Operator\n-                    keys: ds (type: string)\n-                    minReductionHashAggr: 0.99\n-                    mode: hash\n-                    outputColumnNames: _col0\n-                    Statistics: Num rows: 2 Data size: 368 Basic stats: COMPLETE Column stats: COMPLETE\n-                    Reduce Output Operator\n-                      key expressions: _col0 (type: string)\n-                      null sort order: z\n-                      sort order: +\n-                      Map-reduce partition columns: _col0 (type: string)\n+                  Filter Operator", "originalCommit": "c1e35d3e5c0bccd20ce51b223ec6aca189378c64", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIyMDM0NQ==", "url": "https://github.com/apache/hive/pull/1553#discussion_r508220345", "bodyText": "typo. onlt", "author": "jcamachor", "createdAt": "2020-10-20T05:35:32Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java", "diffHunk": "@@ -284,6 +304,54 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {\n     return pctx;\n   }\n \n+  /** SharedWorkOptimization strategy modes */\n+  public enum Mode {\n+    /**\n+     * Merges two identical subtrees.\n+     */\n+    SubtreeMerge,\n+    /**\n+     * Merges a filtered scan into a non-filtered scan.\n+     *\n+     * In case we are already scanning the whole table - we should not scan it twice.\n+     */\n+    RemoveSemijoin,\n+    /**\n+     * Fuses two filtered table scans into a single one.\n+     *\n+     * Dynamic filter subtree is kept on both sides - but the table is onlt scanned once.", "originalCommit": "c1e35d3e5c0bccd20ce51b223ec6aca189378c64", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQzODc3Ng==", "url": "https://github.com/apache/hive/pull/1553#discussion_r508438776", "bodyText": "added fix to HIVE-24241", "author": "kgyrtkirk", "createdAt": "2020-10-20T11:56:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIyMDM0NQ=="}], "type": "inlineReview"}]}