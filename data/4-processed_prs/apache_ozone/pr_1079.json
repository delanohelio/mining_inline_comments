{"pr_number": 1079, "pr_title": "HDDS-3802. Incorrect data returned by reading a FILE_PER_CHUNK block.", "pr_createdAt": "2020-06-16T03:00:53Z", "pr_url": "https://github.com/apache/ozone/pull/1079", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2NzUxNg==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440667516", "bodyText": "Nit: Can we please avoid this dependency?  I think it's better to use StringUtils from Apache Commons Lang, or even simply check explicitly both for null and empty string.", "author": "adoroszlai", "createdAt": "2020-06-16T08:13:27Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -34,11 +34,14 @@\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume;\n import org.apache.hadoop.ozone.container.common.volume.VolumeIOStats;\n import org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.interfaces.BlockManager;\n import org.apache.hadoop.ozone.container.keyvalue.interfaces.ChunkManager;\n import org.apache.hadoop.ozone.container.common.interfaces.Container;\n \n import static org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result.IO_EXCEPTION;\n import static org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result.UNABLE_TO_FIND_CHUNK;\n+\n+import org.eclipse.jetty.util.StringUtil;", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2ODQ2Mg==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440668462", "bodyText": "Please store file.length() in local variable to avoid duplicate call.\nAlso, should we consider moving these checks to ChunkUtils.readData?", "author": "adoroszlai", "createdAt": "2020-06-16T08:15:00Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);\n+      List<ContainerProtos.ChunkInfo> chunks = blockData.getChunks();\n+      Preconditions.checkState(index <= chunks.size());\n+      chunkFileOffset = chunks.get(index - 1).getOffset();\n+    } catch (IOException e) {\n+      throw new StorageContainerException(\n+          \"Cannot find block \" + blockID.toString() + \" for chunk \" +\n+              info.getChunkName(), UNABLE_TO_FIND_CHUNK);\n+    }\n+\n     for (File file : possibleFiles) {\n       try {\n-        // use offset only if file written by old datanode\n-        long offset;\n-        if (file.exists() && file.length() == info.getOffset() + len) {\n-          offset = info.getOffset();\n-        } else {\n-          offset = 0;\n+        if (file.exists()) {\n+          long offset = info.getOffset() - chunkFileOffset;\n+          Preconditions.checkState(offset < file.length());\n+          Preconditions.checkState((offset + len) <= file.length());", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY2OTk2NA==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440669964", "bodyText": "Nit: I think it can be a public static constant.  It's not instance-specific, and it also makes the accessor method unnecessary.", "author": "adoroszlai", "createdAt": "2020-06-16T08:17:26Z", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/impl/AbstractTestChunkManager.java", "diffHunk": "@@ -62,14 +63,17 @@\n   private ChunkInfo chunkInfo;\n   private ByteBuffer data;\n   private byte[] header;\n+  private BlockManager blockManager;\n+  private final String chunkFileNamePattern = \"%d_data_%d\";", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY5MDMzMQ==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440690331", "bodyText": "It seems the combination of increasing chunk size, creating new cluster for each test method, and running for both chunk layouts makes this integration test flaky: https://github.com/apache/hadoop-ozone/pull/1079/checks?check_run_id=775565603\nCan we use KB instead of MB for each size setting?\nadoroszlai@0cc9272", "author": "adoroszlai", "createdAt": "2020-06-16T08:49:46Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -64,25 +71,38 @@\n   private static String volumeName;\n   private static String bucketName;\n   private static String keyString;\n+  private static ChunkLayoutTestInfo chunkLayout;\n+\n+  @Parameterized.Parameters\n+  public static Collection<Object[]> layouts() {\n+    return Arrays.asList(new Object[][] {\n+        {ChunkLayoutTestInfo.FILE_PER_CHUNK},\n+        {ChunkLayoutTestInfo.FILE_PER_BLOCK}\n+    });\n+  }\n \n+  public TestKeyInputStream(ChunkLayoutTestInfo layout) {\n+    this.chunkLayout = layout;\n+  }\n   /**\n    * Create a MiniDFSCluster for testing.\n    * <p>\n    * Ozone is made active by setting OZONE_ENABLED = true\n    *\n    * @throws IOException\n    */\n-  @BeforeClass\n-  public static void init() throws Exception {\n-    chunkSize = 100;\n+  @Before\n+  public void init() throws Exception {\n+    chunkSize = 1024 * 1024 * 4;", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyNjEyNA==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441326124", "bodyText": "I will try to reduce the chunkSize. One limitation here is chunkSize cann't be less than the mininum bytes-per-checksume value which is 256KB, otherwise the TestKeyInputStream will always succeed even without this patch.", "author": "ChenSammi", "createdAt": "2020-06-17T07:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY5MDMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyNDkzOQ==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440724939", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Arrays.equals(tmp1, tmp2);\n          \n          \n            \n                    Assert.assertArrayEquals(tmp1, tmp2);\n          \n          \n            \n                    totalRead += numBytesRead;", "author": "adoroszlai", "createdAt": "2020-06-16T09:46:05Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -331,4 +351,45 @@ public void testCopyLarge() throws Exception {\n       }\n     }\n   }\n+\n+  @Test\n+  public void testReadChunk() throws Exception {\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, 0, objectStore, volumeName, bucketName);\n+\n+    // write data spanning multiple chunks\n+    int dataLength = (2 * chunkSize) + (chunkSize / 2);\n+    byte[] originData = new byte[dataLength];\n+    Random r = new Random();\n+    r.nextBytes(originData);\n+    key.write(originData);\n+    key.close();\n+\n+    // read chunk data\n+    KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName).readKey(keyName)\n+        .getInputStream();\n+\n+    int[] bufferSizeList = {chunkSize / 4, chunkSize / 2, chunkSize - 1,\n+        chunkSize, chunkSize + 1, blockSize - 1, blockSize, blockSize + 1,\n+        blockSize * 2};\n+    for (int bufferSize : bufferSizeList) {\n+      byte[] data = new byte[bufferSize];\n+      int totalRead = 0;\n+      while (totalRead < dataLength) {\n+        int numBytesRead = keyInputStream.read(data);\n+        if (numBytesRead == -1 || numBytesRead == 0) {\n+          break;\n+        }\n+        byte[] tmp1 =\n+            Arrays.copyOfRange(originData, totalRead, totalRead + numBytesRead);\n+        byte[] tmp2 =\n+            Arrays.copyOfRange(data, 0, numBytesRead);\n+        Arrays.equals(tmp1, tmp2);", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4NzYwNA==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441387604", "bodyText": "Thanks.", "author": "ChenSammi", "createdAt": "2020-06-17T08:51:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcyNDkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNTcwMg==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440735702", "bodyText": "Instead of parsing the chunk name for an index, we could find the chunk with the exact same name.  I think it's less fragile, although might be slower if the block has too many chunks.  What's your opinion?\nadoroszlai@398ea1d (note: CI is still in progress, may need tweak based on the result)", "author": "adoroszlai", "createdAt": "2020-06-16T10:04:28Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);\n+      List<ContainerProtos.ChunkInfo> chunks = blockData.getChunks();\n+      Preconditions.checkState(index <= chunks.size());\n+      chunkFileOffset = chunks.get(index - 1).getOffset();", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI2MjQ0MA==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r441262440", "bodyText": "A good point. I think the performance overhead can be ignored.", "author": "ChenSammi", "createdAt": "2020-06-17T03:40:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNTcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDczNjM2NQ==", "url": "https://github.com/apache/ozone/pull/1079#discussion_r440736365", "bodyText": "I think we can skip trying to determine chunk file offset if info.getOffset() == 0.", "author": "adoroszlai", "createdAt": "2020-06-16T10:05:35Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -218,17 +244,36 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     long len = info.getLen();\n     ByteBuffer data = ByteBuffer.allocate((int) len);\n \n+    int index = extractChunkIndex(info.getChunkName());\n+    if (index == -1) {\n+      throw new StorageContainerException(\n+          \"Chunk file name can't be parsed \" + possibleFiles.toString(),\n+          UNABLE_TO_FIND_CHUNK);\n+    }\n+    // Chunk index start from 1\n+    Preconditions.checkState(index > 0);\n+\n+    long chunkFileOffset;\n+    try {\n+      BlockData blockData = blockManager.getBlock(kvContainer, blockID);", "originalCommit": "3a929967b379971a06903d0e1a10dc04b3b347c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7306ec42fe6a0116aec652dd281a74e85e524cd8", "url": "https://github.com/apache/ozone/commit/7306ec42fe6a0116aec652dd281a74e85e524cd8", "message": "HDDS-3802. Incorrect data returned by reading a FILE_PER_CHUNK block.", "committedDate": "2020-06-17T07:17:56Z", "type": "commit"}, {"oid": "a66f56288d52716b664d977a8e0a6ee32e0327c0", "url": "https://github.com/apache/ozone/commit/a66f56288d52716b664d977a8e0a6ee32e0327c0", "message": "fix checkstyle", "committedDate": "2020-06-17T07:17:56Z", "type": "commit"}, {"oid": "1a96a5cb952a432f26b856f4d48100991955365b", "url": "https://github.com/apache/ozone/commit/1a96a5cb952a432f26b856f4d48100991955365b", "message": "address comments", "committedDate": "2020-06-17T07:17:56Z", "type": "commit"}, {"oid": "1a96a5cb952a432f26b856f4d48100991955365b", "url": "https://github.com/apache/ozone/commit/1a96a5cb952a432f26b856f4d48100991955365b", "message": "address comments", "committedDate": "2020-06-17T07:17:56Z", "type": "forcePushed"}]}