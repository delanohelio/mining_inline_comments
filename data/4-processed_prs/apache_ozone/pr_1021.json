{"pr_number": 1021, "pr_title": "HDDS-2665. Implement new Ozone Filesystem scheme ofs://", "pr_createdAt": "2020-06-04T16:07:29Z", "pr_url": "https://github.com/apache/ozone/pull/1021", "timeline": [{"oid": "20bb5b56d6b15ecd47126559374c3391b3661d99", "url": "https://github.com/apache/ozone/commit/20bb5b56d6b15ecd47126559374c3391b3661d99", "message": "HDDS-2840. Implement ofs://: mkdir (#415)", "committedDate": "2020-01-30T20:45:06Z", "type": "commit"}, {"oid": "79c0c1c8c2279b3b83c4b977959f09154de9a20d", "url": "https://github.com/apache/ozone/commit/79c0c1c8c2279b3b83c4b977959f09154de9a20d", "message": "HDDS-2665. Merge master to HDDS-2665-ofs branch (#511)", "committedDate": "2020-01-31T22:38:55Z", "type": "commit"}, {"oid": "3d397239b07c61f98319902366db800f05769949", "url": "https://github.com/apache/ozone/commit/3d397239b07c61f98319902366db800f05769949", "message": "Revert \"HDDS-2665. Merge master to HDDS-2665-ofs branch (#511)\"\n\nThis reverts commit 79c0c1c8c2279b3b83c4b977959f09154de9a20d.", "committedDate": "2020-02-06T22:59:14Z", "type": "commit"}, {"oid": "ec9986b5b769234deb913cea023aae112684c0c2", "url": "https://github.com/apache/ozone/commit/ec9986b5b769234deb913cea023aae112684c0c2", "message": "Merge branch 'master' into HDDS-2665", "committedDate": "2020-02-06T22:59:48Z", "type": "commit"}, {"oid": "87288a2e9ce5c2ab8066052337cb95eb4498e5e2", "url": "https://github.com/apache/ozone/commit/87288a2e9ce5c2ab8066052337cb95eb4498e5e2", "message": "HDDS-2665. Addendum fix for merge master due to HDDS-2188 revert. Contributed by Siyao Meng.", "committedDate": "2020-02-06T23:13:54Z", "type": "commit"}, {"oid": "3e4782d3aaeb5b4d4b7429f39a8ce867d3481be4", "url": "https://github.com/apache/ozone/commit/3e4782d3aaeb5b4d4b7429f39a8ce867d3481be4", "message": "HDDS-2979. Implement ofs://: Fix getFileStatus for mkdir volume (#528)", "committedDate": "2020-02-07T00:51:17Z", "type": "commit"}, {"oid": "7c35d76b2ec22dbd869ec7332e1c333567821e11", "url": "https://github.com/apache/ozone/commit/7c35d76b2ec22dbd869ec7332e1c333567821e11", "message": "HDDS-2928. Implement ofs://: listStatus (#547)", "committedDate": "2020-02-25T05:59:44Z", "type": "commit"}, {"oid": "0610ce39c51a7eba728be418f3d6cd425cafe2ad", "url": "https://github.com/apache/ozone/commit/0610ce39c51a7eba728be418f3d6cd425cafe2ad", "message": "HDDS-3073. Implement ofs://: Fix listStatus continuation (#606)", "committedDate": "2020-02-28T19:03:37Z", "type": "commit"}, {"oid": "c410c4d329a839547bbf9491b5bf4562efcda59e", "url": "https://github.com/apache/ozone/commit/c410c4d329a839547bbf9491b5bf4562efcda59e", "message": "HDDS-2929. Implement ofs://: temp directory mount (#610)", "committedDate": "2020-03-07T01:33:01Z", "type": "commit"}, {"oid": "5616dc38b9e84efe6b0ee198d344c41a7adb0812", "url": "https://github.com/apache/ozone/commit/5616dc38b9e84efe6b0ee198d344c41a7adb0812", "message": "HDDS-2945. Implement ofs://: Add robot tests for mkdir. (#703)", "committedDate": "2020-03-25T19:05:30Z", "type": "commit"}, {"oid": "3c5ec6a0346259fc82130729aabc726a5a3d17be", "url": "https://github.com/apache/ozone/commit/3c5ec6a0346259fc82130729aabc726a5a3d17be", "message": "Merge branch 'master' into HDDS-2665-ofs", "committedDate": "2020-03-27T17:21:56Z", "type": "commit"}, {"oid": "3eaec50a7f1839bf6b3ef88f6904ffb272d60868", "url": "https://github.com/apache/ozone/commit/3eaec50a7f1839bf6b3ef88f6904ffb272d60868", "message": "HDDS-3279. Rebase OFS branch (#731)", "committedDate": "2020-03-27T18:32:54Z", "type": "commit"}, {"oid": "8356a4a382e0584ba39fc89ae082b124c6797789", "url": "https://github.com/apache/ozone/commit/8356a4a382e0584ba39fc89ae082b124c6797789", "message": "HDDS-3390. Adapt OFSPath to master branch changes (#847)", "committedDate": "2020-04-22T20:29:10Z", "type": "commit"}, {"oid": "dd992234b9d3a585dd6bc6119436075a80b2ca8f", "url": "https://github.com/apache/ozone/commit/dd992234b9d3a585dd6bc6119436075a80b2ca8f", "message": "Merge remote-tracking branch 'asf/master' into HDDS-2665-ofs\n\nConflicts:\nhadoop-ozone/dist/src/main/smoketest/ozonefs/ozonefs.robot", "committedDate": "2020-04-23T17:57:47Z", "type": "commit"}, {"oid": "9ed2e15834c8d19c2dac30c2d06788006e31ec82", "url": "https://github.com/apache/ozone/commit/9ed2e15834c8d19c2dac30c2d06788006e31ec82", "message": "HDDS-3390. Rebase OFS branch - 2. Adapt OFS classes to HDDS-3101 (#822)", "committedDate": "2020-04-23T17:59:33Z", "type": "commit"}, {"oid": "0773a4aef90d27720c89cdc34dfcba5b0f7de5d5", "url": "https://github.com/apache/ozone/commit/0773a4aef90d27720c89cdc34dfcba5b0f7de5d5", "message": "HDDS-3494. Implement ofs://: Support volume and bucket deletion (#906)", "committedDate": "2020-05-18T16:55:45Z", "type": "commit"}, {"oid": "28540cb1cc1f27c43f727b797b408125e21a9abb", "url": "https://github.com/apache/ozone/commit/28540cb1cc1f27c43f727b797b408125e21a9abb", "message": "HDDS-3574. Implement ofs://: Override getTrashRoot (#941)", "committedDate": "2020-06-02T14:53:21Z", "type": "commit"}, {"oid": "d6d31c688c53c5886f3b54cc4bc418ef223d1ed5", "url": "https://github.com/apache/ozone/commit/d6d31c688c53c5886f3b54cc4bc418ef223d1ed5", "message": "HDDS-2969. Implement ofs://: Add contract test (#865)", "committedDate": "2020-06-02T14:56:17Z", "type": "commit"}, {"oid": "a00dc185d49192a6627e71d71429935d77d3e814", "url": "https://github.com/apache/ozone/commit/a00dc185d49192a6627e71d71429935d77d3e814", "message": "Merge remote-tracking branch 'asf/master' into HDDS-2665-ofs\n\nConflicts: (HDDS-3385)\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/shell/TestOzoneShellHA.java", "committedDate": "2020-06-03T15:07:58Z", "type": "commit"}, {"oid": "b9dac41380617e1c680da2eb789857a64274ad6b", "url": "https://github.com/apache/ozone/commit/b9dac41380617e1c680da2eb789857a64274ad6b", "message": "HDDS-3709. Rebase OFS branch - 3. Adapt to HDDS-3501 (#1015)", "committedDate": "2020-06-03T23:13:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjczNjM5NA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r436736394", "bodyText": "HDDS-3054 seems to be resolved as far as I see.", "author": "elek", "createdAt": "2020-06-08T14:08:04Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,876 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneAcl;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.VolumeArgs;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.security.acl.IAccessAuthorizer.ACLIdentityType;\n+import org.apache.hadoop.ozone.security.acl.IAccessAuthorizer.ACLType;\n+import org.apache.hadoop.ozone.security.acl.OzoneAclConfig;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.ozone.OzoneAcl.AclScope.ACCESS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.BUCKET_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.VOLUME_NOT_FOUND;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ * TODO: Refactor this and TestOzoneFileSystem later to reduce code duplication.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private OzoneConfiguration conf;\n+  private MiniOzoneCluster cluster = null;\n+  private FileSystem fs;\n+  private RootedOzoneFileSystem ofs;\n+  private ObjectStore objectStore;\n+  private static BasicRootedOzoneClientAdapterImpl adapter;\n+\n+  private String volumeName;\n+  private String bucketName;\n+  // Store path commonly used by tests that test functionality within a bucket\n+  private Path testBucketPath;\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by RootedOzoneFileSystem (OFS)\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+    String testBucketStr =\n+        OZONE_URI_DELIMITER + volumeName + OZONE_URI_DELIMITER + bucketName;\n+    testBucketPath = new Path(testBucketStr);\n+\n+    rootPath = String.format(\"%s://%s/\",\n+        OzoneConsts.OZONE_OFS_URI_SCHEME, conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+    adapter = (BasicRootedOzoneClientAdapterImpl) ofs.getAdapter();\n+  }\n+\n+  @After\n+  public void teardown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration confTestLoader = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    confTestLoader.set(\"fs.ofs.impl\",\n+        \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    Assert.assertEquals(FileSystem.getFileSystemClass(\n+        OzoneConsts.OZONE_OFS_URI_SCHEME, confTestLoader),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKey(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    Assert.assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    Assert.assertEquals(\n+        \"List status of parent should include the 1 child file\",\n+        1L, fs.listStatus(parent).length);\n+    Assert.assertTrue(\n+        \"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    Assert.assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKey(parent, true);\n+    Assert.assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    Assert.assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path parent = new Path(testBucketPath, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(testBucketPath);\n+    Assert.assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    fileStatuses = ofs.listStatus(testBucketPath);\n+    Assert.assertEquals(\"Should have created parent\",\n+        1, fileStatuses.length);\n+    Assert.assertEquals(\"Parent path doesn't match\",\n+        fileStatuses[0].getPath().toUri().getPath(), parent.toString());\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    Assert.assertEquals(\n+        \"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    Assert.assertEquals(\n+        \"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  /**\n+   * OFS: Helper function for tests. Return a volume name that doesn't exist.\n+   */\n+  private String getRandomNonExistVolumeName() throws IOException {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      Assert.fail(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * OFS: Test mkdir on volume, bucket and dir that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucketDir() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    Assert.assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    Assert.assertEquals(\n+        fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    Assert.assertEquals(\n+        fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    Assert.assertEquals(\n+        fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    Assert.assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    Assert.assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * OFS: Test mkdir on a volume and bucket that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path newVolBucket = new Path(\n+        \"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    fs.mkdirs(newVolBucket);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    Assert.assertNotNull(ozoneVolume);\n+    Assert.assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    Iterator<? extends OzoneBucket> iterBuc =\n+        ozoneVolume.listBuckets(\"bucket-\");\n+    OzoneBucket ozoneBucket = iterBuc.next();\n+    Assert.assertNotNull(ozoneBucket);\n+    Assert.assertEquals(bucketNameLocal, ozoneBucket.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * OFS: Test mkdir on a volume that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolume() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    Path newVolume = new Path(\"/\" + volumeNameLocal);\n+    fs.mkdirs(newVolume);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    Assert.assertNotNull(ozoneVolume);\n+    Assert.assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * OFS: Test getFileStatus on root.\n+   */\n+  @Test\n+  public void testGetFileStatusRoot() throws Exception {\n+    Path root = new Path(\"/\");\n+    FileStatus fileStatus = fs.getFileStatus(root);\n+    Assert.assertNotNull(fileStatus);\n+    Assert.assertEquals(new Path(rootPath), fileStatus.getPath());\n+    Assert.assertTrue(fileStatus.isDirectory());\n+    Assert.assertEquals(FsPermission.getDirDefault(),\n+        fileStatus.getPermission());\n+  }\n+\n+  /**\n+   * Test listStatus operation in a bucket.\n+   */\n+  @Test\n+  public void testListStatusInBucket() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    Assert.assertEquals(\n+        \"FileStatus should return only the immediate children\",\n+        2, fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    Assert.assertNotEquals(fileStatus1, dir12.toString());\n+    Assert.assertNotEquals(fileStatus2, dir12.toString());\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = LISTING_PAGE_SIZE + LISTING_PAGE_SIZE / 2;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    Assert.assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      Assert.assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path dir1 = new Path(testBucketPath, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(testBucketPath, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    Assert.assertEquals(\n+        \"FileStatus should return only the immediate children\",\n+        2, fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    Assert.assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    Assert.assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path source = new Path(testBucketPath, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(testBucketPath, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    Assert.assertTrue(fs.rename(leafInsideInterimPath, leafInTarget));\n+\n+    // after rename listStatus for interimPath should succeed and\n+    // interimPath should have no children\n+    FileStatus[] statuses = fs.listStatus(interimPath);\n+    Assert.assertNotNull(\"liststatus returns a null array\", statuses);\n+    Assert.assertEquals(\"Statuses array is not empty\", 0, statuses.length);\n+    FileStatus fileStatus = fs.getFileStatus(interimPath);\n+    Assert.assertEquals(\"FileStatus does not point to interimPath\",\n+        interimPath.getName(), fileStatus.getPath().getName());\n+  }\n+\n+  /**\n+   * OFS: Try to rename a key to a different bucket. The attempt should fail.\n+   */\n+  @Test\n+  public void testRenameToDifferentBucket() throws IOException {\n+    Path source = new Path(testBucketPath, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(testBucketPath, \"target\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(OZONE_URI_DELIMITER + volumeName +\n+        OZONE_URI_DELIMITER + bucketName + \"test\");\n+    Path leafInTargetInAnotherBucket = new Path(bucket2, \"leaf\");\n+    try {\n+      fs.rename(leafInsideInterimPath, leafInTargetInAnotherBucket);\n+      Assert.fail(\n+          \"Should have thrown exception when renaming to a different bucket\");\n+    } catch (IOException ignored) {\n+      // Test passed. Exception thrown as expected.\n+    }\n+  }\n+\n+  private OzoneKeyDetails getKey(Path keyPath, boolean isDirectory)\n+      throws IOException {\n+    String key = ofs.pathToKey(keyPath);\n+    if (isDirectory) {\n+      key = key + OZONE_URI_DELIMITER;\n+    }\n+    OFSPath ofsPath = new OFSPath(key);\n+    String keyInBucket = ofsPath.getKeyName();\n+    return cluster.getClient().getObjectStore().getVolume(volumeName)\n+        .getBucket(bucketName).getKey(keyInBucket);\n+  }\n+\n+  private void assertKeyNotFoundException(IOException ex) {\n+    GenericTestUtils.assertExceptionContains(\"KEY_NOT_FOUND\", ex);\n+  }\n+\n+  /**\n+   * Helper function for testListStatusRootAndVolume*.\n+   * Each call creates one volume, one bucket under that volume,\n+   * two dir under that bucket, one subdir under one of the dirs,\n+   * and one file under the subdir.\n+   */\n+  private Path createRandomVolumeBucketWithDirs() throws IOException {\n+    String volume1 = getRandomNonExistVolumeName();\n+    String bucket1 = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path bucketPath1 = new Path(\n+        OZONE_URI_DELIMITER + volume1 + OZONE_URI_DELIMITER + bucket1);\n+\n+    Path dir1 = new Path(bucketPath1, \"dir1\");\n+    fs.mkdirs(dir1);  // Intentionally creating this \"in-the-middle\" dir key\n+    Path subdir1 = new Path(dir1, \"subdir1\");\n+    fs.mkdirs(subdir1);\n+    Path dir2 = new Path(bucketPath1, \"dir2\");\n+    fs.mkdirs(dir2);\n+\n+    try (FSDataOutputStream stream =\n+        ofs.create(new Path(dir2, \"file1\"))) {\n+      stream.write(1);\n+    }\n+\n+    return bucketPath1;\n+  }\n+\n+  /**\n+   * OFS: Test non-recursive listStatus on root and volume.\n+   */\n+  @Test\n+  public void testListStatusRootAndVolumeNonRecursive() throws Exception {\n+    Path bucketPath1 = createRandomVolumeBucketWithDirs();\n+    createRandomVolumeBucketWithDirs();\n+    // listStatus(\"/volume/bucket\")\n+    FileStatus[] fileStatusBucket = ofs.listStatus(bucketPath1);\n+    Assert.assertEquals(2, fileStatusBucket.length);\n+    // listStatus(\"/volume\")\n+    Path volume = new Path(\n+        OZONE_URI_DELIMITER + new OFSPath(bucketPath1).getVolumeName());\n+    FileStatus[] fileStatusVolume = ofs.listStatus(volume);\n+    Assert.assertEquals(1, fileStatusVolume.length);\n+    // listStatus(\"/\")\n+    Path root = new Path(OZONE_URI_DELIMITER);\n+    FileStatus[] fileStatusRoot = ofs.listStatus(root);\n+    Assert.assertEquals(2, fileStatusRoot.length);\n+  }\n+\n+  /**\n+   * Helper function to do FileSystem#listStatus recursively.\n+   * Simulate what FsShell does, using DFS.\n+   */\n+  private void listStatusRecursiveHelper(Path curPath, List<FileStatus> result)\n+      throws IOException {\n+    FileStatus[] startList = ofs.listStatus(curPath);\n+    for (FileStatus fileStatus : startList) {\n+      result.add(fileStatus);\n+      if (fileStatus.isDirectory()) {\n+        Path nextPath = fileStatus.getPath();\n+        listStatusRecursiveHelper(nextPath, result);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helper function to call listStatus in adapter implementation.\n+   */\n+  private List<FileStatus> callAdapterListStatus(String pathStr,\n+      boolean recursive, String startPath, long numEntries) throws IOException {\n+    return adapter.listStatus(pathStr, recursive, startPath, numEntries,\n+        ofs.getUri(), ofs.getWorkingDirectory(), ofs.getUsername())\n+        .stream().map(ofs::convertFileStatus).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Helper function to compare recursive listStatus results from adapter\n+   * and (simulated) FileSystem.\n+   */\n+  private void listStatusCheckHelper(Path path) throws IOException {\n+    // Get recursive listStatus result directly from adapter impl\n+    List<FileStatus> statusesFromAdapter = callAdapterListStatus(\n+        path.toString(), true, \"\", 1000);\n+    // Get recursive listStatus result with FileSystem API by simulating FsShell\n+    List<FileStatus> statusesFromFS = new ArrayList<>();\n+    listStatusRecursiveHelper(path, statusesFromFS);\n+    // Compare. The results would be in the same order due to assumptions:\n+    // 1. They are both using DFS internally;\n+    // 2. They both return ordered results.\n+    Assert.assertEquals(statusesFromAdapter.size(), statusesFromFS.size());\n+    final int n = statusesFromFS.size();\n+    for (int i = 0; i < n; i++) {\n+      FileStatus statusFromAdapter = statusesFromAdapter.get(i);\n+      FileStatus statusFromFS = statusesFromFS.get(i);\n+      Assert.assertEquals(statusFromAdapter.getPath(), statusFromFS.getPath());\n+      Assert.assertEquals(statusFromAdapter.getLen(), statusFromFS.getLen());\n+      Assert.assertEquals(statusFromAdapter.isDirectory(),\n+          statusFromFS.isDirectory());\n+      // TODO: When HDDS-3054 is in, uncomment the lines below.", "originalCommit": "b9dac41380617e1c680da2eb789857a64274ad6b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1ODI2NA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r437758264", "bodyText": "Addressed in HDDS-3767.", "author": "smengcl", "createdAt": "2020-06-09T22:31:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjczNjM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0MTM3OA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r436741378", "bodyText": "FYI: there is a full, in memory implementation of ObjectStore in s3 project. Can be useful for similar tests if we move it to a common place. (BTW: I like this lightweight test).", "author": "elek", "createdAt": "2020-06-08T14:12:53Z", "path": "hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystemWithMocks.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+import java.net.URI;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Ozone File system tests that are light weight and use mocks.\n+ */\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({ OzoneClientFactory.class, UserGroupInformation.class })\n+@PowerMockIgnore(\"javax.management.*\")\n+public class TestRootedOzoneFileSystemWithMocks {", "originalCommit": "b9dac41380617e1c680da2eb789857a64274ad6b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1ODUyMA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r437758520", "bodyText": "I removed TestRootedOzoneFileSystemWithMocks in HDDS-3767 since TestOzoneFileSystemWithMocks is also removed. We can restore this later.", "author": "smengcl", "createdAt": "2020-06-09T22:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0MTM3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0NDYwMA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r436744600", "bodyText": "Why don't we use the interface instead of implementation?", "author": "elek", "createdAt": "2020-06-08T14:17:35Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,904 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.utils.LegacyHadoopConfigurationSource;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.BUCKET_NOT_EMPTY;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.VOLUME_NOT_EMPTY;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * BasicRootedOzoneFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+  private OzoneClientAdapter adapter;\n+  private BasicRootedOzoneClientAdapterImpl adapterImpl;", "originalCommit": "b9dac41380617e1c680da2eb789857a64274ad6b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1OTYzOQ==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r437759639", "bodyText": "this.adapterImpl = (BasicRootedOzoneClientAdapterImpl) this.adapter;\nthis was intended to make the usage of it cleaner", "author": "smengcl", "createdAt": "2020-06-09T22:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0NDYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0OTE5OA==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r436749198", "bodyText": "The main idea behind the adapter is that we can use only methods on the adapter which provides a clean definition of the used ozone method. I would the usage of getObjectStore() as it leaks the internal methods.\nThe original goal of adapter to support different classloaders, but still seems to be a good design pattern to use an adapter.getVolume instead.", "author": "elek", "createdAt": "2020-06-08T14:24:22Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,904 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdds.conf.ConfigurationSource;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.utils.LegacyHadoopConfigurationSource;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.BUCKET_NOT_EMPTY;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes.VOLUME_NOT_EMPTY;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * BasicRootedOzoneFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+  private OzoneClientAdapter adapter;\n+  private BasicRootedOzoneClientAdapterImpl adapterImpl;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/path/to/key  OR \" +\n+      \"ofs://om-host.example.com/path/to/key  OR \" +\n+      \"ofs://om-host.example.com:5678/path/to/key\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHostOrServiceId;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHostOrServiceId = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      ConfigurationSource source;\n+      if (conf instanceof OzoneConfiguration) {\n+        source = (ConfigurationSource) conf;\n+      } else {\n+        source = new LegacyHadoopConfigurationSource(conf);\n+      }\n+      this.adapter =\n+          createAdapter(source,\n+              omHostOrServiceId, omPort,\n+              isolatedClassloader);\n+      this.adapterImpl = (BasicRootedOzoneClientAdapterImpl) this.adapter;\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected OzoneClientAdapter createAdapter(ConfigurationSource conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      return OzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, replication, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key,\n+        replication, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, short replication,\n+      boolean overwrite, boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key,\n+        replication, overwrite, recursive), statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+    private final OzoneBucket bucket;\n+    private final BasicRootedOzoneClientAdapterImpl adapterImpl;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+      // Initialize bucket here to reduce number of RPC calls\n+      OFSPath ofsPath = new OFSPath(srcPath);\n+      // TODO: Refactor later.\n+      adapterImpl = (BasicRootedOzoneClientAdapterImpl) adapter;\n+      this.bucket = adapterImpl.getBucket(ofsPath, false);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapterImpl.rename(this.bucket, keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    final private boolean recursive;\n+    private final OzoneBucket bucket;\n+    private final BasicRootedOzoneClientAdapterImpl adapterImpl;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+      // Initialize bucket here to reduce number of RPC calls\n+      OFSPath ofsPath = new OFSPath(f);\n+      // TODO: Refactor later.\n+      adapterImpl = (BasicRootedOzoneClientAdapterImpl) adapter;\n+      this.bucket = adapterImpl.getBucket(ofsPath, false);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"Deleting: {}\", keyPath);\n+        boolean succeed = adapterImpl.deleteObject(this.bucket, keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    if (status == null) {\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      OFSPath ofsPath = new OFSPath(key);\n+\n+      // Handle rm root\n+      if (ofsPath.isRoot()) {\n+        // Intentionally drop support for rm root\n+        // because it is too dangerous and doesn't provide much value\n+        LOG.warn(\"delete: OFS does not support rm root. \"\n+            + \"To wipe the cluster, please re-init OM instead.\");\n+        return false;\n+      }\n+\n+      // Handle delete volume\n+      if (ofsPath.isVolume()) {\n+        String volumeName = ofsPath.getVolumeName();\n+        if (recursive) {\n+          // Delete all buckets first\n+          OzoneVolume volume =\n+              adapterImpl.getObjectStore().getVolume(volumeName);", "originalCommit": "b9dac41380617e1c680da2eb789857a64274ad6b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2MDQzMQ==", "url": "https://github.com/apache/ozone/pull/1021#discussion_r437760431", "bodyText": "Yes I agree. When implementing OFS volume and bucket deletion I tried to put the logic in adapter, but I realized if I want to do it the easy way (using recursion) I simply can't put the logic in adapter. Hence the hacky one you see here.\nI will refactor this chunk of code and remove adapterImpl in another refactoring jira later. But I think this shouldn't affect the merge.", "author": "smengcl", "createdAt": "2020-06-09T22:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc0OTE5OA=="}], "type": "inlineReview"}, {"oid": "f229222a3243ca394e394d7e81345265af793bc3", "url": "https://github.com/apache/ozone/commit/f229222a3243ca394e394d7e81345265af793bc3", "message": "Merge remote-tracking branch 'origin/master' into HDDS-2665-ofs\n\nConflicts:\nhadoop-ozone/ozonefs-hadoop2/src/main/java/org/apache/hadoop/fs/ozone/TestOFSPath.java\nhadoop-ozone/ozonefs-hadoop2/src/main/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystemWithMocks.java\nhadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapterFactory.java", "committedDate": "2020-06-09T16:48:36Z", "type": "commit"}, {"oid": "f64315f8bc80987292c82b29eebe591af01520a9", "url": "https://github.com/apache/ozone/commit/f64315f8bc80987292c82b29eebe591af01520a9", "message": "HDDS-3767. [OFS] Address merge conflicts after HDDS-3627\n\nCloses #1046", "committedDate": "2020-06-11T15:43:56Z", "type": "commit"}, {"oid": "2eb3181b552824ea8a5e5956387e4c8b540b97c9", "url": "https://github.com/apache/ozone/commit/2eb3181b552824ea8a5e5956387e4c8b540b97c9", "message": "Merge remote-tracking branch 'origin/master' into HDDS-2665-ofs", "committedDate": "2020-06-11T15:58:41Z", "type": "commit"}, {"oid": "2eb3181b552824ea8a5e5956387e4c8b540b97c9", "url": "https://github.com/apache/ozone/commit/2eb3181b552824ea8a5e5956387e4c8b540b97c9", "message": "Merge remote-tracking branch 'origin/master' into HDDS-2665-ofs", "committedDate": "2020-06-11T15:58:41Z", "type": "forcePushed"}]}