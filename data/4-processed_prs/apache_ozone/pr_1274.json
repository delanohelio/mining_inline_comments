{"pr_number": 1274, "pr_title": "HDDS-3810. Add the logic to distribute open containers among the pipelines of a datanode.", "pr_createdAt": "2020-07-30T07:01:43Z", "pr_url": "https://github.com/apache/ozone/pull/1274", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg3Njc2MA==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r462876760", "bodyText": "Should be added to ozone-default.xml, too (indicated by failure of TestOzoneConfigurationFields).", "author": "adoroszlai", "createdAt": "2020-07-30T09:39:30Z", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "diffHunk": "@@ -307,6 +307,10 @@\n       OZONE_SCM_KEY_VALUE_CONTAINER_DELETION_CHOOSING_POLICY =\n       \"ozone.scm.keyvalue.container.deletion-choosing.policy\";\n \n+  public static final String OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK =\n+      \"ozone.scm.pipeline.per.raft.log.disk\";", "originalCommit": "05dad8bf4fcf8cc56fac9acd837f69e95939cc45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg4MzgyMQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r462883821", "bodyText": "Shouldn't this be same as OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK_DEFAULT?", "author": "adoroszlai", "createdAt": "2020-07-30T09:52:11Z", "path": "hadoop-hdds/common/src/main/resources/ozone-default.xml", "diffHunk": "@@ -821,6 +821,13 @@\n     <description>Number of containers per owner per disk in a pipeline.\n     </description>\n   </property>\n+  <property>\n+    <name>ozone.scm.pipeline.per.raft.log.disk</name>\n+    <value>3</value>", "originalCommit": "a656dc79a01439a1dc00e7f5b30f9caa8f468b9f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c538a7542e3962cf1588a18fc889af494c663bff", "url": "https://github.com/apache/ozone/commit/c538a7542e3962cf1588a18fc889af494c663bff", "message": "HDDS-3810. Add the logic to distribute open containers among the piplelines of a datanode.", "committedDate": "2020-08-28T07:11:42Z", "type": "commit"}, {"oid": "ca4fa4ce75d74db56cef600c150d3dd29c68be0f", "url": "https://github.com/apache/ozone/commit/ca4fa4ce75d74db56cef600c150d3dd29c68be0f", "message": "Addressed review comments.", "committedDate": "2020-08-28T07:11:42Z", "type": "commit"}, {"oid": "331ddcde454da394543afafd3d520277d80c4b1b", "url": "https://github.com/apache/ozone/commit/331ddcde454da394543afafd3d520277d80c4b1b", "message": "changed default config value.", "committedDate": "2020-08-28T07:11:42Z", "type": "commit"}, {"oid": "6b4d8cc371e0e34704ecad7e98b52837fe071a89", "url": "https://github.com/apache/ozone/commit/6b4d8cc371e0e34704ecad7e98b52837fe071a89", "message": "Added logic for raft log disk accouting in pipeline creation.", "committedDate": "2020-08-28T12:15:09Z", "type": "commit"}, {"oid": "6b4d8cc371e0e34704ecad7e98b52837fe071a89", "url": "https://github.com/apache/ozone/commit/6b4d8cc371e0e34704ecad7e98b52837fe071a89", "message": "Added logic for raft log disk accouting in pipeline creation.", "committedDate": "2020-08-28T12:15:09Z", "type": "forcePushed"}, {"oid": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "url": "https://github.com/apache/ozone/commit/3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "message": "Revert pom.xml changes.", "committedDate": "2020-08-28T15:22:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjExMTgyMg==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496111822", "bodyText": "Rename suggestion: getMetadataVolumeCount()", "author": "nandakumar131", "createdAt": "2020-09-28T17:19:00Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java", "diffHunk": "@@ -121,6 +143,19 @@ public int getHealthyVolumeCount() {\n     }\n   }\n \n+  /**\n+   * Returns count of healthy raft log volumes reported from datanode.\n+   * @return count of healthy raft log volumes\n+   */\n+  public int getRaftLogVolumeCount() {", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNzYyNA==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496127624", "bodyText": "Will this work in case of heterogeneous datanodes, where one datanode has 1 Raft log disk with 2 data disk and the other datanode has 5 Raft log disk with 10 data disk?\nAccording to the current logic getOpenContainerCountPerPipeline will return 10, if numContainerPerVolume and numPipelinesPerRaftLogDisk are set to 2.", "author": "nandakumar131", "createdAt": "2020-09-28T17:46:40Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/SCMContainerManager.java", "diffHunk": "@@ -101,12 +103,23 @@ public SCMContainerManager(\n     this.numContainerPerVolume = conf\n         .getInt(ScmConfigKeys.OZONE_SCM_PIPELINE_OWNER_CONTAINER_COUNT,\n             ScmConfigKeys.OZONE_SCM_PIPELINE_OWNER_CONTAINER_COUNT_DEFAULT);\n+    this.numPipelinesPerRaftLogDisk = conf\n+        .getInt(ScmConfigKeys.OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK,\n+            ScmConfigKeys.OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK_DEFAULT);\n \n     loadExistingContainers();\n \n     scmContainerManagerMetrics = SCMContainerManagerMetrics.create();\n   }\n \n+  private int getOpenContainerCountPerPipeline(Pipeline pipeline) {\n+    int totalContainerCountPerDn = numContainerPerVolume *\n+        pipelineManager.getNumHealthyVolumes(pipeline);\n+    int maxPipelineCountPerDn = pipelineManager.maxPipelineLimit(pipeline);\n+    return (int) Math.ceil(\n+        ((double) totalContainerCountPerDn / maxPipelineCountPerDn));\n+  }\n+", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM2MTQwMA==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496361400", "bodyText": "might need change pipeline placement policy to make sure that only allocating pipelines on homogeneous machiens.\nIt one pipeline connects a strong DN and weak DN, its open container number will be volumeNumOfStrongDN * 3 / pipelineNumOfWeakDN.\uff0cpipeline on the same DN may not be able to evenly distribute open containers of that DN.", "author": "GlenGeng", "createdAt": "2020-09-29T03:48:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNzYyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM5NDA3Ng==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496394076", "bodyText": "I agree with @GlenGeng here. The pipeline placement should choose homogeneous datanodes. The choice being made here, have many containers open on a minimal set of pipelines that we can have out of the set of datanodes in the pipelines.", "author": "bshashikant", "createdAt": "2020-09-29T04:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNzYyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzMxMTYxMA==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r497311610", "bodyText": "Changed the logic to take the min of the data disks count of the set of dns in a pipeline while determining the open container limit.", "author": "bshashikant", "createdAt": "2020-09-30T07:53:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEyNzYyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0ODMyNQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496348325", "bodyText": "may replace StorageContainerDatanodeProtocolProtos.MetadataStorageReportProto with MetadataStorageReportProto", "author": "GlenGeng", "createdAt": "2020-09-29T02:54:51Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java", "diffHunk": "@@ -40,6 +43,8 @@\n   private long lastStatsUpdatedTime;\n \n   private List<StorageReportProto> storageReports;\n+  private List<StorageContainerDatanodeProtocolProtos.", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0ODU5Ng==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496348596", "bodyText": "stale java doc, suggestion: Updates the datanode metadata storage reports.", "author": "GlenGeng", "createdAt": "2020-09-29T02:55:52Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java", "diffHunk": "@@ -94,6 +100,22 @@ public void updateStorageReports(List<StorageReportProto> reports) {\n     }\n   }\n \n+  /**\n+   * Updates the datanode storage reports.", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MTgwMQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496351801", "bodyText": "How about OZONE_SCM_PIPELINE_PER_METADATA_DISK ?\nThere are both raft log disk and meta data storage report existing in the code context, they are similar to each other, bring in some redundancy.\nBTW, the former one may lead misunderstanding. One might think that each raft log disk contains one raft log, which is straightforwad, nevertheless, the raft log disk and the raft log is a OneToMany relationship.", "author": "GlenGeng", "createdAt": "2020-09-29T03:08:22Z", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "diffHunk": "@@ -308,6 +308,10 @@\n       OZONE_SCM_KEY_VALUE_CONTAINER_DELETION_CHOOSING_POLICY =\n       \"ozone.scm.keyvalue.container.deletion-choosing.policy\";\n \n+  public static final String OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK =", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzA0OA==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496357048", "bodyText": "method name is maxPipelineLimit, but the logic calculates the min, which is a little bit weird. How about minPipelineLimit", "author": "GlenGeng", "createdAt": "2020-09-29T03:30:20Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java", "diffHunk": "@@ -530,6 +540,43 @@ public int getNumHealthyVolumes(List<DatanodeDetails> dnList) {\n     return Collections.max(volumeCountList);\n   }\n \n+  /**\n+   * Returns the pipeline limit for the datanode.\n+   * if the datanode pipeline limit is set, consider that as the max\n+   * pipeline limit.\n+   * In case, the pipeline limit is not set, the max pipeline limit\n+   * will be based on the no of raft log volume reported and provided\n+   * that it has atleast one healthy data volume.\n+   */\n+  @Override\n+  public int maxPipelineLimit(DatanodeDetails dn) {\n+    try {\n+      if (heavyNodeCriteria > 0) {\n+        return heavyNodeCriteria;\n+      } else if (nodeStateManager.getNode(dn).getHealthyVolumeCount() > 0) {\n+        return numPipelinesPerRaftLogDisk *\n+            nodeStateManager.getNode(dn).getRaftLogVolumeCount();\n+      }\n+    } catch (NodeNotFoundException e) {\n+      LOG.warn(\"Cannot generate NodeStat, datanode {} not found.\",\n+          dn.getUuid());\n+    }\n+    return 0;\n+  }\n+\n+  /**\n+   * Returns the pipeline limit for set of datanodes.\n+   */\n+  @Override\n+  public int maxPipelineLimit(List<DatanodeDetails> dnList) {", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1ODI5NQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r496358295", "bodyText": "pipelineManager.getMaxHealthyVolumeNum(pipeline) / pipelineManager.getMinPipelineLimit(pipeline) may be more expressive.", "author": "GlenGeng", "createdAt": "2020-09-29T03:35:34Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/SCMContainerManager.java", "diffHunk": "@@ -101,12 +103,23 @@ public SCMContainerManager(\n     this.numContainerPerVolume = conf\n         .getInt(ScmConfigKeys.OZONE_SCM_PIPELINE_OWNER_CONTAINER_COUNT,\n             ScmConfigKeys.OZONE_SCM_PIPELINE_OWNER_CONTAINER_COUNT_DEFAULT);\n+    this.numPipelinesPerRaftLogDisk = conf\n+        .getInt(ScmConfigKeys.OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK,\n+            ScmConfigKeys.OZONE_SCM_PIPELINE_PER_RAFT_LOG_DISK_DEFAULT);\n \n     loadExistingContainers();\n \n     scmContainerManagerMetrics = SCMContainerManagerMetrics.create();\n   }\n \n+  private int getOpenContainerCountPerPipeline(Pipeline pipeline) {\n+    int totalContainerCountPerDn = numContainerPerVolume *\n+        pipelineManager.getNumHealthyVolumes(pipeline);", "originalCommit": "3ddc28746bc31fcd52d3f8302e3f31c7e8bf1caa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5338e15f1cbd791697f8bd395a08eaa511a09dd0", "url": "https://github.com/apache/ozone/commit/5338e15f1cbd791697f8bd395a08eaa511a09dd0", "message": "Addressed review comments.", "committedDate": "2020-09-29T05:26:09Z", "type": "commit"}, {"oid": "7ce53bc48b3ac6932e9bce5bac658edf080b62a4", "url": "https://github.com/apache/ozone/commit/7ce53bc48b3ac6932e9bce5bac658edf080b62a4", "message": "Addressed unit test failures", "committedDate": "2020-09-29T08:52:07Z", "type": "commit"}, {"oid": "3ae1a5f6856a13b9723b4ab438e8f8d5963f70cb", "url": "https://github.com/apache/ozone/commit/3ae1a5f6856a13b9723b4ab438e8f8d5963f70cb", "message": "Addressed Integeration test failure", "committedDate": "2020-09-29T09:11:26Z", "type": "commit"}, {"oid": "9e710be6ea896724ca75c7e3967dc66e885e44c4", "url": "https://github.com/apache/ozone/commit/9e710be6ea896724ca75c7e3967dc66e885e44c4", "message": "Addressed test failure", "committedDate": "2020-09-29T11:36:07Z", "type": "commit"}, {"oid": "576c1ab93a8114f601e25d445013f87b7c0a3653", "url": "https://github.com/apache/ozone/commit/576c1ab93a8114f601e25d445013f87b7c0a3653", "message": "Addressed Checkstyle issues.", "committedDate": "2020-09-29T14:39:56Z", "type": "commit"}, {"oid": "9ef14e007f4410bde398bd0e66e617f5d674b239", "url": "https://github.com/apache/ozone/commit/9ef14e007f4410bde398bd0e66e617f5d674b239", "message": "Changed the logic for calculating healthyVolume count per pipeline.", "committedDate": "2020-09-30T07:51:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM1MDEwNQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r497350105", "bodyText": "method name suggestion: minHealthyVolumeNum", "author": "GlenGeng", "createdAt": "2020-09-30T08:55:10Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java", "diffHunk": "@@ -511,7 +521,7 @@ private SCMNodeStat getNodeStatInternal(DatanodeDetails datanodeDetails) {\n   }\n \n   /**\n-   * Returns the max of no healthy volumes reported out of the set\n+   * Returns the min of no healthy volumes reported out of the set", "originalCommit": "9ef14e007f4410bde398bd0e66e617f5d674b239", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM1MDY4Ng==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r497350686", "bodyText": "name suggestion: totalContainerCountPerDn  -> minContainerCountPerDn", "author": "GlenGeng", "createdAt": "2020-09-30T08:56:04Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/SCMContainerManager.java", "diffHunk": "@@ -107,6 +107,14 @@ public SCMContainerManager(\n     scmContainerManagerMetrics = SCMContainerManagerMetrics.create();\n   }\n \n+  private int getOpenContainerCountPerPipeline(Pipeline pipeline) {\n+    int totalContainerCountPerDn = numContainerPerVolume *", "originalCommit": "9ef14e007f4410bde398bd0e66e617f5d674b239", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM1MjMyMQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r497352321", "bodyText": "member var name suggestion: numPipelinesPerMetadataVolume", "author": "GlenGeng", "createdAt": "2020-09-30T08:58:31Z", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java", "diffHunk": "@@ -130,6 +133,11 @@ public SCMNodeManager(OzoneConfiguration conf,\n     this.useHostname = conf.getBoolean(\n         DFSConfigKeysLegacy.DFS_DATANODE_USE_DN_HOSTNAME,\n         DFSConfigKeysLegacy.DFS_DATANODE_USE_DN_HOSTNAME_DEFAULT);\n+    this.numPipelinesPerRaftLogDisk =", "originalCommit": "9ef14e007f4410bde398bd0e66e617f5d674b239", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM1MzQ1OQ==", "url": "https://github.com/apache/ozone/pull/1274#discussion_r497353459", "bodyText": "name suggestion: OZONE_SCM_PIPELINE_PER_METADATA_VOLUME.\nWe'd better use concept volume instead of concept disk in SCM, the former one is a logical concept, the latter is physical one, and volume is already widely used in context.", "author": "GlenGeng", "createdAt": "2020-09-30T09:00:12Z", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "diffHunk": "@@ -308,6 +308,10 @@\n       OZONE_SCM_KEY_VALUE_CONTAINER_DELETION_CHOOSING_POLICY =\n       \"ozone.scm.keyvalue.container.deletion-choosing.policy\";\n \n+  public static final String OZONE_SCM_PIPELINE_PER_METADATA_DISK =", "originalCommit": "9ef14e007f4410bde398bd0e66e617f5d674b239", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5ed4460a840e860d89472915f3680f4b25a39fc5", "url": "https://github.com/apache/ozone/commit/5ed4460a840e860d89472915f3680f4b25a39fc5", "message": "Addressed review comments.", "committedDate": "2020-09-30T09:31:49Z", "type": "commit"}, {"oid": "a8160fe710d0faa761b7127e873405c6d81a3509", "url": "https://github.com/apache/ozone/commit/a8160fe710d0faa761b7127e873405c6d81a3509", "message": "Fix compilation issue.", "committedDate": "2020-09-30T10:21:29Z", "type": "commit"}, {"oid": "229daca6931e2be50e3ff363e792da6cdacf3dbc", "url": "https://github.com/apache/ozone/commit/229daca6931e2be50e3ff363e792da6cdacf3dbc", "message": "Trigger CI check", "committedDate": "2020-09-30T11:34:16Z", "type": "commit"}]}