{"pr_number": 508, "pr_title": "HDDS-2950. Upgrade jetty to the latest 9.4 release", "pr_createdAt": "2020-01-30T14:45:03Z", "pr_url": "https://github.com/apache/ozone/pull/508", "timeline": [{"oid": "009066f7c081f7fa77a08a0177fd7d76d71f1540", "url": "https://github.com/apache/ozone/commit/009066f7c081f7fa77a08a0177fd7d76d71f1540", "message": "HDDS-2950. Upgrade jetty to the latest 9.4 release", "committedDate": "2020-01-28T12:06:28Z", "type": "commit"}, {"oid": "39b27423bc4eed088ad760d0a0a6180fc591b4de", "url": "https://github.com/apache/ozone/commit/39b27423bc4eed088ad760d0a0a6180fc591b4de", "message": "fix findbugs error", "committedDate": "2020-01-28T13:05:43Z", "type": "commit"}, {"oid": "48175b715194d47cbb435b0b40879e1a9e84d5d2", "url": "https://github.com/apache/ozone/commit/48175b715194d47cbb435b0b40879e1a9e84d5d2", "message": "disable spnego filter for s3g", "committedDate": "2020-01-28T13:44:05Z", "type": "commit"}, {"oid": "ba3b4ea8bd8ee76625859bbfc5ca9f2d0e07f749", "url": "https://github.com/apache/ozone/commit/ba3b4ea8bd8ee76625859bbfc5ca9f2d0e07f749", "message": "fix checkstyle + server filter initialization", "committedDate": "2020-01-29T09:46:58Z", "type": "commit"}, {"oid": "ef0d720e5b97f2a54c639ef2fddb976f137cd456", "url": "https://github.com/apache/ozone/commit/ef0d720e5b97f2a54c639ef2fddb976f137cd456", "message": "Import proper 3.2 version from HttpServer2 and dependencies + fix all the checkstyle / findbug errors", "committedDate": "2020-01-30T13:10:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3Nzk0Mg==", "url": "https://github.com/apache/ozone/pull/508#discussion_r373177942", "bodyText": "This can be changed to private as well?", "author": "smengcl", "createdAt": "2020-01-30T20:31:04Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";", "originalCommit": "ef0d720e5b97f2a54c639ef2fddb976f137cd456", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NTc2MA==", "url": "https://github.com/apache/ozone/pull/508#discussion_r373995760", "bodyText": "sure, done.", "author": "elek", "createdAt": "2020-02-03T09:27:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3Nzk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDA2Nw==", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180067", "bodyText": "hadoop.http.sni.host.check.enabled is added in HADOOP-16718 in trunk. We might want it here as well?", "author": "smengcl", "createdAt": "2020-01-30T20:36:01Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+", "originalCommit": "ef0d720e5b97f2a54c639ef2fddb976f137cd456", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDU1OQ==", "url": "https://github.com/apache/ozone/pull/508#discussion_r373180559", "bodyText": "I see .clone() is added here. For safe practice?", "author": "smengcl", "createdAt": "2020-01-30T20:37:12Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/http/HttpServer2.java", "diffHunk": "@@ -0,0 +1,1701 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdds.server.http;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.io.PrintStream;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.servlet.Filter;\n+import javax.servlet.FilterChain;\n+import javax.servlet.FilterConfig;\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletRequest;\n+import javax.servlet.ServletResponse;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletRequestWrapper;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.sun.jersey.spi.container.servlet.ServletContainer;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.ConfServlet;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configuration.IntegerRanges;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.jmx.JMXJsonServlet;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.security.AuthenticationFilterInitializer;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n+import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n+import org.apache.hadoop.security.authorize.AccessControlList;\n+import org.apache.hadoop.security.ssl.SSLFactory;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+import org.eclipse.jetty.http.HttpVersion;\n+import org.eclipse.jetty.server.ConnectionFactory;\n+import org.eclipse.jetty.server.Connector;\n+import org.eclipse.jetty.server.Handler;\n+import org.eclipse.jetty.server.HttpConfiguration;\n+import org.eclipse.jetty.server.HttpConnectionFactory;\n+import org.eclipse.jetty.server.RequestLog;\n+import org.eclipse.jetty.server.SecureRequestCustomizer;\n+import org.eclipse.jetty.server.Server;\n+import org.eclipse.jetty.server.ServerConnector;\n+import org.eclipse.jetty.server.SslConnectionFactory;\n+import org.eclipse.jetty.server.handler.ContextHandlerCollection;\n+import org.eclipse.jetty.server.handler.HandlerCollection;\n+import org.eclipse.jetty.server.handler.RequestLogHandler;\n+import org.eclipse.jetty.server.session.SessionHandler;\n+import org.eclipse.jetty.servlet.DefaultServlet;\n+import org.eclipse.jetty.servlet.FilterHolder;\n+import org.eclipse.jetty.servlet.FilterMapping;\n+import org.eclipse.jetty.servlet.ServletContextHandler;\n+import org.eclipse.jetty.servlet.ServletHandler;\n+import org.eclipse.jetty.servlet.ServletHolder;\n+import org.eclipse.jetty.servlet.ServletMapping;\n+import org.eclipse.jetty.util.ArrayUtil;\n+import org.eclipse.jetty.util.MultiException;\n+import org.eclipse.jetty.util.ssl.SslContextFactory;\n+import org.eclipse.jetty.util.thread.QueuedThreadPool;\n+import org.eclipse.jetty.webapp.WebAppContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal is\n+ * to serve up status information for the server. There are three contexts:\n+ * \"/logs/\" -> points to the log directory \"/static/\" -> points to common static\n+ * files (src/webapps/static) \"/\" -> the jsp server code from\n+ * (src/webapps/<name>)\n+ *\n+ * This class is a fork of the old HttpServer. HttpServer exists for\n+ * compatibility reasons. See HBASE-10336 for more details.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public final class HttpServer2 implements FilterContainer {\n+  public static final Logger LOG = LoggerFactory.getLogger(HttpServer2.class);\n+\n+  private static final String HTTP_SCHEME = \"http\";\n+  public static final String HTTPS_SCHEME = \"https\";\n+\n+  public static final String HTTP_MAX_REQUEST_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.request.header.size\";\n+  public static final int HTTP_MAX_REQUEST_HEADER_SIZE_DEFAULT = 65536;\n+  public static final String HTTP_MAX_RESPONSE_HEADER_SIZE_KEY =\n+      \"hadoop.http.max.response.header.size\";\n+  public static final int HTTP_MAX_RESPONSE_HEADER_SIZE_DEFAULT = 65536;\n+\n+  public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n+      \"hadoop.http.socket.backlog.size\";\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n+  public static final String HTTP_ACCEPTOR_COUNT_KEY =\n+      \"hadoop.http.acceptor.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_ACCEPTOR_COUNT_DEFAULT = -1;\n+  public static final String HTTP_SELECTOR_COUNT_KEY =\n+      \"hadoop.http.selector.count\";\n+  // -1 to use default behavior of setting count based on CPU core count\n+  public static final int HTTP_SELECTOR_COUNT_DEFAULT = -1;\n+  // idle timeout in milliseconds\n+  public static final String HTTP_IDLE_TIMEOUT_MS_KEY =\n+      \"hadoop.http.idle_timeout.ms\";\n+  public static final int HTTP_IDLE_TIMEOUT_MS_DEFAULT = 10000;\n+  public static final String HTTP_TEMP_DIR_KEY = \"hadoop.http.temp.dir\";\n+\n+  public static final String FILTER_INITIALIZER_PROPERTY\n+      = \"ozone.http.filter.initializers\";\n+\n+  // The ServletContext attribute where the daemon Configuration\n+  // gets stored.\n+  public static final String CONF_CONTEXT_ATTRIBUTE = \"hadoop.conf\";\n+  public static final String ADMINS_ACL = \"admins.acl\";\n+  public static final String SPNEGO_FILTER = \"SpnegoFilter\";\n+  public static final String NO_CACHE_FILTER = \"NoCacheFilter\";\n+\n+  public static final String BIND_ADDRESS = \"bind.address\";\n+\n+  private final AccessControlList adminsAcl;\n+\n+  private final Server webServer;\n+\n+  private final HandlerCollection handlers;\n+\n+  private final List<ServerConnector> listeners = Lists.newArrayList();\n+\n+  private final WebAppContext webAppContext;\n+  private final boolean findPort;\n+  private final IntegerRanges portRanges;\n+  private final Map<ServletContextHandler, Boolean> defaultContexts =\n+      new HashMap<>();\n+  private final List<String> filterNames = new ArrayList<>();\n+  static final String STATE_DESCRIPTION_ALIVE = \" - alive\";\n+  static final String STATE_DESCRIPTION_NOT_LIVE = \" - not live\";\n+  private final SignerSecretProvider secretProvider;\n+  private XFrameOption xFrameOption;\n+  private boolean xFrameOptionIsEnabled;\n+  public static final String HTTP_HEADER_PREFIX = \"hadoop.http.header.\";\n+  private static final String HTTP_HEADER_REGEX =\n+      \"hadoop\\\\.http\\\\.header\\\\.([a-zA-Z\\\\-_]+)\";\n+  static final String X_XSS_PROTECTION =\n+      \"X-XSS-Protection:1; mode=block\";\n+  static final String X_CONTENT_TYPE_OPTIONS =\n+      \"X-Content-Type-Options:nosniff\";\n+  private static final String X_FRAME_OPTIONS = \"X-FRAME-OPTIONS\";\n+  private static final Pattern PATTERN_HTTP_HEADER_REGEX =\n+      Pattern.compile(HTTP_HEADER_REGEX);\n+  /**\n+   * Class to construct instances of HTTP server with specific options.\n+   */\n+  public static class Builder {\n+    private ArrayList<URI> endpoints = Lists.newArrayList();\n+    private String name;\n+    private Configuration conf;\n+    private Configuration sslConf;\n+    private String[] pathSpecs;\n+    private AccessControlList adminsAcl;\n+    private boolean securityEnabled = false;\n+    private String usernameConfKey;\n+    private String keytabConfKey;\n+    private boolean needsClientAuth;\n+    private String trustStore;\n+    private String trustStorePassword;\n+    private String trustStoreType;\n+\n+    private String keyStore;\n+    private String keyStorePassword;\n+    private String keyStoreType;\n+\n+    // The -keypass option in keytool\n+    private String keyPassword;\n+\n+    private boolean findPort;\n+    private IntegerRanges portRanges = null;\n+\n+    private String hostName;\n+    private boolean disallowFallbackToRandomSignerSecretProvider;\n+    private String authFilterConfigurationPrefix =\n+        \"hadoop.http.authentication.\";\n+    private String excludeCiphers;\n+\n+    private boolean xFrameEnabled;\n+    private XFrameOption xFrameOption = XFrameOption.SAMEORIGIN;\n+\n+    public Builder setName(String serverName) {\n+      this.name = serverName;\n+      return this;\n+    }\n+\n+    /**\n+     * Add an endpoint that the HTTP server should listen to.\n+     *\n+     * @param endpoint\n+     *          the endpoint of that the HTTP server should listen to. The\n+     *          scheme specifies the protocol (i.e. HTTP / HTTPS), the host\n+     *          specifies the binding address, and the port specifies the\n+     *          listening port. Unspecified or zero port means that the server\n+     *          can listen to any port.\n+     */\n+    public Builder addEndpoint(URI endpoint) {\n+      endpoints.add(endpoint);\n+      return this;\n+    }\n+\n+    /**\n+     * Set the hostname of the http server. The host name is used to resolve the\n+     * _HOST field in Kerberos principals. The hostname of the first listener\n+     * will be used if the name is unspecified.\n+     */\n+    public Builder hostName(String host) {\n+      this.hostName = host;\n+      return this;\n+    }\n+\n+    public Builder trustStore(String location, String password, String type) {\n+      this.trustStore = location;\n+      this.trustStorePassword = password;\n+      this.trustStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyStore(String location, String password, String type) {\n+      this.keyStore = location;\n+      this.keyStorePassword = password;\n+      this.keyStoreType = type;\n+      return this;\n+    }\n+\n+    public Builder keyPassword(String password) {\n+      this.keyPassword = password;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify whether the server should authorize the client in SSL\n+     * connections.\n+     */\n+    public Builder needsClientAuth(boolean value) {\n+      this.needsClientAuth = value;\n+      return this;\n+    }\n+\n+    public Builder setFindPort(boolean portFind) {\n+      this.findPort = portFind;\n+      return this;\n+    }\n+\n+    public Builder setPortRanges(IntegerRanges ranges) {\n+      this.portRanges = ranges;\n+      return this;\n+    }\n+\n+    public Builder setConf(Configuration configuration) {\n+      this.conf = configuration;\n+      return this;\n+    }\n+\n+    /**\n+     * Specify the SSL configuration to load. This API provides an alternative\n+     * to keyStore/keyPassword/trustStore.\n+     */\n+    public Builder setSSLConf(Configuration sslCnf) {\n+      this.sslConf = sslCnf;\n+      return this;\n+    }\n+\n+    public Builder setPathSpec(String[] pathSpec) {\n+      this.pathSpecs = pathSpec.clone();", "originalCommit": "ef0d720e5b97f2a54c639ef2fddb976f137cd456", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NjA5MQ==", "url": "https://github.com/apache/ozone/pull/508#discussion_r373996091", "bodyText": "Yes, it was a findbugs violation (internal representation is leaked...)", "author": "elek", "createdAt": "2020-02-03T09:28:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE4MDU1OQ=="}], "type": "inlineReview"}, {"oid": "8e9db69e2230cd89743344fc30cf30fe17309a32", "url": "https://github.com/apache/ozone/commit/8e9db69e2230cd89743344fc30cf30fe17309a32", "message": "importing null checks from HADOOP-16727.", "committedDate": "2020-02-03T09:26:24Z", "type": "commit"}, {"oid": "1f98b9bcf29868924ed4b550a58807217d43c05b", "url": "https://github.com/apache/ozone/commit/1f98b9bcf29868924ed4b550a58807217d43c05b", "message": "make private constants private", "committedDate": "2020-02-03T09:27:32Z", "type": "commit"}, {"oid": "aebe028343181ed9dafa4f2acbdbcd0d2ee6821b", "url": "https://github.com/apache/ozone/commit/aebe028343181ed9dafa4f2acbdbcd0d2ee6821b", "message": "Merge remote-tracking branch 'origin/master' into HDDS-2950", "committedDate": "2020-02-07T09:40:20Z", "type": "commit"}]}