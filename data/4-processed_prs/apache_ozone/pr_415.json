{"pr_number": 415, "pr_title": "HDDS-2840. Implement ofs://: mkdir", "pr_createdAt": "2020-01-07T18:32:06Z", "pr_url": "https://github.com/apache/ozone/pull/415", "timeline": [{"oid": "0d7100604334f06fe304b143a3b906c9cbfc88dd", "url": "https://github.com/apache/ozone/commit/0d7100604334f06fe304b143a3b906c9cbfc88dd", "message": "testListStatusOnLargeDirectory: numDirs reduced from 5111 to 1024+512.", "committedDate": "2020-01-30T19:14:02Z", "type": "commit"}, {"oid": "343f2cdd8a0ca2a6168b935e865a6dcb201f8233", "url": "https://github.com/apache/ozone/commit/343f2cdd8a0ca2a6168b935e865a6dcb201f8233", "message": "Copy OzoneFileSystem -> OFileSystem, BasicOzoneFileSystem -> BasicOFileSystem.", "committedDate": "2020-01-10T21:48:32Z", "type": "commit"}, {"oid": "0707c68c8c0d2aae4a5dd51e0c4bae7c3821f262", "url": "https://github.com/apache/ozone/commit/0707c68c8c0d2aae4a5dd51e0c4bae7c3821f262", "message": "Copy TestOzoneFileSystemWithMocks -> TestOFileSystemWithMocks.", "committedDate": "2020-01-10T21:48:32Z", "type": "commit"}, {"oid": "c9d22b34fae03688c5de9965caa0df3cda2d8a0d", "url": "https://github.com/apache/ozone/commit/c9d22b34fae03688c5de9965caa0df3cda2d8a0d", "message": "Copy TestOzoneFileSystem -> TestOFileSystem.", "committedDate": "2020-01-10T21:48:32Z", "type": "commit"}, {"oid": "08cee598a397ac10911240640b7c595060532413", "url": "https://github.com/apache/ozone/commit/08cee598a397ac10911240640b7c595060532413", "message": "Copy BasicOzoneClientAdapterImpl -> BasicOzoneClientOFSAdapterImpl, OzoneClientAdapterImpl -> OzoneClientOFSAdapterImpl.", "committedDate": "2020-01-10T21:48:32Z", "type": "commit"}, {"oid": "0bee28acadfb2c358c6f008173e7eca6ed7fa23f", "url": "https://github.com/apache/ozone/commit/0bee28acadfb2c358c6f008173e7eca6ed7fa23f", "message": "Copy OzoneClientAdapter -> RootedOzoneClientAdapter.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "5252f459276bb26bea254ab29a9a94e61af601dd", "url": "https://github.com/apache/ozone/commit/5252f459276bb26bea254ab29a9a94e61af601dd", "message": "Change to new scheme name \"ofs\".", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "7e66a20d8b50971aa25755e7a235d0468e560cf4", "url": "https://github.com/apache/ozone/commit/7e66a20d8b50971aa25755e7a235d0468e560cf4", "message": "Fix typo. Tested that ofs:// scheme is registered:\n\nmvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\nalias docc=docker-compose\ndocc up -d --scale datanode=3\ndocc exec om /bin/bash\n\nozone sh volume create vol1\nozone sh bucket create vol1/buc1\n\nozone fs -put README.md o3fs://buc1.vol1/\nozone fs -ls o3fs://buc1.vol1/\n\n# Currently tested\nozone fs -ls ofs://buc1.vol1/\n\n# Goal\nozone fs -ls ofs://vol1/buc1/", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "24c38187e7d423a143db19be0e137abc34013332", "url": "https://github.com/apache/ozone/commit/24c38187e7d423a143db19be0e137abc34013332", "message": "Mod TestOFileSystemWithMocks#testFSUriWithHostPortOverrides.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "407093077ea18b487be02e19ec00c4395433f673", "url": "https://github.com/apache/ozone/commit/407093077ea18b487be02e19ec00c4395433f673", "message": "Fix TestOFileSystemWithMocks#testFSUriWithHostPortOverrides.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "34ce7e87e519402ae4db21afd8ac8c06897ccff5", "url": "https://github.com/apache/ozone/commit/34ce7e87e519402ae4db21afd8ac8c06897ccff5", "message": "Mod TestOFileSystem.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "1eb19366cdc2e20335b33f1fe11784bfde8f6da2", "url": "https://github.com/apache/ozone/commit/1eb19366cdc2e20335b33f1fe11784bfde8f6da2", "message": "Implement parsing. Next: Test with TestOFileSystem.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "bc581f1cfc9670f1601fd7de92cc98e669e51c94", "url": "https://github.com/apache/ozone/commit/bc581f1cfc9670f1601fd7de92cc98e669e51c94", "message": "Remove unused regex imports.", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "a958afc749af5ba9b838c35a88f6fcf8e934fcf5", "url": "https://github.com/apache/ozone/commit/a958afc749af5ba9b838c35a88f6fcf8e934fcf5", "message": "Mod pathToKey. Successfully tested with -put and -ls with ofs://\n\n```\nozone sh volume create vol1\nozone sh bucket create vol1/buc1\n\nozone fs -put README.md ofs://om/vol1/buc1/2.md\nozone fs -ls ofs://om/vol1/buc1/\n```", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "61dd45a60ff0e4b55e0f39ae89d450eedc6d99c1", "url": "https://github.com/apache/ozone/commit/61dd45a60ff0e4b55e0f39ae89d450eedc6d99c1", "message": "Mod BasicOFileSystem#toString", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "1464bcb406e9b74d33b1f0e58dd9d58de7a59456", "url": "https://github.com/apache/ozone/commit/1464bcb406e9b74d33b1f0e58dd9d58de7a59456", "message": "Mod TestOFileSystem. Note: during \"fs = FileSystem.get(conf)\", protobuf isn't decoding the getServiceInfo request correctly:\n\n2019-12-11 09:10:46,622 [Thread-0] WARN  ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:submitRequest(391)) - OMRequest payload =\ncmdType: ServiceList\ntraceID: \"15dc425ffcc948a2:15dc425ffcc948a2:0:1\"\nclientId: \"client-EFF2A03AAB22\"\nserviceListRequest {\n}\n\n2019-12-11 09:10:46,699 [qtp799408188-143] WARN  http.HttpParser (HttpParser.java:<init>(1859)) - Illegal character 0x9 in state=METHOD for buffer HeapByteBuffer@7d1994d5[p=5,l=280,c=8192,r=275]={hrpc\\t<<<\\x00\\x00\\x00\\x00\\x00_\\x1a\\x08\\x02\\x10\\x00\\x18\\x05\"\\x10>x...EFF2A03AAB22\\x9a\\x03\\x00>>>\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\n2019-12-11 09:10:46,700 [qtp799408188-143] WARN  http.HttpParser (HttpParser.java:parseNext(1454)) - bad HTTP parsed: 400 Illegal character 0x9 for HttpChannelOverHttp@9e9d902{r=0,c=false,a=IDLE,uri=null}\n2019-12-11 09:10:46,705 [Thread-0] WARN  net.NetUtils (NetUtils.java:wrapWithMessage(834)) - Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor\njava.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)\n\tat java.lang.Class.getConstructor0(Class.java:3082)\n\tat java.lang.Class.getConstructor(Class.java:1825)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:830)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy38.submitRequest(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy38.submitRequest(Unknown Source)\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:393)\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)\n\tat com.sun.proxy.$Proxy39.getServiceInfo(Unknown Source)\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:158)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:165)\n\tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)\n\tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\n\tat org.apache.hadoop.fs.ozone.OFileSystem.createAdapter(OFileSystem.java:102)\n\tat org.apache.hadoop.fs.ozone.BasicOFileSystem.initialize(BasicOFileSystem.java:156)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:227)\n\tat org.apache.hadoop.fs.ozone.TestOFileSystem.init(TestOFileSystem.java:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "d12a0c632b450488f52edf61719b954734d9ee41", "url": "https://github.com/apache/ozone/commit/d12a0c632b450488f52edf61719b954734d9ee41", "message": "Basic listStatus works now: TestOFileSystem#testListStatus", "committedDate": "2020-01-10T22:11:05Z", "type": "commit"}, {"oid": "6794e8c5c2572f300f9cc62989da672358bdcf7b", "url": "https://github.com/apache/ozone/commit/6794e8c5c2572f300f9cc62989da672358bdcf7b", "message": "Impl class OFSPath.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "a7720ad272ab78e20de1a27ae63aee8fb6c7f8dc", "url": "https://github.com/apache/ozone/commit/a7720ad272ab78e20de1a27ae63aee8fb6c7f8dc", "message": "Overload createAdapter for OFSPath. Use it in all FileSystem ops.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "7e145f4b8a4b97e69b399dacec1ff487aa7cf207", "url": "https://github.com/apache/ozone/commit/7e145f4b8a4b97e69b399dacec1ff487aa7cf207", "message": "Test testListStatus fully passed.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "07097bd0a367ba4033500b32f556d6422491e262", "url": "https://github.com/apache/ozone/commit/07097bd0a367ba4033500b32f556d6422491e262", "message": "Start working on other TestOFileSystem tests.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "2561a58c814bddf227a6b132a155bcd6558eae84", "url": "https://github.com/apache/ozone/commit/2561a58c814bddf227a6b132a155bcd6558eae84", "message": "Close previous adapter before every new adapter initialization. TODO: 1. Can be more concise; 2. Can reuse the previous adapter if volume and bucket don't change.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "83c0e9f10f41b40c3dec223c25bcd49506b7c376", "url": "https://github.com/apache/ozone/commit/83c0e9f10f41b40c3dec223c25bcd49506b7c376", "message": "Test testListStatusOnRoot now passes (hacky).", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "3f0e2e8c91cb9d4e791afd7d0a491a8889cded83", "url": "https://github.com/apache/ozone/commit/3f0e2e8c91cb9d4e791afd7d0a491a8889cded83", "message": "Fix test. testListStatusOnSubDirs, testCreateDoesNotAddParentDirKeys, testDeleteCreatesFakeParentDir now pass.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "530d26e1dffa0b12193674766d6a77c355e7ea59", "url": "https://github.com/apache/ozone/commit/530d26e1dffa0b12193674766d6a77c355e7ea59", "message": "Fix rename operation. testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved now passes.\nAll existing TestOFileSystem test cases now pass. Next:\n1. Rename some existing test cases to better names;\n2. Add new test cases (e.g. renaming across buckets shall fail);\n3. Clean up existing code.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "b7072fe493c9afcc7043361514bf9fd85d150b3c", "url": "https://github.com/apache/ozone/commit/b7072fe493c9afcc7043361514bf9fd85d150b3c", "message": "Fix checkstyle.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "d2f3771629c72e783c90b0821bd7c1594dd58ab1", "url": "https://github.com/apache/ozone/commit/d2f3771629c72e783c90b0821bd7c1594dd58ab1", "message": "Remove debug warn in OzoneManagerProtocolClientSideTranslatorPB.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "b075046b3dd8fde2310c2d420f100500a3b85e33", "url": "https://github.com/apache/ozone/commit/b075046b3dd8fde2310c2d420f100500a3b85e33", "message": "Clean up code.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "1e6d333e6d71df9d6f87ffda15baae113dfe3fd8", "url": "https://github.com/apache/ozone/commit/1e6d333e6d71df9d6f87ffda15baae113dfe3fd8", "message": "Rename overloaded createAdapter to checkAndCreateAdapter. Refactor a bit to make it cleaner.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "bbd0e9c3420130189962545fc87141f839fa6bdf", "url": "https://github.com/apache/ozone/commit/bbd0e9c3420130189962545fc87141f839fa6bdf", "message": "Improve URI_EXCEPTION_TEXT. https://github.com/apache/hadoop-ozone/pull/367#discussion_r359004295", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "952474cc9b22f2d7d8cf04edcd7c559ea2eb0c33", "url": "https://github.com/apache/ozone/commit/952474cc9b22f2d7d8cf04edcd7c559ea2eb0c33", "message": "Mkdir: Will now (try to) create volume and bucket if they don't exist.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "e6d11054e5ec9f23d67353777f62a2004393fc98", "url": "https://github.com/apache/ozone/commit/e6d11054e5ec9f23d67353777f62a2004393fc98", "message": "Manually applied HDDS-2777 b5008d04b896f17d4abfa5c476d79115581fcae6 into BasicOFileSystem in order to compile due to FSDataInputStream constructor change.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "ba3a21ecc2e27690e8b823bd72fa8d8976ffdb43", "url": "https://github.com/apache/ozone/commit/ba3a21ecc2e27690e8b823bd72fa8d8976ffdb43", "message": "Update getFileStatus adapter impl. since mkdir under shell invokes getFileStatus first; moved TestOFileSystem under integration-test as in HDDS-2785 28cefc6000c198726d6437db5efe4a87b766fc21.\n\nTested under shell;\nmvn clean install -Pdist -DskipTests -e -Dmaven.javadoc.skip=true -DskipShade -DskipRecon -Dmaven.test.skip=true\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\ndocker-compose up -d --scale datanode=3\ndocker-compose exec om /bin/bash\nozone fs -mkdir -p ofs://om/vol1/buc1/dir1/\n\nThe above mkdir -p command will create volume vol1, bucket buc1 then dir1.\n\nNote that mkdir without -p will fail as expected.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "6903f5bf6c59f2430ff42ef6cda2d986d6107518", "url": "https://github.com/apache/ozone/commit/6903f5bf6c59f2430ff42ef6cda2d986d6107518", "message": "Clean up code.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "f6772d760df91b639b2bd6e5ebf4f5056708b5e3", "url": "https://github.com/apache/ozone/commit/f6772d760df91b639b2bd6e5ebf4f5056708b5e3", "message": "Improve getVolumeAndBucket(): Check exception result.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "129aa9ceb6122e19f19945976bcf4029c4371d64", "url": "https://github.com/apache/ozone/commit/129aa9ceb6122e19f19945976bcf4029c4371d64", "message": "Refactor BasicOzoneClientOFSAdapterImpl to have no volume/bucket input in the constructor. All public functions in Adapter impl should take a path (includes volume & bucket information or pseudo bucket info) instead of a key.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "7c348f2aae9d48cc7d35d911e939333507e4d7ad", "url": "https://github.com/apache/ozone/commit/7c348f2aae9d48cc7d35d911e939333507e4d7ad", "message": "(1) Rename OFS class files with \"Rooted\" keyword in them;\n(2) Copy and mod OzoneClientAdapterFactory -> RootedOzoneClientAdapterFactory;\n(3) Finish refactoring AdapterImpl and RootedOzoneFileSystem.\n\nThere are failing TestRootedOzoneFileSystem tests after the refactoring, working on them now.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "33ec9d63c8796b7a7773b07d553e588b0cf065c6", "url": "https://github.com/apache/ozone/commit/33ec9d63c8796b7a7773b07d553e588b0cf065c6", "message": "Fix all failing TestRootedOzoneFileSystem tests:\n(1) listStatus and getFileStatus should be returning correct path with volume and bucket now, previously it was missing volume and bucket name in the result;\n(2) rename should be working now (fixed a typo and processKey), but delete doesn't work for now (solution known, to be fixed);\n(3) Renamed renameKey to renamePath for semantics;\n(4) OFSPath class is moved to a new independent file. It is used in tests.\n(5) Renamed listStatus param startKey -> startPath for correct semantics, and fixed the continuation.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "b3d2003117900dac4c66b12808edb13d58812883", "url": "https://github.com/apache/ozone/commit/b3d2003117900dac4c66b12808edb13d58812883", "message": "Add Apache license header to OFSPath.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "3c7b634f2a4cdd12ca36dfef84a7e849286f1603", "url": "https://github.com/apache/ozone/commit/3c7b634f2a4cdd12ca36dfef84a7e849286f1603", "message": "Clean up code. Fix checkstyle and findbugs.", "committedDate": "2020-01-10T22:11:06Z", "type": "commit"}, {"oid": "9f1539e3c99bf02aa99670f12b3e220bc33f85bc", "url": "https://github.com/apache/ozone/commit/9f1539e3c99bf02aa99670f12b3e220bc33f85bc", "message": "Fix checkstyle.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "8405b2d754a0f559ad07815379b04db6292407f5", "url": "https://github.com/apache/ozone/commit/8405b2d754a0f559ad07815379b04db6292407f5", "message": "Fix checkstyle.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "068863984077dd29d2228c6f39bbf0feb17f79ed", "url": "https://github.com/apache/ozone/commit/068863984077dd29d2228c6f39bbf0feb17f79ed", "message": "Fix recursive delete by adjusting OzoneListingIterator#processKey -> processKeyPath.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "82d39526903771e59363e13a7417505326b63fa6", "url": "https://github.com/apache/ozone/commit/82d39526903771e59363e13a7417505326b63fa6", "message": "Finish testMkdirOnNonExistentVolumeBucket.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "f24bcf4cef7c5a73e760fc299f0d23d20150917d", "url": "https://github.com/apache/ozone/commit/f24bcf4cef7c5a73e760fc299f0d23d20150917d", "message": "Merge HDDS-2188 changes to BasicRootedOzoneFileSystem and BasicRootedOzoneClientAdapterImpl due to other API changes.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "url": "https://github.com/apache/ozone/commit/2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "message": "Clean up code.", "committedDate": "2020-01-10T22:11:07Z", "type": "commit"}, {"oid": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "url": "https://github.com/apache/ozone/commit/2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "message": "Clean up code.", "committedDate": "2020-01-10T22:11:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE5NjUzMA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r367196530", "bodyText": "Can we rename this method to setVolumeAndBucket as we are setting the values here.", "author": "hanishakoneru", "createdAt": "2020-01-16T01:52:24Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,602 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ObjectStore objectStore;\n+  private OzoneVolume volume;\n+  private OzoneBucket bucket;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public void setVolume(String volumeString) throws IOException {\n+    this.volume = objectStore.getVolume(volumeString);\n+  }\n+\n+  public void setBucket(String bucketString) throws IOException {\n+    this.bucket = volume.getBucket(bucketString);\n+  }\n+\n+  private void getVolumeAndBucket(OFSPath ofsPath,\n+      boolean createIfNotExist) throws IOException {\n+    getVolumeAndBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Apply volumeStr and bucketStr stored in the object instance before\n+   * executing a FileSystem operation.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException\n+   */\n+  private void getVolumeAndBucket(String volumeStr, String bucketStr,", "originalCommit": "2c0c879ccd447ade27dbd0a2d0673d8d09d34d33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIwNTE1Mg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r367205152", "bodyText": "done", "author": "smengcl", "createdAt": "2020-01-16T02:34:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE5NjUzMA=="}], "type": "inlineReview"}, {"oid": "b932fa8c131e9821ef4dc1d41be84a23d78eb28b", "url": "https://github.com/apache/ozone/commit/b932fa8c131e9821ef4dc1d41be84a23d78eb28b", "message": "Rename getVolumeAndBucket -> setVolumeAndBucket.", "committedDate": "2020-01-16T02:34:19Z", "type": "commit"}, {"oid": "9fb74809a662efe38bba30cb280abd4df2b22b64", "url": "https://github.com/apache/ozone/commit/9fb74809a662efe38bba30cb280abd4df2b22b64", "message": "Fix checkstyle.", "committedDate": "2020-01-16T19:45:28Z", "type": "commit"}, {"oid": "f409519797b5fcc230516c25590f1d23d6074629", "url": "https://github.com/apache/ozone/commit/f409519797b5fcc230516c25590f1d23d6074629", "message": "OFSPath: Use defined OFS_MOUNT_NAME_TMP instead of hard-coded \"tmp\".", "committedDate": "2020-01-16T22:28:03Z", "type": "commit"}, {"oid": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "url": "https://github.com/apache/ozone/commit/d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "message": "Each operation will now get a separate OzoneBucket object to operate in.\nThis should make the implementation thread-safe and faster.", "committedDate": "2020-01-16T22:32:35Z", "type": "commit"}, {"oid": "7efa5504e836b8a822089cadc60f94239634ba41", "url": "https://github.com/apache/ozone/commit/7efa5504e836b8a822089cadc60f94239634ba41", "message": "Clean up code.", "committedDate": "2020-01-16T23:26:02Z", "type": "commit"}, {"oid": "e86728221247038f7e266afe3e971a98bf4dd1e6", "url": "https://github.com/apache/ozone/commit/e86728221247038f7e266afe3e971a98bf4dd1e6", "message": "Rename TestOFileSystemWithMocks -> TestRootedOzoneFileSystemWithMocks.", "committedDate": "2020-01-16T23:27:26Z", "type": "commit"}, {"oid": "47e9f8786b9be9bc5b96cf9df5f27105a491a6df", "url": "https://github.com/apache/ozone/commit/47e9f8786b9be9bc5b96cf9df5f27105a491a6df", "message": "Address checkstyle/findbugs. Clean up code.", "committedDate": "2020-01-16T23:34:18Z", "type": "commit"}, {"oid": "bfe5660e2c19de6944eba6220e51b4bcc27cb259", "url": "https://github.com/apache/ozone/commit/bfe5660e2c19de6944eba6220e51b4bcc27cb259", "message": "listKeys() now throws IOException.", "committedDate": "2020-01-16T23:37:17Z", "type": "commit"}, {"oid": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "url": "https://github.com/apache/ozone/commit/bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "message": "Rename check (same bucket policy) is now performed in both BROFS and BROCAdapterImpl. Test added.", "committedDate": "2020-01-17T18:22:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI5OTAwMA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369299000", "bodyText": "OFileSystem -> RootedOzoneFileSystem", "author": "hanishakoneru", "createdAt": "2020-01-21T23:27:47Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem", "originalCommit": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI5OTkyOQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369299929", "bodyText": "If this is going to change, can we please add a TODO here to keep track.", "author": "hanishakoneru", "createdAt": "2020-01-21T23:30:55Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:", "originalCommit": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQxNzUyOA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r370417528", "bodyText": "I forgot to clean up this line of comment. the rootPath below should be good.", "author": "smengcl", "createdAt": "2020-01-24T00:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI5OTkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwMjkxOQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369302919", "bodyText": "rootBucket is used globally in this Test suit. Can we make it a global variable?\nAlso, rootBucket gives the impression that it's the bucket at the root of the FS. Could we rename it to testBucket or volumeAndBucketPath or something else?", "author": "hanishakoneru", "createdAt": "2020-01-21T23:41:29Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // TODO: FileSystem#loadFileSystems is not loading ofs:// class by default\n+    //  hence this workaround. Might need to add some config in hadoop source.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // TODO: Address this properly.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME,\n+            conf), RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);", "originalCommit": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyMjA3NQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r370422075", "bodyText": "Yep this looks duplicated for those existing test cases modified from O3FS tests.\nThe point is there will definitely be more OFS-specific test cases (like testMkdirOnNonExistentVolumeBucket()) introduced into the this test class which won't use rootBucket.\nI would make this class global and rename it.\nDone in 7a3de51.", "author": "smengcl", "createdAt": "2020-01-24T00:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwMjkxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwNDA5NQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369304095", "bodyText": "We can add one more check later that listStatus(rootBucket) returns 1 fileStatus object - parent.", "author": "hanishakoneru", "createdAt": "2020-01-21T23:45:33Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,395 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // TODO: FileSystem#loadFileSystems is not loading ofs:// class by default\n+    //  hence this workaround. Might need to add some config in hadoop source.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // TODO: Address this properly.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME,\n+            conf), RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);", "originalCommit": "d2a01d19f0cfa2cdce5e252be2dd33fcb0d4fb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwODU1Ng==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369308556", "bodyText": "Let's test \"rename to different bucket\" in a separate method.", "author": "hanishakoneru", "createdAt": "2020-01-22T00:00:55Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwODgzMA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369308830", "bodyText": "I think we can avoid printing this here. Just a comment would suffice.", "author": "hanishakoneru", "createdAt": "2020-01-22T00:01:51Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");\n+    Path leafInTargetInAnotherBucket = new Path(bucket2, \"leaf\");\n+    try {\n+      fs.rename(leafInsideInterimPath, leafInTargetInAnotherBucket);\n+      fail(\"Should have thrown exception when renaming to a different bucket\");\n+    } catch (IOException ex) {\n+      System.out.println(\"Exception thrown as expected\");", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMwOTE3Mw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369309173", "bodyText": "getKeyInBucket looks like we are getting key from a specific bucket and not the global bucket. Can we rename this back to getKey?", "author": "hanishakoneru", "createdAt": "2020-01-22T00:03:09Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by OFileSystem\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+\n+    // For now:\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() throws IOException {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKeyInBucket(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path grandparent = new Path(rootBucket, \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKeyInBucket(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKeyInBucket(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path parent = new Path(rootBucket, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(rootBucket);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Tests Mkdir operation on a volume that doesn't exist.\n+   * Expect Mkdir to create the volume and bucket.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;\n+    for(int i = 0; i < numDirs; i++) {\n+      Path p = new Path(root, String.valueOf(i));\n+      fs.mkdirs(p);\n+      paths.add(p.getName());\n+    }\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\n+        \"Total directories listed do not match the existing directories\",\n+        numDirs, fileStatuses.length);\n+\n+    for (int i=0; i < numDirs; i++) {\n+      assertTrue(paths.contains(fileStatuses[i].getPath().getName()));\n+    }\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(rootBucket, \"dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(rootBucket, \"dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  @Test\n+  public void testNonExplicitlyCreatedPathExistsAfterItsLeafsWereRemoved()\n+      throws Exception {\n+    Path rootBucket = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path source = new Path(rootBucket, \"source\");\n+    Path interimPath = new Path(source, \"interimPath\");\n+    Path leafInsideInterimPath = new Path(interimPath, \"leaf\");\n+    Path target = new Path(rootBucket, \"target\");\n+    Path leafInTarget = new Path(target, \"leaf\");\n+\n+    fs.mkdirs(source);\n+    fs.mkdirs(target);\n+    fs.mkdirs(leafInsideInterimPath);\n+\n+    // Attempt to rename the key to a different bucket\n+    Path bucket2 = new Path(\"/\" + volumeName + \"/\" + bucketName + \"test\");\n+    Path leafInTargetInAnotherBucket = new Path(bucket2, \"leaf\");\n+    try {\n+      fs.rename(leafInsideInterimPath, leafInTargetInAnotherBucket);\n+      fail(\"Should have thrown exception when renaming to a different bucket\");\n+    } catch (IOException ex) {\n+      System.out.println(\"Exception thrown as expected\");\n+    }\n+\n+    assertTrue(fs.rename(leafInsideInterimPath, leafInTarget));\n+\n+    // after rename listStatus for interimPath should succeed and\n+    // interimPath should have no children\n+    FileStatus[] statuses = fs.listStatus(interimPath);\n+    assertNotNull(\"liststatus returns a null array\", statuses);\n+    assertEquals(\"Statuses array is not empty\", 0, statuses.length);\n+    FileStatus fileStatus = fs.getFileStatus(interimPath);\n+    assertEquals(\"FileStatus does not point to interimPath\",\n+        interimPath.getName(), fileStatus.getPath().getName());\n+  }\n+\n+  private OzoneKeyDetails getKeyInBucket(Path keyPath, boolean isDirectory)", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyNTQ4NA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369325484", "bodyText": "Do we want to convert startPath also to ofsPath here? Most likely, the startPath would only have the keyName prefix as the full path with volume and bucket is already given in pathStr.\nI am ok either way but we should add JavaDocs to explain this. Otherwise, user may assume it to be keyName prefix but we take it as volume/bucket/key or vice versa.\nIt would be good to add JavaDoc to all the methods. Atleast all the methods in OzoneClientAdapter should have a JavaDoc as they are client facing.", "author": "hanishakoneru", "createdAt": "2020-01-22T01:06:25Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,604 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  private OzoneBucket getBucket(OFSPath ofsPath,\n+      boolean createIfNotExist) throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException volEx) {\n+            if (volEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              objectStore.createVolume(volumeStr);\n+            } else {\n+              throw volEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          volume.createBucket(bucketStr);\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      bucket.createDirectory(keyStr);\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  public FileStatusAdapter getFileStatus(String path, URI uri,\n+      Path qualifiedPath, String userName)\n+      throws IOException {\n+    OFSPath ofsPath = new OFSPath(path);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_QUERY);\n+      OzoneFileStatus status = bucket.getFileStatus(key);\n+      // Note: qualifiedPath passed in is good from\n+      //  BasicRootedOzoneFileSystem#getFileStatus. No need to prepend here.\n+      makeQualified(status, uri, qualifiedPath, userName);\n+      return toFileStatusAdapter(status);\n+\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_NOT_FOUND) {\n+        throw new\n+            FileNotFoundException(key + \": No such file or directory!\");\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  public void makeQualified(FileStatus status, URI uri, Path path,\n+      String username) {\n+    if (status instanceof OzoneFileStatus) {\n+      ((OzoneFileStatus) status)\n+          .makeQualified(uri, path,\n+              username, username);\n+    }\n+  }\n+\n+  @Override\n+  public Iterator<BasicKeyInfo> listKeys(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_LIST);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    return new IteratorAdapter(bucket.listKeys(key));\n+  }\n+\n+  public List<FileStatusAdapter> listStatus(String pathStr, boolean recursive,\n+      String startPath, long numEntries, URI uri,\n+      Path workingDir, String username) throws IOException {\n+\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String keyName = ofsPath.getKeyName();\n+    OFSPath ofsStartPath = new OFSPath(startPath);\n+    String startKey = ofsStartPath.getKeyName();", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE5MTQ0MQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371191441", "bodyText": "Since I made all the input path params (pathStr) for BasicRootedOzoneClientAdapterImpl methods full path (volume+bucket+key), I believe it's best to keep startPath consistent. - That's why I intentionally renamed the parameter name in API definition from startKey to startPath.\nTotally agree that doc is required. This should've been done in o3fs BasicOzoneClientAdapterImpl as well.", "author": "smengcl", "createdAt": "2020-01-27T11:35:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyNTQ4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE5NjMyOA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371196328", "bodyText": "Done in b21005e.", "author": "smengcl", "createdAt": "2020-01-27T11:47:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyNTQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODE5NA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328194", "bodyText": "Commented variables", "author": "hanishakoneru", "createdAt": "2020-01-22T01:17:48Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODMxMA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328310", "bodyText": "OFileSystem -> RootedOzoneFileSystem", "author": "hanishakoneru", "createdAt": "2020-01-22T01:18:18Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODczNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369328737", "bodyText": "The URL can also have the path, right?", "author": "hanishakoneru", "createdAt": "2020-01-22T01:20:05Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE5NjM0Ng==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371196346", "bodyText": "Yep. Done: da12d48", "author": "smengcl", "createdAt": "2020-01-27T11:47:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMyODczNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMzMDIwMQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369330201", "bodyText": "We are assigning serviceID also to omHost variable. Can we either change the variable name to reflect this or add a comment explaining this.", "author": "hanishakoneru", "createdAt": "2020-01-22T01:26:32Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE5NzUxNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371197517", "bodyText": "Done, omHost -> omHostOrServiceId: 8e34c5f\nBTW this should be applied to o3fs as well.", "author": "smengcl", "createdAt": "2020-01-27T11:50:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTMzMDIwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5NTM5MA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369795390", "bodyText": "I am confused about pathToKey() functionality. If the input path is not an absolute path, it prepends the working dir (/user/username/) to the path. Please correct me if my understanding is wrong.\nFor o3fs, if the input key is lets say \"dir1/dir2/key1\", then pathToKey would be /user/username/dir1/dir2/key1. The volume and bucket would have been set already while creating the FS object.\nThe same example with ofs would result in volume being set as \"user\" and bucket as \"username\" and key as \"dir1/dir2/key1\". Here, we are making an assumption of the volume and bucket. Would it not be better to not allow non-absolute paths in ofs?", "author": "hanishakoneru", "createdAt": "2020-01-22T20:50:29Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg1NzI5NA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369857294", "bodyText": "This is a design decision which we can leave out in this Jira and probably add a TODO to keep track, if you agree.", "author": "hanishakoneru", "createdAt": "2020-01-22T23:14:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5NTM5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIwNTU5Mw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371205593", "bodyText": "This pathToKey is added in HDDS-177 by Anu. Honestly what I did here was fixing the typo.\nYour understanding is right. It should convert a relative path into an absolute one (if not). Then remove leading /.\nBut I think defaulting volume to user and bucket name to <USERNAME> in this case is exactly the way we intend it to be? So when user run ozone fs -put textfile.txt doc/ should automatically put the file under doc/ in user's home directory. (workingDir should be renamed to homeDir IMO) CMIIW.\nYes we are assuming that admin would create a volume named user and created a bucket for each user beforehand. That is indeed the implication. Let's discuss it.", "author": "smengcl", "createdAt": "2020-01-27T12:10:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5NTM5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMTUwNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369801507", "bodyText": "When renaming a directory, we iterate through all the keys in that directory. For each key, we are going to call adapter.renamePath(). This function would in turn get BucketInfo from OM each time. We should optimize this to avoid the extra getBucketInfo calls.", "author": "hanishakoneru", "createdAt": "2020-01-22T21:03:25Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI0NTIzNg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371245236", "bodyText": "Yep this is a problem.\nI attempted to address it in commit dfc50cc8b0f9d35aa5d9dfcea3e4beedb4930341.\nI do realize my solution still have 2 minor issues:\n\ncode duplication, e.g. overloaded renamePath/deleteObject shares mostly the same code;\nusing key in processKeyPath would be more efficient -- but this would break my intention of using ONLY full path outside of AdapterImpl.\n\nThose two issues above could be solved with some degree of refactoring. I'm doing it this way just to show one of the ways to solve the RPC call problem. Comments?", "author": "smengcl", "createdAt": "2020-01-27T13:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMTUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMTgwNA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369821804", "bodyText": "When deleting a directory, we iterate through all the keys in that directory. For each key, we are going to call adapter.deleteObject(). This function would in turn get BucketInfo from OM each time. We should optimize this to avoid the extra getBucketInfo calls.", "author": "hanishakoneru", "createdAt": "2020-01-22T21:47:40Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI0NTE5Nw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371245197", "bodyText": "Attempted to address it in commit dfc50cc8b0f9d35aa5d9dfcea3e4beedb4930341. See my comment above.", "author": "smengcl", "createdAt": "2020-01-27T13:42:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMTgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyNzkyMw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369827923", "bodyText": "Do we need to convert pathKey to ofsPath here? We make this conversion when corresponding function (rename/delete) is called in processKeyPath anyway.", "author": "hanishakoneru", "createdAt": "2020-01-22T22:01:14Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");\n+    if (!path.isAbsolute()) {\n+      path = new Path(workingDir, path);\n+    }\n+    // removing leading '/' char\n+    String key = path.toUri().getPath().substring(1);\n+    LOG.trace(\"path for key: {} is: {}\", key, path);\n+    return key;\n+  }\n+\n+  /**\n+   * Add trailing delimiter to path if it is already not present.\n+   *\n+   * @param key the ozone Key which needs to be appended\n+   * @return delimiter appended key\n+   */\n+  private String addTrailingSlashIfNeeded(String key) {\n+    if (!isEmpty(key) && !key.endsWith(OZONE_URI_DELIMITER)) {\n+      return key + OZONE_URI_DELIMITER;\n+    } else {\n+      return key;\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"RootedOzoneFileSystem{URI=\" + uri + \", \"\n+        + \"workingDir=\" + workingDir + \", \"\n+        + \"userName=\" + userName + \", \"\n+        + \"statistics=\" + statistics\n+        + \"}\";\n+  }\n+\n+  /**\n+   * This class provides an interface to iterate through all the keys in the\n+   * bucket prefixed with the input path key and process them.\n+   * <p>\n+   * Each implementing class should define how the keys should be processed\n+   * through the processKeyPath() function.\n+   */\n+  private abstract class OzoneListingIterator {\n+    private final Path path;\n+    private final FileStatus status;\n+    private String pathKey;\n+    private Iterator<BasicKeyInfo> keyIterator;\n+\n+    OzoneListingIterator(Path path)\n+        throws IOException {\n+      this.path = path;\n+      this.status = getFileStatus(path);\n+      this.pathKey = pathToKey(path);\n+      if (status.isDirectory()) {\n+        this.pathKey = addTrailingSlashIfNeeded(pathKey);\n+      }\n+      keyIterator = adapter.listKeys(pathKey);\n+    }\n+\n+    /**\n+     * The output of processKey determines if further iteration through the\n+     * keys should be done or not.\n+     *\n+     * @return true if we should continue iteration of keys, false otherwise.\n+     * @throws IOException\n+     */\n+    abstract boolean processKeyPath(String keyPath) throws IOException;\n+\n+    /**\n+     * Iterates through all the keys prefixed with the input path's key and\n+     * processes the key though processKey().\n+     * If for any key, the processKey() returns false, then the iteration is\n+     * stopped and returned with false indicating that all the keys could not\n+     * be processed successfully.\n+     *\n+     * @return true if all keys are processed successfully, false otherwise.\n+     * @throws IOException\n+     */\n+    boolean iterate() throws IOException {\n+      LOG.trace(\"Iterating path: {}\", path);\n+      if (status.isDirectory()) {\n+        LOG.trace(\"Iterating directory: {}\", pathKey);\n+        OFSPath ofsPath = new OFSPath(pathKey);", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI0ODc0Nw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371248747", "bodyText": "Yes since I intend to convert key to full path before feeding it to processKeyPath(). I could change it back but again, my idea of \"using only full path outside of AdapterImpl\" will be broken. <- this could probably be solved with refactoring but I don't think that is a priority at the moment.\nAdded comment and TODO in 10fe834.", "author": "smengcl", "createdAt": "2020-01-27T13:49:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyNzkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMDU5OA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369830598", "bodyText": "I did not understand what we are doing here. Could you please give an example.", "author": "hanishakoneru", "createdAt": "2020-01-22T22:07:27Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,805 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.CreateFlag;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathIsNotEmptyDirectoryException;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.http.client.utils.URIBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.EnumSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.fs.ozone.Constants.LISTING_PAGE_SIZE;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_DEFAULT_USER;\n+import static org.apache.hadoop.fs.ozone.Constants.OZONE_USER_DIR;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_OFS_URI_SCHEME;\n+\n+/**\n+ * The minimal Ozone Filesystem implementation.\n+ * <p>\n+ * This is a basic version which doesn't extend\n+ * KeyProviderTokenIssuer and doesn't include statistics. It can be used\n+ * from older hadoop version. For newer hadoop version use the full featured\n+ * OFileSystem.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Evolving\n+public class BasicRootedOzoneFileSystem extends FileSystem {\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneFileSystem.class);\n+\n+  /**\n+   * The Ozone client for connecting to Ozone server.\n+   */\n+\n+  private URI uri;\n+  private String userName;\n+  private Path workingDir;\n+//  private String gOmHost;\n+//  private int gOmPort;\n+//  private Configuration gConf;\n+//  private boolean gIsolatedClassloader;\n+\n+  private RootedOzoneClientAdapter adapter;\n+//  private String adapterPath;\n+\n+  private static final String URI_EXCEPTION_TEXT =\n+      \"URL should be one of the following formats: \" +\n+      \"ofs://om-service-id/  OR \" +\n+      \"ofs://om-host.example.com/  OR \" +\n+      \"ofs://om-host.example.com:5678/\";\n+\n+  @Override\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+    setConf(conf);\n+    Objects.requireNonNull(name.getScheme(), \"No scheme provided in \" + name);\n+    Preconditions.checkArgument(getScheme().equals(name.getScheme()),\n+        \"Invalid scheme provided in \" + name);\n+\n+    String authority = name.getAuthority();\n+    if (authority == null) {\n+      // authority is null when fs.defaultFS is not a qualified ofs URI and\n+      // ofs:/// is passed to the client. matcher will NPE if authority is null\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+\n+    String omHost;\n+    int omPort = -1;\n+    // Parse hostname and port\n+    String[] parts = authority.split(\":\");\n+    if (parts.length > 2) {\n+      throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+    }\n+    omHost = parts[0];\n+    if (parts.length == 2) {\n+      try {\n+        omPort = Integer.parseInt(parts[1]);\n+      } catch (NumberFormatException e) {\n+        throw new IllegalArgumentException(URI_EXCEPTION_TEXT);\n+      }\n+    }\n+\n+    try {\n+      uri = new URIBuilder().setScheme(OZONE_OFS_URI_SCHEME)\n+          .setHost(authority)\n+          .build();\n+      LOG.trace(\"Ozone URI for OFS initialization is \" + uri);\n+\n+      //isolated is the default for ozonefs-lib-legacy which includes the\n+      // /ozonefs.txt, otherwise the default is false. It could be overridden.\n+      boolean defaultValue =\n+          BasicRootedOzoneFileSystem.class.getClassLoader()\n+              .getResource(\"ozonefs.txt\") != null;\n+\n+      //Use string here instead of the constant as constant may not be available\n+      //on the classpath of a hadoop 2.7\n+      boolean isolatedClassloader =\n+          conf.getBoolean(\"ozone.fs.isolated-classloader\", defaultValue);\n+\n+      // adapter should be initialized in operations.\n+      this.adapter = createAdapter(conf, omHost, omPort, isolatedClassloader);\n+\n+      try {\n+        this.userName =\n+            UserGroupInformation.getCurrentUser().getShortUserName();\n+      } catch (IOException e) {\n+        this.userName = OZONE_DEFAULT_USER;\n+      }\n+      this.workingDir = new Path(OZONE_USER_DIR, this.userName)\n+          .makeQualified(this.uri, this.workingDir);\n+    } catch (URISyntaxException ue) {\n+      final String msg = \"Invalid Ozone endpoint \" + name;\n+      LOG.error(msg, ue);\n+      throw new IOException(msg, ue);\n+    }\n+  }\n+\n+  protected RootedOzoneClientAdapter createAdapter(Configuration conf,\n+      String omHost, int omPort, boolean isolatedClassloader)\n+      throws IOException {\n+\n+    if (isolatedClassloader) {\n+      // TODO: Check how this code path need to be changed, for legacy Hadoop?\n+      return RootedOzoneClientAdapterFactory.createAdapter();\n+    } else {\n+      return new BasicRootedOzoneClientAdapterImpl(omHost, omPort, conf);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    try {\n+      adapter.close();\n+    } finally {\n+      super.close();\n+    }\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return OZONE_OFS_URI_SCHEME;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_OPEN);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"open() path: {}\", path);\n+    final String key = pathToKey(path);\n+    return new FSDataInputStream(\n+        new OzoneFSInputStream(adapter.readFile(key), statistics));\n+  }\n+\n+  protected void incrementCounter(Statistic statistic) {\n+    //don't do anything in this default implementation.\n+  }\n+\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    LOG.trace(\"create() path:{}\", f);\n+    incrementCounter(Statistic.INVOCATION_CREATE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(f);\n+    return createOutputStream(key, overwrite, true);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream createNonRecursive(Path path,\n+      FsPermission permission,\n+      EnumSet<CreateFlag> flags,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_CREATE_NON_RECURSIVE);\n+    statistics.incrementWriteOps(1);\n+    final String key = pathToKey(path);\n+    return createOutputStream(key, flags.contains(CreateFlag.OVERWRITE), false);\n+  }\n+\n+  private FSDataOutputStream createOutputStream(String key, boolean overwrite,\n+      boolean recursive) throws IOException {\n+    return new FSDataOutputStream(adapter.createFile(key, overwrite, recursive),\n+        statistics);\n+  }\n+\n+  @Override\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new UnsupportedOperationException(\"append() Not implemented by the \"\n+        + getClass().getSimpleName() + \" FileSystem implementation\");\n+  }\n+\n+  private class RenameIterator extends OzoneListingIterator {\n+    private final String srcPath;\n+    private final String dstPath;\n+\n+    RenameIterator(Path srcPath, Path dstPath)\n+        throws IOException {\n+      super(srcPath);\n+      this.srcPath = pathToKey(srcPath);\n+      this.dstPath = pathToKey(dstPath);\n+      LOG.trace(\"rename from:{} to:{}\", this.srcPath, this.dstPath);\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      String newPath = dstPath.concat(keyPath.substring(srcPath.length()));\n+      adapter.renamePath(keyPath, newPath);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Check whether the source and destination path are valid and then perform\n+   * rename from source path to destination path.\n+   * <p>\n+   * The rename operation is performed by renaming the keys with src as prefix.\n+   * For such keys the prefix is changed from src to dst.\n+   *\n+   * @param src source path for rename\n+   * @param dst destination path for rename\n+   * @return true if rename operation succeeded or\n+   * if the src and dst have the same path and are of the same type\n+   * @throws IOException on I/O errors or if the src/dst paths are invalid.\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_RENAME);\n+    statistics.incrementWriteOps(1);\n+    if (src.equals(dst)) {\n+      return true;\n+    }\n+\n+    LOG.trace(\"rename() from: {} to: {}\", src, dst);\n+    if (src.isRoot()) {\n+      // Cannot rename root of file system\n+      LOG.trace(\"Cannot rename the root of a filesystem\");\n+      return false;\n+    }\n+\n+    // src and dst should be in the same bucket\n+    OFSPath ofsSrc = new OFSPath(src);\n+    OFSPath ofsDst = new OFSPath(dst);\n+    if (!ofsSrc.isInSameBucketAs(ofsDst)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    // Cannot rename a directory to its own subdirectory\n+    Path dstParent = dst.getParent();\n+    while (dstParent != null && !src.equals(dstParent)) {\n+      dstParent = dstParent.getParent();\n+    }\n+    Preconditions.checkArgument(dstParent == null,\n+        \"Cannot rename a directory to its own subdirectory\");\n+    // Check if the source exists\n+    FileStatus srcStatus;\n+    try {\n+      srcStatus = getFileStatus(src);\n+    } catch (FileNotFoundException fnfe) {\n+      // source doesn't exist, return\n+      return false;\n+    }\n+\n+    // Check if the destination exists\n+    FileStatus dstStatus;\n+    try {\n+      dstStatus = getFileStatus(dst);\n+    } catch (FileNotFoundException fnde) {\n+      dstStatus = null;\n+    }\n+\n+    if (dstStatus == null) {\n+      // If dst doesn't exist, check whether dst parent dir exists or not\n+      // if the parent exists, the source can still be renamed to dst path\n+      dstStatus = getFileStatus(dst.getParent());\n+      if (!dstStatus.isDirectory()) {\n+        throw new IOException(String.format(\n+            \"Failed to rename %s to %s, %s is a file\", src, dst,\n+            dst.getParent()));\n+      }\n+    } else {\n+      // if dst exists and source and destination are same,\n+      // check both the src and dst are of same type\n+      if (srcStatus.getPath().equals(dstStatus.getPath())) {\n+        return !srcStatus.isDirectory();\n+      } else if (dstStatus.isDirectory()) {\n+        // If dst is a directory, rename source as subpath of it.\n+        // for example rename /source to /dst will lead to /dst/source\n+        dst = new Path(dst, src.getName());\n+        FileStatus[] statuses;\n+        try {\n+          statuses = listStatus(dst);\n+        } catch (FileNotFoundException fnde) {\n+          statuses = null;\n+        }\n+\n+        if (statuses != null && statuses.length > 0) {\n+          // If dst exists and not a directory not empty\n+          throw new FileAlreadyExistsException(String.format(\n+              \"Failed to rename %s to %s, file already exists or not empty!\",\n+              src, dst));\n+        }\n+      } else {\n+        // If dst is not a directory\n+        throw new FileAlreadyExistsException(String.format(\n+            \"Failed to rename %s to %s, file already exists!\", src, dst));\n+      }\n+    }\n+\n+    if (srcStatus.isDirectory()) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n+        LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n+        return false;\n+      }\n+    }\n+    RenameIterator iterator = new RenameIterator(src, dst);\n+    boolean result = iterator.iterate();\n+    if (result) {\n+      createFakeParentDirectory(src);\n+    }\n+    return result;\n+  }\n+\n+  private class DeleteIterator extends OzoneListingIterator {\n+    private boolean recursive;\n+\n+    DeleteIterator(Path f, boolean recursive)\n+        throws IOException {\n+      super(f);\n+      this.recursive = recursive;\n+      if (getStatus().isDirectory()\n+          && !this.recursive\n+          && listStatus(f).length != 0) {\n+        throw new PathIsNotEmptyDirectoryException(f.toString());\n+      }\n+    }\n+\n+    @Override\n+    boolean processKeyPath(String keyPath) throws IOException {\n+      if (keyPath.equals(\"\")) {\n+        LOG.trace(\"Skipping deleting root directory\");\n+        return true;\n+      } else {\n+        LOG.trace(\"deleting key path:\" + keyPath);\n+        boolean succeed = adapter.deleteObject(keyPath);\n+        // if recursive delete is requested ignore the return value of\n+        // deleteObject and issue deletes for other keys.\n+        return recursive || succeed;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Deletes the children of the input dir path by iterating though the\n+   * DeleteIterator.\n+   *\n+   * @param f directory path to be deleted\n+   * @return true if successfully deletes all required keys, false otherwise\n+   * @throws IOException\n+   */\n+  private boolean innerDelete(Path f, boolean recursive) throws IOException {\n+    LOG.trace(\"delete() path:{} recursive:{}\", f, recursive);\n+    try {\n+      DeleteIterator iterator = new DeleteIterator(f, recursive);\n+      return iterator.iterate();\n+    } catch (FileNotFoundException e) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Couldn't delete {} - does not exist\", f);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_DELETE);\n+    statistics.incrementWriteOps(1);\n+    LOG.debug(\"Delete path {} - recursive {}\", f, recursive);\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException ex) {\n+      LOG.warn(\"delete: Path does not exist: {}\", f);\n+      return false;\n+    }\n+\n+    String key = pathToKey(f);\n+    boolean result;\n+\n+    if (status.isDirectory()) {\n+      LOG.debug(\"delete: Path is a directory: {}\", f);\n+      key = addTrailingSlashIfNeeded(key);\n+\n+      if (key.equals(\"/\")) {\n+        LOG.warn(\"Cannot delete root directory.\");\n+        return false;\n+      }\n+\n+      result = innerDelete(f, recursive);\n+    } else {\n+      LOG.debug(\"delete: Path is a file: {}\", f);\n+      result = adapter.deleteObject(key);\n+    }\n+\n+    if (result) {\n+      // If this delete operation removes all files/directories from the\n+      // parent direcotry, then an empty parent directory must be created.\n+      createFakeParentDirectory(f);\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Create a fake parent directory key if it does not already exist and no\n+   * other child of this parent directory exists.\n+   *\n+   * @param f path to the fake parent directory\n+   * @throws IOException\n+   */\n+  private void createFakeParentDirectory(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !parent.isRoot()) {\n+      createFakeDirectoryIfNecessary(parent);\n+    }\n+  }\n+\n+  /**\n+   * Create a fake directory key if it does not already exist.\n+   *\n+   * @param f path to the fake directory\n+   * @throws IOException\n+   */\n+  private void createFakeDirectoryIfNecessary(Path f) throws IOException {\n+    String key = pathToKey(f);\n+    if (!key.isEmpty() && !o3Exists(f)) {\n+      LOG.debug(\"Creating new fake directory at {}\", f);\n+      String dirKey = addTrailingSlashIfNeeded(key);\n+      adapter.createDirectory(dirKey);\n+    }\n+  }\n+\n+  /**\n+   * Check if a file or directory exists corresponding to given path.\n+   *\n+   * @param f path to file/directory.\n+   * @return true if it exists, false otherwise.\n+   * @throws IOException\n+   */\n+  private boolean o3Exists(final Path f) throws IOException {\n+    Path path = makeQualified(f);\n+    try {\n+      getFileStatus(path);\n+      return true;\n+    } catch (FileNotFoundException ex) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_LIST_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"listStatus() path:{}\", f);\n+    int numEntries = LISTING_PAGE_SIZE;\n+    LinkedList<FileStatus> statuses = new LinkedList<>();\n+    List<FileStatus> tmpStatusList;\n+    String startPath = \"\";\n+\n+    do {\n+      tmpStatusList =\n+          adapter.listStatus(pathToKey(f), false, startPath,\n+              numEntries, uri, workingDir, getUsername())\n+              .stream()\n+              .map(this::convertFileStatus)\n+              .collect(Collectors.toList());\n+\n+      if (!tmpStatusList.isEmpty()) {\n+        if (startPath.isEmpty()) {\n+          statuses.addAll(tmpStatusList);\n+        } else {\n+          statuses.addAll(tmpStatusList.subList(1, tmpStatusList.size()));\n+        }\n+        startPath = pathToKey(statuses.getLast().getPath());\n+      }\n+      // listStatus returns entries numEntries in size if available.\n+      // Any lesser number of entries indicate that the required entries have\n+      // exhausted.\n+    } while (tmpStatusList.size() == numEntries);\n+\n+    return statuses.toArray(new FileStatus[0]);\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public Token<?> getDelegationToken(String renewer) throws IOException {\n+    return adapter.getDelegationToken(renewer);\n+  }\n+\n+  /**\n+   * Get a canonical service name for this file system. If the URI is logical,\n+   * the hostname part of the URI will be returned.\n+   *\n+   * @return a service string that uniquely identifies this file system.\n+   */\n+  @Override\n+  public String getCanonicalServiceName() {\n+    return adapter.getCanonicalServiceName();\n+  }\n+\n+  /**\n+   * Get the username of the FS.\n+   *\n+   * @return the short name of the user who instantiated the FS\n+   */\n+  public String getUsername() {\n+    return userName;\n+  }\n+\n+  /**\n+   * Creates a directory. Directory is represented using a key with no value.\n+   *\n+   * @param path directory path to be created\n+   * @return true if directory exists or created successfully.\n+   * @throws IOException\n+   */\n+  private boolean mkdir(Path path) throws IOException {\n+    return adapter.createDirectory(pathToKey(path));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    LOG.trace(\"mkdir() path:{} \", f);\n+    String key = pathToKey(f);\n+    if (isEmpty(key)) {\n+      return false;\n+    }\n+    return mkdir(f);\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    incrementCounter(Statistic.INVOCATION_GET_FILE_STATUS);\n+    statistics.incrementReadOps(1);\n+    LOG.trace(\"getFileStatus() path:{}\", f);\n+    Path qualifiedPath = f.makeQualified(uri, workingDir);\n+    String key = pathToKey(qualifiedPath);\n+    FileStatus fileStatus = null;\n+    try {\n+      fileStatus = convertFileStatus(\n+          adapter.getFileStatus(key, uri, qualifiedPath, getUsername()));\n+    } catch (OMException ex) {\n+      if (ex.getResult().equals(OMException.ResultCodes.KEY_NOT_FOUND)) {\n+        throw new FileNotFoundException(\"File not found. path:\" + f);\n+      }\n+    }\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus fileStatus,\n+      long start, long len)\n+      throws IOException {\n+    if (fileStatus instanceof LocatedFileStatus) {\n+      return ((LocatedFileStatus) fileStatus).getBlockLocations();\n+    } else {\n+      return super.getFileBlockLocations(fileStatus, start, len);\n+    }\n+  }\n+\n+  /**\n+   * Turn a path (relative or otherwise) into an Ozone key.\n+   *\n+   * @param path the path of the file.\n+   * @return the key of the object that represents the file.\n+   */\n+  public String pathToKey(Path path) {\n+    Objects.requireNonNull(path, \"Path can't be null!\");\n+    if (!path.isAbsolute()) {\n+      path = new Path(workingDir, path);\n+    }\n+    // removing leading '/' char\n+    String key = path.toUri().getPath().substring(1);\n+    LOG.trace(\"path for key: {} is: {}\", key, path);\n+    return key;\n+  }\n+\n+  /**\n+   * Add trailing delimiter to path if it is already not present.\n+   *\n+   * @param key the ozone Key which needs to be appended\n+   * @return delimiter appended key\n+   */\n+  private String addTrailingSlashIfNeeded(String key) {\n+    if (!isEmpty(key) && !key.endsWith(OZONE_URI_DELIMITER)) {\n+      return key + OZONE_URI_DELIMITER;\n+    } else {\n+      return key;\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"RootedOzoneFileSystem{URI=\" + uri + \", \"\n+        + \"workingDir=\" + workingDir + \", \"\n+        + \"userName=\" + userName + \", \"\n+        + \"statistics=\" + statistics\n+        + \"}\";\n+  }\n+\n+  /**\n+   * This class provides an interface to iterate through all the keys in the\n+   * bucket prefixed with the input path key and process them.\n+   * <p>\n+   * Each implementing class should define how the keys should be processed\n+   * through the processKeyPath() function.\n+   */\n+  private abstract class OzoneListingIterator {\n+    private final Path path;\n+    private final FileStatus status;\n+    private String pathKey;\n+    private Iterator<BasicKeyInfo> keyIterator;\n+\n+    OzoneListingIterator(Path path)\n+        throws IOException {\n+      this.path = path;\n+      this.status = getFileStatus(path);\n+      this.pathKey = pathToKey(path);\n+      if (status.isDirectory()) {\n+        this.pathKey = addTrailingSlashIfNeeded(pathKey);\n+      }\n+      keyIterator = adapter.listKeys(pathKey);\n+    }\n+\n+    /**\n+     * The output of processKey determines if further iteration through the\n+     * keys should be done or not.\n+     *\n+     * @return true if we should continue iteration of keys, false otherwise.\n+     * @throws IOException\n+     */\n+    abstract boolean processKeyPath(String keyPath) throws IOException;\n+\n+    /**\n+     * Iterates through all the keys prefixed with the input path's key and\n+     * processes the key though processKey().\n+     * If for any key, the processKey() returns false, then the iteration is\n+     * stopped and returned with false indicating that all the keys could not\n+     * be processed successfully.\n+     *\n+     * @return true if all keys are processed successfully, false otherwise.\n+     * @throws IOException\n+     */\n+    boolean iterate() throws IOException {\n+      LOG.trace(\"Iterating path: {}\", path);\n+      if (status.isDirectory()) {\n+        LOG.trace(\"Iterating directory: {}\", pathKey);\n+        OFSPath ofsPath = new OFSPath(pathKey);\n+        while (keyIterator.hasNext()) {\n+          BasicKeyInfo key = keyIterator.next();\n+          String keyPath = ofsPath.getNonKeyPathNoPrefixDelim() +\n+              OZONE_URI_DELIMITER + key.getName();\n+          LOG.trace(\"iterating key path: {}\", keyPath);\n+          if (!processKeyPath(keyPath)) {\n+            return false;\n+          }\n+        }\n+        return true;\n+      } else {\n+        LOG.trace(\"iterating file: {}\", path);\n+        return processKeyPath(pathKey);\n+      }\n+    }\n+\n+    String getPathKey() {\n+      return pathKey;\n+    }\n+\n+    boolean pathIsDirectory() {\n+      return status.isDirectory();\n+    }\n+\n+    FileStatus getStatus() {\n+      return status;\n+    }\n+  }\n+\n+  public RootedOzoneClientAdapter getAdapter() {\n+    return adapter;\n+  }\n+\n+  public boolean isEmpty(CharSequence cs) {\n+    return cs == null || cs.length() == 0;\n+  }\n+\n+  public boolean isNumber(String number) {\n+    try {\n+      Integer.parseInt(number);\n+    } catch (NumberFormatException ex) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private FileStatus convertFileStatus(FileStatusAdapter fileStatusAdapter) {\n+    Path symLink = null;\n+    try {\n+      fileStatusAdapter.getSymlink();\n+    } catch (Exception ex) {\n+      //NOOP: If not symlink symlink remains null.\n+    }\n+\n+    // Process path.\n+    URI newUri = fileStatusAdapter.getPath().toUri();\n+    try {\n+      newUri = new URIBuilder().setScheme(newUri.getScheme())\n+          .setHost(newUri.getAuthority())\n+          .setPath(newUri.getPath())\n+          .build();\n+    } catch (URISyntaxException e) {\n+    }", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI1MzEyOA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371253128", "bodyText": "This chunk of code is useless now. It was previously added for stitching the listStatus result. Just removed it.", "author": "smengcl", "createdAt": "2020-01-27T13:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMDU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMjMzNg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369832336", "bodyText": "Can you please add comments explaining how vol, bucket and mounts are used.", "author": "hanishakoneru", "createdAt": "2020-01-22T22:11:16Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OFSPath.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.yetus.audience.InterfaceStability;\n+import java.util.StringTokenizer;\n+\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+\n+/**\n+ * Utility class for Rooted Ozone Filesystem (OFS) path processing.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class OFSPath {\n+  private String volumeName;\n+  private String bucketName;\n+  private String mountName;\n+  private String keyName;", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MjY1Ng==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371262656", "bodyText": "I added a table for illustration: 4bb7ed1\nNote the table might change in the future. This just shows what it does at the moment.", "author": "smengcl", "createdAt": "2020-01-27T14:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzMjMzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzODI5NA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r369838294", "bodyText": "testFSUriHostVersionDefault() and testFSUriWithHostPortUnspecified() need to be modified to test ofs.", "author": "hanishakoneru", "createdAt": "2020-01-22T22:24:31Z", "path": "hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystemWithMocks.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+import java.net.URI;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Ozone File system tests that are light weight and use mocks.\n+ */\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({ OzoneClientFactory.class, UserGroupInformation.class })\n+@PowerMockIgnore(\"javax.management.*\")\n+public class TestRootedOzoneFileSystemWithMocks {\n+\n+  @Test\n+  public void testFSUriWithHostPortOverrides() throws Exception {\n+    Configuration conf = new OzoneConfiguration();\n+    OzoneClient ozoneClient = mock(OzoneClient.class);\n+    ObjectStore objectStore = mock(ObjectStore.class);\n+    OzoneVolume volume = mock(OzoneVolume.class);\n+    OzoneBucket bucket = mock(OzoneBucket.class);\n+\n+    when(ozoneClient.getObjectStore()).thenReturn(objectStore);\n+    when(objectStore.getVolume(eq(\"volume1\"))).thenReturn(volume);\n+    when(volume.getBucket(\"bucket1\")).thenReturn(bucket);\n+\n+    PowerMockito.mockStatic(OzoneClientFactory.class);\n+    PowerMockito.when(OzoneClientFactory.getRpcClient(eq(\"local.host\"),\n+        eq(5899), eq(conf))).thenReturn(ozoneClient);\n+\n+    UserGroupInformation ugi = mock(UserGroupInformation.class);\n+    PowerMockito.mockStatic(UserGroupInformation.class);\n+    PowerMockito.when(UserGroupInformation.getCurrentUser()).thenReturn(ugi);\n+    when(ugi.getShortUserName()).thenReturn(\"user1\");\n+\n+    // Note: FileSystem#loadFileSystems doesn't load OFS class because\n+    //  META-INF still points to org.apache.hadoop.fs.ozone.OzoneFileSystem\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    URI uri = new URI(\"ofs://local.host:5899/volume1/bucket1\");\n+\n+    FileSystem fileSystem = FileSystem.get(uri, conf);\n+    RootedOzoneFileSystem ofs = (RootedOzoneFileSystem) fileSystem;\n+\n+    assertEquals(ofs.getUri().getAuthority(), \"local.host:5899\");\n+    PowerMockito.verifyStatic();\n+    OzoneClientFactory.getRpcClient(\"local.host\", 5899, conf);\n+  }\n+\n+  @Test\n+  public void testFSUriWithHostPortUnspecified() throws Exception {\n+    Configuration conf = new OzoneConfiguration();\n+    final int omPort = OmUtils.getOmRpcPort(conf);\n+\n+    OzoneClient ozoneClient = mock(OzoneClient.class);\n+    ObjectStore objectStore = mock(ObjectStore.class);\n+    OzoneVolume volume = mock(OzoneVolume.class);\n+    OzoneBucket bucket = mock(OzoneBucket.class);\n+\n+    when(ozoneClient.getObjectStore()).thenReturn(objectStore);\n+    when(objectStore.getVolume(eq(\"volume1\"))).thenReturn(volume);\n+    when(volume.getBucket(\"bucket1\")).thenReturn(bucket);\n+\n+    PowerMockito.mockStatic(OzoneClientFactory.class);\n+    PowerMockito.when(OzoneClientFactory.getRpcClient(eq(\"local.host\"),\n+        eq(omPort), eq(conf))).thenReturn(ozoneClient);\n+\n+    UserGroupInformation ugi = mock(UserGroupInformation.class);\n+    PowerMockito.mockStatic(UserGroupInformation.class);\n+    PowerMockito.when(UserGroupInformation.getCurrentUser()).thenReturn(ugi);\n+    when(ugi.getShortUserName()).thenReturn(\"user1\");\n+\n+    URI uri = new URI(\"o3fs://bucket1.volume1.local.host\");\n+", "originalCommit": "bdffa6ab5c1f59cedc968bf36c2ccd286cc6288f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI3NTE2Ng==", "url": "https://github.com/apache/ozone/pull/415#discussion_r371275166", "bodyText": "Done in 5a20f9d.", "author": "smengcl", "createdAt": "2020-01-27T14:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgzODI5NA=="}], "type": "inlineReview"}, {"oid": "ab64a98fadd52b483136f99031d8ed9d401b36c9", "url": "https://github.com/apache/ozone/commit/ab64a98fadd52b483136f99031d8ed9d401b36c9", "message": "Ignore potential VOLUME_ALREADY_EXISTS and BUCKET_ALREADY_EXISTS in getBucket(), addressing comment https://github.com/apache/hadoop-ozone/pull/415#commitcomment-36906753", "committedDate": "2020-01-23T23:56:36Z", "type": "commit"}, {"oid": "7a3de51e6e23abba3e91971f80e8666c1fc6c108", "url": "https://github.com/apache/ozone/commit/7a3de51e6e23abba3e91971f80e8666c1fc6c108", "message": "Make testBucketPath class global, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369302919", "committedDate": "2020-01-24T00:18:01Z", "type": "commit"}, {"oid": "f3f6d459b8bfe0435cfee20cab3e7e25263db1ee", "url": "https://github.com/apache/ozone/commit/f3f6d459b8bfe0435cfee20cab3e7e25263db1ee", "message": "Clean up OFSPath.", "committedDate": "2020-01-27T08:12:46Z", "type": "commit"}, {"oid": "c323b6cb8051907bf6e2c456191eb07aafc4d860", "url": "https://github.com/apache/ozone/commit/c323b6cb8051907bf6e2c456191eb07aafc4d860", "message": "Improved testListStatus(), addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369304095", "committedDate": "2020-01-27T11:22:13Z", "type": "commit"}, {"oid": "11ac3a9c4c4118005017dd5c1c1cbe59ffbfd4e3", "url": "https://github.com/apache/ozone/commit/11ac3a9c4c4118005017dd5c1c1cbe59ffbfd4e3", "message": "Move rename to different bucket test case to a new unit test `testRenameToDifferentBucket()`, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369308556", "committedDate": "2020-01-27T11:26:31Z", "type": "commit"}, {"oid": "ae95babbefcb67acd9e33a736600086348fa3598", "url": "https://github.com/apache/ozone/commit/ae95babbefcb67acd9e33a736600086348fa3598", "message": "Remove print from testRenameToDifferentBucket success, addressing https://github.com/apache/hadoop-ozone/pull/415#discussion_r369308830", "committedDate": "2020-01-27T11:28:05Z", "type": "commit"}, {"oid": "f6001f660dc7884dc0aa14efaf4aa5a84d41784a", "url": "https://github.com/apache/ozone/commit/f6001f660dc7884dc0aa14efaf4aa5a84d41784a", "message": "TROFS: Rename getKeyInBucket -> getKey. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369309173", "committedDate": "2020-01-27T11:29:12Z", "type": "commit"}, {"oid": "b21005ebacf25f5a3a6bfdb97348a2e2e567423d", "url": "https://github.com/apache/ozone/commit/b21005ebacf25f5a3a6bfdb97348a2e2e567423d", "message": "Add comment for BROCAI#listStatus. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369325484", "committedDate": "2020-01-27T11:44:51Z", "type": "commit"}, {"oid": "da12d487fe5c5bb05f013c33cb2d02aba96b63c3", "url": "https://github.com/apache/ozone/commit/da12d487fe5c5bb05f013c33cb2d02aba96b63c3", "message": "Clean up code. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369328737", "committedDate": "2020-01-27T11:47:29Z", "type": "commit"}, {"oid": "8e34c5f6718680c35b683ca4b0969286e975e318", "url": "https://github.com/apache/ozone/commit/8e34c5f6718680c35b683ca4b0969286e975e318", "message": "omHost -> omHostOrServiceId. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369330201", "committedDate": "2020-01-27T11:50:20Z", "type": "commit"}, {"oid": "881e9a49d7af59821dea1161a7db1da91d38231b", "url": "https://github.com/apache/ozone/commit/881e9a49d7af59821dea1161a7db1da91d38231b", "message": "Reduce # of RPC calls in rename and delete iterator. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369801507 and https://github.com/apache/hadoop-ozone/pull/415#discussion_r369821804", "committedDate": "2020-01-27T13:42:24Z", "type": "commit"}, {"oid": "10fe834c08cf72911b6d0409fb1138ff7d23294e", "url": "https://github.com/apache/ozone/commit/10fe834c08cf72911b6d0409fb1138ff7d23294e", "message": "Add comment&TODO for OzoneListingIterator key to full path conversion. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369827923", "committedDate": "2020-01-27T13:49:24Z", "type": "commit"}, {"oid": "fa73d519e68e1dff5b98ad40bb7a69d9eb697efb", "url": "https://github.com/apache/ozone/commit/fa73d519e68e1dff5b98ad40bb7a69d9eb697efb", "message": "Remove useless code in convertFileStatus. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369830598", "committedDate": "2020-01-27T13:58:29Z", "type": "commit"}, {"oid": "4bb7ed19674dcb959703fe633e940ab4bf17873e", "url": "https://github.com/apache/ozone/commit/4bb7ed19674dcb959703fe633e940ab4bf17873e", "message": "Add a table for OFSPath. https://github.com/apache/hadoop-ozone/pull/415#discussion_r369832336", "committedDate": "2020-01-27T14:14:21Z", "type": "commit"}, {"oid": "5a20f9da30a25fd40fa1a73e6b0e742f40647a94", "url": "https://github.com/apache/ozone/commit/5a20f9da30a25fd40fa1a73e6b0e742f40647a94", "message": "Remove vol/buc path from testFSUriWithHostPortOverrides.\nMod testFSUriWithHostPortUnspecified to test OFS.\nRemove testFSUriHostVersionDefault as omitting host name is not allowed in OFS.\nhttps://github.com/apache/hadoop-ozone/pull/415#discussion_r369838294", "committedDate": "2020-01-27T14:35:22Z", "type": "commit"}, {"oid": "3c9727dbfc6221b8e44decef56f836af78eeef4e", "url": "https://github.com/apache/ozone/commit/3c9727dbfc6221b8e44decef56f836af78eeef4e", "message": "Unused imports - address checkstyle.", "committedDate": "2020-01-27T14:39:45Z", "type": "commit"}, {"oid": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "url": "https://github.com/apache/ozone/commit/60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "message": "Fix a case where `mkdir -p /volume1` or `mkdir -p /volume1/bucket2` doesn't work: https://github.com/apache/hadoop-ozone/pull/415#issuecomment-578954095\nAdd a few more mkdir test cases for above.\nAdd TestOFSPath.", "committedDate": "2020-01-28T02:34:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMTU3OA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372001578", "bodyText": "The test cases are very similar to TestOzoneFileSystem, most of the difference are in setup.\nCan we refactor to have a base class that contains all the common test methods and leave only the setup in the subclass for different file system? (This is the approach taken in the contract test to avoid duplicate code)", "author": "xiaoyuyao", "createdAt": "2020-01-28T19:12:42Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,477 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ */\n+public class TestRootedOzoneFileSystem {", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA5NTM2OQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372095369", "bodyText": "Indeed most code are the same, but not just in the setup. In quite a few existing test cases I changed some subtle variable to make it run on o3fs, meaning we likely would need to change o3fs's unit test a bit as well (potentially more conflicts when merging this branch with trunk). I would suggest adding a TODO for this class and do the refactoring at a later point. What do you say?", "author": "smengcl", "createdAt": "2020-01-28T22:30:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU0MzU4Nw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372543587", "bodyText": "Added TODO.", "author": "smengcl", "createdAt": "2020-01-29T18:05:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMTU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNTUyNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372015527", "bodyText": "should we do getVolumeDetails/create first and if that fails, there is no need to create bucket.\nAs createVolume is an admin only operation, I would expect most of the client driven operation will fail here.", "author": "xiaoyuyao", "createdAt": "2020-01-28T19:40:12Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA2MjkxOA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372062918", "bodyText": "@xiaoyuyao Actually this part was coded previously that it checks if the volume exists first. Then @bharatviswa504 argued that this way we would always have at least two round trips for each getBucket() (first getVolumeDetails, then getBucketDetails) regardless of whether the bucket already exists. This change is made in d2a01d1#diff-1982358804d0a7b88f575ecd1e3b3013R202", "author": "smengcl", "createdAt": "2020-01-28T21:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNTUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNzEyNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372017127", "bodyText": "Even the volume creation succeed, the bucket creation might fail if the ACLs are not set properly when security and acls are enabled. With o3fs, this is OK because the volume/bucket are provisioned and acls setup before it is mounted. With ofs, this may not work without ACL changes.", "author": "xiaoyuyao", "createdAt": "2020-01-28T19:43:20Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU0NzI1MA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372547250", "bodyText": "As we discussed, currently in Ozone createVolume will set owner to current user when volArg owner is null: https://github.com/smengcl/hadoop-ozone/blob/046a06f02783da516179ee8d8d1bed862d22f78d/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/RpcClient.java#L268\nCan we resolve this one?", "author": "smengcl", "createdAt": "2020-01-29T18:13:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNzEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNzcyMQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372017721", "bodyText": "what is permission denied, do we recover the environment by delete the volume?", "author": "xiaoyuyao", "createdAt": "2020-01-28T19:44:29Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNjgyNw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372026827", "bodyText": "In the /tmp case, the bucketName length is also 0? do we want to create a /tmp volume here?", "author": "xiaoyuyao", "createdAt": "2020-01-28T20:02:39Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU0OTYxNg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372549616", "bodyText": "Done in e16a52e.", "author": "smengcl", "createdAt": "2020-01-29T18:18:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNjgyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNzA3OQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372027079", "bodyText": "what if keyStr is empty(length 0) should we check that before getBucket?", "author": "xiaoyuyao", "createdAt": "2020-01-28T20:03:10Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1MjE3Ng==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372552176", "bodyText": "Actually here we need to put the keyStr.length() check after getBucket, since getBucket need to create volume and bucket if they don't exist. If we check it before try clause it wouldn't create the volume and bucket then.", "author": "smengcl", "createdAt": "2020-01-29T18:23:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNzA3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1MjkyMQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372552921", "bodyText": "Comment updated in c0e2ed7.", "author": "smengcl", "createdAt": "2020-01-29T18:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyNzA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372028254", "bodyText": "check keyName is empty?", "author": "xiaoyuyao", "createdAt": "2020-01-28T20:05:49Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      // if keyStr is empty, it indicates that only volume or volume+bucket is\n+      // given in pathStr, so getBucket() above should've handled the creation\n+      // of volume/bucket already.\n+      if (keyStr != null && keyStr.length() > 0) {\n+        bucket.createDirectory(keyStr);\n+      }\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODU5OA==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372028598", "bodyText": "Do we support delete volume/bucket via OFS?", "author": "xiaoyuyao", "createdAt": "2020-01-28T20:06:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1NTM3NQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372555375", "bodyText": "keyName check done in ac0cd77.", "author": "smengcl", "createdAt": "2020-01-29T18:29:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1NjAxOQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372556019", "bodyText": "On supporting delete volume/bucket though, this is a good question. I'd argue it is weird to support delete volume/bucket via rm interface but from a flat filesystem point of view it should be a feature. What's you opinion? @arp7", "author": "smengcl", "createdAt": "2020-01-29T18:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc1Mjk4Mg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372752982", "bodyText": "Let's follow up that on separate JIRA.", "author": "xiaoyuyao", "createdAt": "2020-01-30T04:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEzODE4MQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r373138181", "bodyText": "Agree", "author": "smengcl", "createdAt": "2020-01-30T19:08:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyODI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyOTQ4Mw==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372029483", "bodyText": "should we move the counter incr before getBucket so that we can accurately count the ops even though getBucket may fail?", "author": "xiaoyuyao", "createdAt": "2020-01-28T20:08:42Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+\n+    // Check path and newPathName are in the same volume and same bucket.\n+    // This should have been checked in BasicRootedOzoneFileSystem#rename\n+    // already via regular call path unless bypassed.\n+    if (!ofsPath.isInSameBucketAs(ofsNewPath)) {\n+      throw new IOException(\"Cannot rename a key to a different bucket\");\n+    }\n+\n+    OzoneBucket bucket = getBucket(ofsPath, false);\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Existing key path.\n+   * @param newPath New key path.\n+   * @throws IOException IOException from bucket.renameKey().\n+   */\n+  void renamePath(OzoneBucket bucket, String path, String newPath)\n+      throws IOException {\n+    incrementCounter(Statistic.OBJECTS_RENAMED);\n+    OFSPath ofsPath = new OFSPath(path);\n+    OFSPath ofsNewPath = new OFSPath(newPath);\n+    // No same-bucket policy check here since this call path is controlled\n+    String key = ofsPath.getKeyName();\n+    String newKey = ofsNewPath.getKeyName();\n+    bucket.renameKey(key, newKey);\n+  }\n+\n+  /**\n+   * Helper method to create an directory specified by key name in bucket.\n+   *\n+   * @param pathStr path to be created as directory\n+   * @return true if the key is created, false otherwise\n+   */\n+  @Override\n+  public boolean createDirectory(String pathStr) throws IOException {\n+    LOG.trace(\"creating dir for path: {}\", pathStr);\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    // Volume name unspecified, return failure\n+    if (ofsPath.getVolumeName().length() == 0) {\n+      return false;\n+    }\n+    // Handle where only volume is specified in pathStr\n+    if (ofsPath.getBucketName().length() == 0) {\n+      objectStore.createVolume(ofsPath.getVolumeName());\n+      return true;\n+    }\n+    String keyStr = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, true);\n+      // if keyStr is empty, it indicates that only volume or volume+bucket is\n+      // given in pathStr, so getBucket() above should've handled the creation\n+      // of volume/bucket already.\n+      if (keyStr != null && keyStr.length() > 0) {\n+        bucket.createDirectory(keyStr);\n+      }\n+    } catch (OMException e) {\n+      if (e.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS) {\n+        throw new FileAlreadyExistsException(e.getMessage());\n+      }\n+      throw e;\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Helper method to delete an object specified by key name in bucket.\n+   *\n+   * @param path path to a key to be deleted\n+   * @return true if the key is deleted, false otherwise\n+   */\n+  @Override\n+  public boolean deleteObject(String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  /**\n+   * Package-private helper function to reduce calls to getBucket().\n+   * @param bucket Bucket to operate in.\n+   * @param path Path to delete.\n+   * @return true if operation succeeded, false upon IOException.\n+   */\n+  boolean deleteObject(OzoneBucket bucket, String path) {\n+    LOG.trace(\"issuing delete for path to key: {}\", path);\n+    OFSPath ofsPath = new OFSPath(path);\n+    String keyName = ofsPath.getKeyName();\n+    try {\n+      incrementCounter(Statistic.OBJECTS_DELETED);\n+      bucket.deleteKey(keyName);\n+      return true;\n+    } catch (IOException ioe) {\n+      LOG.error(\"delete key failed \" + ioe.getMessage());\n+      return false;\n+    }\n+  }\n+\n+  public FileStatusAdapter getFileStatus(String path, URI uri,\n+      Path qualifiedPath, String userName)\n+      throws IOException {\n+    OFSPath ofsPath = new OFSPath(path);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      incrementCounter(Statistic.OBJECTS_QUERY);", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1NjMwMg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372556302", "bodyText": "Done in a9ef567", "author": "smengcl", "createdAt": "2020-01-29T18:31:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAyOTQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA3NDk5NQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372074995", "bodyText": "How is the /tempVol/tempBucket created? Do we provide a CLI to admin?", "author": "xiaoyuyao", "createdAt": "2020-01-28T21:44:12Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OFSPath.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.yetus.audience.InterfaceStability;\n+import java.util.StringTokenizer;\n+\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_URI_DELIMITER;\n+\n+/**\n+ * Utility class for Rooted Ozone Filesystem (OFS) path processing.\n+ */\n+@InterfaceAudience.Private\n+@InterfaceStability.Unstable\n+class OFSPath {\n+  /**\n+   * Here is a table illustrating what each name variable is given an input path\n+   * Assuming /tmp is mounted to /tempVol/tempBucket", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA5NDM0Mg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372094342", "bodyText": "This is undecided. Yes we could give admin a CLI command to do the initialization. But we might want to leave this for jira HDDS-2929.", "author": "smengcl", "createdAt": "2020-01-28T22:27:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA3NDk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEwNTA5MQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372105091", "bodyText": "Let's change renamePath to rename", "author": "xiaoyuyao", "createdAt": "2020-01-28T22:55:35Z", "path": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicRootedOzoneClientAdapterImpl.java", "diffHunk": "@@ -0,0 +1,694 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ozone;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import org.apache.commons.collections.CollectionUtils;\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.crypto.key.KeyProvider;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdds.client.ReplicationFactor;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ozone.OmUtils;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClient;\n+import org.apache.hadoop.ozone.client.OzoneClientFactory;\n+import org.apache.hadoop.ozone.client.OzoneKey;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.ozone.client.io.OzoneOutputStream;\n+import org.apache.hadoop.ozone.client.protocol.ClientProtocol;\n+import org.apache.hadoop.ozone.om.exceptions.OMException;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfo;\n+import org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup;\n+import org.apache.hadoop.ozone.om.helpers.OzoneFileStatus;\n+import org.apache.hadoop.ozone.security.OzoneTokenIdentifier;\n+import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.security.token.TokenRenewer;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_ALREADY_EXISTS;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .VOLUME_NOT_FOUND;\n+import static org.apache.hadoop.ozone.om.exceptions.OMException.ResultCodes\n+    .BUCKET_NOT_FOUND;\n+\n+/**\n+ * Basic Implementation of the OzoneFileSystem calls.\n+ * <p>\n+ * This is the minimal version which doesn't include any statistics.\n+ * <p>\n+ * For full featured version use OzoneClientAdapterImpl.\n+ */\n+public class BasicRootedOzoneClientAdapterImpl\n+    implements RootedOzoneClientAdapter {\n+\n+  static final Logger LOG =\n+      LoggerFactory.getLogger(BasicRootedOzoneClientAdapterImpl.class);\n+\n+  private OzoneClient ozoneClient;\n+  private ClientProtocol proxy;\n+  private ObjectStore objectStore;\n+  private ReplicationType replicationType;\n+  private ReplicationFactor replicationFactor;\n+  private boolean securityEnabled;\n+  private int configuredDnPort;\n+\n+  /**\n+   * Create new OzoneClientAdapter implementation.\n+   *\n+   * @throws IOException In case of a problem.\n+   */\n+  public BasicRootedOzoneClientAdapterImpl() throws IOException {\n+    this(createConf());\n+  }\n+\n+  private static OzoneConfiguration createConf() {\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+    try {\n+      return new OzoneConfiguration();\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(OzoneConfiguration conf)\n+      throws IOException {\n+    this(null, -1, conf);\n+  }\n+\n+  public BasicRootedOzoneClientAdapterImpl(String omHost, int omPort,\n+      Configuration hadoopConf) throws IOException {\n+\n+    ClassLoader contextClassLoader =\n+        Thread.currentThread().getContextClassLoader();\n+    Thread.currentThread().setContextClassLoader(null);\n+\n+    try {\n+      OzoneConfiguration conf = OzoneConfiguration.of(hadoopConf);\n+\n+      if (omHost == null && OmUtils.isServiceIdsDefined(conf)) {\n+        // When the host name or service id isn't given\n+        // but ozone.om.service.ids is defined, declare failure.\n+\n+        // This is a safety precaution that prevents the client from\n+        // accidentally failing over to an unintended OM.\n+        throw new IllegalArgumentException(\"Service ID or host name must not\"\n+            + \" be omitted when ozone.om.service.ids is defined.\");\n+      }\n+\n+      if (omPort != -1) {\n+        // When the port number is specified, perform the following check\n+        if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+          // If omHost is a service id, it shouldn't use a port\n+          throw new IllegalArgumentException(\"Port \" + omPort +\n+              \" specified in URI but host '\" + omHost + \"' is a \"\n+              + \"logical (HA) OzoneManager and does not use port information.\");\n+        }\n+      } else {\n+        // When port number is not specified, read it from config\n+        omPort = OmUtils.getOmRpcPort(conf);\n+      }\n+\n+      SecurityConfig secConfig = new SecurityConfig(conf);\n+\n+      if (secConfig.isSecurityEnabled()) {\n+        this.securityEnabled = true;\n+      }\n+\n+      String replicationTypeConf =\n+          conf.get(OzoneConfigKeys.OZONE_REPLICATION_TYPE,\n+              OzoneConfigKeys.OZONE_REPLICATION_TYPE_DEFAULT);\n+\n+      int replicationCountConf = conf.getInt(OzoneConfigKeys.OZONE_REPLICATION,\n+          OzoneConfigKeys.OZONE_REPLICATION_DEFAULT);\n+\n+      if (OmUtils.isOmHAServiceId(conf, omHost)) {\n+        // omHost is listed as one of the service ids in the config,\n+        // thus we should treat omHost as omServiceId\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, conf);\n+      } else if (StringUtils.isNotEmpty(omHost) && omPort != -1) {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(omHost, omPort, conf);\n+      } else {\n+        this.ozoneClient =\n+            OzoneClientFactory.getRpcClient(conf);\n+      }\n+      objectStore = ozoneClient.getObjectStore();\n+      proxy = objectStore.getClientProxy();\n+      this.replicationType = ReplicationType.valueOf(replicationTypeConf);\n+      this.replicationFactor = ReplicationFactor.valueOf(replicationCountConf);\n+      this.configuredDnPort = conf.getInt(\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n+          OzoneConfigKeys.DFS_CONTAINER_IPC_PORT_DEFAULT);\n+    } finally {\n+      Thread.currentThread().setContextClassLoader(contextClassLoader);\n+    }\n+  }\n+\n+  OzoneBucket getBucket(OFSPath ofsPath, boolean createIfNotExist)\n+      throws IOException {\n+\n+    return getBucket(ofsPath.getVolumeName(), ofsPath.getBucketName(),\n+        createIfNotExist);\n+  }\n+\n+  /**\n+   * Get OzoneBucket object to operate in.\n+   * Optionally create volume and bucket if not found.\n+   *\n+   * @param createIfNotExist Set this to true if the caller is a write operation\n+   *                         in order to create the volume and bucket.\n+   * @throws IOException Exceptions other than OMException with result code\n+   *                     VOLUME_NOT_FOUND or BUCKET_NOT_FOUND.\n+   */\n+  private OzoneBucket getBucket(String volumeStr, String bucketStr,\n+      boolean createIfNotExist) throws IOException {\n+\n+    OzoneBucket bucket;\n+    try {\n+      bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+    } catch (OMException ex) {\n+      if (createIfNotExist) {\n+        // Note: getBucketDetails always throws BUCKET_NOT_FOUND, even if\n+        // the volume doesn't exist.\n+        if (ex.getResult().equals(BUCKET_NOT_FOUND)) {\n+          OzoneVolume volume;\n+          try {\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          } catch (OMException getVolEx) {\n+            if (getVolEx.getResult().equals(VOLUME_NOT_FOUND)) {\n+              // Volume doesn't exist. Create it\n+              try {\n+                objectStore.createVolume(volumeStr);\n+              } catch (OMException newVolEx) {\n+                // Ignore the case where another client created the volume\n+                if (!newVolEx.getResult().equals(VOLUME_ALREADY_EXISTS)) {\n+                  throw newVolEx;\n+                }\n+              }\n+            } else {\n+              throw getVolEx;\n+            }\n+            // Try get volume again\n+            volume = proxy.getVolumeDetails(volumeStr);\n+          }\n+          // Create the bucket\n+          try {\n+            volume.createBucket(bucketStr);\n+          } catch (OMException newBucEx) {\n+            // Ignore the case where another client created the bucket\n+            if (!newBucEx.getResult().equals(BUCKET_ALREADY_EXISTS)) {\n+              throw newBucEx;\n+            }\n+          }\n+        }\n+        // Try get bucket again\n+        bucket = proxy.getBucketDetails(volumeStr, bucketStr);\n+      } else {\n+        throw ex;\n+      }\n+    }\n+\n+    return bucket;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    ozoneClient.close();\n+  }\n+\n+  @Override\n+  public InputStream readFile(String pathStr) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_READ);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      return bucket.readFile(key).getInputStream();\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_NOT_FOUND\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileNotFoundException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  protected void incrementCounter(Statistic objectsRead) {\n+    //noop: Use OzoneClientAdapterImpl which supports statistics.\n+  }\n+\n+  @Override\n+  public OzoneFSOutputStream createFile(String pathStr, boolean overWrite,\n+      boolean recursive) throws IOException {\n+    incrementCounter(Statistic.OBJECTS_CREATED);\n+    OFSPath ofsPath = new OFSPath(pathStr);\n+    String key = ofsPath.getKeyName();\n+    try {\n+      OzoneBucket bucket = getBucket(ofsPath, false);\n+      OzoneOutputStream ozoneOutputStream = bucket.createFile(\n+          key, 0, replicationType, replicationFactor, overWrite, recursive);\n+      return new OzoneFSOutputStream(ozoneOutputStream.getOutputStream());\n+    } catch (OMException ex) {\n+      if (ex.getResult() == OMException.ResultCodes.FILE_ALREADY_EXISTS\n+          || ex.getResult() == OMException.ResultCodes.NOT_A_FILE) {\n+        throw new FileAlreadyExistsException(\n+            ex.getResult().name() + \": \" + ex.getMessage());\n+      } else {\n+        throw ex;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void renamePath(String path, String newPath) throws IOException {", "originalCommit": "60b4bf8e6008cbd53d95c3e0f249f33227209c1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU1OTE4MQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r372559181", "bodyText": "Done in 5c1c081", "author": "smengcl", "createdAt": "2020-01-29T18:37:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEwNTA5MQ=="}], "type": "inlineReview"}, {"oid": "90c57d18c2e7bb8b5c59db333c1d6dd92392a6be", "url": "https://github.com/apache/ozone/commit/90c57d18c2e7bb8b5c59db333c1d6dd92392a6be", "message": "Add TODO for test refactoring; new OFS tests are prefixed with \"OFS: \" in javadoc comment.", "committedDate": "2020-01-29T18:16:29Z", "type": "commit"}, {"oid": "e16a52ee7629ce89a185be00ab71742f56f1b7ce", "url": "https://github.com/apache/ozone/commit/e16a52ee7629ce89a185be00ab71742f56f1b7ce", "message": "OFSPath: hard-code volume and bucket name for now so that the rename logic isn't compromised. Addressed comment https://github.com/apache/hadoop-ozone/pull/415#discussion_r372026827", "committedDate": "2020-01-29T18:17:36Z", "type": "commit"}, {"oid": "c0e2ed7f4cf08490b151613fdb3415b097ee5ac0", "url": "https://github.com/apache/ozone/commit/c0e2ed7f4cf08490b151613fdb3415b097ee5ac0", "message": "Update comment for createDirectory.", "committedDate": "2020-01-29T18:24:20Z", "type": "commit"}, {"oid": "a9ef567103a13fd57d2c63197e7e356b447cafd7", "url": "https://github.com/apache/ozone/commit/a9ef567103a13fd57d2c63197e7e356b447cafd7", "message": "Place incrementCounter at the beginning of operations.", "committedDate": "2020-01-29T18:26:20Z", "type": "commit"}, {"oid": "ac0cd77f9c22c19c7442f7e248ab401bbb9ffb73", "url": "https://github.com/apache/ozone/commit/ac0cd77f9c22c19c7442f7e248ab401bbb9ffb73", "message": "Check keyName empty in deleteObject.", "committedDate": "2020-01-29T18:29:07Z", "type": "commit"}, {"oid": "5c1c08178a4d230ded059c8f11e82f19a375a862", "url": "https://github.com/apache/ozone/commit/5c1c08178a4d230ded059c8f11e82f19a375a862", "message": "renamePath -> rename. https://github.com/apache/hadoop-ozone/pull/415#discussion_r372105091", "committedDate": "2020-01-29T18:34:05Z", "type": "commit"}, {"oid": "2d365d2a355daef244319038be7896ec2266f7a1", "url": "https://github.com/apache/ozone/commit/2d365d2a355daef244319038be7896ec2266f7a1", "message": "OFSPath: Fix typo; fix test case.", "committedDate": "2020-01-29T22:38:06Z", "type": "commit"}, {"oid": "98887e710d799d345b3ee106299a4d24a9d154fc", "url": "https://github.com/apache/ozone/commit/98887e710d799d345b3ee106299a4d24a9d154fc", "message": "Move ofsPath.getNonKeyPath*() call out of the loop to speed up / lower memory pressure.", "committedDate": "2020-01-29T22:59:38Z", "type": "commit"}, {"oid": "741c6aa4b21baddcc69cd337acf22a871e36c3f0", "url": "https://github.com/apache/ozone/commit/741c6aa4b21baddcc69cd337acf22a871e36c3f0", "message": "Fix incompatibility for listKeys in RootedOzoneClientAdapter.", "committedDate": "2020-01-29T23:14:41Z", "type": "commit"}, {"oid": "5e12395a0ee8257d135fd4205363fc46d340bd87", "url": "https://github.com/apache/ozone/commit/5e12395a0ee8257d135fd4205363fc46d340bd87", "message": "Clean up comments.", "committedDate": "2020-01-29T23:21:59Z", "type": "commit"}, {"oid": "09a85c2da5e5270c2e6576682c8b9e77c669b29c", "url": "https://github.com/apache/ozone/commit/09a85c2da5e5270c2e6576682c8b9e77c669b29c", "message": "De-duplicate RootedOzoneClientAdapter by extending OzoneClientAdapter.", "committedDate": "2020-01-29T23:26:55Z", "type": "commit"}, {"oid": "0e58f83a722c17d995f5afc5591f4bd63c89329c", "url": "https://github.com/apache/ozone/commit/0e58f83a722c17d995f5afc5591f4bd63c89329c", "message": " new ArrayList<OzoneKey>().iterator() -> Collections.emptyIterator() to eliminate warning.", "committedDate": "2020-01-29T23:34:47Z", "type": "commit"}, {"oid": "364e4142abdde20ea18b86525e70eda340fc8145", "url": "https://github.com/apache/ozone/commit/364e4142abdde20ea18b86525e70eda340fc8145", "message": "Remove RootedOzoneClientAdapterFactory;\nPlace new createAdapter() inside OzoneClientAdapterFactory.", "committedDate": "2020-01-29T23:45:20Z", "type": "commit"}, {"oid": "40d146fc6edbc01dbb5d6b7315ed6d93a71073d4", "url": "https://github.com/apache/ozone/commit/40d146fc6edbc01dbb5d6b7315ed6d93a71073d4", "message": "Localize adapterImpl into RenameIterator/DeleteIterator for now.", "committedDate": "2020-01-29T23:48:38Z", "type": "commit"}, {"oid": "780ebaf5b5279e921a9eed5a43db34027c9b1aa4", "url": "https://github.com/apache/ozone/commit/780ebaf5b5279e921a9eed5a43db34027c9b1aa4", "message": "Refine comments.", "committedDate": "2020-01-30T00:01:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEyMDQyNg==", "url": "https://github.com/apache/ozone/pull/415#discussion_r373120426", "bodyText": "Consider reducing the numDirs as it will unnecessarily slow down the PR.", "author": "xiaoyuyao", "createdAt": "2020-01-30T18:34:08Z", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestRootedOzoneFileSystem.java", "diffHunk": "@@ -0,0 +1,482 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ozone;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.RandomStringUtils;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.contract.ContractTestUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.TestDataUtil;\n+import org.apache.hadoop.ozone.client.ObjectStore;\n+import org.apache.hadoop.ozone.client.OzoneBucket;\n+import org.apache.hadoop.ozone.client.OzoneClientException;\n+import org.apache.hadoop.ozone.client.OzoneKeyDetails;\n+import org.apache.hadoop.ozone.client.OzoneVolume;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import static org.apache.hadoop.ozone.om.OMConfigKeys.OZONE_OM_ADDRESS_KEY;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Ozone file system tests that are not covered by contract tests.\n+ *\n+ * TODO: Refactor this and TestOzoneFileSystem to eliminate most\n+ *  code duplication.\n+ */\n+public class TestRootedOzoneFileSystem {\n+\n+  @Rule\n+  public Timeout globalTimeout = new Timeout(300_000);\n+\n+  private static MiniOzoneCluster cluster = null;\n+\n+  private static FileSystem fs;\n+  private static RootedOzoneFileSystem ofs;\n+\n+  private static ObjectStore objectStore;\n+\n+  private String volumeName;\n+  private String bucketName;\n+\n+  private String rootPath;\n+\n+  // Store path commonly used by tests that test functionality within a bucket\n+  private String testBucketStr;\n+  private Path testBucketPath;\n+\n+  @Before\n+  public void init() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setNumDatanodes(3)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    objectStore = cluster.getClient().getObjectStore();\n+\n+    // create a volume and a bucket to be used by RootedOzoneFileSystem (OFS)\n+    OzoneBucket bucket = TestDataUtil.createVolumeAndBucket(cluster);\n+    volumeName = bucket.getVolumeName();\n+    bucketName = bucket.getName();\n+    testBucketStr = \"/\" + volumeName + \"/\" + bucketName;\n+    testBucketPath = new Path(testBucketStr);\n+\n+    rootPath = String.format(\"%s://%s/\", OzoneConsts.OZONE_OFS_URI_SCHEME,\n+        conf.get(OZONE_OM_ADDRESS_KEY));\n+\n+    // Set the fs.defaultFS and start the filesystem\n+    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, rootPath);\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    fs = FileSystem.get(conf);\n+    ofs = (RootedOzoneFileSystem) fs;\n+  }\n+\n+  @After\n+  public void teardown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+    IOUtils.closeQuietly(fs);\n+  }\n+\n+  @Test\n+  public void testOzoneFsServiceLoader() throws IOException {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    // Note: FileSystem#loadFileSystems won't load OFS class due to META-INF\n+    //  hence this workaround.\n+    conf.set(\"fs.ofs.impl\", \"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem\");\n+    assertEquals(\n+        FileSystem.getFileSystemClass(OzoneConsts.OZONE_OFS_URI_SCHEME, conf),\n+        RootedOzoneFileSystem.class);\n+  }\n+\n+  @Test\n+  public void testCreateDoesNotAddParentDirKeys() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testCreateDoesNotAddParentDirKeys\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    OzoneKeyDetails key = getKey(child, false);\n+    OFSPath childOFSPath = new OFSPath(child);\n+    assertEquals(key.getName(), childOFSPath.getKeyName());\n+\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // List status on the parent should show the child file\n+    assertEquals(\"List status of parent should include the 1 child file\", 1L,\n+        (long)fs.listStatus(parent).length);\n+    assertTrue(\"Parent directory does not appear to be a directory\",\n+        fs.getFileStatus(parent).isDirectory());\n+  }\n+\n+  @Test\n+  public void testDeleteCreatesFakeParentDir() throws Exception {\n+    Path grandparent = new Path(testBucketPath,\n+        \"testDeleteCreatesFakeParentDir\");\n+    Path parent = new Path(grandparent, \"parent\");\n+    Path child = new Path(parent, \"child\");\n+    ContractTestUtils.touch(fs, child);\n+\n+    // Verify that parent dir key does not exist\n+    // Creating a child should not add parent keys to the bucket\n+    try {\n+      getKey(parent, true);\n+    } catch (IOException ex) {\n+      assertKeyNotFoundException(ex);\n+    }\n+\n+    // Delete the child key\n+    assertTrue(fs.delete(child, false));\n+\n+    // Deleting the only child should create the parent dir key if it does\n+    // not exist\n+    OFSPath parentOFSPath = new OFSPath(parent);\n+    String parentKey = parentOFSPath.getKeyName() + \"/\";\n+    OzoneKeyDetails parentKeyInfo = getKey(parent, true);\n+    assertEquals(parentKey, parentKeyInfo.getName());\n+\n+    // Recursive delete with DeleteIterator\n+    assertTrue(fs.delete(grandparent, true));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    Path parent = new Path(testBucketPath, \"testListStatus\");\n+    Path file1 = new Path(parent, \"key1\");\n+    Path file2 = new Path(parent, \"key2\");\n+\n+    FileStatus[] fileStatuses = ofs.listStatus(testBucketPath);\n+    assertEquals(\"Should be empty\", 0, fileStatuses.length);\n+\n+    ContractTestUtils.touch(fs, file1);\n+    ContractTestUtils.touch(fs, file2);\n+\n+    fileStatuses = ofs.listStatus(testBucketPath);\n+    assertEquals(\"Should have created parent\",\n+        1, fileStatuses.length);\n+    assertEquals(\"Parent path doesn't match\",\n+        fileStatuses[0].getPath().toUri().getPath(), parent.toString());\n+\n+    // ListStatus on a directory should return all subdirs along with\n+    // files, even if there exists a file and sub-dir with the same name.\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        2, fileStatuses.length);\n+\n+    // ListStatus should return only the immediate children of a directory.\n+    Path file3 = new Path(parent, \"dir1/key3\");\n+    Path file4 = new Path(parent, \"dir1/key4\");\n+    ContractTestUtils.touch(fs, file3);\n+    ContractTestUtils.touch(fs, file4);\n+    fileStatuses = ofs.listStatus(parent);\n+    assertEquals(\"FileStatus did not return all children of the directory\",\n+        3, fileStatuses.length);\n+  }\n+\n+  /**\n+   * OFS: Helper function for tests. Return a volume name that doesn't exist.\n+   */\n+  private String getRandomNonExistVolumeName() throws Exception {\n+    final int numDigit = 5;\n+    long retriesLeft = Math.round(Math.pow(10, 5));\n+    String name = null;\n+    while (name == null && retriesLeft-- > 0) {\n+      name = \"volume-\" + RandomStringUtils.randomNumeric(numDigit);\n+      // Check volume existence.\n+      Iterator<? extends OzoneVolume> iter =\n+          objectStore.listVolumesByUser(null, name, null);\n+      if (iter.hasNext()) {\n+        // If there is a match, try again.\n+        // Note that volume name prefix match doesn't equal volume existence\n+        //  but the check is sufficient for this test.\n+        name = null;\n+      }\n+    }\n+    if (retriesLeft <= 0) {\n+      throw new Exception(\n+          \"Failed to generate random volume name that doesn't exist already.\");\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * OFS: Test mkdir on volume, bucket and dir that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirOnNonExistentVolumeBucketDir() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path root = new Path(\"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // Check volume and bucket existence, they should both be created.\n+    OzoneVolume ozoneVolume = objectStore.getVolume(volumeNameLocal);\n+    OzoneBucket ozoneBucket = ozoneVolume.getBucket(bucketNameLocal);\n+    OFSPath ofsPathDir1 = new OFSPath(dir12);\n+    String key = ofsPathDir1.getKeyName() + \"/\";\n+    OzoneKeyDetails ozoneKeyDetails = ozoneBucket.getKey(key);\n+    assertEquals(key, ozoneKeyDetails.getName());\n+\n+    // Verify that directories are created.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir1.toString());\n+    assertEquals(fileStatuses[1].getPath().toUri().getPath(), dir2.toString());\n+\n+    fileStatuses = ofs.listStatus(dir1);\n+    assertEquals(fileStatuses[0].getPath().toUri().getPath(), dir12.toString());\n+    fileStatuses = ofs.listStatus(dir12);\n+    assertEquals(fileStatuses.length, 0);\n+    fileStatuses = ofs.listStatus(dir2);\n+    assertEquals(fileStatuses.length, 0);\n+  }\n+\n+  /**\n+   * OFS: Tests mkdir on a volume and bucket that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolumeBucket() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    String bucketNameLocal = \"bucket-\" + RandomStringUtils.randomNumeric(5);\n+    Path newVolBucket = new Path(\n+        \"/\" + volumeNameLocal + \"/\" + bucketNameLocal);\n+    fs.mkdirs(newVolBucket);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    assertNotNull(ozoneVolume);\n+    assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    Iterator<? extends OzoneBucket> iterBuc =\n+        ozoneVolume.listBuckets(\"bucket-\");\n+    OzoneBucket ozoneBucket = iterBuc.next();\n+    assertNotNull(ozoneBucket);\n+    assertEquals(bucketNameLocal, ozoneBucket.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * OFS: Tests mkdir on a volume that doesn't exist.\n+   */\n+  @Test\n+  public void testMkdirNonExistentVolume() throws Exception {\n+    String volumeNameLocal = getRandomNonExistVolumeName();\n+    Path newVolume = new Path(\"/\" + volumeNameLocal);\n+    fs.mkdirs(newVolume);\n+\n+    // Verify with listVolumes and listBuckets\n+    Iterator<? extends OzoneVolume> iterVol =\n+        objectStore.listVolumesByUser(null, volumeNameLocal, null);\n+    OzoneVolume ozoneVolume = iterVol.next();\n+    assertNotNull(ozoneVolume);\n+    assertEquals(volumeNameLocal, ozoneVolume.getName());\n+\n+    // TODO: Use listStatus to check volume and bucket creation in HDDS-2928.\n+  }\n+\n+  /**\n+   * Tests listStatus operation in a bucket.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = ofs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnLargeDirectory() throws Exception {\n+    Path root = new Path(\"/\" + volumeName + \"/\" + bucketName);\n+    Set<String> paths = new TreeSet<>();\n+    int numDirs = 5111;", "originalCommit": "780ebaf5b5279e921a9eed5a43db34027c9b1aa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE0MTAwMQ==", "url": "https://github.com/apache/ozone/pull/415#discussion_r373141001", "bodyText": "I'm not sure why o3fs test chose this random number. But basically numDirs only need to be larger than LISTING_PAGE_SIZE so it tests pagination. And BTW I believe integration tests are disabled in PR by default now. (They are not run in PR checks.)\nReduced in 0d71006.", "author": "smengcl", "createdAt": "2020-01-30T19:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEyMDQyNg=="}], "type": "inlineReview"}]}