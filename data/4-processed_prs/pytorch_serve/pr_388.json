{"pr_number": 388, "pr_title": "Document the docker settings for running in production mode", "pr_createdAt": "2020-05-22T22:15:33Z", "pr_url": "https://github.com/pytorch/serve/pull/388", "timeline": [{"oid": "67af153709f9cbf86a2414c1d6ae675ccc7a07b1", "url": "https://github.com/pytorch/serve/commit/67af153709f9cbf86a2414c1d6ae675ccc7a07b1", "message": "Docker Production Options #234", "committedDate": "2020-05-22T22:05:21Z", "type": "commit"}, {"oid": "f771e3f6bb007d5c5dd00a1df47f67cfbae89b25", "url": "https://github.com/pytorch/serve/commit/f771e3f6bb007d5c5dd00a1df47f67cfbae89b25", "message": "Docker Production Options - Typo Fixes / Fit & Finish  #234", "committedDate": "2020-05-22T22:11:44Z", "type": "commit"}, {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "url": "https://github.com/pytorch/serve/commit/ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "message": "Typos - Docker Docs #234", "committedDate": "2020-05-22T22:12:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDU2NA==", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480564", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n          \n          \n            \n            You may want to consider the following aspects / docker options when deploying torchserve in Production with Docker.", "author": "maaquib", "createdAt": "2020-05-22T22:37:56Z", "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.", "originalCommit": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDk5MQ==", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480991", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n          \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses default ports 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "author": "maaquib", "createdAt": "2020-05-22T22:40:42Z", "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "originalCommit": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MTA0Ng==", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429481046", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.\n          \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want to consider using a shared volume if you prefer pre populating models in model-store directory.", "author": "maaquib", "createdAt": "2020-05-22T22:40:59Z", "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n+    * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.", "originalCommit": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "935d9d248b5042e72995a700e586ef9ab1bb56bc", "url": "https://github.com/pytorch/serve/commit/935d9d248b5042e72995a700e586ef9ab1bb56bc", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>", "committedDate": "2020-05-22T23:13:46Z", "type": "commit"}, {"oid": "87f261c925e203e1b6908c2e2873705688be0e71", "url": "https://github.com/pytorch/serve/commit/87f261c925e203e1b6908c2e2873705688be0e71", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>", "committedDate": "2020-05-22T23:14:03Z", "type": "commit"}, {"oid": "2be662e93e25bbf977ec567f10b80a0499e2eaf1", "url": "https://github.com/pytorch/serve/commit/2be662e93e25bbf977ec567f10b80a0499e2eaf1", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>", "committedDate": "2020-05-22T23:14:16Z", "type": "commit"}, {"oid": "a7a4c0a8f671705ebebd3f9754b05fb23e1c5982", "url": "https://github.com/pytorch/serve/commit/a7a4c0a8f671705ebebd3f9754b05fb23e1c5982", "message": "Merge branch 'staging_0_1_1' into issue_234", "committedDate": "2020-05-22T23:14:33Z", "type": "commit"}, {"oid": "e490bd18d2b07f8d1be73f07a3a7a4292b95d006", "url": "https://github.com/pytorch/serve/commit/e490bd18d2b07f8d1be73f07a3a7a4292b95d006", "message": "Merge branch 'staging_0_1_1' into issue_234", "committedDate": "2020-05-22T23:41:00Z", "type": "commit"}]}