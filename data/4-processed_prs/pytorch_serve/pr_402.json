{"pr_number": 402, "pr_title": "added backend profiling and fixed benchmark's install dependency scripts", "pr_createdAt": "2020-05-26T09:36:50Z", "pr_url": "https://github.com/pytorch/serve/pull/402", "timeline": [{"oid": "889b08b5e12cbfbc99aa5e6e573d077a64efa4b2", "url": "https://github.com/pytorch/serve/commit/889b08b5e12cbfbc99aa5e6e573d077a64efa4b2", "message": "added backend profiling", "committedDate": "2020-05-26T06:29:30Z", "type": "commit"}, {"oid": "c65921dae7f8b1dbe7a4b82c80e9577059b0c97e", "url": "https://github.com/pytorch/serve/commit/c65921dae7f8b1dbe7a4b82c80e9577059b0c97e", "message": "updated install dependency scripts to make them jmeter version independent", "committedDate": "2020-05-26T09:18:00Z", "type": "commit"}, {"oid": "674ed488e01fbb087215d41428ca2c1c6dc37981", "url": "https://github.com/pytorch/serve/commit/674ed488e01fbb087215d41428ca2c1c6dc37981", "message": "updated readme", "committedDate": "2020-05-26T11:58:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMzAzMQ==", "url": "https://github.com/pytorch/serve/pull/402#discussion_r430603031", "bodyText": "It will be good to give the example launch command here. Also shouldn't the docker container be created after updating the BENCHMARK flag in the code and use that updated code to build the container. To reduce confusion for users, can the BENCHMARK flag be handled as an Environment variable that one sets before starting torchserve to run in benchmark mode?", "author": "chauhang", "createdAt": "2020-05-26T17:59:21Z", "path": "benchmarks/README.md", "diffHunk": "@@ -150,7 +150,7 @@ Once you have stopped recording, you should be able to analyze the data.  One us\n The benchmarks can also be used to analyze the backend performance using cProfile.  It does not require any additional packages to run the benchmark, but viewing the logs does require an additional package.  Run `pip install snakeviz` to install this.  To run the python profiling, follow these steps:\n \n 1. In the file `ts/model_service_worker.py`, set the constant BENCHMARK to true at the top to enable benchmarking.\n-2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag.\n+2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag. If you are running `TorchServe` inside docker container make sure you map the `/tmp` directory to some local directory on your machine while starting docker.", "originalCommit": "674ed488e01fbb087215d41428ca2c1c6dc37981", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMzQ3Nw==", "url": "https://github.com/pytorch/serve/pull/402#discussion_r430603477", "bodyText": "Same here, will be useful to give example of actual launch command", "author": "chauhang", "createdAt": "2020-05-26T17:59:49Z", "path": "benchmarks/README.md", "diffHunk": "@@ -150,7 +150,7 @@ Once you have stopped recording, you should be able to analyze the data.  One us\n The benchmarks can also be used to analyze the backend performance using cProfile.  It does not require any additional packages to run the benchmark, but viewing the logs does require an additional package.  Run `pip install snakeviz` to install this.  To run the python profiling, follow these steps:\n \n 1. In the file `ts/model_service_worker.py`, set the constant BENCHMARK to true at the top to enable benchmarking.\n-2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag.\n+2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag. If you are running `TorchServe` inside docker container make sure you map the `/tmp` directory to some local directory on your machine while starting docker.\n 3. Run TorchServe directly through gradle (do not use docker).  This can be done either on your machine or on a remote machine accessible through SSH.", "originalCommit": "674ed488e01fbb087215d41428ca2c1c6dc37981", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwNDkzMQ==", "url": "https://github.com/pytorch/serve/pull/402#discussion_r430604931", "bodyText": "Also add steps in readme for installing snakeviz using pip install snakeviz", "author": "chauhang", "createdAt": "2020-05-26T18:01:21Z", "path": "benchmarks/README.md", "diffHunk": "@@ -150,7 +150,7 @@ Once you have stopped recording, you should be able to analyze the data.  One us\n The benchmarks can also be used to analyze the backend performance using cProfile.  It does not require any additional packages to run the benchmark, but viewing the logs does require an additional package.  Run `pip install snakeviz` to install this.  To run the python profiling, follow these steps:\n \n 1. In the file `ts/model_service_worker.py`, set the constant BENCHMARK to true at the top to enable benchmarking.\n-2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag.\n+2. Run the benchmark and TorchServe.  They can either be done automatically inside the docker container or separately with the \"--ts\" flag. If you are running `TorchServe` inside docker container make sure you map the `/tmp` directory to some local directory on your machine while starting docker.\n 3. Run TorchServe directly through gradle (do not use docker).  This can be done either on your machine or on a remote machine accessible through SSH.\n 4. Run the Benchmark script targeting your running TorchServe instance.  It might run something like `./benchmark.py throughput --ts https://127.0.0.1:8443`.  It can be run on either your local machine or a remote machine (if you are running remote), but we recommend running the benchmark on the same machine as the model server to avoid confounding network latencies.\n 5. Run `snakeviz /tmp/tsPythonProfile.prof` to view the profiling data.  It should start up a web server on your machine and automatically open the page.", "originalCommit": "674ed488e01fbb087215d41428ca2c1c6dc37981", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "80edb4b5cc617f2f31ca560da5c9b87bd00218b4", "url": "https://github.com/pytorch/serve/commit/80edb4b5cc617f2f31ca560da5c9b87bd00218b4", "message": "Fix benchmarks/README typo and JMeter versioning issue", "committedDate": "2020-05-28T02:01:51Z", "type": "commit"}, {"oid": "9af97df73824b84a8c454095d46378b397eceb65", "url": "https://github.com/pytorch/serve/commit/9af97df73824b84a8c454095d46378b397eceb65", "message": "Merge branch 'staging_0_1_1' into issue_400", "committedDate": "2020-05-29T04:20:01Z", "type": "commit"}, {"oid": "819e0398ac2abb3a02a9df8cf95e85bcc60f79d7", "url": "https://github.com/pytorch/serve/commit/819e0398ac2abb3a02a9df8cf95e85bcc60f79d7", "message": "merged #412 and resolved conflicts", "committedDate": "2020-05-29T04:32:45Z", "type": "commit"}, {"oid": "d166682a5740aa8f614b3f833f6ebe5e2ceae66f", "url": "https://github.com/pytorch/serve/commit/d166682a5740aa8f614b3f833f6ebe5e2ceae66f", "message": "updated documentation", "committedDate": "2020-05-29T04:44:57Z", "type": "commit"}]}