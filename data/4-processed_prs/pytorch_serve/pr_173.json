{"pr_number": 173, "pr_title": "doc review: configuration.md and custom_service.md", "pr_createdAt": "2020-04-09T18:40:55Z", "pr_url": "https://github.com/pytorch/serve/pull/173", "timeline": [{"oid": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "url": "https://github.com/pytorch/serve/commit/c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "message": "doc review: configuration and custom_service", "committedDate": "2020-04-09T18:38:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0NTAwNw==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406445007", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            **Note:** Environment variables have higher priority than command line or config.properties.\n          \n          \n            \n            **Note:** Environment variables have higher priority than command line or `config.properties`.\n          \n      \n    \n    \n  \n\nAlso, this file should be introduced before calling it out in a note.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:01:29Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MzkwMg==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406453902", "bodyText": "I'd put this in the intro and give the full order, rather than an \"or\".\nproperties < CLI < Env Var\nReading through this I still had to ask about the behavior.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0NTAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0NjA3Ng==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406446076", "bodyText": "Introduce the different config vehicles: env var, properties file, and CLI.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:03:38Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0NjQ3Ng==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406446476", "bodyText": "Does running this change the properties file?", "author": "aaronmarkham", "createdAt": "2020-04-09T20:04:28Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0Njk0Mw==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406446943", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set\n          \n          \n            \n            * **--ts-config** TorchServe loads the specified configuration file if the `TS_CONFIG_FILE` environment variable is not set", "author": "aaronmarkham", "createdAt": "2020-04-09T20:05:24Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:\n \n-* **--ts-config** TorchServe will load specified configuration file if TS_CONFIG_FILE is not set.\n-* **--model-store** This parameter will override `model_store` property in config.properties file.\n-* **--models** This parameter will override `load_models' property in config.properties.\n-* **--log-config** This parameter will override default log4j.properties.\n-* **--foreground** This parameter will run the TorchServe in foreground. If this option is\n-                        disabled, the TorchServe will run in the background.\n+* **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MDE5Nw==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406450197", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * `blacklist_env_vars`: a regular expression to filter out environment variable names. default: all environment variables are visible to backend workers.\n          \n          \n            \n            * `blacklist_env_vars`: a regular expression to filter out environment variable names. Default: all environment variables are visible to backend workers.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:11:42Z", "path": "docs/configuration.md", "diffHunk": "@@ -148,30 +153,31 @@ cors_allowed_methods=GET, POST, PUT, OPTIONS\n cors_allowed_headers=X-Custom-Header\n ```\n \n-\n ### Restrict backend worker to access environment variable\n \n-Environment variable may contains sensitive information like AWS credentials. Backend worker will execute arbitrary model's custom code, which may expose security risk. TorchServe provides a `blacklist_env_vars` property which allows user to restrict which environment variable can be accessed by backend worker.\n+Environment variables might contain sensitive information, like AWS credentials. Backend workers execute an arbitrary model's custom code,\n+which might expose a security risk. TorchServe provides a `blacklist_env_vars` property that allows you to restrict which environment variables can be accessed by backend workers.\n \n-* blacklist_env_vars: a regular expression to filter out environment variable names, default: all environment variable will be visible to backend worker.\n+* `blacklist_env_vars`: a regular expression to filter out environment variable names. default: all environment variables are visible to backend workers.", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MDQ3Ng==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406450476", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * `number_of_gpu`: Maximum number of GPUs that TorchServe can use for inference. default: all available GPUs in system.\n          \n          \n            \n            * `number_of_gpu`: Maximum number of GPUs that TorchServe can use for inference. Default: all available GPUs.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:12:17Z", "path": "docs/configuration.md", "diffHunk": "@@ -148,30 +153,31 @@ cors_allowed_methods=GET, POST, PUT, OPTIONS\n cors_allowed_headers=X-Custom-Header\n ```\n \n-\n ### Restrict backend worker to access environment variable\n \n-Environment variable may contains sensitive information like AWS credentials. Backend worker will execute arbitrary model's custom code, which may expose security risk. TorchServe provides a `blacklist_env_vars` property which allows user to restrict which environment variable can be accessed by backend worker.\n+Environment variables might contain sensitive information, like AWS credentials. Backend workers execute an arbitrary model's custom code,\n+which might expose a security risk. TorchServe provides a `blacklist_env_vars` property that allows you to restrict which environment variables can be accessed by backend workers.\n \n-* blacklist_env_vars: a regular expression to filter out environment variable names, default: all environment variable will be visible to backend worker.\n+* `blacklist_env_vars`: a regular expression to filter out environment variable names. default: all environment variables are visible to backend workers.\n \n ### Limit GPU usage\n-By default, TorchServe will use all available GPUs for inference, you use `number_of_gpu` to limit the usage of GPUs.\n \n-* number_of_gpu: max number of GPUs that TorchServe can use for inference, default: available GPUs in system.\n+By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu` to limit the usage of GPUs.\n+\n+* `number_of_gpu`: Maximum number of GPUs that TorchServe can use for inference. default: all available GPUs in system.", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MDkxOA==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406450918", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Restrict backend worker to access environment variable\n          \n          \n            \n            ### Restrict backend worker access to environment variables", "author": "aaronmarkham", "createdAt": "2020-04-09T20:13:09Z", "path": "docs/configuration.md", "diffHunk": "@@ -148,30 +153,31 @@ cors_allowed_methods=GET, POST, PUT, OPTIONS\n cors_allowed_headers=X-Custom-Header\n ```\n \n-\n ### Restrict backend worker to access environment variable", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NTY1Ng==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406455656", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Preprocess input data before it is sent to the model for inference\n          \n          \n            \n            * Pre-process input data before it is sent to the model for inference\n          \n      \n    \n    \n  \n\nnit... doesn't matter, but bugs me when post-process is right next to it", "author": "aaronmarkham", "createdAt": "2020-04-09T20:22:58Z", "path": "docs/custom_service.md", "diffHunk": "@@ -1,25 +1,28 @@\n # Custom Service\n \n+Customize the behavior of TorchServe by writing a Python script that you package with\n+the model when you use the model archiver. TorchServe executes this code when it runs.\n+\n+Provide a custom script to:\n+\n+* Preprocess input data before it is sent to the model for inference", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1ODE5MQ==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406458191", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * management_address: management API binding address. Default: http://127.0.0.1:8081\n          \n          \n            \n            * `management_address`: management API binding address. Default: http://127.0.0.1:8081", "author": "aaronmarkham", "createdAt": "2020-04-09T20:28:03Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:\n \n-* **--ts-config** TorchServe will load specified configuration file if TS_CONFIG_FILE is not set.\n-* **--model-store** This parameter will override `model_store` property in config.properties file.\n-* **--models** This parameter will override `load_models' property in config.properties.\n-* **--log-config** This parameter will override default log4j.properties.\n-* **--foreground** This parameter will run the TorchServe in foreground. If this option is\n-                        disabled, the TorchServe will run in the background.\n+* **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set\n+* **--model-store** Overrides the `model_store` property in config.properties file\n+* **--models** Overrides the `load_models` property in config.properties\n+* **--log-config** Overrides the default log4j.properties\n+* **--foreground** Runs TorchServe in the foreground. If this option is\n+                        disabled, TorchServe runs in the background\n \n-See [Running the TorchServe](server.md) for detail.\n+For more detailed information about `torchserve` command line options, see [Serve Models with TorchServe](server.md).\n \n ## config.properties file\n \n-TorchServe use a `config.properties` file to store configurations. TorchServe use following order to locate this `config.properties` file:\n-1. if `TS_CONFIG_FILE` environment variable is set, TorchServe will load the configuration from the environment variable.\n-2. if `--ts-config` parameter is passed to `torchserve`, TorchServe will load the configuration from the parameter.\n-3. if there is a `config.properties` in current folder where user start the `torchserve`, TorchServe will load the `config.properties` file form current working directory.\n-4. If none of above is specified, TorchServe will load built-in configuration with default values.\n+TorchServe uses a `config.properties` file to store configurations. TorchServe uses following, in order of priority, to locate this `config.properties` file:\n+\n+1. If the `TS_CONFIG_FILE` environment variable is set, TorchServe loads the configuration from the path specified by the environment variable.\n+2. If `--ts-config` parameter is passed to `torchserve`, TorchServe loads the configuration from the path specified by the parameter.\n+3. If there is a `config.properties` in the folder where you call `torchserve`, TorchServe loads the `config.properties` file from the current working directory.\n+4. If none of the above is specified, TorchServe loads a built-in configuration with default values.\n \n ### Customize JVM options\n \n-The restrict TorchServe frontend memory footprint, certain JVM options is set via **vmargs** property in `config.properties` file\n+To control TorchServe frontend memory footprint, configure the **vmargs** property in the `config.properties` file\n \n * default: N/A, use JVM default options\n \n-User can adjust those JVM options for fit their memory requirement if needed.\n+Adjust JVM options to fit your memory requirement.\n \n ### Load models at startup\n \n-User can configure load models while TorchServe startup. TorchServe can load models from `model_store` or from HTTP(s) URL.\n-\n-* model_store\n-\t* standalone: default: N/A, load models from local disk is disabled. Following syntax can be used to configure lost of models to be loaded on startup :\n-\t\n-```python\n-# load all models present in model store\n-load_models=all\n-# load models from model store with mar names only\n-load_models=model1.mar,model2.mar\n-# load models from model store with model name and mar file\n-load_models=model1=model1.mar,model2=model2.mar\n-```\n+You can configure TorchServe to load models during startup by setting the `model_store` and `load_models` properties.\n+The following values are valid:\n \n-* load_models\n-\t* standalone: default: N/A, no models will be load on startup.\n-```python\n-model_store=<path to model store directory which stores the local mar files.>\n-```\n+* `load_models`\n+  * `standalone`: default: N/A, No models are loaded at start up.\n+  * `all`: Load all models present in `model_store`.\n+  * `model1.mar, model2.mar`: Load models in the specified MAR files from `model_store`.\n+  * `model1=model1.mar, model2=model2.mar`: Load models with the specified names and MAR files from `model_store`.\n \n-**Note:** `model_store` and `load_models` property can be override by command line parameters.\n+* `model_store`\n+  * `standalone`: default: N/A, Loading models from the local disk is disabled.\n+  * `pathname`: The model store location is specified by the value of `pathname`.\n+\n+**Note:** `model_store` and `load_models` properties are overridden by command line parameters, if specified.\n \n ### Configure TorchServe listening address and port\n \n-TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default. Inference API is listening on 8080 port and accepting HTTP request. Management API is listening on 8081 port and accepting HTTP request. See [Enable SSL](#enable-ssl) for configuring HTTPS.\n+TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default.\n+The inference API is listening on port 8080 accepting HTTP requests. The management API is listening on port 8081 port and accepting HTTP request.\n+See [Enable SSL](#enable-ssl) to configure HTTPS.\n \n-* inference_address: inference API binding address, default: http://127.0.0.1:8080\n-* management_address: management API binding address, default: http://127.0.0.1:8081\n-* In order to run predictions on models via public-ip specify IP address as `0.0.0.0` to make is accessible over all network interfaces or to the explicit IP-address as shown in example below.\n+* `inference_address`: Inference API binding address. Default: http://127.0.0.1:8080\n+* management_address: management API binding address. Default: http://127.0.0.1:8081", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1ODUxNQ==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406458515", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * To run predictions on models via public-ip, specify the IP address as `0.0.0.0`\n          \n          \n            \n            * To run predictions on models on a public IP address, specify the IP address as `0.0.0.0`\n          \n      \n    \n    \n  \n\nCurious how this works... would it pick one or bind to all public interfaces?", "author": "aaronmarkham", "createdAt": "2020-04-09T20:28:42Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:\n \n-* **--ts-config** TorchServe will load specified configuration file if TS_CONFIG_FILE is not set.\n-* **--model-store** This parameter will override `model_store` property in config.properties file.\n-* **--models** This parameter will override `load_models' property in config.properties.\n-* **--log-config** This parameter will override default log4j.properties.\n-* **--foreground** This parameter will run the TorchServe in foreground. If this option is\n-                        disabled, the TorchServe will run in the background.\n+* **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set\n+* **--model-store** Overrides the `model_store` property in config.properties file\n+* **--models** Overrides the `load_models` property in config.properties\n+* **--log-config** Overrides the default log4j.properties\n+* **--foreground** Runs TorchServe in the foreground. If this option is\n+                        disabled, TorchServe runs in the background\n \n-See [Running the TorchServe](server.md) for detail.\n+For more detailed information about `torchserve` command line options, see [Serve Models with TorchServe](server.md).\n \n ## config.properties file\n \n-TorchServe use a `config.properties` file to store configurations. TorchServe use following order to locate this `config.properties` file:\n-1. if `TS_CONFIG_FILE` environment variable is set, TorchServe will load the configuration from the environment variable.\n-2. if `--ts-config` parameter is passed to `torchserve`, TorchServe will load the configuration from the parameter.\n-3. if there is a `config.properties` in current folder where user start the `torchserve`, TorchServe will load the `config.properties` file form current working directory.\n-4. If none of above is specified, TorchServe will load built-in configuration with default values.\n+TorchServe uses a `config.properties` file to store configurations. TorchServe uses following, in order of priority, to locate this `config.properties` file:\n+\n+1. If the `TS_CONFIG_FILE` environment variable is set, TorchServe loads the configuration from the path specified by the environment variable.\n+2. If `--ts-config` parameter is passed to `torchserve`, TorchServe loads the configuration from the path specified by the parameter.\n+3. If there is a `config.properties` in the folder where you call `torchserve`, TorchServe loads the `config.properties` file from the current working directory.\n+4. If none of the above is specified, TorchServe loads a built-in configuration with default values.\n \n ### Customize JVM options\n \n-The restrict TorchServe frontend memory footprint, certain JVM options is set via **vmargs** property in `config.properties` file\n+To control TorchServe frontend memory footprint, configure the **vmargs** property in the `config.properties` file\n \n * default: N/A, use JVM default options\n \n-User can adjust those JVM options for fit their memory requirement if needed.\n+Adjust JVM options to fit your memory requirement.\n \n ### Load models at startup\n \n-User can configure load models while TorchServe startup. TorchServe can load models from `model_store` or from HTTP(s) URL.\n-\n-* model_store\n-\t* standalone: default: N/A, load models from local disk is disabled. Following syntax can be used to configure lost of models to be loaded on startup :\n-\t\n-```python\n-# load all models present in model store\n-load_models=all\n-# load models from model store with mar names only\n-load_models=model1.mar,model2.mar\n-# load models from model store with model name and mar file\n-load_models=model1=model1.mar,model2=model2.mar\n-```\n+You can configure TorchServe to load models during startup by setting the `model_store` and `load_models` properties.\n+The following values are valid:\n \n-* load_models\n-\t* standalone: default: N/A, no models will be load on startup.\n-```python\n-model_store=<path to model store directory which stores the local mar files.>\n-```\n+* `load_models`\n+  * `standalone`: default: N/A, No models are loaded at start up.\n+  * `all`: Load all models present in `model_store`.\n+  * `model1.mar, model2.mar`: Load models in the specified MAR files from `model_store`.\n+  * `model1=model1.mar, model2=model2.mar`: Load models with the specified names and MAR files from `model_store`.\n \n-**Note:** `model_store` and `load_models` property can be override by command line parameters.\n+* `model_store`\n+  * `standalone`: default: N/A, Loading models from the local disk is disabled.\n+  * `pathname`: The model store location is specified by the value of `pathname`.\n+\n+**Note:** `model_store` and `load_models` properties are overridden by command line parameters, if specified.\n \n ### Configure TorchServe listening address and port\n \n-TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default. Inference API is listening on 8080 port and accepting HTTP request. Management API is listening on 8081 port and accepting HTTP request. See [Enable SSL](#enable-ssl) for configuring HTTPS.\n+TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default.\n+The inference API is listening on port 8080 accepting HTTP requests. The management API is listening on port 8081 port and accepting HTTP request.\n+See [Enable SSL](#enable-ssl) to configure HTTPS.\n \n-* inference_address: inference API binding address, default: http://127.0.0.1:8080\n-* management_address: management API binding address, default: http://127.0.0.1:8081\n-* In order to run predictions on models via public-ip specify IP address as `0.0.0.0` to make is accessible over all network interfaces or to the explicit IP-address as shown in example below.\n+* `inference_address`: Inference API binding address. Default: http://127.0.0.1:8080\n+* management_address: management API binding address. Default: http://127.0.0.1:8081\n+* To run predictions on models via public-ip, specify the IP address as `0.0.0.0`", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ2MDg0Mw==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406460843", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The inference API is listening on port 8080 accepting HTTP requests. The management API is listening on port 8081 port and accepting HTTP request.\n          \n          \n            \n            The inference API is listening on port 8080. The management API is listening on port 8081. Both expect HTTP requests. These are the default ports.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:33:27Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:\n \n-* **--ts-config** TorchServe will load specified configuration file if TS_CONFIG_FILE is not set.\n-* **--model-store** This parameter will override `model_store` property in config.properties file.\n-* **--models** This parameter will override `load_models' property in config.properties.\n-* **--log-config** This parameter will override default log4j.properties.\n-* **--foreground** This parameter will run the TorchServe in foreground. If this option is\n-                        disabled, the TorchServe will run in the background.\n+* **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set\n+* **--model-store** Overrides the `model_store` property in config.properties file\n+* **--models** Overrides the `load_models` property in config.properties\n+* **--log-config** Overrides the default log4j.properties\n+* **--foreground** Runs TorchServe in the foreground. If this option is\n+                        disabled, TorchServe runs in the background\n \n-See [Running the TorchServe](server.md) for detail.\n+For more detailed information about `torchserve` command line options, see [Serve Models with TorchServe](server.md).\n \n ## config.properties file\n \n-TorchServe use a `config.properties` file to store configurations. TorchServe use following order to locate this `config.properties` file:\n-1. if `TS_CONFIG_FILE` environment variable is set, TorchServe will load the configuration from the environment variable.\n-2. if `--ts-config` parameter is passed to `torchserve`, TorchServe will load the configuration from the parameter.\n-3. if there is a `config.properties` in current folder where user start the `torchserve`, TorchServe will load the `config.properties` file form current working directory.\n-4. If none of above is specified, TorchServe will load built-in configuration with default values.\n+TorchServe uses a `config.properties` file to store configurations. TorchServe uses following, in order of priority, to locate this `config.properties` file:\n+\n+1. If the `TS_CONFIG_FILE` environment variable is set, TorchServe loads the configuration from the path specified by the environment variable.\n+2. If `--ts-config` parameter is passed to `torchserve`, TorchServe loads the configuration from the path specified by the parameter.\n+3. If there is a `config.properties` in the folder where you call `torchserve`, TorchServe loads the `config.properties` file from the current working directory.\n+4. If none of the above is specified, TorchServe loads a built-in configuration with default values.\n \n ### Customize JVM options\n \n-The restrict TorchServe frontend memory footprint, certain JVM options is set via **vmargs** property in `config.properties` file\n+To control TorchServe frontend memory footprint, configure the **vmargs** property in the `config.properties` file\n \n * default: N/A, use JVM default options\n \n-User can adjust those JVM options for fit their memory requirement if needed.\n+Adjust JVM options to fit your memory requirement.\n \n ### Load models at startup\n \n-User can configure load models while TorchServe startup. TorchServe can load models from `model_store` or from HTTP(s) URL.\n-\n-* model_store\n-\t* standalone: default: N/A, load models from local disk is disabled. Following syntax can be used to configure lost of models to be loaded on startup :\n-\t\n-```python\n-# load all models present in model store\n-load_models=all\n-# load models from model store with mar names only\n-load_models=model1.mar,model2.mar\n-# load models from model store with model name and mar file\n-load_models=model1=model1.mar,model2=model2.mar\n-```\n+You can configure TorchServe to load models during startup by setting the `model_store` and `load_models` properties.\n+The following values are valid:\n \n-* load_models\n-\t* standalone: default: N/A, no models will be load on startup.\n-```python\n-model_store=<path to model store directory which stores the local mar files.>\n-```\n+* `load_models`\n+  * `standalone`: default: N/A, No models are loaded at start up.\n+  * `all`: Load all models present in `model_store`.\n+  * `model1.mar, model2.mar`: Load models in the specified MAR files from `model_store`.\n+  * `model1=model1.mar, model2=model2.mar`: Load models with the specified names and MAR files from `model_store`.\n \n-**Note:** `model_store` and `load_models` property can be override by command line parameters.\n+* `model_store`\n+  * `standalone`: default: N/A, Loading models from the local disk is disabled.\n+  * `pathname`: The model store location is specified by the value of `pathname`.\n+\n+**Note:** `model_store` and `load_models` properties are overridden by command line parameters, if specified.\n \n ### Configure TorchServe listening address and port\n \n-TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default. Inference API is listening on 8080 port and accepting HTTP request. Management API is listening on 8081 port and accepting HTTP request. See [Enable SSL](#enable-ssl) for configuring HTTPS.\n+TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default.\n+The inference API is listening on port 8080 accepting HTTP requests. The management API is listening on port 8081 port and accepting HTTP request.", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ2NDc1Mg==", "url": "https://github.com/pytorch/serve/pull/173#discussion_r406464752", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              to make it accessible over all network interfaces, or set it to the explicit IP-address as shown in example below:\n          \n          \n            \n            \n          \n          \n            \n            *  To run predictions on models on a specific IP address, specify the IP address and port.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:41:32Z", "path": "docs/configuration.md", "diffHunk": "@@ -1,80 +1,81 @@\n # Advanced configuration\n \n-One of design goal of TorchServe is easy to use. The default settings form TorchServe should be sufficient for most of use cases. This document describe advanced configurations that allows user to deep customize TorchServe's behavior.\n+TorchServe is meant to be easy to use. The default settings form TorchServe should be sufficient for most use cases.\n+If you want to customize TorchServe, the configuration options described in this topic are available.\n \n ## Environment variables\n \n-User can set environment variables to change TorchServe behavior, following is a list of variables that user can set for TorchServe:\n+You can change TorchServe behavior by setting the following environment variables:\n+\n * JAVA_HOME\n * PYTHONPATH\n * TS_CONFIG_FILE\n * LOG_LOCATION\n * METRICS_LOCATION\n \n-**Note:** environment variable has higher priority that command line or config.properties. It will override other property values.\n+**Note:** Environment variables have higher priority than command line or config.properties.\n+The value of an environment variable overrides other property values.\n \n ## Command line parameters\n \n-User can following parameters to start TorchServe, those parameters will override default TorchServe behavior:\n+Customize TorchServe behavior by using the following command line arguments when you call `torchserve`:\n \n-* **--ts-config** TorchServe will load specified configuration file if TS_CONFIG_FILE is not set.\n-* **--model-store** This parameter will override `model_store` property in config.properties file.\n-* **--models** This parameter will override `load_models' property in config.properties.\n-* **--log-config** This parameter will override default log4j.properties.\n-* **--foreground** This parameter will run the TorchServe in foreground. If this option is\n-                        disabled, the TorchServe will run in the background.\n+* **--ts-config** TorchServe loads the specified configuration file if `TS_CONFIG_FILE` is not set\n+* **--model-store** Overrides the `model_store` property in config.properties file\n+* **--models** Overrides the `load_models` property in config.properties\n+* **--log-config** Overrides the default log4j.properties\n+* **--foreground** Runs TorchServe in the foreground. If this option is\n+                        disabled, TorchServe runs in the background\n \n-See [Running the TorchServe](server.md) for detail.\n+For more detailed information about `torchserve` command line options, see [Serve Models with TorchServe](server.md).\n \n ## config.properties file\n \n-TorchServe use a `config.properties` file to store configurations. TorchServe use following order to locate this `config.properties` file:\n-1. if `TS_CONFIG_FILE` environment variable is set, TorchServe will load the configuration from the environment variable.\n-2. if `--ts-config` parameter is passed to `torchserve`, TorchServe will load the configuration from the parameter.\n-3. if there is a `config.properties` in current folder where user start the `torchserve`, TorchServe will load the `config.properties` file form current working directory.\n-4. If none of above is specified, TorchServe will load built-in configuration with default values.\n+TorchServe uses a `config.properties` file to store configurations. TorchServe uses following, in order of priority, to locate this `config.properties` file:\n+\n+1. If the `TS_CONFIG_FILE` environment variable is set, TorchServe loads the configuration from the path specified by the environment variable.\n+2. If `--ts-config` parameter is passed to `torchserve`, TorchServe loads the configuration from the path specified by the parameter.\n+3. If there is a `config.properties` in the folder where you call `torchserve`, TorchServe loads the `config.properties` file from the current working directory.\n+4. If none of the above is specified, TorchServe loads a built-in configuration with default values.\n \n ### Customize JVM options\n \n-The restrict TorchServe frontend memory footprint, certain JVM options is set via **vmargs** property in `config.properties` file\n+To control TorchServe frontend memory footprint, configure the **vmargs** property in the `config.properties` file\n \n * default: N/A, use JVM default options\n \n-User can adjust those JVM options for fit their memory requirement if needed.\n+Adjust JVM options to fit your memory requirement.\n \n ### Load models at startup\n \n-User can configure load models while TorchServe startup. TorchServe can load models from `model_store` or from HTTP(s) URL.\n-\n-* model_store\n-\t* standalone: default: N/A, load models from local disk is disabled. Following syntax can be used to configure lost of models to be loaded on startup :\n-\t\n-```python\n-# load all models present in model store\n-load_models=all\n-# load models from model store with mar names only\n-load_models=model1.mar,model2.mar\n-# load models from model store with model name and mar file\n-load_models=model1=model1.mar,model2=model2.mar\n-```\n+You can configure TorchServe to load models during startup by setting the `model_store` and `load_models` properties.\n+The following values are valid:\n \n-* load_models\n-\t* standalone: default: N/A, no models will be load on startup.\n-```python\n-model_store=<path to model store directory which stores the local mar files.>\n-```\n+* `load_models`\n+  * `standalone`: default: N/A, No models are loaded at start up.\n+  * `all`: Load all models present in `model_store`.\n+  * `model1.mar, model2.mar`: Load models in the specified MAR files from `model_store`.\n+  * `model1=model1.mar, model2=model2.mar`: Load models with the specified names and MAR files from `model_store`.\n \n-**Note:** `model_store` and `load_models` property can be override by command line parameters.\n+* `model_store`\n+  * `standalone`: default: N/A, Loading models from the local disk is disabled.\n+  * `pathname`: The model store location is specified by the value of `pathname`.\n+\n+**Note:** `model_store` and `load_models` properties are overridden by command line parameters, if specified.\n \n ### Configure TorchServe listening address and port\n \n-TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default. Inference API is listening on 8080 port and accepting HTTP request. Management API is listening on 8081 port and accepting HTTP request. See [Enable SSL](#enable-ssl) for configuring HTTPS.\n+TorchServe doesn't support authentication natively. To avoid unauthorized access, TorchServe only allows localhost access by default.\n+The inference API is listening on port 8080 accepting HTTP requests. The management API is listening on port 8081 port and accepting HTTP request.\n+See [Enable SSL](#enable-ssl) to configure HTTPS.\n \n-* inference_address: inference API binding address, default: http://127.0.0.1:8080\n-* management_address: management API binding address, default: http://127.0.0.1:8081\n-* In order to run predictions on models via public-ip specify IP address as `0.0.0.0` to make is accessible over all network interfaces or to the explicit IP-address as shown in example below.\n+* `inference_address`: Inference API binding address. Default: http://127.0.0.1:8080\n+* management_address: management API binding address. Default: http://127.0.0.1:8081\n+* To run predictions on models via public-ip, specify the IP address as `0.0.0.0`\n+  to make it accessible over all network interfaces, or set it to the explicit IP-address as shown in example below:", "originalCommit": "c75ecd42746895fadf4b55f77c477b0a11a1cbd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2bc2c094a3a68dede15f75f625096556c3573ab8", "url": "https://github.com/pytorch/serve/commit/2bc2c094a3a68dede15f75f625096556c3573ab8", "message": "addressed review comments", "committedDate": "2020-04-09T22:05:34Z", "type": "commit"}]}