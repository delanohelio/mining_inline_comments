{"pr_number": 644, "pr_title": "TorchServe on AKS", "pr_createdAt": "2020-08-25T01:45:43Z", "pr_url": "https://github.com/pytorch/serve/pull/644", "timeline": [{"oid": "4623034a030d990cbb040d7b57e7c1c9693d8215", "url": "https://github.com/pytorch/serve/commit/4623034a030d990cbb040d7b57e7c1c9693d8215", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:51Z", "type": "commit"}, {"oid": "bf71d6e802398687fa1cbc2013447673e0021cde", "url": "https://github.com/pytorch/serve/commit/bf71d6e802398687fa1cbc2013447673e0021cde", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:53Z", "type": "commit"}, {"oid": "6b4eaf863fef63f4730a6ecabc8783cb3a0ad88b", "url": "https://github.com/pytorch/serve/commit/6b4eaf863fef63f4730a6ecabc8783cb3a0ad88b", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:55Z", "type": "commit"}, {"oid": "82a895f84408459402e835c369bd74f1d8fea7df", "url": "https://github.com/pytorch/serve/commit/82a895f84408459402e835c369bd74f1d8fea7df", "message": "add result to bash block", "committedDate": "2020-11-19T05:13:57Z", "type": "commit"}, {"oid": "eb1c33c7eec53c3e5b0ff189012fbb167e36bcdd", "url": "https://github.com/pytorch/serve/commit/eb1c33c7eec53c3e5b0ff189012fbb167e36bcdd", "message": "add result to bash block", "committedDate": "2020-11-19T05:13:59Z", "type": "commit"}, {"oid": "814a1c52255ff1a2895a0822f4b0d175c6108014", "url": "https://github.com/pytorch/serve/commit/814a1c52255ff1a2895a0822f4b0d175c6108014", "message": "Update with section to remove aks cluster and resource group", "committedDate": "2020-11-19T05:14:01Z", "type": "commit"}, {"oid": "814a1c52255ff1a2895a0822f4b0d175c6108014", "url": "https://github.com/pytorch/serve/commit/814a1c52255ff1a2895a0822f4b0d175c6108014", "message": "Update with section to remove aks cluster and resource group", "committedDate": "2020-11-19T05:14:01Z", "type": "forcePushed"}, {"oid": "de263f92fa6b5469043dc6c4324650c25dd04ba7", "url": "https://github.com/pytorch/serve/commit/de263f92fa6b5469043dc6c4324650c25dd04ba7", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-19T21:28:54Z", "type": "commit"}, {"oid": "8838ef7e5c228747ba13af7e827e4c3e8930b7bc", "url": "https://github.com/pytorch/serve/commit/8838ef7e5c228747ba13af7e827e4c3e8930b7bc", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-21T02:51:26Z", "type": "commit"}, {"oid": "d19973ad72da4cdd6b00f2c6642aeb180b839927", "url": "https://github.com/pytorch/serve/commit/d19973ad72da4cdd6b00f2c6642aeb180b839927", "message": "Remove dup archiveTest() test case", "committedDate": "2020-11-21T03:18:44Z", "type": "commit"}, {"oid": "1eac0220303992bf58cb976857ffb8767286002a", "url": "https://github.com/pytorch/serve/commit/1eac0220303992bf58cb976857ffb8767286002a", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-25T07:04:57Z", "type": "commit"}, {"oid": "f9f38629c8f163d9449cd62da103387edd9ee88b", "url": "https://github.com/pytorch/serve/commit/f9f38629c8f163d9449cd62da103387edd9ee88b", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-25T07:36:57Z", "type": "commit"}, {"oid": "7152881531e759783bb10f402d57f5a203a25839", "url": "https://github.com/pytorch/serve/commit/7152881531e759783bb10f402d57f5a203a25839", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-25T20:35:30Z", "type": "commit"}, {"oid": "7fdb331eef23a8e347dbab9a681617af9d286182", "url": "https://github.com/pytorch/serve/commit/7fdb331eef23a8e347dbab9a681617af9d286182", "message": "use UUID as the name of model directory", "committedDate": "2020-07-20T02:17:43Z", "type": "commit"}, {"oid": "50e57c2813be378df3121bdafb4a9ed1ddadd903", "url": "https://github.com/pytorch/serve/commit/50e57c2813be378df3121bdafb4a9ed1ddadd903", "message": "fit", "committedDate": "2020-07-20T09:28:02Z", "type": "commit"}, {"oid": "16fee7b7e21630d9bf90260df2b84997d025ecc6", "url": "https://github.com/pytorch/serve/commit/16fee7b7e21630d9bf90260df2b84997d025ecc6", "message": "fit", "committedDate": "2020-07-20T09:28:27Z", "type": "commit"}, {"oid": "6eb1e6a5e9bfee2f43c3a50c4797978282e04839", "url": "https://github.com/pytorch/serve/commit/6eb1e6a5e9bfee2f43c3a50c4797978282e04839", "message": "fit", "committedDate": "2020-07-20T09:32:27Z", "type": "commit"}, {"oid": "998fd0a5cf491dde23865d387385ff2cee430af3", "url": "https://github.com/pytorch/serve/commit/998fd0a5cf491dde23865d387385ff2cee430af3", "message": "fit", "committedDate": "2020-07-20T10:03:30Z", "type": "commit"}, {"oid": "b6133488dbb6a65892ce742fd59b00f6e5065868", "url": "https://github.com/pytorch/serve/commit/b6133488dbb6a65892ce742fd59b00f6e5065868", "message": "fit", "committedDate": "2020-07-20T10:13:45Z", "type": "commit"}, {"oid": "27fe8d20df583442429322bbd84e1de526523aa0", "url": "https://github.com/pytorch/serve/commit/27fe8d20df583442429322bbd84e1de526523aa0", "message": "fit", "committedDate": "2020-07-21T01:21:46Z", "type": "commit"}, {"oid": "b7df01f64c949dd95be9eb09ca529e407893a729", "url": "https://github.com/pytorch/serve/commit/b7df01f64c949dd95be9eb09ca529e407893a729", "message": "fit", "committedDate": "2020-07-21T01:34:22Z", "type": "commit"}, {"oid": "e4647637c6f129625aacbe1dcefdd23e180bc78a", "url": "https://github.com/pytorch/serve/commit/e4647637c6f129625aacbe1dcefdd23e180bc78a", "message": "fit", "committedDate": "2020-07-21T01:37:19Z", "type": "commit"}, {"oid": "ee6600fe3d4c1c247c66236002d8b64b90a2eea9", "url": "https://github.com/pytorch/serve/commit/ee6600fe3d4c1c247c66236002d8b64b90a2eea9", "message": "fit", "committedDate": "2020-07-21T02:01:29Z", "type": "commit"}, {"oid": "34a4058027693065e8515a0833f76067b572509d", "url": "https://github.com/pytorch/serve/commit/34a4058027693065e8515a0833f76067b572509d", "message": "fit", "committedDate": "2020-07-21T02:08:42Z", "type": "commit"}, {"oid": "34116b90d171d25cb0de12e5cbcd593ace3b7b59", "url": "https://github.com/pytorch/serve/commit/34116b90d171d25cb0de12e5cbcd593ace3b7b59", "message": "fit", "committedDate": "2020-07-21T02:14:03Z", "type": "commit"}, {"oid": "ddf72d5ec394bee13f1de4952e3c567919b83d5b", "url": "https://github.com/pytorch/serve/commit/ddf72d5ec394bee13f1de4952e3c567919b83d5b", "message": "fit", "committedDate": "2020-07-21T02:22:06Z", "type": "commit"}, {"oid": "9c508c60f53401c824a64829e23b23850da86f8d", "url": "https://github.com/pytorch/serve/commit/9c508c60f53401c824a64829e23b23850da86f8d", "message": "fit", "committedDate": "2020-07-21T02:25:20Z", "type": "commit"}, {"oid": "a27f976d09d9fa29e3b6aaa3e7757a65afa30dca", "url": "https://github.com/pytorch/serve/commit/a27f976d09d9fa29e3b6aaa3e7757a65afa30dca", "message": "fit", "committedDate": "2020-07-21T02:45:41Z", "type": "commit"}, {"oid": "62f94003db6cbb1a7d4179134e3333d541af5b00", "url": "https://github.com/pytorch/serve/commit/62f94003db6cbb1a7d4179134e3333d541af5b00", "message": "fit", "committedDate": "2020-07-21T02:48:12Z", "type": "commit"}, {"oid": "735f1568d8470b6a07c432db5dfb4229b52f1581", "url": "https://github.com/pytorch/serve/commit/735f1568d8470b6a07c432db5dfb4229b52f1581", "message": "fit", "committedDate": "2020-07-21T02:55:32Z", "type": "commit"}, {"oid": "ad3a1f92e3bea303dad6b6b9ea22569ad041d7dd", "url": "https://github.com/pytorch/serve/commit/ad3a1f92e3bea303dad6b6b9ea22569ad041d7dd", "message": "fit", "committedDate": "2020-07-21T03:04:42Z", "type": "commit"}, {"oid": "e7d1d88d16b34e5585bca5dcb82e8f72c8f575f7", "url": "https://github.com/pytorch/serve/commit/e7d1d88d16b34e5585bca5dcb82e8f72c8f575f7", "message": "fit", "committedDate": "2020-07-21T03:14:41Z", "type": "commit"}, {"oid": "0b9fae276fecbe56763eb4423e5e6e5ca833df42", "url": "https://github.com/pytorch/serve/commit/0b9fae276fecbe56763eb4423e5e6e5ca833df42", "message": "fit", "committedDate": "2020-07-21T03:18:56Z", "type": "commit"}, {"oid": "3151b3e6e8c482dedd9afe21bdaa779425671b95", "url": "https://github.com/pytorch/serve/commit/3151b3e6e8c482dedd9afe21bdaa779425671b95", "message": "fit", "committedDate": "2020-07-21T03:22:06Z", "type": "commit"}, {"oid": "6aa27c2ad0e274adb86f0118a15edbc76dcd8b7c", "url": "https://github.com/pytorch/serve/commit/6aa27c2ad0e274adb86f0118a15edbc76dcd8b7c", "message": "fit", "committedDate": "2020-07-21T03:25:51Z", "type": "commit"}, {"oid": "7022173e47e53981bff64c75e2a2f15e61e8b2a5", "url": "https://github.com/pytorch/serve/commit/7022173e47e53981bff64c75e2a2f15e61e8b2a5", "message": "fit", "committedDate": "2020-07-21T03:28:53Z", "type": "commit"}, {"oid": "cf0844519f6a3e14e61067f44454b935cd7bfce2", "url": "https://github.com/pytorch/serve/commit/cf0844519f6a3e14e61067f44454b935cd7bfce2", "message": "fit:", "committedDate": "2020-07-21T03:33:26Z", "type": "commit"}, {"oid": "0d3461c31e54f9c983c559e38904c25b195c9e68", "url": "https://github.com/pytorch/serve/commit/0d3461c31e54f9c983c559e38904c25b195c9e68", "message": "Name the model file with UUID", "committedDate": "2020-07-21T03:49:03Z", "type": "commit"}, {"oid": "582b7ce729b3decf1637c4ea7e4c585fc115bb4d", "url": "https://github.com/pytorch/serve/commit/582b7ce729b3decf1637c4ea7e4c585fc115bb4d", "message": "Name model files whith UUID", "committedDate": "2020-07-21T06:18:18Z", "type": "commit"}, {"oid": "6f64c575d757511d5cf3665dd0dfe6394ecf6212", "url": "https://github.com/pytorch/serve/commit/6f64c575d757511d5cf3665dd0dfe6394ecf6212", "message": "fix", "committedDate": "2020-07-21T08:50:47Z", "type": "commit"}, {"oid": "43e334428b7e9bbca1b47951eaee910343f46810", "url": "https://github.com/pytorch/serve/commit/43e334428b7e9bbca1b47951eaee910343f46810", "message": "remove .idea files", "committedDate": "2020-07-21T09:15:40Z", "type": "commit"}, {"oid": "ca0912a3c40aee70b2452d68ba05dcd5c7229715", "url": "https://github.com/pytorch/serve/commit/ca0912a3c40aee70b2452d68ba05dcd5c7229715", "message": "delete .idea files", "committedDate": "2020-07-21T09:26:30Z", "type": "commit"}, {"oid": "3a669ce2de7294979d07ba8eaf2643232e95ead6", "url": "https://github.com/pytorch/serve/commit/3a669ce2de7294979d07ba8eaf2643232e95ead6", "message": "delete .gitignore", "committedDate": "2020-07-21T09:28:10Z", "type": "commit"}, {"oid": "f39ca7c6740f08baa65c3e3a497710fc2fc7bd79", "url": "https://github.com/pytorch/serve/commit/f39ca7c6740f08baa65c3e3a497710fc2fc7bd79", "message": "fit", "committedDate": "2020-07-21T13:41:32Z", "type": "commit"}, {"oid": "a89b2fbcb0b2ec1232705bbf34f7fd8772fbba60", "url": "https://github.com/pytorch/serve/commit/a89b2fbcb0b2ec1232705bbf34f7fd8772fbba60", "message": "fit", "committedDate": "2020-07-21T13:43:28Z", "type": "commit"}, {"oid": "33b046c3f5b4d318cfc59e24eeb884a4e2e078ff", "url": "https://github.com/pytorch/serve/commit/33b046c3f5b4d318cfc59e24eeb884a4e2e078ff", "message": "Merge branch 'master' into master", "committedDate": "2020-07-28T04:49:15Z", "type": "commit"}, {"oid": "3ff22d3172f457beeda19c9c454b819249220bf5", "url": "https://github.com/pytorch/serve/commit/3ff22d3172f457beeda19c9c454b819249220bf5", "message": "Merge branch 'master' into master", "committedDate": "2020-07-30T03:23:41Z", "type": "commit"}, {"oid": "a770a9dd10de10c4f53386e734c44f17b8845b4f", "url": "https://github.com/pytorch/serve/commit/a770a9dd10de10c4f53386e734c44f17b8845b4f", "message": "Merge branch 'master' into master", "committedDate": "2020-07-31T23:48:18Z", "type": "commit"}, {"oid": "1a47ecef9170b43119ff1f34c024bfb0ee0f41ea", "url": "https://github.com/pytorch/serve/commit/1a47ecef9170b43119ff1f34c024bfb0ee0f41ea", "message": "Merge branch 'master' into master", "committedDate": "2020-08-04T02:06:23Z", "type": "commit"}, {"oid": "6d4197690a735b704452b54a58674565b626155f", "url": "https://github.com/pytorch/serve/commit/6d4197690a735b704452b54a58674565b626155f", "message": "Merge branch 'master' into master", "committedDate": "2020-08-06T15:49:58Z", "type": "commit"}, {"oid": "3686231d5ef152ebc6377cb5edff2ac021ebfbfe", "url": "https://github.com/pytorch/serve/commit/3686231d5ef152ebc6377cb5edff2ac021ebfbfe", "message": "TorchServe on AKS", "committedDate": "2020-08-25T01:42:31Z", "type": "commit"}, {"oid": "43208afba832476c5038d504a4996765720d6ebf", "url": "https://github.com/pytorch/serve/commit/43208afba832476c5038d504a4996765720d6ebf", "message": "fit", "committedDate": "2020-08-25T02:16:36Z", "type": "commit"}, {"oid": "2f616289ebddfd7065060621be73d17364b4b78a", "url": "https://github.com/pytorch/serve/commit/2f616289ebddfd7065060621be73d17364b4b78a", "message": "fit", "committedDate": "2020-08-25T02:34:16Z", "type": "commit"}, {"oid": "bf75ce7eebce14f4f8c2cda68ad0e261c35cb2bd", "url": "https://github.com/pytorch/serve/commit/bf75ce7eebce14f4f8c2cda68ad0e261c35cb2bd", "message": "fit", "committedDate": "2020-08-25T08:48:02Z", "type": "commit"}, {"oid": "7e1da852c2e38b731640b2975c184346595db916", "url": "https://github.com/pytorch/serve/commit/7e1da852c2e38b731640b2975c184346595db916", "message": "fit", "committedDate": "2020-08-25T09:01:08Z", "type": "commit"}, {"oid": "985b4f041dba79b47f485677b4fad93c4c798f00", "url": "https://github.com/pytorch/serve/commit/985b4f041dba79b47f485677b4fad93c4c798f00", "message": "fit", "committedDate": "2020-08-26T01:14:18Z", "type": "commit"}, {"oid": "985b4f041dba79b47f485677b4fad93c4c798f00", "url": "https://github.com/pytorch/serve/commit/985b4f041dba79b47f485677b4fad93c4c798f00", "message": "fit", "committedDate": "2020-08-26T01:14:18Z", "type": "forcePushed"}, {"oid": "60debba1acf30f20d02dd4deacec0f032b1941e3", "url": "https://github.com/pytorch/serve/commit/60debba1acf30f20d02dd4deacec0f032b1941e3", "message": "Merge branch 'master' into aks", "committedDate": "2020-08-26T01:16:43Z", "type": "commit"}, {"oid": "69d652cb2885175c06586d38b2cb16e029de7831", "url": "https://github.com/pytorch/serve/commit/69d652cb2885175c06586d38b2cb16e029de7831", "message": "add document", "committedDate": "2020-08-31T12:14:48Z", "type": "commit"}, {"oid": "c8923f2a058ff581be4a55de4788f0381012bc8e", "url": "https://github.com/pytorch/serve/commit/c8923f2a058ff581be4a55de4788f0381012bc8e", "message": "add TorchServe-GPU", "committedDate": "2020-09-01T06:40:32Z", "type": "commit"}, {"oid": "952956fa97fc4ad5a557ec12d05bb5e8f112cbab", "url": "https://github.com/pytorch/serve/commit/952956fa97fc4ad5a557ec12d05bb5e8f112cbab", "message": "Merge branch 'master' into aks", "committedDate": "2020-09-03T02:16:25Z", "type": "commit"}, {"oid": "89114a2911ae785713e8f94816dc86bfcb7a4d55", "url": "https://github.com/pytorch/serve/commit/89114a2911ae785713e8f94816dc86bfcb7a4d55", "message": "Share Helm Chart between EKS & AKS", "committedDate": "2020-09-03T07:28:06Z", "type": "commit"}, {"oid": "f01033505c1a084ceb3734c9b12cd853b88cb073", "url": "https://github.com/pytorch/serve/commit/f01033505c1a084ceb3734c9b12cd853b88cb073", "message": "Merge branch 'master' into aks", "committedDate": "2020-09-09T18:55:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5MzQzNQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r485893435", "bodyText": "Just curious.\nCan you use helm to install this as used in EKS.\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin &&\nhelm repo update &&\nhelm install \\\n    --version=0.6.0 \\\n    --generate-name \\\n    nvdp/nvidia-device-plugin", "author": "dhanainme", "createdAt": "2020-09-09T20:10:10Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,259 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### \u4e00\u3001Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/zh-cn/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+`az login`\n+\n+`az account set -s your-subscription-ID`\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/zh-cn/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+`az group create --name myResourceGroup --location eastus`\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+`az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1`\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+`az aks install-cli`\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+`az aks get-credentials --resource-group myResourceGroup --name myAKSCluster`\n+\n+#### 1.5 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+`kubectl apply -f nvidia-device-plugin-ds.yaml`", "originalCommit": "f01033505c1a084ceb3734c9b12cd853b88cb073", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NDQ3NQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r485894475", "bodyText": "Please consider moving all the yaml template files to a template folder.\nAlso consider renaming pod.yaml to something more contextual", "author": "dhanainme", "createdAt": "2020-09-09T20:12:23Z", "path": "kubernetes/AKS/pod.yaml", "diffHunk": "@@ -0,0 +1,24 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: model-store-pod\n+spec:", "originalCommit": "f01033505c1a084ceb3734c9b12cd853b88cb073", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NjQwOQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r485896409", "bodyText": "This file could be shared between EKS & AKS.", "author": "dhanainme", "createdAt": "2020-09-09T20:16:21Z", "path": "kubernetes/EKS/config.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+inference_address=http://0.0.0.0:8080\n+management_address=http://0.0.0.0:8081\n+NUM_WORKERS=1\n+number_of_gpu=1\n+number_of_netty_threads=32\n+job_queue_size=1000\n+model_store=/home/model-server/shared/model-store\n+model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}", "originalCommit": "f01033505c1a084ceb3734c9b12cd853b88cb073", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTkwMDU5MQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r485900591", "bodyText": "Can you please help me with some info on how storage classes work in AKS.  For EKS i had to install a driver and make the storageClass to a shared volume. How does default map to a shared volume.\nDid find some similar ideas in - https://docs.microsoft.com/en-us/azure/aks/concepts-storage (AzureFiles). Wondering how these would be used.", "author": "dhanainme", "createdAt": "2020-09-09T20:24:38Z", "path": "kubernetes/AKS/pvc.yaml", "diffHunk": "@@ -0,0 +1,11 @@\n+kind: PersistentVolumeClaim\n+apiVersion: v1\n+metadata:\n+  name: model-store-claim\n+spec:\n+  storageClassName: default", "originalCommit": "f01033505c1a084ceb3734c9b12cd853b88cb073", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTkwMTU3Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r485901573", "bodyText": "Were you able to test this for a multi node / pod deployment.\nWith a single node we many not be able to test if the shared volume is being used.", "author": "dhanainme", "createdAt": "2020-09-09T20:26:31Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,259 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### \u4e00\u3001Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/zh-cn/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+`az login`\n+\n+`az account set -s your-subscription-ID`\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/zh-cn/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+`az group create --name myResourceGroup --location eastus`\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+`az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1`\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+`az aks install-cli`\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/zh-cn/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+`az aks get-credentials --resource-group myResourceGroup --name myAKSCluster`\n+\n+#### 1.5 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+`kubectl apply -f nvidia-device-plugin-ds.yaml`\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 1.6 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### \u4e8c\u3001Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+`git clone https://github.com/pytorch/serve.git`\n+\n+`cd serve/kubernetes/MicrosoftAzure`\n+\n+#### 2.2 Create PersistentVolume\n+\n+`kubectl apply -f pvc.yaml`\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+`kubectl get pvc,pv`\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.3 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+`kubectl apply -f pod.yaml`\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+`kubectl get po`\n+\n+Your output should look similar to\n+\n+NAME                               READY   STATUS    RESTARTS   AGE\n+model-store-pod                    1/1     Running   0          39s", "originalCommit": "f01033505c1a084ceb3734c9b12cd853b88cb073", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5353b27cd850f3fd33b0c52e57bbae200b466e12", "url": "https://github.com/pytorch/serve/commit/5353b27cd850f3fd33b0c52e57bbae200b466e12", "message": "add Azure File as PV", "committedDate": "2020-09-10T12:29:50Z", "type": "commit"}, {"oid": "d7c81cb55db4213827f1c7c4080b2c58bffffbf5", "url": "https://github.com/pytorch/serve/commit/d7c81cb55db4213827f1c7c4080b2c58bffffbf5", "message": "fit", "committedDate": "2020-09-10T12:53:10Z", "type": "commit"}, {"oid": "669d6b9f9a340651f8da600667d72b0f41f8e968", "url": "https://github.com/pytorch/serve/commit/669d6b9f9a340651f8da600667d72b0f41f8e968", "message": "delete .DS_store", "committedDate": "2020-09-10T12:56:09Z", "type": "commit"}, {"oid": "ad4f0bdbd45fcf4f6b589c1793a878e19cb15c41", "url": "https://github.com/pytorch/serve/commit/ad4f0bdbd45fcf4f6b589c1793a878e19cb15c41", "message": "Merge branch 'master' into aks", "committedDate": "2020-09-14T02:29:45Z", "type": "commit"}, {"oid": "534a6712ee31a880b221573ec1c8c5275ad1d249", "url": "https://github.com/pytorch/serve/commit/534a6712ee31a880b221573ec1c8c5275ad1d249", "message": "Update kubernetes/AKS/README.md\r\n\r\nRemove reference to locale-specific documentation", "committedDate": "2020-09-19T22:08:11Z", "type": "commit"}, {"oid": "91ef6ee8f4987290cc9ca3096fa710463e0a1b9a", "url": "https://github.com/pytorch/serve/commit/91ef6ee8f4987290cc9ca3096fa710463e0a1b9a", "message": "EN-US", "committedDate": "2020-09-21T03:49:47Z", "type": "commit"}, {"oid": "abccc6fc8d0c953885b5d510263ed1984783e1bc", "url": "https://github.com/pytorch/serve/commit/abccc6fc8d0c953885b5d510263ed1984783e1bc", "message": "Merge branch 'master' into aks", "committedDate": "2020-09-23T03:15:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDM3Mg==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r493874372", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              cd kubernetes/\n          \n          \n            \n              cd kubernetes/EKS", "author": "jeremiahschung", "createdAt": "2020-09-23T20:26:55Z", "path": "kubernetes/EKS/README.md", "diffHunk": "@@ -0,0 +1,814 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/", "originalCommit": "abccc6fc8d0c953885b5d510263ed1984783e1bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwOTgzOA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r493909838", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n          \n          \n            \n              You should be able to see a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)", "author": "jeremiahschung", "createdAt": "2020-09-23T21:37:29Z", "path": "kubernetes/EKS/README.md", "diffHunk": "@@ -0,0 +1,814 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `source ./setup_efs.sh`. This would also set all the env variables which might be used for deletion at a later time\n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)", "originalCommit": "abccc6fc8d0c953885b5d510263ed1984783e1bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d43bb7f0f4b0cca1b26a38f5da562223a4f084b4", "url": "https://github.com/pytorch/serve/commit/d43bb7f0f4b0cca1b26a38f5da562223a4f084b4", "message": "Update kubernetes/EKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-09-24T01:16:28Z", "type": "commit"}, {"oid": "8576978b9a99877ef68ddcde1a227fed199bc0eb", "url": "https://github.com/pytorch/serve/commit/8576978b9a99877ef68ddcde1a227fed199bc0eb", "message": "Update kubernetes/EKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-09-24T01:16:48Z", "type": "commit"}, {"oid": "ef5445cde2a73cad0da5f83fe6043fb68306fd16", "url": "https://github.com/pytorch/serve/commit/ef5445cde2a73cad0da5f83fe6043fb68306fd16", "message": "Merge branch 'master' into aks", "committedDate": "2020-09-24T01:21:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5NTk0OQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r496995949", "bodyText": "Now that we have a common directory in Helm to share some files, let's remove these files that are no longer needed:\n\nEKS/.helmignore (not needed since our values.yaml now lives in /Helm)\nEKS/Chart.yaml (same as above)\nEKS/templates/torchserve.yaml (same as above)", "author": "jeremiahschung", "createdAt": "2020-09-29T19:35:36Z", "path": "kubernetes/EKS/.helmignore", "diffHunk": "@@ -0,0 +1,5 @@\n+../templates/eks_cluster.yaml", "originalCommit": "ef5445cde2a73cad0da5f83fe6043fb68306fd16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI3Mzc2Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497273763", "bodyText": "@jeremiahschung Is this file, EKS/values.yaml,  still needed?", "author": "MengMeng96", "createdAt": "2020-09-30T06:37:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5NTk0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzMwNTU5Ng==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497305596", "bodyText": "Good catch! EKS/values.yaml is replaced by Helm/values.yaml so we can remove it as well.", "author": "jeremiahschung", "createdAt": "2020-09-30T07:43:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5NTk0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5Njg0NA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r496996844", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              To install Torchserve run ```helm install ts .```  \n          \n          \n            \n              To install Torchserve, move to the Helm folder ```cd ../Helm``` and run ```helm install ts .```", "author": "jeremiahschung", "createdAt": "2020-09-29T19:37:11Z", "path": "kubernetes/EKS/README.md", "diffHunk": "@@ -0,0 +1,814 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/EKS\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `source ./setup_efs.sh`. This would also set all the env variables which might be used for deletion at a later time\n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to see a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n+\n+  \n+\n+  ![security_group](images/security_group.png)\n+\n+  \n+\n+  You should also find Mount Points in your EFS console for every region where there is a Node in the Node Group.\n+\n+  \n+\n+  ![](images/efs_mount.png)\n+\n+  \n+\n+  \n+\n+  **Prepare PersistentVolume for Deployment**\n+\n+  We use the [ELF Provisioner Helm Chart](https://github.com/helm/charts/tree/master/stable/efs-provisioner) to create a PersistentVolume backed by EFS. Run the following command to set this up.\n+\n+  ```bash\n+  helm repo add stable https://kubernetes-charts.storage.googleapis.com\n+  helm install stable/efs-provisioner --set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name\n+  ```\n+\n+  \n+\n+  you should get an output similar to \n+\n+  \n+\n+  ```bash\n+  NAME: efs-provisioner-1596010253\n+  LAST DEPLOYED: Wed Jul 29 08:10:56 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  NOTES:\n+  You can provision an EFS-backed persistent volume with a persistent volume claim like below:\n+  \n+  kind: PersistentVolumeClaim\n+  apiVersion: v1\n+  metadata:\n+    name: my-efs-vol-1\n+    annotations:\n+      volume.beta.kubernetes.io/storage-class: aws-efs\n+  spec:\n+    storageClassName: aws-efs\n+    accessModes:\n+      - ReadWriteMany\n+    resources:\n+      requests:\n+        storage: 1Mi\n+  \n+  ```\n+\n+  \n+\n+  Verify that your EFS Provisioner installation is succesfull by invoking ```kubectl get pods```. Your output should look similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get pods\n+  NAME                                          READY   STATUS    RESTARTS   AGE\n+  efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          109s\n+  ```\n+\n+ Update `templates/efs_pv_claim.yaml` - `resources.request.storage` with the size of your PVC claim based on the size of the models you plan to use.  Now run, ```kubectl apply -f templates/efs_pv_claim.yaml```. This would also create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files in the same folder structure described above. \n+\n+  \n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl apply -f templates/efs_pv_claim.yaml\n+  persistentvolumeclaim/model-store-claim created\n+  pod/model-store-pod created\n+  ```\n+\n+  \n+\n+  Verify that the PVC / Pod is created  by excuting.   ```kubectl get service,po,daemonset,pv,pvc --all-namespaces``` \n+\n+  You should see\n+\n+  * ```Running``` status for ```pod/model-store-pod```  \n+  * ```Bound``` status for ```default/model-store-claim``` and ```persistentvolumeclaim/model-store-claim```\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         107m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   107m\n+  \n+  NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE\n+  default       pod/efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          4m49s\n+  default       pod/model-store-pod                               1/1     Running   0          8s\n+  kube-system   pod/aws-node-xx8kp                                1/1     Running   0          99m\n+  kube-system   pod/coredns-5c97f79574-tchfg                      1/1     Running   0          107m\n+  kube-system   pod/coredns-5c97f79574-thzqw                      1/1     Running   0          106m\n+  kube-system   pod/kube-proxy-4l8mw                              1/1     Running   0          99m\n+  kube-system   pod/nvidia-device-plugin-daemonset-dbhgq          1/1     Running   0          94m\n+  \n+  NAMESPACE     NAME                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node                         1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/kube-proxy                       1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          94m\n+  \n+  NAMESPACE   NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\n+              persistentvolume/pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            Delete           Bound    default/model-store-claim   aws-efs                 8s\n+  \n+  NAMESPACE   NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+  default     persistentvolumeclaim/model-store-claim   Bound    pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            aws-efs        8s\n+  ```\n+\n+  \n+\n+  Now edit the TS config file `config.properties` that would be used for the deployment. Any changes to this config should also have corresponding changes in Torchserve Helm Chart that we install in the next section. This config would be used by every torchserve instance in worker pods.\n+\n+  \n+\n+  The default config starts **squeezenet1_1** and **mnist** from the model zoo with 3, 5 workers.\n+\n+  \n+\n+  ```yaml\n+  inference_address=http://0.0.0.0:8080\n+  management_address=http://0.0.0.0:8081\n+  NUM_WORKERS=1\n+  number_of_gpu=1\n+  number_of_netty_threads=32\n+  job_queue_size=1000\n+  model_store=/home/model-server/shared/model-store\n+  model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+  \n+\n+  Now copy the files over to PersistentVolume using the following commands.\n+\n+  \n+\n+  ```bash\n+  wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+  wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+  kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+  kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+  \n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+  kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+  ```\n+\n+  \n+\n+  Verify that the files have been copied by executing ```kubectl exec --tty pod/model-store-pod -- find /pv/``` . You should get an output similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec --tty pod/model-store-pod -- find /pv/\n+  /pv/\n+  /pv/config\n+  /pv/config/config.properties\n+  /pv/model-store\n+  /pv/model-store/squeezenet1_1.mar\n+  /pv/model-store/mnist.mar\n+  ```\n+\n+  \n+\n+  Finally terminate the pod - `kubectl delete pod/model-store-pod`.\n+\n+  \n+\n+  ## Deploy TorchServe using Helm Charts\n+\n+  \n+  The following table describes all the parameters for the Helm Chart.\n+\n+  | Parameter          | Description              | Default                         |\n+  | ------------------ | ------------------------ | ------------------------------- |\n+  | `image`            | Torchserve Serving image | `pytorch/torchserve:latest-gpu` |\n+  | `management-port`  | TS Inference port        | `8080`                          |\n+  | `inference-port`   | TS Management port       | `8081`                          |\n+  | `replicas`         | K8S deployment replicas  | `1`                             |\n+  | `model-store`      | EFS mountpath            | `/home/model-server/shared/`    |\n+  | `persistence.size` | Storage size to request  | `1Gi`                           |\n+  | `n_gpu`            | Number of GPU in a TS Pod| `1`                             |\n+  | `n_cpu`            | Number of CPU in a TS Pod| `1`                             |\n+  | `memory_limit`     | TS Pod memory limit      | `4Gi`                           |\n+  | `memory_request`   | TS Pod memory request    | `1Gi`                           |\n+\n+\n+  Edit the values in `values.yaml` with the right parameters.  Somethings to consider,\n+  \n+  * Set torchserve_image to the `pytorch/torchserve:latest` if your nodes are CPU.\n+  * Set `persistence.size` based on the size of your models.\n+  * The value of `replicas` should be less than number of Nodes in the Node group.\n+  * `n_gpu` would be exposed to TS container by docker. This should be set to `number_of_gpu` in `config.properties` above.\n+  * `n_gpu` & `n_cpu` values are used on a per pod level and not in the entire cluster level\n+\n+  ```yaml\n+  # Default values for torchserve helm chart.\n+  \n+  torchserve_image: pytorch/torchserve:latest-gpu\n+  \n+  namespace: torchserve\n+  \n+  torchserve:\n+    management_port: 8081\n+    inference_port: 8080\n+    pvd_mount: /home/model-server/shared/\n+    n_gpu: 1\n+    n_cpu: 1\n+    memory_limit: 4Gi\n+    memory_request: 1Gi\n+  \n+  deployment:\n+    replicas: 1 # Changes this to number of node in Node Group\n+  \n+  persitant_volume:\n+    size: 1Gi\n+  ```\n+\n+\n+  To install Torchserve run ```helm install ts .```  ", "originalCommit": "ef5445cde2a73cad0da5f83fe6043fb68306fd16", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "970b9c0a5d85f6cc54140a508cbbad21a12b0135", "url": "https://github.com/pytorch/serve/commit/970b9c0a5d85f6cc54140a508cbbad21a12b0135", "message": "Update kubernetes/EKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-09-30T06:16:10Z", "type": "commit"}, {"oid": "24acfc3183df93c578e7465bf09a67331eca60dc", "url": "https://github.com/pytorch/serve/commit/24acfc3183df93c578e7465bf09a67331eca60dc", "message": "merge", "committedDate": "2020-09-30T06:31:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NzI2Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497677263", "bodyText": "NIT: Please use a descriptive name", "author": "maaquib", "createdAt": "2020-09-30T17:19:53Z", "path": "kubernetes/AKS/templates/AKS_pv_claim.yaml", "diffHunk": "@@ -0,0 +1,11 @@\n+kind: PersistentVolumeClaim\n+apiVersion: v1\n+metadata:\n+  name: model-store-claim\n+spec:\n+  storageClassName: my-azurefile", "originalCommit": "24acfc3183df93c578e7465bf09a67331eca60dc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NjIzNw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r502256237", "bodyText": "The name of the storageclass has been changed to \"persistent-volume-azurefile\". The name of the PersistentVolumeClaim is the same as that in EKS/templates/efs_pv_claim.yaml, is it necessary to change it?", "author": "MengMeng96", "createdAt": "2020-10-09T08:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NzI2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NzU4Ng==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497677586", "bodyText": "NIT: Please use a descriptive name", "author": "maaquib", "createdAt": "2020-09-30T17:20:26Z", "path": "kubernetes/AKS/templates/Azure_file_sc.yaml", "diffHunk": "@@ -0,0 +1,14 @@\n+kind: StorageClass\n+apiVersion: storage.k8s.io/v1\n+metadata:\n+  name: my-azurefile", "originalCommit": "24acfc3183df93c578e7465bf09a67331eca60dc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NDgxMw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r502254813", "bodyText": "Thanks for your comment. The name of the storageclass has been changed to \"persistent-volume-azurefile\"", "author": "MengMeng96", "createdAt": "2020-10-09T08:00:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NzU4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3OTIzNw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497679237", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### 1\u3001Create an AKS cluster\n          \n          \n            \n            ### 1 Create an AKS cluster", "author": "maaquib", "createdAt": "2020-09-30T17:23:13Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1\u3001Create an AKS cluster", "originalCommit": "24acfc3183df93c578e7465bf09a67331eca60dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3OTU1Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r497679553", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### 2\u3001Deploy TorchServe on AKS\n          \n          \n            \n            ### 2 Deploy TorchServe on AKS", "author": "maaquib", "createdAt": "2020-09-30T17:23:52Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1\u3001Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 1.6 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2\u3001Deploy TorchServe on AKS", "originalCommit": "24acfc3183df93c578e7465bf09a67331eca60dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "00e26377161ffd511d7e8b0fe47daf9de71fb2c0", "url": "https://github.com/pytorch/serve/commit/00e26377161ffd511d7e8b0fe47daf9de71fb2c0", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>", "committedDate": "2020-10-01T07:49:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA2ODAxMQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r499068011", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ```kubectl apply -f templates/model-store-pod.yaml```\n          \n          \n            \n            ```kubectl apply -f templates/model_store_pod.yaml```\n          \n      \n    \n    \n  \n\nThe checked in yaml file uses underscores instead", "author": "jeremiahschung", "createdAt": "2020-10-02T22:01:08Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 1.6 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2\u3001Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.3 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.4 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model-store-pod.yaml```", "originalCommit": "00e26377161ffd511d7e8b0fe47daf9de71fb2c0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA2ODM4Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r499068383", "bodyText": "We don't have the .yaml file yet in this step because we clone the repo in step 2.1. Can we change the order so that the repo is cloned earlier or the NVidia plugin is applied later?", "author": "jeremiahschung", "createdAt": "2020-10-02T22:02:29Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f nvidia-device-plugin-ds.yaml```", "originalCommit": "00e26377161ffd511d7e8b0fe47daf9de71fb2c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1Mzk0MA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r502253940", "bodyText": "Thanks for your comments. The order of AKS/README.md has been changed.", "author": "MengMeng96", "createdAt": "2020-10-09T07:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA2ODM4Mw=="}], "type": "inlineReview"}, {"oid": "bf41c6e246f405bf71409875d34b3c87bc7a9435", "url": "https://github.com/pytorch/serve/commit/bf41c6e246f405bf71409875d34b3c87bc7a9435", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-10-09T03:22:22Z", "type": "commit"}, {"oid": "3e4019c4705c37faf682848db7f00a0a0d383594", "url": "https://github.com/pytorch/serve/commit/3e4019c4705c37faf682848db7f00a0a0d383594", "message": "Merge branch 'master' into aks", "committedDate": "2020-10-09T03:27:44Z", "type": "commit"}, {"oid": "a5494269753cd8a9d3547a659cbfffa5c8c2b640", "url": "https://github.com/pytorch/serve/commit/a5494269753cd8a9d3547a659cbfffa5c8c2b640", "message": "change the order of AKS/README.md", "committedDate": "2020-10-09T03:34:17Z", "type": "commit"}, {"oid": "32765dbb08cd8e16d5561041f41b2cebff44237a", "url": "https://github.com/pytorch/serve/commit/32765dbb08cd8e16d5561041f41b2cebff44237a", "message": "change azurefile name", "committedDate": "2020-10-09T03:54:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA4MDQ1Mw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505080453", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            `kubectl get pods` should show something similar to:", "author": "jeremiahschung", "createdAt": "2020-10-14T23:50:59Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA4ODY5MQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505088691", "bodyText": "nit: Since the EKS document puts log results in boxes, let's do the same to keep the docs consistent.\n(Note: I'm unable to actually suggest the change because of how markdown works)\nNAME                  READY  STATUS  RESTARTS  AGE\nnvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s", "author": "jeremiahschung", "createdAt": "2020-10-15T00:03:54Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA4ODg2NQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505088865", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            persistentvolumeclaim/model-store-claim created\n          \n          \n            \n            ```persistentvolumeclaim/model-store-claim created```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:04:23Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA4OTI4Mg==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505089282", "bodyText": "nit: wrap in a box with ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:05:50Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MDAyNg==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505090026", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            pod/model-store-pod created\n          \n          \n            \n            ```pod/model-store-pod created```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:08:31Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MDQ1OA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505090458", "bodyText": "nit: wrap in box with ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:09:54Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MDg1MA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505090850", "bodyText": "torchserve pod not created yet in this step\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NAME                                   READY   STATUS    RESTARTS   AGE\n          \n          \n            \n            model-store-pod                        1/1     Running   0          143m\n          \n          \n            \n            nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n          \n          \n            \n            torchserve-576df559ff-tww7q            1/1     Running   0          141m\n          \n          \n            \n            NAME                                   READY   STATUS    RESTARTS   AGE\n          \n          \n            \n            model-store-pod                        1/1     Running   0          143m\n          \n          \n            \n            nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m", "author": "jeremiahschung", "createdAt": "2020-10-15T00:11:22Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MTE5MQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505091191", "bodyText": "Use new pytorch.org URLs for mar files\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n          \n          \n            \n            wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n          \n          \n            \n            wget https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar\n          \n          \n            \n            wget https://torchserve.pytorch.org/mar_files/mnist.mar", "author": "jeremiahschung", "createdAt": "2020-10-15T00:12:31Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MTcxNA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505091714", "bodyText": "nit: wrap in ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:14:25Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MTk3Mg==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505091972", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Enter the Helm directory and install TorchServe using Helm Charts\n          \n          \n            \n            Enter the Helm directory and install TorchServe using Helm Charts.", "author": "jeremiahschung", "createdAt": "2020-10-15T00:15:12Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MjA4MQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505092081", "bodyText": "wrap in ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:15:33Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts\n+```cd ../Helm```\n+\n+```helm install ts .```\n+\n+Your output should look similar to\n+\n+NAME: ts\n+LAST DEPLOYED: Thu Aug 20 02:07:38 2020\n+NAMESPACE: default\n+STATUS: deployed\n+REVISION: 1\n+TEST SUITE: None", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MjM5OQ==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505092399", "bodyText": "wrap in ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:16:35Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts\n+```cd ../Helm```\n+\n+```helm install ts .```\n+\n+Your output should look similar to\n+\n+NAME: ts\n+LAST DEPLOYED: Thu Aug 20 02:07:38 2020\n+NAMESPACE: default\n+STATUS: deployed\n+REVISION: 1\n+TEST SUITE: None\n+\n+#### 2.8 Check the status of TorchServe\n+\n+```kubectl get po```\n+\n+The installation will take a few minutes. Output like this means the installation is not completed yet.\n+\n+NAME                               READY   STATUS              RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        0/1     ContainerCreating   0          6s", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MjQzNw==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505092437", "bodyText": "wrap in ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:16:42Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts\n+```cd ../Helm```\n+\n+```helm install ts .```\n+\n+Your output should look similar to\n+\n+NAME: ts\n+LAST DEPLOYED: Thu Aug 20 02:07:38 2020\n+NAMESPACE: default\n+STATUS: deployed\n+REVISION: 1\n+TEST SUITE: None\n+\n+#### 2.8 Check the status of TorchServe\n+\n+```kubectl get po```\n+\n+The installation will take a few minutes. Output like this means the installation is not completed yet.\n+\n+NAME                               READY   STATUS              RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        0/1     ContainerCreating   0          6s\n+\n+Output like this means the installation is completed.\n+\n+NAME                               READY   STATUS    RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        1/1     Running   0          2m36s", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTA5MjUwMA==", "url": "https://github.com/pytorch/serve/pull/644#discussion_r505092500", "bodyText": "wrap in ```", "author": "jeremiahschung", "createdAt": "2020-10-15T00:16:53Z", "path": "kubernetes/AKS/README.md", "diffHunk": "@@ -0,0 +1,262 @@\n+## TorchServe on Azure Kubernetes Service (AKS)\n+\n+### 1 Create an AKS cluster\n+\n+This quickstart requires that you are running the Azure CLI version 2.0.64 or later. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli).\n+\n+#### 1.1 Set Azure account information\n+\n+```az login``` \n+\n+```az account set -s your-subscription-ID```\n+\n+#### 1.2 Create a resource group\n+\n+An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored, it is also where your resources run in Azure if you don't specify another region during resource creation. Create a resource group using the [az group create](https://docs.microsoft.com/en-us/cli/azure/group#az-group-create) command.\n+\n+The following example creates a resource group named *myResourceGroup* in the *eastus* location.\n+\n+```az group create --name myResourceGroup --location eastus```\n+\n+#### 1.3 Create AKS cluster\n+\n+Use the [az aks create](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-create) command to create an AKS cluster. The following example creates a cluster named *myAKSCluster* with one node. This will take several minutes to complete.\n+\n+```az aks create  --resource-group myResourceGroup  --name myAKSCluster --node-vm-size Standard_NC6   --node-count 1```\n+\n+#### 1.4 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use Azure Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [az aks install-cli](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli) command:\n+\n+```az aks install-cli```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [az aks get-credentials](https://docs.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```az aks get-credentials --resource-group myResourceGroup --name myAKSCluster```\n+\n+#### 1.5 Install helm\n+\n+```\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on AKS\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```git clone https://github.com/pytorch/serve.git```\n+\n+```cd serve/kubernetes/AKS```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```kubectl apply -f templates/nvidia-device-plugin-ds.yaml```\n+\n+NAME                  READY  STATUS  RESTARTS  AGE\n+\n+nvidia-device-plugin-daemonset-7lvxd  1/1   Running  0     42s\n+\n+#### 2.3 Create a storage class\n+\n+A storage class is used to define how an Azure file share is created. If multiple pods need concurrent access to the same storage volume, you need Azure Files. Create the storage class with the following kubectl apply command:\n+\n+```kubectl apply -f templates/Azure_file_sc.yaml```\n+\n+#### 2.4 Create PersistentVolume\n+\n+```kubectl apply -f templates/AKS_pv_claim.yaml```\n+\n+Your output should look similar to\n+\n+persistentvolumeclaim/model-store-claim created\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```kubectl get pvc,pv```\n+\n+Your output should look similar to\n+\n+NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n+persistentvolumeclaim/model-store-claim   Bound    pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            managed-premium   29s\n+\n+NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS      REASON   AGE\n+persistentvolume/pvc-c9e235a8-ca2b-4d04-8f25-8262de1bb915   1Gi        RWO            Delete           Bound    default/model-store-claim   managed-premium            28s\n+\n+#### 2.5 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```kubectl apply -f templates/model_store_pod.yaml```\n+\n+Your output should look similar to\n+\n+pod/model-store-pod created\n+\n+Verify that the pod is created by excuting.\n+\n+```kubectl get po```\n+\n+Your output should look similar to\n+\n+NAME                                   READY   STATUS    RESTARTS   AGE\n+model-store-pod                        1/1     Running   0          143m\n+nvidia-device-plugin-daemonset-qccgn   1/1     Running   0          144m\n+torchserve-576df559ff-tww7q            1/1     Running   0          141m\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```shell\n+wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/mnt/azure/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/mnt/azure/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /mnt/azure/config/\n+kubectl cp config.properties model-store-pod:/mnt/azure/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```kubectl exec --tty pod/model-store-pod -- find /mnt/azure/```\n+\n+Your output should look similar to\n+\n+/mnt/azure/\n+/mnt/azure/config\n+/mnt/azure/config/config.properties\n+/mnt/azure/lost+found\n+/mnt/azure/model-store\n+/mnt/azure/model-store/mnist.mar\n+/mnt/azure/model-store/squeezenet1_1.mar\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts\n+```cd ../Helm```\n+\n+```helm install ts .```\n+\n+Your output should look similar to\n+\n+NAME: ts\n+LAST DEPLOYED: Thu Aug 20 02:07:38 2020\n+NAMESPACE: default\n+STATUS: deployed\n+REVISION: 1\n+TEST SUITE: None\n+\n+#### 2.8 Check the status of TorchServe\n+\n+```kubectl get po```\n+\n+The installation will take a few minutes. Output like this means the installation is not completed yet.\n+\n+NAME                               READY   STATUS              RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        0/1     ContainerCreating   0          6s\n+\n+Output like this means the installation is completed.\n+\n+NAME                               READY   STATUS    RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        1/1     Running   0          2m36s\n+\n+### 3 Test Torchserve Installation\n+\n+#### 3.1 Fetch the Load Balancer Extenal IP\n+\n+Fetch the Load Balancer Extenal IP by executing.\n+\n+```kubectl get svc```\n+\n+Your output should look similar to\n+\n+NAME               TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                         AGE\n+kubernetes         ClusterIP      10.0.0.1       <none>         443/TCP                         5d19h\n+torchserve         LoadBalancer   10.0.39.88     your-external-IP   8080:30306/TCP,8081:30442/TCP   48s", "originalCommit": "32765dbb08cd8e16d5561041f41b2cebff44237a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e79e281b64fc33bedca1738fdc9031e90b72f4c7", "url": "https://github.com/pytorch/serve/commit/e79e281b64fc33bedca1738fdc9031e90b72f4c7", "message": "Merge branch 'master' of https://github.com/pytorch/serve into pytorch-master", "committedDate": "2020-11-19T05:07:09Z", "type": "commit"}, {"oid": "954843d7e7b00198204f7d0caf6b1cb2f12e086c", "url": "https://github.com/pytorch/serve/commit/954843d7e7b00198204f7d0caf6b1cb2f12e086c", "message": "Merge branch 'pytorch-master' into master", "committedDate": "2020-11-19T05:07:37Z", "type": "commit"}, {"oid": "c40639d2d8012024741cf27d61b92c3c61545f69", "url": "https://github.com/pytorch/serve/commit/c40639d2d8012024741cf27d61b92c3c61545f69", "message": "Merge branch 'master' into aks", "committedDate": "2020-11-19T05:13:38Z", "type": "commit"}, {"oid": "f8c9dde88d70f8c04721931ff602b725dcb97ef7", "url": "https://github.com/pytorch/serve/commit/f8c9dde88d70f8c04721931ff602b725dcb97ef7", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:45Z", "type": "commit"}, {"oid": "a8de326aa4dd2fa37ce1ea7c2bc64c4d50517916", "url": "https://github.com/pytorch/serve/commit/a8de326aa4dd2fa37ce1ea7c2bc64c4d50517916", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:47Z", "type": "commit"}, {"oid": "1f5604a9f5c54970fa7fe6f2eb101469f8e52ec5", "url": "https://github.com/pytorch/serve/commit/1f5604a9f5c54970fa7fe6f2eb101469f8e52ec5", "message": "Update kubernetes/AKS/README.md\n\nCo-authored-by: jeremiahschung <70922646+jeremiahschung@users.noreply.github.com>", "committedDate": "2020-11-19T05:13:49Z", "type": "commit"}]}