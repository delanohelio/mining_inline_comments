{"pr_number": 140, "pr_title": "Eslesar documentation review - issue #127", "pr_createdAt": "2020-03-31T02:59:05Z", "pr_url": "https://github.com/pytorch/serve/pull/140", "timeline": [{"oid": "68b2f96a29e8f2893393e9ec1e51ad717fa90831", "url": "https://github.com/pytorch/serve/commit/68b2f96a29e8f2893393e9ec1e51ad717fa90831", "message": "initial comit for docs review", "committedDate": "2020-03-30T20:48:07Z", "type": "commit"}, {"oid": "98d74049bbceef99cd1024b8daf2deab86cbd86d", "url": "https://github.com/pytorch/serve/commit/98d74049bbceef99cd1024b8daf2deab86cbd86d", "message": "Merge branch 'stage_release' of https://github.com/pytorch/serve into eslesar-issue-127", "committedDate": "2020-03-30T20:48:25Z", "type": "commit"}, {"oid": "4501d1cfaed7f49c93800124b287fd48645d8ea4", "url": "https://github.com/pytorch/serve/commit/4501d1cfaed7f49c93800124b287fd48645d8ea4", "message": "completed first batch of doc review", "committedDate": "2020-03-31T02:45:14Z", "type": "commit"}, {"oid": "b3564ef10e277ac8f1e266de986bf4b85cab16b1", "url": "https://github.com/pytorch/serve/commit/b3564ef10e277ac8f1e266de986bf4b85cab16b1", "message": "moved sections from install topic to main readme and removed install.md", "committedDate": "2020-03-31T02:55:29Z", "type": "commit"}, {"oid": "2a7ebfcae533ae320cc3b46297388803da9749d3", "url": "https://github.com/pytorch/serve/commit/2a7ebfcae533ae320cc3b46297388803da9749d3", "message": "resolved merge conflicts", "committedDate": "2020-04-01T20:49:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1MTY4Mw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404451683", "bodyText": "We going to sentence case?", "author": "aaronmarkham", "createdAt": "2020-04-06T23:38:26Z", "path": "README.md", "diffHunk": "@@ -107,30 +112,27 @@ pip install .\n ```\n \n **Notes:**\n-* In case `pip install .` step fails, try using `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n+\n+* If `pip install .`  fails, run `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n * See the [advanced installation](docs/install.md) page for more options and troubleshooting.\n \n-### Installing torch-model-archiver\n+### Install TorchServe for Development", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1MzA2OA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404453068", "bodyText": "This is running together in the preview. An extra line break should fix it.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.\n          \n          \n            \n            \n          \n          \n            \n            TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.", "author": "aaronmarkham", "createdAt": "2020-04-06T23:42:37Z", "path": "README.md", "diffHunk": "@@ -65,12 +69,12 @@ Refer to the [Virtualenv documentation](https://virtualenv.pypa.io/en/stable/) f\n [Download anaconda distribution](https://www.anaconda.com/distribution/#download-section)\n \n Then create a virtual environment using conda.\n+\n ```bash\n conda create -n myenv\n source activate myenv\n ```\n \n-\n **Step 2:** Install torch\n TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NDcxOQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404454719", "bodyText": "This is for development mode. See comments on install.md... I think that stuff need to be synced up here.", "author": "aaronmarkham", "createdAt": "2020-04-06T23:47:45Z", "path": "README.md", "diffHunk": "@@ -107,30 +112,27 @@ pip install .\n ```\n \n **Notes:**\n-* In case `pip install .` step fails, try using `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n+\n+* If `pip install .`  fails, run `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n * See the [advanced installation](docs/install.md) page for more options and troubleshooting.\n \n-### Installing torch-model-archiver\n+### Install TorchServe for Development\n \n-*Install torch-model-archiver as follows:\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n ```bash\n cd model-archiver\n pip install .\n ```\n \n-**Note** \n-* Once torch-model-arvchiver is available in Python Package Index (PyPi), it will be a part of dependency in TorchServe installation.\n-* See the [detailed documentation](model-archiver/README.md) page for more options and troubleshooting.\n-### Serve a Model\n+To upgrade TorchServe from source code and make changes executable, run:\n \n-Once installed, you can get TorchServe model server up and running very quickly. Try out `--help` to see all the CLI options available.\n \n ```bash\n-torchserve --help\n+pip install -U -e .", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NTU5NQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404455595", "bodyText": "This would be for archiver, not dev mode.", "author": "aaronmarkham", "createdAt": "2020-04-06T23:50:19Z", "path": "README.md", "diffHunk": "@@ -107,30 +112,27 @@ pip install .\n ```\n \n **Notes:**\n-* In case `pip install .` step fails, try using `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n+\n+* If `pip install .`  fails, run `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n * See the [advanced installation](docs/install.md) page for more options and troubleshooting.\n \n-### Installing torch-model-archiver\n+### Install TorchServe for Development\n \n-*Install torch-model-archiver as follows:\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n ```bash", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NTgxNQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404455815", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            *Install torch-model-archiver as follows:\n          \n          \n            \n            * Install torch-model-archiver as follows:", "author": "aaronmarkham", "createdAt": "2020-04-06T23:50:59Z", "path": "README.md", "diffHunk": "@@ -141,68 +143,16 @@ mv densenet161.mar model_store/\n torchserve --start --model-store model_store --models densenet161=densenet161.mar\n ```\n \n-With the command above executed, you have TorchServe running on your host, listening for inference requests. **Please note, that if you specify model(s) during TorchServe start - it will automatically scale backend workers to the number equal to available vCPUs (if you run on CPU instance) or to the number of available GPUs (if you run on GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs) this start up and autoscaling process might take considerable time. If you would like to minimize TorchServe start up time you can try to avoid registering and scaling up model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model) calls (this allows finer grain control to how much resources are allocated for any particular model).**\n-\n-To test it out, you can open a new terminal window next to the one running TorchServe. Then you can use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images) and curl's `-o` flag will name it `kitten.jpg` for you. Then you will `curl` a `POST` to the TorchServe predict endpoint with the kitten's image.\n-\n-![kitten](docs/images/kitten_small.jpg)\n-\n-In the example below, we provide a shortcut for these steps.\n-\n-```bash\n-curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n-curl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n-```\n-\n-The predict endpoint will return a prediction response in JSON. It will look something like the following result:\n-\n-```json\n-[\n-  {\n-    \"tiger_cat\": 0.46933549642562866\n-  },\n-  {\n-    \"tabby\": 0.4633878469467163\n-  },\n-  {\n-    \"Egyptian_cat\": 0.06456148624420166\n-  },\n-  {\n-    \"lynx\": 0.0012828214094042778\n-  },\n-  {\n-    \"plastic_bag\": 0.00023323034110944718\n-  }\n-]\n-```\n-\n-You will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n+### Install torch-model-archiver\n \n-Now you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n+*Install torch-model-archiver as follows:", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NzQxMw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404457413", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To get predictions from the default version of each loaded model, make a REST call to `/predictions/{model_name}`:\n          \n          \n            \n            To get predictions from the model, make a REST call to the predictions endpoint using hte model's name, e.g. `/predictions/{model_name}`. You can also get a prediction from a specific version of the model by adding the version name to the call. Examples:", "author": "aaronmarkham", "createdAt": "2020-04-06T23:55:49Z", "path": "docs/inference_api.md", "diffHunk": "@@ -38,11 +38,11 @@ Your response, if the server is running should be:\n \n ## Predictions API\n \n-To run inference on the default version of each loaded model, user can make REST call to URI: /predictions/{model_name}. \n+To get predictions from the default version of each loaded model, make a REST call to `/predictions/{model_name}`:", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODA1Mw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404458053", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * POST /predictions/{model_name}\n          \n          \n            \n            * POST /predictions/{model_name}\n          \n          \n            \n            * POST /predictions/{model_name}/{version}", "author": "aaronmarkham", "createdAt": "2020-04-06T23:57:53Z", "path": "docs/inference_api.md", "diffHunk": "@@ -38,11 +38,11 @@ Your response, if the server is running should be:\n \n ## Predictions API\n \n-To run inference on the default version of each loaded model, user can make REST call to URI: /predictions/{model_name}. \n+To get predictions from the default version of each loaded model, make a REST call to `/predictions/{model_name}`:\n \n * POST /predictions/{model_name}", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODQwMw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404458403", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### curl Example\n          \n          \n            \n            ### Example prediction using the default model", "author": "aaronmarkham", "createdAt": "2020-04-06T23:58:56Z", "path": "docs/inference_api.md", "diffHunk": "@@ -38,11 +38,11 @@ Your response, if the server is running should be:\n \n ## Predictions API\n \n-To run inference on the default version of each loaded model, user can make REST call to URI: /predictions/{model_name}. \n+To get predictions from the default version of each loaded model, make a REST call to `/predictions/{model_name}`:\n \n * POST /predictions/{model_name}\n \n-**curl Example**\n+### curl Example", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODUzOA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404458538", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## curl Example\n          \n          \n            \n            ### Example prediction using a specific version of a model", "author": "aaronmarkham", "createdAt": "2020-04-06T23:59:20Z", "path": "docs/inference_api.md", "diffHunk": "@@ -54,11 +54,11 @@ or:\n curl -X POST http://localhost:8080/predictions/resnet-18 -F \"data=@kitten.jpg\"\n ```\n \n-To run inference on the specific version of each loaded model, user can make REST call to URI: /predictions/{model_name}/{version}. \n+To get predictions from a specific version of each loaded model, make a REST call to `/predictions/{model_name}/{version}`:\n \n * POST /predictions/{model_name}/{version}\n \n-**curl Example**\n+## curl Example", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODczMQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404458731", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \n          \n          \n            \n            Version 2.0 is going to be used in this example.", "author": "aaronmarkham", "createdAt": "2020-04-06T23:59:53Z", "path": "docs/inference_api.md", "diffHunk": "@@ -54,11 +54,11 @@ or:\n curl -X POST http://localhost:8080/predictions/resnet-18 -F \"data=@kitten.jpg\"\n ```\n \n-To run inference on the specific version of each loaded model, user can make REST call to URI: /predictions/{model_name}/{version}. \n+To get predictions from a specific version of each loaded model, make a REST call to `/predictions/{model_name}/{version}`:\n \n * POST /predictions/{model_name}/{version}\n \n-**curl Example**\n+## curl Example\n ", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MTQzMQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404461431", "bodyText": "Agree with consolidating installation info, but I think you're removing some valuable info here...", "author": "aaronmarkham", "createdAt": "2020-04-07T00:09:02Z", "path": "docs/install.md", "diffHunk": "@@ -1,90 +0,0 @@\n-\n-# Install TorchServe\n-\n-## Prerequisites\n-\n-* **Python**: Required. Model Server for PyTorch (TorchServe) works with Python 3.  When installing TorchServe, we recommend that you use a Python and Conda environment to avoid conflicts with your other Torch installations.\n-\n-* **java 8**: Required. TorchServe use java to serve HTTP requests. You must install java 8 (or later) and make sure java is on available in $PATH environment variable *before* installing TorchServe. If you have multiple java installed, you can use $JAVA_HOME environment variable to control which java to use.", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MTU4OA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404461588", "bodyText": "Port this stuff to readme.", "author": "aaronmarkham", "createdAt": "2020-04-07T00:09:30Z", "path": "docs/install.md", "diffHunk": "@@ -1,90 +0,0 @@\n-\n-# Install TorchServe\n-\n-## Prerequisites\n-\n-* **Python**: Required. Model Server for PyTorch (TorchServe) works with Python 3.  When installing TorchServe, we recommend that you use a Python and Conda environment to avoid conflicts with your other Torch installations.\n-\n-* **java 8**: Required. TorchServe use java to serve HTTP requests. You must install java 8 (or later) and make sure java is on available in $PATH environment variable *before* installing TorchServe. If you have multiple java installed, you can use $JAVA_HOME environment variable to control which java to use.\n-\n-For Ubuntu:\n-```bash\n-sudo apt-get install openjdk-11-jdk\n-```\n-\n-For CentOS:\n-```bash\n-openjdk-11-jdk\n-sudo yum install java-11-openjdk\n-```\n-\n-For macOS:\n-```bash\n-brew tap AdoptOpenJDK/openjdk\n-brew cask install adoptopenjdk11\n-```\n-\n-You can also download and install [Oracle JDK](https://www.oracle.com/technetwork/java/javase/overview/index.html) manually if you have trouble with above commands.\n-\n-* **Torch**: Recommended. TorchServe won't install `torch` by default. Torch is required for most of examples in this project. TorchServe won't install torch engine by default. And you can also choose specific version of torch if you want.\n-\n-* For virtualenv\n-\n-```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n-```\n-\n-* For conda\n-\n-```bash\n-#For CPU\n-conda install psutil pytorch torchvision torchtext -c pytorch\n-```\n-\n-```bash\n-#For GPU\n-conda install future psutil pytorch torchvision cudatoolkit=10.1 torchtext -c pytorch\n-```\n-\n-\n-* **Curl**: Optional. Curl is used in all of the examples. Install it with your preferred package manager.\n-\n-* **Unzip**: Optional. Unzip allows you to easily extract model files and inspect their content. If you choose to use it, associate it with `.mar` extensions.\n-\n-\n-## Install TorchServe from Source Code", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MTcwNw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404461707", "bodyText": "This too... more accurate than what's in the readme.", "author": "aaronmarkham", "createdAt": "2020-04-07T00:09:50Z", "path": "docs/install.md", "diffHunk": "@@ -1,90 +0,0 @@\n-\n-# Install TorchServe\n-\n-## Prerequisites\n-\n-* **Python**: Required. Model Server for PyTorch (TorchServe) works with Python 3.  When installing TorchServe, we recommend that you use a Python and Conda environment to avoid conflicts with your other Torch installations.\n-\n-* **java 8**: Required. TorchServe use java to serve HTTP requests. You must install java 8 (or later) and make sure java is on available in $PATH environment variable *before* installing TorchServe. If you have multiple java installed, you can use $JAVA_HOME environment variable to control which java to use.\n-\n-For Ubuntu:\n-```bash\n-sudo apt-get install openjdk-11-jdk\n-```\n-\n-For CentOS:\n-```bash\n-openjdk-11-jdk\n-sudo yum install java-11-openjdk\n-```\n-\n-For macOS:\n-```bash\n-brew tap AdoptOpenJDK/openjdk\n-brew cask install adoptopenjdk11\n-```\n-\n-You can also download and install [Oracle JDK](https://www.oracle.com/technetwork/java/javase/overview/index.html) manually if you have trouble with above commands.\n-\n-* **Torch**: Recommended. TorchServe won't install `torch` by default. Torch is required for most of examples in this project. TorchServe won't install torch engine by default. And you can also choose specific version of torch if you want.\n-\n-* For virtualenv\n-\n-```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n-```\n-\n-* For conda\n-\n-```bash\n-#For CPU\n-conda install psutil pytorch torchvision torchtext -c pytorch\n-```\n-\n-```bash\n-#For GPU\n-conda install future psutil pytorch torchvision cudatoolkit=10.1 torchtext -c pytorch\n-```\n-\n-\n-* **Curl**: Optional. Curl is used in all of the examples. Install it with your preferred package manager.\n-\n-* **Unzip**: Optional. Unzip allows you to easily extract model files and inspect their content. If you choose to use it, associate it with `.mar` extensions.\n-\n-\n-## Install TorchServe from Source Code\n-\n-If you prefer, you can clone TorchServe from source code. First, run the following command:\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-pip install .\n-```\n-\n-**Notes:**\n-* In case `pip install .` step fails, try using `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n-\n-## Install TorchServe for Development", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MjMwOA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404462308", "bodyText": "I think we need a more detailed description of the parts (torchserve and archiver) plus a mini TOC to the main docs. Having them only available from a link in a sentence is hard to find.", "author": "aaronmarkham", "createdAt": "2020-04-07T00:11:44Z", "path": "README.md", "diffHunk": "@@ -1,37 +1,40 @@\n-TorchServe\n-=======\n+# TorchServe\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-A quick overview and examples for both serving and packaging are provided below. Detailed documentation and examples are provided in the [docs folder](docs/README.md).\n+For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MzA1Mg==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404463052", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * [Install TorchServe](#install-torchserve)\n          \n          \n            \n            * [Install TorchServe](#install-torchserve)\n          \n          \n            \n            * [Install Torch model archiver](#install-torch-model-archiver)", "author": "aaronmarkham", "createdAt": "2020-04-07T00:14:12Z", "path": "README.md", "diffHunk": "@@ -1,37 +1,40 @@\n-TorchServe\n-=======\n+# TorchServe\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-A quick overview and examples for both serving and packaging are provided below. Detailed documentation and examples are provided in the [docs folder](docs/README.md).\n+For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n \n ## Contents of this Document\n-* [Quick Start](#quick-start)\n-* [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+\n+* [Install TorchServe](#install-torchserve)", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MzE5Mw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404463193", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Install torch-model-archiver\n          \n          \n            \n            ### Install Torch model archiver", "author": "aaronmarkham", "createdAt": "2020-04-07T00:14:45Z", "path": "README.md", "diffHunk": "@@ -141,68 +143,16 @@ mv densenet161.mar model_store/\n torchserve --start --model-store model_store --models densenet161=densenet161.mar\n ```\n \n-With the command above executed, you have TorchServe running on your host, listening for inference requests. **Please note, that if you specify model(s) during TorchServe start - it will automatically scale backend workers to the number equal to available vCPUs (if you run on CPU instance) or to the number of available GPUs (if you run on GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs) this start up and autoscaling process might take considerable time. If you would like to minimize TorchServe start up time you can try to avoid registering and scaling up model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model) calls (this allows finer grain control to how much resources are allocated for any particular model).**\n-\n-To test it out, you can open a new terminal window next to the one running TorchServe. Then you can use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images) and curl's `-o` flag will name it `kitten.jpg` for you. Then you will `curl` a `POST` to the TorchServe predict endpoint with the kitten's image.\n-\n-![kitten](docs/images/kitten_small.jpg)\n-\n-In the example below, we provide a shortcut for these steps.\n-\n-```bash\n-curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n-curl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n-```\n-\n-The predict endpoint will return a prediction response in JSON. It will look something like the following result:\n-\n-```json\n-[\n-  {\n-    \"tiger_cat\": 0.46933549642562866\n-  },\n-  {\n-    \"tabby\": 0.4633878469467163\n-  },\n-  {\n-    \"Egyptian_cat\": 0.06456148624420166\n-  },\n-  {\n-    \"lynx\": 0.0012828214094042778\n-  },\n-  {\n-    \"plastic_bag\": 0.00023323034110944718\n-  }\n-]\n-```\n-\n-You will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n+### Install torch-model-archiver", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NDg5NQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404464895", "bodyText": "That's installing it from source, not pip. That should be a separate section.", "author": "aaronmarkham", "createdAt": "2020-04-07T00:20:37Z", "path": "README.md", "diffHunk": "@@ -107,30 +112,27 @@ pip install .\n ```", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NTQyMg==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404465422", "bodyText": "Why lead with archiver when you can lead with a .mar file and just do TorchServe?", "author": "aaronmarkham", "createdAt": "2020-04-07T00:22:34Z", "path": "docs/quick_start.md", "diffHunk": "@@ -0,0 +1,96 @@\n+\n+# Quick start\n+\n+This topic shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver. \n+For installation instructions, see the [TorchServe README](../README.md)\n+\n+## Store a Model", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDI1MQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r405694251", "bodyText": "Because customers aren't typically going to have a mar file out of the gate. This is why the archiver is included.", "author": "eslesar-aws", "createdAt": "2020-04-08T17:30:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NTQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NTY4MA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404465680", "bodyText": "Reference a URL to a model zoo file with a mar file there... rather than make the new user make their own...", "author": "aaronmarkham", "createdAt": "2020-04-07T00:23:25Z", "path": "docs/quick_start.md", "diffHunk": "@@ -0,0 +1,96 @@\n+\n+# Quick start\n+\n+This topic shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver. \n+For installation instructions, see the [TorchServe README](../README.md)\n+\n+## Store a Model\n+\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n+\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n+\n+```bash\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n+mkdir model_store\n+mv densenet161.mar model_store/\n+```\n+\n+For more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n+\n+## Serve a Model\n+\n+After you archive and store the model, use the `torchserve` command to serve the model.\n+\n+```bash\n+torchserve --start --model-store model_store --models densenet161=densenet161.mar", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5Mzg2MA==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r405693860", "bodyText": "I don't think this is a realistic scenario. PyTorch model zoo doesn't contain mar files.", "author": "eslesar-aws", "createdAt": "2020-04-08T17:30:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NTY4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NjI2Mw==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404466263", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Comlete the following steps:\n          \n          \n            \n            Complete the following steps:", "author": "aaronmarkham", "createdAt": "2020-04-07T00:25:22Z", "path": "docs/quick_start.md", "diffHunk": "@@ -0,0 +1,96 @@\n+\n+# Quick start\n+\n+This topic shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver. \n+For installation instructions, see the [TorchServe README](../README.md)\n+\n+## Store a Model\n+\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n+\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n+\n+```bash\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n+mkdir model_store\n+mv densenet161.mar model_store/\n+```\n+\n+For more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n+\n+## Serve a Model\n+\n+After you archive and store the model, use the `torchserve` command to serve the model.\n+\n+```bash\n+torchserve --start --model-store model_store --models densenet161=densenet161.mar\n+```\n+\n+After you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n+\n+**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n+\n+## Get predictions from a model\n+\n+To test the model server, send a request to the server's `predictions` API.\n+\n+Comlete the following steps:", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2NjY1MQ==", "url": "https://github.com/pytorch/serve/pull/140#discussion_r404466651", "bodyText": "Needs to be somewhere else and not in quickstart. Like a prod deployment page of its own...", "author": "aaronmarkham", "createdAt": "2020-04-07T00:26:47Z", "path": "docs/quick_start.md", "diffHunk": "@@ -0,0 +1,96 @@\n+\n+# Quick start\n+\n+This topic shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver. \n+For installation instructions, see the [TorchServe README](../README.md)\n+\n+## Store a Model\n+\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n+\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n+\n+```bash\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n+mkdir model_store\n+mv densenet161.mar model_store/\n+```\n+\n+For more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n+\n+## Serve a Model\n+\n+After you archive and store the model, use the `torchserve` command to serve the model.\n+\n+```bash\n+torchserve --start --model-store model_store --models densenet161=densenet161.mar\n+```\n+\n+After you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n+\n+**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n+\n+## Get predictions from a model\n+\n+To test the model server, send a request to the server's `predictions` API.\n+\n+Comlete the following steps:\n+* Open a new terminal window (other than the one running TorchServe).\n+* Use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images)\n+  and use the  `-o` flag to name it `kitten.jpg` for you.\n+* Use `curl` to send `POST` to the TorchServe `predict` endpoint with the kitten's image.\n+\n+![kitten](docs/images/kitten_small.jpg)\n+\n+The following code completes all three steps:\n+\n+```bash\n+curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n+curl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n+```\n+\n+The predict endpoint returns a prediction response in JSON. It will look something like the following result:\n+\n+```json\n+[\n+  {\n+    \"tiger_cat\": 0.46933549642562866\n+  },\n+  {\n+    \"tabby\": 0.4633878469467163\n+  },\n+  {\n+    \"Egyptian_cat\": 0.06456148624420166\n+  },\n+  {\n+    \"lynx\": 0.0012828214094042778\n+  },\n+  {\n+    \"plastic_bag\": 0.00023323034110944718\n+  }\n+]\n+```\n+\n+You will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n+\n+Now you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n+\n+## Stop the running TorchServe\n+\n+To stop the currently running TorchServe instance, run the following command:\n+\n+```bash\n+torchserve --stop\n+```\n+\n+You see output specifying that TorchServe has stopped.\n+\n+## Recommended production deployments", "originalCommit": "2a7ebfcae533ae320cc3b46297388803da9749d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1e01637424d7ad1581dc802c3d6ab9df33e7fd6e", "url": "https://github.com/pytorch/serve/commit/1e01637424d7ad1581dc802c3d6ab9df33e7fd6e", "message": "Merge branch 'stage_release' of https://github.com/pytorch/serve into eslesar-issue-127", "committedDate": "2020-04-07T16:27:05Z", "type": "commit"}, {"oid": "a7d2565df972da1b8d235286b95293bc60b6d937", "url": "https://github.com/pytorch/serve/commit/a7d2565df972da1b8d235286b95293bc60b6d937", "message": "Update README.md\n\nCo-Authored-By: Aaron Markham <markhama@amazon.com>", "committedDate": "2020-04-07T16:32:22Z", "type": "commit"}, {"oid": "3dda69bbcb51ec880a074ccf88cfef6bf40f993a", "url": "https://github.com/pytorch/serve/commit/3dda69bbcb51ec880a074ccf88cfef6bf40f993a", "message": "Incorporate feedback in README.md\n\nCo-Authored-By: Aaron Markham <markhama@amazon.com>", "committedDate": "2020-04-07T16:35:20Z", "type": "commit"}, {"oid": "8cff1ce0b4703c4554fd9c072162410f6d3a41fb", "url": "https://github.com/pytorch/serve/commit/8cff1ce0b4703c4554fd9c072162410f6d3a41fb", "message": "started incorporating review", "committedDate": "2020-04-07T20:43:51Z", "type": "commit"}, {"oid": "3fb35ff0acbb04d98c8c1c10a1a32141e2dc16aa", "url": "https://github.com/pytorch/serve/commit/3fb35ff0acbb04d98c8c1c10a1a32141e2dc16aa", "message": "address review comments and resolve merge conflicts", "committedDate": "2020-04-08T17:43:21Z", "type": "commit"}, {"oid": "961b078e8e1a6c200c05424f0efa19749f6ee26b", "url": "https://github.com/pytorch/serve/commit/961b078e8e1a6c200c05424f0efa19749f6ee26b", "message": "Merge branch 'eslesar-issue-127' of https://github.com/eslesar-aws/serve into eslesar-issue-127", "committedDate": "2020-04-08T17:46:17Z", "type": "commit"}, {"oid": "8378297c74d91bda8e6e65de67ed79544affdcb3", "url": "https://github.com/pytorch/serve/commit/8378297c74d91bda8e6e65de67ed79544affdcb3", "message": "moved archiver installation before troubleshooting", "committedDate": "2020-04-08T17:51:02Z", "type": "commit"}, {"oid": "49342103cc370315bd3828e50c27095cf75f3525", "url": "https://github.com/pytorch/serve/commit/49342103cc370315bd3828e50c27095cf75f3525", "message": "resolved merge conflicts", "committedDate": "2020-04-08T18:18:06Z", "type": "commit"}]}