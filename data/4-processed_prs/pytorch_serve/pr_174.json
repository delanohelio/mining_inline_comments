{"pr_number": 174, "pr_title": "doc review management_api.md and server.md", "pr_createdAt": "2020-04-09T20:29:07Z", "pr_url": "https://github.com/pytorch/serve/pull/174", "timeline": [{"oid": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "url": "https://github.com/pytorch/serve/commit/7242ef5bc2b33c4054d1792a0c3724b118feec84", "message": "doc review management_api.md and server.md", "committedDate": "2020-04-09T20:25:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MTQ1MQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406471451", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n          \n          \n            \n            The Management API listens on port 8081 and is only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).", "author": "aaronmarkham", "createdAt": "2020-04-09T20:54:23Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MTc1OQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406471759", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n          \n          \n            \n            Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with the OpenAPI 3.0 specification.", "author": "aaronmarkham", "createdAt": "2020-04-09T20:55:03Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n \n-## Management APIs\n+Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MjM5Ng==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406472396", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              * a local model archive (.mar); the file must be directly in model_store folder.\n          \n          \n            \n              * a local model archive (.mar); the file must be in the `model_store` folder (and not in a subfolder).\n          \n      \n    \n    \n  \n\nClarifying \"directly\"", "author": "aaronmarkham", "createdAt": "2020-04-09T20:56:20Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n \n-## Management APIs\n+Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n \n-### Register a model\n+## Register a model\n \n `POST /models`\n+\n * url - Model archive download url. Supports the following locations:\n-    * a local model archive (.mar); the file must be directly in model_store folder.\n-    * a URI using the HTTP(s) protocol. TorchServe can download .mar files from the Internet.\n+  * a local model archive (.mar); the file must be directly in model_store folder.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NTI1Nw==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406475257", "bodyText": "This line introduces the manifest file out of context. I looked for info on it and it looks like it's not included from the source repo. Save this task for later.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * model_name - the name of the model; this name will be used as {model_name} in other API as path. If this parameter is not present, modelName in MANIFEST.json will be used.\n          \n          \n            \n            * `model_name` - the name of the model; this name will be used as {model_name} in other APIs as part of the path. If this parameter is not present, `modelName` in MANIFEST.json will be used.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:01:59Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n \n-## Management APIs\n+Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n \n-### Register a model\n+## Register a model\n \n `POST /models`\n+\n * url - Model archive download url. Supports the following locations:\n-    * a local model archive (.mar); the file must be directly in model_store folder.\n-    * a URI using the HTTP(s) protocol. TorchServe can download .mar files from the Internet.\n+  * a local model archive (.mar); the file must be directly in model_store folder.\n+  * a URI using the HTTP(s) protocol. TorchServe can download .mar files from the Internet.\n * model_name - the name of the model; this name will be used as {model_name} in other API as path. If this parameter is not present, modelName in MANIFEST.json will be used.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NTQ0MQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406475441", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * url - Model archive download url. Supports the following locations:\n          \n          \n            \n            * `url` - Model archive download url. Supports the following locations:", "author": "aaronmarkham", "createdAt": "2020-04-09T21:02:22Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n \n-## Management APIs\n+Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n \n-### Register a model\n+## Register a model\n \n `POST /models`\n+\n * url - Model archive download url. Supports the following locations:", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NTg4NA==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406475884", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * handler - the inference handler entry-point. This value will override `handler` in MANIFEST.json if present. **NOTE: Make sure that the given `handler` is in the `PYTHONPATH`. The format of handler is `module_name:method_name`.**\n          \n          \n            \n            * `handler` - the inference handler entry-point. This value will override `handler` in MANIFEST.json if present. **NOTE: Make sure that the given `handler` is in the `PYTHONPATH`. The format of handler is `module_name:method_name`.**", "author": "aaronmarkham", "createdAt": "2020-04-09T21:03:15Z", "path": "docs/management_api.md", "diffHunk": "@@ -1,25 +1,25 @@\n # Management API\n \n-TorchServe provides a set of API allow user to manage models at runtime:\n+TorchServe provides the following APIs that allows you to manage models at runtime:\n+\n 1. [Register a model](#register-a-model)\n 2. [Increase/decrease number of workers for specific model](#scale-workers)\n 3. [Describe a model's status](#describe-model)\n 4. [Unregister a model](#unregister-a-model)\n 5. [List registered models](#list-models)\n 6. [Set default version of a model](#set-default-version)\n \n-Management API is listening on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n-\n-Similar as [Inference API](inference_api.md), Management API also provide a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n+The Management API listens on port 8081 and only accessible from localhost by default. To change the default setting, see [TorchServe Configuration](configuration.md).\n \n-## Management APIs\n+Similar to the [Inference API](inference_api.md), the Management API provides a [API description](#api-description) to describe management APIs with OpenAPI 3.0 specification.\n \n-### Register a model\n+## Register a model\n \n `POST /models`\n+\n * url - Model archive download url. Supports the following locations:\n-    * a local model archive (.mar); the file must be directly in model_store folder.\n-    * a URI using the HTTP(s) protocol. TorchServe can download .mar files from the Internet.\n+  * a local model archive (.mar); the file must be directly in model_store folder.\n+  * a URI using the HTTP(s) protocol. TorchServe can download .mar files from the Internet.\n * model_name - the name of the model; this name will be used as {model_name} in other API as path. If this parameter is not present, modelName in MANIFEST.json will be used.\n * handler - the inference handler entry-point. This value will override `handler` in MANIFEST.json if present. **NOTE: Make sure that the given `handler` is in the `PYTHONPATH`. The format of handler is `module_name:method_name`.**", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NjI0Mg==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406476242", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.\n          \n          \n            \n            * `min_worker` - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:03:56Z", "path": "docs/management_api.md", "diffHunk": "@@ -65,25 +66,25 @@ curl -v -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true\n < x-request-id: c4b2804e-42b1-4d6f-9e8f-1e8901fc2c6c\n < content-length: 32\n < connection: keep-alive\n-< \n+<\n {\n   \"status\": \"Worker scaled\"\n }\n ```\n \n-\n-### Scale workers\n+## Scale workers\n \n `PUT /models/{model_name}`\n+\n * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NjMyMA==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406476320", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * max_worker - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.\n          \n          \n            \n            * `max_worker` - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:04:05Z", "path": "docs/management_api.md", "diffHunk": "@@ -65,25 +66,25 @@ curl -v -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true\n < x-request-id: c4b2804e-42b1-4d6f-9e8f-1e8901fc2c6c\n < content-length: 32\n < connection: keep-alive\n-< \n+<\n {\n   \"status\": \"Worker scaled\"\n }\n ```\n \n-\n-### Scale workers\n+## Scale workers\n \n `PUT /models/{model_name}`\n+\n * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.\n * max_worker - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NjM4Nw==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406476387", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * number_gpu - (optional) the number of GPU worker processes to create. The default value is `0`. If number_gpu exceeds the number of available GPUs, the rest of workers will run on CPU.\n          \n          \n            \n            * `number_gpu` - (optional) the number of GPU worker processes to create. The default value is `0`. If number_gpu exceeds the number of available GPUs, the rest of workers will run on CPU.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:04:13Z", "path": "docs/management_api.md", "diffHunk": "@@ -65,25 +66,25 @@ curl -v -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true\n < x-request-id: c4b2804e-42b1-4d6f-9e8f-1e8901fc2c6c\n < content-length: 32\n < connection: keep-alive\n-< \n+<\n {\n   \"status\": \"Worker scaled\"\n }\n ```\n \n-\n-### Scale workers\n+## Scale workers\n \n `PUT /models/{model_name}`\n+\n * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.\n * max_worker - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.\n * number_gpu - (optional) the number of GPU worker processes to create. The default value is `0`. If number_gpu exceeds the number of available GPUs, the rest of workers will run on CPU.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NjQzOQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406476439", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * synchronous - whether or not the call is synchronous. The default value is `false`.\n          \n          \n            \n            * `synchronous` - whether or not the call is synchronous. The default value is `false`.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:04:22Z", "path": "docs/management_api.md", "diffHunk": "@@ -65,25 +66,25 @@ curl -v -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true\n < x-request-id: c4b2804e-42b1-4d6f-9e8f-1e8901fc2c6c\n < content-length: 32\n < connection: keep-alive\n-< \n+<\n {\n   \"status\": \"Worker scaled\"\n }\n ```\n \n-\n-### Scale workers\n+## Scale workers\n \n `PUT /models/{model_name}`\n+\n * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.\n * max_worker - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.\n * number_gpu - (optional) the number of GPU worker processes to create. The default value is `0`. If number_gpu exceeds the number of available GPUs, the rest of workers will run on CPU.\n * synchronous - whether or not the call is synchronous. The default value is `false`.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NjUwNw==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406476507", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * timeout - the specified wait time for a worker to complete all pending requests. If exceeded, the work process will be terminated. Use `0` to terminate the backend worker process immediately. Use `-1` to wait infinitely. The default value is `-1`. \n          \n          \n            \n            * `timeout` - the specified wait time for a worker to complete all pending requests. If exceeded, the work process will be terminated. Use `0` to terminate the backend worker process immediately. Use `-1` to wait infinitely. The default value is `-1`.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:04:30Z", "path": "docs/management_api.md", "diffHunk": "@@ -65,25 +66,25 @@ curl -v -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true\n < x-request-id: c4b2804e-42b1-4d6f-9e8f-1e8901fc2c6c\n < content-length: 32\n < connection: keep-alive\n-< \n+<\n {\n   \"status\": \"Worker scaled\"\n }\n ```\n \n-\n-### Scale workers\n+## Scale workers\n \n `PUT /models/{model_name}`\n+\n * min_worker - (optional) the minimum number of worker processes. TorchServe will try to maintain this minimum for specified model. The default value is `1`.\n * max_worker - (optional) the maximum number of worker processes. TorchServe will make no more that this number of workers for the specified model. The default is the same as the setting for `min_worker`.\n * number_gpu - (optional) the number of GPU worker processes to create. The default value is `0`. If number_gpu exceeds the number of available GPUs, the rest of workers will run on CPU.\n * synchronous - whether or not the call is synchronous. The default value is `false`.\n * timeout - the specified wait time for a worker to complete all pending requests. If exceeded, the work process will be terminated. Use `0` to terminate the backend worker process immediately. Use `-1` to wait infinitely. The default value is `-1`. ", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NzE1OQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406477159", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * limit - (optional) the maximum number of items to return. It is passed as a query parameter. The default value is `100`.\n          \n          \n            \n            * `limit` - (optional) the maximum number of items to return. It is passed as a query parameter. The default value is `100`.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:05:55Z", "path": "docs/management_api.md", "diffHunk": "@@ -262,9 +263,10 @@ curl -X DELETE http://localhost:8081/models/noop/1.0\n }\n ```\n \n-### List models\n+## List models\n \n `GET /models`\n+\n * limit - (optional) the maximum number of items to return. It is passed as a query parameter. The default value is `100`.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NzI1MQ==", "url": "https://github.com/pytorch/serve/pull/174#discussion_r406477251", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * next_page_token - (optional) queries for next page. It is passed as a query parameter. This value is return by a previous API call.\n          \n          \n            \n            * `next_page_token` - (optional) queries for next page. It is passed as a query parameter. This value is return by a previous API call.", "author": "aaronmarkham", "createdAt": "2020-04-09T21:06:05Z", "path": "docs/management_api.md", "diffHunk": "@@ -262,9 +263,10 @@ curl -X DELETE http://localhost:8081/models/noop/1.0\n }\n ```\n \n-### List models\n+## List models\n \n `GET /models`\n+\n * limit - (optional) the maximum number of items to return. It is passed as a query parameter. The default value is `100`.\n * next_page_token - (optional) queries for next page. It is passed as a query parameter. This value is return by a previous API call.", "originalCommit": "7242ef5bc2b33c4054d1792a0c3724b118feec84", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "423b6465c21fba994b9b6a8806bad6924e97630e", "url": "https://github.com/pytorch/serve/commit/423b6465c21fba994b9b6a8806bad6924e97630e", "message": "Apply suggestions from code review\r\n\r\nIncorporated feedback from @aaronmarkham\n\nCo-Authored-By: Aaron Markham <markhama@amazon.com>", "committedDate": "2020-04-09T23:02:07Z", "type": "commit"}]}