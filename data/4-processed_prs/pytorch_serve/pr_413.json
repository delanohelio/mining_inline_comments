{"pr_number": 413, "pr_title": "Adding system diagram and brief explanation of workings", "pr_createdAt": "2020-05-28T23:04:03Z", "pr_url": "https://github.com/pytorch/serve/pull/413", "timeline": [{"oid": "0456ad6cc1d26bbc45444c9f205ba977b588f26a", "url": "https://github.com/pytorch/serve/commit/0456ad6cc1d26bbc45444c9f205ba977b588f26a", "message": "Adding system diagram and brief explanation of workings", "committedDate": "2020-05-28T23:03:23Z", "type": "commit"}, {"oid": "d3946db5421a3ba6815531cfb58d4ad9137eaca8", "url": "https://github.com/pytorch/serve/commit/d3946db5421a3ba6815531cfb58d4ad9137eaca8", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-05-29T16:23:53Z", "type": "commit"}, {"oid": "cfcdcd07a3e698938150f1fc115539f9ad33ad7b", "url": "https://github.com/pytorch/serve/commit/cfcdcd07a3e698938150f1fc115539f9ad33ad7b", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-06-01T16:24:53Z", "type": "commit"}, {"oid": "6d518fa085a2bc58dd60bf056a0a8e8266dcfb2d", "url": "https://github.com/pytorch/serve/commit/6d518fa085a2bc58dd60bf056a0a8e8266dcfb2d", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-06-03T00:30:49Z", "type": "commit"}, {"oid": "47000b37fac09496bcf74db4156f94e3908e25a0", "url": "https://github.com/pytorch/serve/commit/47000b37fac09496bcf74db4156f94e3908e25a0", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-06-05T19:44:39Z", "type": "commit"}, {"oid": "e96e28df873b8062002529d7a6f9e800a8e38857", "url": "https://github.com/pytorch/serve/commit/e96e28df873b8062002529d7a6f9e800a8e38857", "message": "Adding number of worker recommendations", "committedDate": "2020-06-05T21:22:31Z", "type": "commit"}, {"oid": "160be78cebb3bbb02dcde10ae905afaffd4367a9", "url": "https://github.com/pytorch/serve/commit/160be78cebb3bbb02dcde10ae905afaffd4367a9", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-06-08T17:34:18Z", "type": "commit"}, {"oid": "82596ef575cc75bed2f69ed9b7b8d7a3f3842db6", "url": "https://github.com/pytorch/serve/commit/82596ef575cc75bed2f69ed9b7b8d7a3f3842db6", "message": "Merge branch 'staging_0_1_1' into issue_206", "committedDate": "2020-06-08T23:32:00Z", "type": "commit"}, {"oid": "1ce19249daeb9b3183682752c7329753899e82dc", "url": "https://github.com/pytorch/serve/commit/1ce19249daeb9b3183682752c7329753899e82dc", "message": "Adding description of netty configuration params", "committedDate": "2020-06-08T23:32:44Z", "type": "commit"}, {"oid": "142fc595b4b8a4ed10c22f4cd100b33acc22195d", "url": "https://github.com/pytorch/serve/commit/142fc595b4b8a4ed10c22f4cd100b33acc22195d", "message": "Update header for worker threads readme section", "committedDate": "2020-06-08T23:47:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NTY5OQ==", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437085699", "bodyText": "Maybe add a bit of background about how Netty is used in TorchServe", "author": "mycpuorg", "createdAt": "2020-06-09T01:22:50Z", "path": "docs/configuration.md", "diffHunk": "@@ -188,8 +188,8 @@ By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu\n Most of the following properties are designed for performance tuning. Adjusting these numbers will impact scalability and throughput.\n \n * `enable_envvars_config`: Enable configuring TorchServe through environment variables. When this option is set to \"true\", all the static configurations of TorchServe can come through environment variables as well. Default: false\n-* `number_of_netty_threads`: number frontend netty thread. Default: number of logical processors available to the JVM.\n-* `netty_client_threads`: number of backend netty thread. Default: number of logical processors available to the JVM.\n+* `number_of_netty_threads`: number frontend netty thread. This specifies the numer of threads in the child [EventLoopGroup](https://livebook.manning.com/book/netty-in-action/chapter-8) of the frontend netty server. This group provides EventLoops for processing Netty Channel events (namely inference and management requests) from accepted connections. Default: number of logical processors available to the JVM.", "originalCommit": "142fc595b4b8a4ed10c22f4cd100b33acc22195d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5MzgwMA==", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437793800", "bodyText": "Yes that will be helpful to get some clarity on the internal workings and how to think about tuning the different threads related configuration parameters", "author": "chauhang", "createdAt": "2020-06-10T00:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NTY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI0MDkzNw==", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437240937", "bodyText": "In the diagram, we should name Thread1/Thread2 as Worker1/Worker2.\nAlso, will it be a good idea to define 'Model Handler' as it has been referred to in the diagram.\nAnd some reference to MAR under 'Model' terminology.", "author": "dhaniram-kshirsagar", "createdAt": "2020-06-09T08:48:12Z", "path": "README.md", "diffHunk": "@@ -4,6 +4,16 @@ TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n **For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n+## TorchServe Architecture\n+![Architecture Diagram](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)\n+\n+### Terminology:\n+* **Frontend**: The request/response handling component of TorchServe. This portion of the serving component handles both request/response coming from clients as well manages the models lifecycle.\n+* **Model Workers**: These workers are responsible for running the actual inference on the models. These are actual running instances of the models.", "originalCommit": "142fc595b4b8a4ed10c22f4cd100b33acc22195d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI0NTU3OA==", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437245578", "bodyText": "Though not related -> 'Install Java 11' step has been repeated in 'install using pip' and 'install using conda'.", "author": "dhaniram-kshirsagar", "createdAt": "2020-06-09T08:55:18Z", "path": "README.md", "diffHunk": "@@ -4,6 +4,16 @@ TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n **For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n+## TorchServe Architecture\n+![Architecture Diagram](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)\n+\n+### Terminology:\n+* **Frontend**: The request/response handling component of TorchServe. This portion of the serving component handles both request/response coming from clients as well manages the models lifecycle.\n+* **Model Workers**: These workers are responsible for running the actual inference on the models. These are actual running instances of the models.\n+* **Model**: Models could be a `script_module` (JIT saved models) or `eager_mode_models`. These models can provide custom pre- and post-processing of data along with any other model artifacts such as state_dicts. Models can be loaded from cloud storage or from local hosts.\n+* **Plugins**: These are custom endpoints or authz/authn or batching algorithms that can be dropped into TorchServe at startup time.\n+* **Model Store**: This is a directory in which all the loadable models exist.\n+", "originalCommit": "142fc595b4b8a4ed10c22f4cd100b33acc22195d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}