{"pr_number": 4171, "pr_title": "DB-9770/DB-10267 Fix External Table Row Counts after cross-join insert", "pr_createdAt": "2020-09-22T07:57:42Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/4171", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTUwNw==", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492571507", "bodyText": "Can you make it a Set? Since we are going to call lots of .contains() on it I think that makes sense.", "author": "dgomezferro", "createdAt": "2020-09-22T08:48:01Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1114,64 +1116,98 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n         return (JavaRDD<V>) dataSet.javaRDD().map(new RowToLocatedRowFunction(context));\n     }\n \n+    private DataSet<ExecRow> getRowsWritten(OperationContext context) {\n+        ValueRow valueRow = new ValueRow(1);\n+        valueRow.setColumn(1, new SQLLongint(context.getRecordsWritten()));\n+        return new SparkDataSet<>(SpliceSpark.getContext()\n+                .parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n     @Override\n-    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp,\n-                                             int[] partitionBy,\n-                                             String location,\n-                                             String compression,\n-                                             OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n+    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                             String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .parquet(location);\n+        }\n \n-        // construct a DF using schema of data\n-        Dataset<Row> insertDF = SpliceSpark.getSession()\n-                .createDataFrame(dataset.rdd(), tableSchema);\n+        return getRowsWritten(context);\n+    }\n \n-        List<String> partitionByCols = new ArrayList();\n-        for (int i = 0; i < partitionBy.length; i++) {\n-            partitionByCols.add(tableSchema.fields()[partitionBy[i]].name());\n-        }\n-        if (partitionBy.length > 0) {\n-            List<Column> repartitionCols = new ArrayList();\n-            for (int i = 0; i < partitionBy.length; i++) {\n-                repartitionCols.add(new Column(tableSchema.fields()[partitionBy[i]].name()));\n-            }\n-            insertDF = insertDF.repartition(scala.collection.JavaConversions.asScalaBuffer(repartitionCols).toList());\n+    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                          String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            compression = SparkDataSet.getAvroCompression(compression);\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .format(\"com.databricks.spark.avro\").save(location);\n         }\n-        insertDF.write().option(SPARK_COMPRESSION_OPTION,compression)\n-                .partitionBy(partitionByCols.toArray(new String[partitionByCols.size()]))\n-                .mode(SaveMode.Append).parquet(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        return getRowsWritten(context);\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,\n-                                          int[] partitionBy,\n-                                          String location,\n-                                          String compression,\n-                                          OperationContext context) throws StandardException\n-    {\n-        compression = SparkDataSet.getAvroCompression(compression);\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).format(\"com.databricks.spark.avro\").save(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    public DataSet<ExecRow> writeTextFile(int[] partitionBy, String location, CsvOptions csvOptions,\n+                                          OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .options(getCsvOptions(csvOptions))\n+                    .csv(location);\n+        }\n+        return getRowsWritten(context);\n     }\n-\n-\n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n     public DataSet<ExecRow> writeORCFile(int[] baseColumnMap, int[] partitionBy, String location,  String compression,\n                                                     OperationContext context) throws StandardException {\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).orc(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .orc(location);\n+        }\n+        return getRowsWritten(context);\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, String compression, OperationContext context) throws StandardException {\n+    class CountingListener extends SparkListener implements AutoCloseable\n+    {\n+        OperationContext context;\n+        SparkContext sc;\n+        String uuid;\n+        List<Integer> stageIdsToWatch;", "originalCommit": "2b3d3a2adbe99b963bad460690729f70683af015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3NTU2MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492575561", "bodyText": "yes will do", "author": "martinrupp", "createdAt": "2020-09-22T08:54:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MjczNw==", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492572737", "bodyText": "Why the change? If this is deliberate we'd need to change the test name too", "author": "dgomezferro", "createdAt": "2020-09-22T08:49:57Z", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -2409,15 +2446,15 @@ public void testParquetPartitionColumnName() throws Exception {\n     public void testOrcColumnName() throws Exception {\n         String tablePath = getExternalResourceDirectory()+\"orc_colname\";\n         methodWatcher.execute(String.format(\"create external table t_orc (col1 int, col2 varchar(5))\" +\n-                \" STORED AS ORC LOCATION '%s'\", tablePath));\n+                \" STORED AS PARQUET LOCATION '%s'\", tablePath));", "originalCommit": "2b3d3a2adbe99b963bad460690729f70683af015", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3NTQxMA==", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492575410", "bodyText": "good catch, that wasn't intentional.", "author": "martinrupp", "createdAt": "2020-09-22T08:54:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MjczNw=="}], "type": "inlineReview"}, {"oid": "a34fd0c451848475f9022e89c281c2dc17d1b57c", "url": "https://github.com/splicemachine/spliceengine/commit/a34fd0c451848475f9022e89c281c2dc17d1b57c", "message": "DB-10267 address code review", "committedDate": "2020-09-23T09:28:18Z", "type": "forcePushed"}, {"oid": "b39d843594c0bb8455186245a96f6ffd11de0351", "url": "https://github.com/splicemachine/spliceengine/commit/b39d843594c0bb8455186245a96f6ffd11de0351", "message": "DB-10267 address code review", "committedDate": "2020-09-24T10:55:46Z", "type": "forcePushed"}, {"oid": "924330eb77553233fd9690af6b7d6be92e311aed", "url": "https://github.com/splicemachine/spliceengine/commit/924330eb77553233fd9690af6b7d6be92e311aed", "message": "DB-10267 address code review", "committedDate": "2020-09-24T10:56:29Z", "type": "forcePushed"}, {"oid": "03e5534b370f170e73e93f9bcb118f8824b3fa83", "url": "https://github.com/splicemachine/spliceengine/commit/03e5534b370f170e73e93f9bcb118f8824b3fa83", "message": "DB-10267 address code review", "committedDate": "2020-09-28T21:32:59Z", "type": "forcePushed"}, {"oid": "e72b07f3ade56909e7d9a8ce8017574339c64682", "url": "https://github.com/splicemachine/spliceengine/commit/e72b07f3ade56909e7d9a8ce8017574339c64682", "message": "DB-9770 Fix External Table Row Counts after cross-join insert", "committedDate": "2020-09-30T08:09:49Z", "type": "commit"}, {"oid": "b9e2d21b7f4feb6b4b8b333ffa86793f20fb30c4", "url": "https://github.com/splicemachine/spliceengine/commit/b9e2d21b7f4feb6b4b8b333ffa86793f20fb30c4", "message": "DB-9770 refactoring", "committedDate": "2020-09-30T08:09:49Z", "type": "commit"}, {"oid": "8fa12bf99601a26961577e8d878b29810accdf3e", "url": "https://github.com/splicemachine/spliceengine/commit/8fa12bf99601a26961577e8d878b29810accdf3e", "message": "DB-10267 improving write count speed", "committedDate": "2020-09-30T08:09:49Z", "type": "commit"}, {"oid": "000f754b0152cc03fe24b7ddb39c43f0abe5c990", "url": "https://github.com/splicemachine/spliceengine/commit/000f754b0152cc03fe24b7ddb39c43f0abe5c990", "message": "DB-10267 address code review", "committedDate": "2020-09-30T08:09:49Z", "type": "commit"}, {"oid": "4ed5d801e7318e343187c14f8322508a4c9a73fa", "url": "https://github.com/splicemachine/spliceengine/commit/4ed5d801e7318e343187c14f8322508a4c9a73fa", "message": "DB-10267 fix test failure for CSV", "committedDate": "2020-09-30T08:09:49Z", "type": "commit"}, {"oid": "4ed5d801e7318e343187c14f8322508a4c9a73fa", "url": "https://github.com/splicemachine/spliceengine/commit/4ed5d801e7318e343187c14f8322508a4c9a73fa", "message": "DB-10267 fix test failure for CSV", "committedDate": "2020-09-30T08:09:49Z", "type": "forcePushed"}]}