{"pr_number": 3671, "pr_title": "DB-9599 Use bulk column APIs in Spark", "pr_createdAt": "2020-06-10T10:44:04Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/3671", "timeline": [{"oid": "7701184fb8474af19a290d5abd68e024abd6239e", "url": "https://github.com/splicemachine/spliceengine/commit/7701184fb8474af19a290d5abd68e024abd6239e", "message": "DB-9599 Use bulk column APIs in Spark", "committedDate": "2020-06-10T10:42:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA4ODQ5NQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3671#discussion_r438088495", "bodyText": "col could be replaced with _ maybe?", "author": "hatyo", "createdAt": "2020-06-10T12:39:02Z", "path": "scala_util/src/main/scala/org/apache/spark/sql/WithColumns.scala", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package org.apache.spark.sql\n+\n+object WithColumns {\n+\n+  def withColumns(colNames: Seq[String], cols: Seq[Column], ds: DataFrame): DataFrame = {\n+    val resolver = ds.sparkSession.sessionState.analyzer.resolver\n+    val output = ds.queryExecution.analyzed.output\n+\n+    val columnMap = colNames.zip(cols).toMap\n+\n+    val replacedAndExistingColumns = output.map { field =>\n+      columnMap.find { case (colName, _) =>\n+        resolver(field.name, colName)\n+      } match {\n+        case Some((colName: String, col: Column)) => col.as(colName)\n+        case _ => Column(field)\n+      }\n+    }\n+\n+    val newColumns = columnMap.filter { case (colName, col) =>", "originalCommit": "7701184fb8474af19a290d5abd68e024abd6239e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ0ODk2MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3671#discussion_r439448960", "bodyText": "I'd rather leave it like that because I copied this directly from Spark code: Dataset.withColumns(), which is only available in Spark 2.3+\nI added a clarifying comment in case we ever need to update this to match Spark", "author": "dgomezferro", "createdAt": "2020-06-12T14:21:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA4ODQ5NQ=="}], "type": "inlineReview"}, {"oid": "42ed97f64297e60a3a96a29fae9b0d06425d15c6", "url": "https://github.com/splicemachine/spliceengine/commit/42ed97f64297e60a3a96a29fae9b0d06425d15c6", "message": "DB-9599 Add clarifying comment", "committedDate": "2020-06-12T14:19:16Z", "type": "commit"}]}