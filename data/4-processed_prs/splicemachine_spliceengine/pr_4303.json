{"pr_number": 4303, "pr_title": "DB-9924 NSDS v2 Streaming Performance", "pr_createdAt": "2020-10-15T15:23:48Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/4303", "timeline": [{"oid": "503b07c72096ce9da2b2b199b60f20d7ccf67f2d", "url": "https://github.com/splicemachine/spliceengine/commit/503b07c72096ce9da2b2b199b60f20d7ccf67f2d", "message": "DB-9924 Performance tuning.", "committedDate": "2020-08-07T00:44:31Z", "type": "commit"}, {"oid": "a32d004fe21667a20bc7feaa45c597626807cd95", "url": "https://github.com/splicemachine/spliceengine/commit/a32d004fe21667a20bc7feaa45c597626807cd95", "message": "DB-9924 Shutting down Zookeeper last helps Kafka restart reliably more often.", "committedDate": "2020-08-07T00:46:47Z", "type": "commit"}, {"oid": "d4954ab5e846469a054bca0b02d18634f5f1687c", "url": "https://github.com/splicemachine/spliceengine/commit/d4954ab5e846469a054bca0b02d18634f5f1687c", "message": "DB-9924 Enable kafka topic creation with num partitions and replication factor. Random partition assignment when sending data to Kafka.", "committedDate": "2020-08-07T00:52:21Z", "type": "commit"}, {"oid": "e67ce530cd65e189a224ab5693e2ef172c74d1d8", "url": "https://github.com/splicemachine/spliceengine/commit/e67ce530cd65e189a224ab5693e2ef172c74d1d8", "message": "DB-9924 Added sleep to ensure topics are created and ready.", "committedDate": "2020-08-15T00:31:26Z", "type": "commit"}, {"oid": "3bad6f2d865739646547280930e769bd60983b52", "url": "https://github.com/splicemachine/spliceengine/commit/3bad6f2d865739646547280930e769bd60983b52", "message": "DB-9924 Changed random partition assignment to default in kafka producer.", "committedDate": "2020-08-15T00:34:10Z", "type": "commit"}, {"oid": "9b559ee23f37fb504fb37af8733497b312332ae7", "url": "https://github.com/splicemachine/spliceengine/commit/9b559ee23f37fb504fb37af8733497b312332ae7", "message": "DB-9924 Latest NSDS updates.", "committedDate": "2020-09-09T19:14:58Z", "type": "commit"}, {"oid": "09d48fbc73d8073239ed81a381c1c462980f52c9", "url": "https://github.com/splicemachine/spliceengine/commit/09d48fbc73d8073239ed81a381c1c462980f52c9", "message": "DB-9924 Latest NSDS updates.", "committedDate": "2020-09-16T16:40:02Z", "type": "commit"}, {"oid": "1b7781042b1eac3978e035a3ff61e7ccd522f2bb", "url": "https://github.com/splicemachine/spliceengine/commit/1b7781042b1eac3978e035a3ff61e7ccd522f2bb", "message": "DB-9924 Added end of batch marker when passing data from nsds to the DB.", "committedDate": "2020-09-17T16:29:09Z", "type": "commit"}, {"oid": "29c861da0263fdf28b8efba4f5b26b51ca22bf59", "url": "https://github.com/splicemachine/spliceengine/commit/29c861da0263fdf28b8efba4f5b26b51ca22bf59", "message": "DB-9924 Latest update for NSDS.", "committedDate": "2020-09-18T14:37:03Z", "type": "commit"}, {"oid": "6460c6705de54874f07b4923f04e39bb58d91f7f", "url": "https://github.com/splicemachine/spliceengine/commit/6460c6705de54874f07b4923f04e39bb58d91f7f", "message": "DB-9924 NSDS v2 latest updates.", "committedDate": "2020-09-24T17:39:57Z", "type": "commit"}, {"oid": "87597157af221ac7613a1472913352239fecaf10", "url": "https://github.com/splicemachine/spliceengine/commit/87597157af221ac7613a1472913352239fecaf10", "message": "DB-9924 NSDS v2 Added kryo serialization.", "committedDate": "2020-10-15T08:40:41Z", "type": "commit"}, {"oid": "e8304c61d5723be2181b8b20a48b9a393032cc40", "url": "https://github.com/splicemachine/spliceengine/commit/e8304c61d5723be2181b8b20a48b9a393032cc40", "message": "DB-9924 NSDS v2 Added kryo serialization.", "committedDate": "2020-10-15T09:10:02Z", "type": "commit"}, {"oid": "e668872b8a18b12903a04f001db83cd3e75898ef", "url": "https://github.com/splicemachine/spliceengine/commit/e668872b8a18b12903a04f001db83cd3e75898ef", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-15T10:41:54Z", "type": "commit"}, {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "url": "https://github.com/splicemachine/spliceengine/commit/08e8c53eea34e7b7116af18293f24fe0f59d75db", "message": "DB-9924 NSDS v2 copy latest updates to spark3.0", "committedDate": "2020-10-15T11:02:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r506768338", "bodyText": "Is this our naming convention for kafka partition?", "author": "jyuanca", "createdAt": "2020-10-17T00:07:23Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1268,13 +1268,29 @@ public void decrementOpDepth() {\n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n-        List ps = consumer.partitionsFor(topicName);\n-        List<Integer> partitions = new ArrayList<>(ps.size());\n-        for (int i = 0; i < ps.size(); ++i) {\n-            partitions.add(i);\n+        List<Integer> partitions;\n+        String topicName;", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4OTMxOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509389319", "bodyText": "Often the first batch received by SSDS at the beginning of a data run does not have data in every partition.  In this case, to the topic name SSDS appends :: followed by a comma-separated list of the partition numbers that have data, so that the consumer in the DB will not poll empty partitions, which results in long timeouts.\nKafka's convention for a partition name is topic name hyphen partition number.", "author": "jpanko1", "createdAt": "2020-10-21T15:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508341049", "bodyText": "What happens when we reach 10 retries? It looks like we silently drop data.", "author": "dgomezferro", "createdAt": "2020-10-20T09:16:08Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQxMDkyMg==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509410922", "bodyText": "Each retry has a 1 minute timeout, so after 10 minutes of retrying, if it hasn't received more data within a batch, it stops polling for data and will LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");.  I've seen data within a batch delayed for several seconds but not as long as minutes, so normally 10 minutes will allow picking up the rest of the data.  In a failure scenario in which the upstream code is unable to send the end of a batch, without the eventual timeout this code will be in an infinite loop tying up a thread / spark task.", "author": "jpanko1", "createdAt": "2020-10-21T16:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYwMzYwMA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r513603600", "bodyText": "The problem is we are not failing the operation (as far as I can see). So after 10 minutes we complete the write without error. I think we should have a shorter timeout (1 minute?) and raise an exception.", "author": "dgomezferro", "createdAt": "2020-10-28T16:50:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMwNDYyNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r514304624", "bodyText": "Added exception and shortened timeout in commit 063e869", "author": "jpanko1", "createdAt": "2020-10-29T14:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NTQzMw==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508345433", "bodyText": "Serializing ValueRows directly with Kryo still has quite a bit of overhead (we are serializing the schema with each row, for instance). I opened https://splicemachine.atlassian.net/browse/DB-10543 to track this improvement", "author": "dgomezferro", "createdAt": "2020-10-20T09:22:32Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;\n+            int retries = 0;\n+            final Duration shortTimeout = java.time.Duration.ofMillis(500L);\n+            final Duration longTimeout = java.time.Duration.ofMinutes(1L);\n+            \n+            private ConsumerRecords<Integer, byte[]> kafkaRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n                 int attempt = 1;\n-                ConsumerRecords<Integer, Externalizable> records = null;\n+                ConsumerRecords<Integer, byte[]> records = null;\n                 do {\n-                    records = consumer.poll(java.time.Duration.ofMillis(1000));\n+                    records = consumer.poll(timeout);\n                     if (TaskContext.get().isInterrupted()) {\n-                        consumer.close();\n+                        LOG.warn( id+\" KRF.call kafkaRecords Spark TaskContext Interrupted\");\n+                        //consumer.close();\n                         throw new TaskKilledException();\n                     }\n                 } while( noRecords.test(records) && attempt++ < maxAttempts );\n                 \n                 return records;\n             }\n             \n-            private boolean hasMoreRecords(int maxAttempts) throws TaskKilledException {\n-                ConsumerRecords<Integer, Externalizable> records = kafkaRecords(maxAttempts);\n+            private boolean hasMoreRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n+                ConsumerRecords<Integer, byte[]> records = kafkaRecords(maxAttempts, timeout);\n                 if( noRecords.test(records) ) {\n-                    consumer.close();\n+                    if( !prevMessage.last() && retries < maxRetries ) {\n+                        retries++;\n+                        Duration retryTimeout = longTimeout;\n+                        LOG.warn( id+\" KRF.call Missed rcds, got \"+totalCount+\" retry \"+retries+\" for up to \"+retryTimeout );\n+                        return hasMoreRecords(\n+                            maxAttempts,\n+                            retryTimeout\n+                        );\n+                    }\n+                    //consumer.close();\n+                    if( !prevMessage.last() ) {\n+                        LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");\n+                    }\n                     return false;\n                 } else {\n+                    int ct = records.count();\n+                    totalCount += ct;\n+                    LOG.trace( id+\" KRF.call p \"+partition+\" t \"+topicName+\" records \"+ct );\n+                    retries = 0;\n+                    \n                     it = records.iterator();\n                     return it.hasNext();\n                 }\n             }\n-\n+            \n             @Override\n             public boolean hasNext() {\n-                if (it == null) {\n-                    return hasMoreRecords(60);\n-                }\n-                if (it.hasNext()) {\n-                    return true;\n+                boolean more = false;\n+                \n+                if (it != null && it.hasNext()) {\n+                    more = true;\n+                } else if (!prevMessage.last()) {\n+                    more = hasMoreRecords(1, shortTimeout);\n                 }\n-                else {\n-                    return hasMoreRecords(1);\n+\n+                if (!more) {\n+                    consumer.close();\n+                    kryo.close();\n                 }\n+\n+                return more;\n             }\n \n             @Override\n             public ExecRow next() {\n-                return (ExecRow)it.next().value();\n+                Message m = (Message)kryo.deserialize( it.next().value() );", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508346600", "bodyText": "remove", "author": "dgomezferro", "createdAt": "2020-10-20T09:24:14Z", "path": "splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala", "diffHunk": "@@ -17,28 +17,37 @@ package com.splicemachine.nsds.kafka\n \n import java.util.Properties\n import java.util.concurrent.TimeUnit\n-import org.apache.kafka.clients.admin.AdminClient\n-import org.apache.kafka.clients.admin.AdminClientConfig\n+\n+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig, NewTopic}\n import org.apache.kafka.common.KafkaFuture\n+\n import scala.collection.JavaConverters._\n \n-class KafkaAdmin(kafkaServers: String) {\n+@SerialVersionUID(20200722241L)\n+class KafkaAdmin(kafkaServers: String) extends Serializable {\n   val props = new Properties()\n   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n   val admin = AdminClient.create( props )\n   \n+  def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = {\n+    admin.createTopics(\n+      topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava\n+    ).all.get\n+//    Thread.sleep(1000)", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxODI0MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509618241", "bodyText": "Made change in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala\n    \n    \n        Lines 32 to 36\n      in\n      0263ed8\n    \n    \n    \n    \n\n        \n          \n           def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = { \n        \n\n        \n          \n             admin.createTopics( \n        \n\n        \n          \n               topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava \n        \n\n        \n          \n             ).all.get \n        \n\n        \n          \n           }", "author": "jpanko1", "createdAt": "2020-10-21T19:41:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDA0MA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874040", "bodyText": "Pushed code to this branch.", "author": "jpanko1", "createdAt": "2020-10-22T04:34:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508350832", "bodyText": "We shouldn't print to console, use Debugging output instead", "author": "dgomezferro", "createdAt": "2020-10-20T09:30:19Z", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow\n         }\n \n         val props = new Properties\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n         props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-s2s-smc-\"+java.util.UUID.randomUUID() )\n         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n-        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n-\n-        val producer = new KafkaProducer[Integer, Externalizable](props)\n-\n-        while( itrRow.hasNext ) {\n-          producer.send( new ProducerRecord(topicName, msgIdx, externalizable(itrRow.next, schema)) )\n-          msgIdx += 1\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)\n+//        // Throughput performance?\n+//        println(s\"SMC.sendData batch 1MB linger 750ms\")\n+//        props.put(ProducerConfig.BATCH_SIZE_CONFIG, (1000*1000).toString )\n+//        props.put(ProducerConfig.LINGER_MS_CONFIG, \"500\")\n+\n+        val producer = new KafkaProducer[Integer, Array[Byte]](props)\n+        \n+        val rowK = new RowForKafka(topicName, partition, schema)\n+        rowK.sparkRow = if( itr.hasNext ) {\n+          msgCount += 1\n+          Some(itr.next)\n+        } else None\n+        rowK.msgCount = msgCount\n+        \n+        val kryo = new KryoSerialization()\n+        kryo.init\n+        \n+        while( itr.hasNext ) {\n+          rowK.valueRow = Some(externalizable(rowK.sparkRow.get, schema, partition))\n+          rowK.send(producer, kryo)\n+          msgCount += 1\n+          rowK.sparkRow = Some(itr.next)\n+          rowK.msgCount = msgCount\n         }\n+        lastRowsToSend.add(rowK)\n+        \n+        kryo.close\n \n+        insAccum.add(msgCount)\n+\n+        println(s\"$id SMC.sendData t $topicName p $partition records $msgCount\")", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyNTAyNQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509625025", "bodyText": "Changed all printlns to logs in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala\n    \n    \n        Lines 799 to 801\n      in\n      0263ed8\n    \n    \n    \n    \n\n        \n          \n           insAccum.add(msgCount) \n        \n\n        \n          \n            \n        \n\n        \n          \n           debug(s\"$id SMC.sendData t $topicName p $partition records $msgCount\") \n        \n    \n  \n\n\nAlso included some other code improvements.", "author": "jpanko1", "createdAt": "2020-10-21T19:50:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDEwNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874104", "bodyText": "Pushed code to this branch.", "author": "jpanko1", "createdAt": "2020-10-22T04:35:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508354298", "bodyText": "Can you explain a bit what's going on here? The naming of variables is a bit confusing.", "author": "dgomezferro", "createdAt": "2020-10-20T09:35:32Z", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc4MzQ5NQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509783495", "bodyText": "Added comments to the code in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala\n    \n    \n        Lines 738 to 768\n      in\n      8e1479e\n    \n    \n    \n    \n\n        \n          \n               // Convert last message in Kafka to a ValueRow (lastVR) \n        \n\n        \n          \n               val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr \n        \n\n        \n          \n               // Get the hash code (lastKHash) of lastVR based on the columns that are in lastVR (hashCols) \n        \n\n        \n          \n               val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) } \n        \n\n        \n          \n               val lastKHash = lastVR.hashCode(hashCols) \n        \n\n        \n          \n               // Define function (hash) for converting a spark row to a ValueRow and getting its hash code \n        \n\n        \n          \n               def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols) \n        \n\n        \n          \n            \n        \n\n        \n          \n               // Get a pair of iterators (itr34) of rdd data to use for different purposes \n        \n\n        \n          \n               //val itr34 = itr12._2.duplicate \n        \n\n        \n          \n               val itr34 = itrRow.duplicate   //itr12.duplicate \n        \n\n        \n          \n                \n        \n\n        \n          \n               // Use span to split itr34._1 into a pair of iterators (inKafka_NotInKafka). \n        \n\n        \n          \n               // inKafka_NotInKafka._1 will contain all of the rows from the beginning of itrRow whose hash != lastKHash. \n        \n\n        \n          \n               // inKafka_NotInKafka._2 will contain all of the rows from the one whose hash == lastKHash to the end of itrRow. \n        \n\n        \n          \n               // So inKafka_NotInKafka._1 will contain rows already in Kafka, and inKafka_NotInKafka._2 will contain the  \n        \n\n        \n          \n               //  last row in Kafka followed by rows that are not in Kafka. \n        \n\n        \n          \n               val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash ) \n        \n\n        \n          \n               if( inKafka_NotInKafka._2.hasNext ) { \n        \n\n        \n          \n                 inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it \n        \n\n        \n          \n                 msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure \n        \n\n        \n          \n                 inKafka_NotInKafka._2 \n        \n\n        \n          \n               } else { \n        \n\n        \n          \n                 // In this case, itrRow didn't contain the last row of Kafka, so inKafka_NotInKafka._2 is empty. \n        \n\n        \n          \n                 // This happens when itrRow starts after the last item added to Kafka. \n        \n\n        \n          \n                 itr34._2 \n        \n\n        \n          \n               } \n        \n\n        \n          \n             } \n        \n\n        \n          \n           } else { \n        \n\n        \n          \n             itrRow \n        \n\n        \n          \n           }", "author": "jpanko1", "createdAt": "2020-10-21T23:01:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDE5Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874192", "bodyText": "Pushed code to this branch.", "author": "jpanko1", "createdAt": "2020-10-22T04:35:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA=="}], "type": "inlineReview"}, {"oid": "bf69fd0c65e44b224c6777a872c38cde5c19b3c4", "url": "https://github.com/splicemachine/spliceengine/commit/bf69fd0c65e44b224c6777a872c38cde5c19b3c4", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-20T22:19:13Z", "type": "commit"}, {"oid": "94f13b7a9ad85ce06d28fe8e783a87ef3ab9f5a6", "url": "https://github.com/splicemachine/spliceengine/commit/94f13b7a9ad85ce06d28fe8e783a87ef3ab9f5a6", "message": "DB-9924 Remove unnecessary comment.", "committedDate": "2020-10-21T19:13:51Z", "type": "commit"}, {"oid": "0263ed86b9c614882b6cde88a47c3caf87ca7027", "url": "https://github.com/splicemachine/spliceengine/commit/0263ed86b9c614882b6cde88a47c3caf87ca7027", "message": "DB-9924 Change printlns to logs. Align kafka partition count to that of the input rdd. Limit last row accumulation in sendData to streaming.", "committedDate": "2020-10-21T19:20:19Z", "type": "commit"}, {"oid": "4997a486eb3fabc2872718be5cf427bd5af784e3", "url": "https://github.com/splicemachine/spliceengine/commit/4997a486eb3fabc2872718be5cf427bd5af784e3", "message": "DB-9924 NSDS v2 Added comments in sendData retry logic.", "committedDate": "2020-10-21T22:49:13Z", "type": "commit"}, {"oid": "7885afdc7924c3ef628140113ea0199336f450d1", "url": "https://github.com/splicemachine/spliceengine/commit/7885afdc7924c3ef628140113ea0199336f450d1", "message": "DB-9924 Fix Spotbugs.", "committedDate": "2020-10-22T04:11:06Z", "type": "commit"}, {"oid": "01f1e76fc165e188fafad4fac71d4a128ff8f46b", "url": "https://github.com/splicemachine/spliceengine/commit/01f1e76fc165e188fafad4fac71d4a128ff8f46b", "message": "DB-9924 Moved creation of AdminClient.", "committedDate": "2020-10-23T04:16:54Z", "type": "commit"}, {"oid": "da8552190acc7bc107ef2b7f9de49c4430c948ad", "url": "https://github.com/splicemachine/spliceengine/commit/da8552190acc7bc107ef2b7f9de49c4430c948ad", "message": "DB-9924 NSDS v2 Added logic to handle empty partitions and RDDs with no partitions.", "committedDate": "2020-10-23T04:24:02Z", "type": "commit"}, {"oid": "c47077b8db5b743454ad520f9291a57e4688403a", "url": "https://github.com/splicemachine/spliceengine/commit/c47077b8db5b743454ad520f9291a57e4688403a", "message": "DB-9924 NSDS v2 Returning partition info from sendData_streaming. Made two utility functions public. Migrated other recent changes from spark2.4 code to the code for other spark versions.", "committedDate": "2020-10-28T07:06:23Z", "type": "commit"}, {"oid": "4815d2c9140af470edcea230e8c0cd8bbf5d3a8d", "url": "https://github.com/splicemachine/spliceengine/commit/4815d2c9140af470edcea230e8c0cd8bbf5d3a8d", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-28T15:24:09Z", "type": "commit"}, {"oid": "063e869b22c07e62976c7e10a8b969397010b898", "url": "https://github.com/splicemachine/spliceengine/commit/063e869b22c07e62976c7e10a8b969397010b898", "message": "DB-9924 Throwing exception when timing out during record retrieval from Kafka. Decreased long timeout.", "committedDate": "2020-10-29T03:40:05Z", "type": "commit"}, {"oid": "776fe353b07dc2d50d8381ec3e2fde2ff350510a", "url": "https://github.com/splicemachine/spliceengine/commit/776fe353b07dc2d50d8381ec3e2fde2ff350510a", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-29T17:06:16Z", "type": "commit"}]}