{"pr_number": 2249, "pr_title": "HBASE-24871 Replication may loss data when refresh recovered replicat\u2026", "pr_createdAt": "2020-08-12T07:21:58Z", "pr_url": "https://github.com/apache/hbase/pull/2249", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzOTcyOA==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r469839728", "bodyText": "So the problem is that logs from recovered queues were getting added to the new \"normal\" queue? And these logs are never read, then?\nNit: maybe worth rename the variable from line #516 from replicationSource to recoveredReplicationSource, for further clarity?\nI endorse the call for an additional UT, here.", "author": "wchevreuil", "createdAt": "2020-08-13T10:00:56Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java", "diffHunk": "@@ -516,7 +516,7 @@ public void refreshSources(String peerId) throws IOException {\n         ReplicationSourceInterface replicationSource = createSource(queueId, peer);\n         this.oldsources.add(replicationSource);\n         for (SortedSet<String> walsByGroup : walsByIdRecoveredQueues.get(queueId).values()) {\n-          walsByGroup.forEach(wal -> src.enqueueLog(new Path(wal)));\n+          walsByGroup.forEach(wal -> replicationSource.enqueueLog(new Path(wal)));", "originalCommit": "a1acbb8ab647267248a599b7d642bcd00e4df116", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM2NjM5Nw==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r470366397", "bodyText": "So the problem is that logs from recovered queues were getting added to the new \"normal\" queue? And these logs are never read, then?\n\nYes. These log will be added to normal source. And will read by normal source. So this should not loss data but only affect the replicate order.\n@ddupg I thought the issue title need to be changed.", "author": "infraio", "createdAt": "2020-08-14T01:47:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzOTcyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczNzIyMw==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r470737223", "bodyText": "Thanks for clarifying, @infraio . Yes, let's change the title, please.", "author": "wchevreuil", "createdAt": "2020-08-14T16:45:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzOTcyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAzMTY2Nw==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r473031667", "bodyText": "Thank @wchevreuil @infraio for reviewing. In this case, it will loss data and duplicate data at the same time.\nAfter we added these log to normal source, the normal source replicate the first WAL entries batch and updateLogPosition via zk.\nIt will successfully replicate first WAL entries batch, but data may be out-of date. And then failed to updateLogPosition in zk, which aborting RS, so we will also loss data.", "author": "ddupg", "createdAt": "2020-08-19T13:32:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgzOTcyOA=="}], "type": "inlineReview"}, {"oid": "8a238a0fc7452845fccbcaf2a85fa88c8e3b7173", "url": "https://github.com/apache/hbase/commit/8a238a0fc7452845fccbcaf2a85fa88c8e3b7173", "message": "HBASE-24871 Replication may loss data when refresh recovered replication sources", "committedDate": "2020-08-19T03:47:41Z", "type": "forcePushed"}, {"oid": "fd5bad91bc2c30af81eca50702bd4615fff10143", "url": "https://github.com/apache/hbase/commit/fd5bad91bc2c30af81eca50702bd4615fff10143", "message": "HBASE-24871 Replication may loss data when refresh recovered replication sources", "committedDate": "2020-08-19T13:22:34Z", "type": "forcePushed"}, {"oid": "c8e1785fc324417637ac6de111f25f3cce5d8cc1", "url": "https://github.com/apache/hbase/commit/c8e1785fc324417637ac6de111f25f3cce5d8cc1", "message": "HBASE-24871 Replication may loss data when refresh recovered replication sources", "committedDate": "2020-08-20T02:02:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTAzODEyNA==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r475038124", "bodyText": "Which assert will failed if no this fix?", "author": "infraio", "createdAt": "2020-08-22T03:03:53Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestRefreshRecoveredReplication.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication.regionserver;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.replication.TestReplicationBase;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Testcase for HBASE-24871.\n+ */\n+@Category({ ReplicationTests.class, MediumTests.class })\n+public class TestRefreshRecoveredReplication extends TestReplicationBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestRefreshRecoveredReplication.class);\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRefreshRecoveredReplication.class);\n+\n+  private static final int BATCH = 50;\n+\n+  @Rule\n+  public TestName name = new TestName();\n+\n+  private TableName tablename;\n+  private Table table1;\n+  private Table table2;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    NUM_SLAVES1 = 2;\n+    // replicate slowly\n+    Configuration conf1 = UTIL1.getConfiguration();\n+    conf1.setInt(HConstants.REPLICATION_SOURCE_TOTAL_BUFFER_KEY, 100);\n+    TestReplicationBase.setUpBeforeClass();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    TestReplicationBase.tearDownAfterClass();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    setUpBase();\n+\n+    tablename = TableName.valueOf(name.getMethodName());\n+    TableDescriptor table = TableDescriptorBuilder.newBuilder(tablename)\n+        .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(famName)\n+            .setScope(HConstants.REPLICATION_SCOPE_GLOBAL).build())\n+        .build();\n+\n+    UTIL1.getAdmin().createTable(table);\n+    UTIL2.getAdmin().createTable(table);\n+    UTIL1.waitTableAvailable(tablename);\n+    UTIL2.waitTableAvailable(tablename);\n+    table1 = UTIL1.getConnection().getTable(tablename);\n+    table2 = UTIL2.getConnection().getTable(tablename);\n+  }\n+\n+  @After\n+  public void teardown() throws Exception {\n+    tearDownBase();\n+\n+    UTIL1.deleteTableIfAny(tablename);\n+    UTIL2.deleteTableIfAny(tablename);\n+  }\n+\n+  @Test\n+  public void testReplicationRefreshSource() throws Exception {\n+    // put some data\n+    for (int i = 0; i < BATCH; i++) {\n+      byte[] r = Bytes.toBytes(i);\n+      table1.put(new Put(r).addColumn(famName, famName, r));\n+    }\n+\n+    // kill rs holding table region\n+    Optional<RegionServerThread> server = UTIL1.getMiniHBaseCluster().getLiveRegionServerThreads()\n+        .stream()\n+        .filter(rst -> CollectionUtils.isNotEmpty(rst.getRegionServer().getRegions(tablename)))\n+        .findAny();\n+    Assert.assertTrue(server.isPresent());\n+    server.get().getRegionServer().abort(\"stopping for test\");\n+    UTIL1.waitFor(60000, () ->\n+        UTIL1.getMiniHBaseCluster().getLiveRegionServerThreads().size() == NUM_SLAVES1 - 1);\n+    UTIL1.waitTableAvailable(tablename);\n+\n+    // waiting for recovered peer to start\n+    Replication replication = (Replication) UTIL1.getMiniHBaseCluster()\n+        .getLiveRegionServerThreads().get(0).getRegionServer().getReplicationSourceService();\n+    UTIL1.waitFor(60000, () ->\n+        !replication.getReplicationManager().getOldSources().isEmpty());\n+\n+    // disable peer to trigger refreshSources\n+    hbaseAdmin.disableReplicationPeer(PEER_ID2);", "originalCommit": "c8e1785fc324417637ac6de111f25f3cce5d8cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwNzg5NA==", "url": "https://github.com/apache/hbase/pull/2249#discussion_r475307894", "bodyText": "The UT will timeout in last UTIL2.waitFor without this fix because of losing data.", "author": "ddupg", "createdAt": "2020-08-24T02:16:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTAzODEyNA=="}], "type": "inlineReview"}, {"oid": "5eb40848bcba7dc38259f69330fb9c82d8a1f2d0", "url": "https://github.com/apache/hbase/commit/5eb40848bcba7dc38259f69330fb9c82d8a1f2d0", "message": "HBASE-24871 Replication may loss data when refresh recovered replication sources", "committedDate": "2020-08-24T03:36:22Z", "type": "commit"}, {"oid": "5eb40848bcba7dc38259f69330fb9c82d8a1f2d0", "url": "https://github.com/apache/hbase/commit/5eb40848bcba7dc38259f69330fb9c82d8a1f2d0", "message": "HBASE-24871 Replication may loss data when refresh recovered replication sources", "committedDate": "2020-08-24T03:36:22Z", "type": "forcePushed"}]}