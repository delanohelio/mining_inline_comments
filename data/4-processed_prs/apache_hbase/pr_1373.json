{"pr_number": 1373, "pr_title": "HBASE-24052 Add debug+fix to TestMasterShutdown", "pr_createdAt": "2020-03-27T16:38:18Z", "pr_url": "https://github.com/apache/hbase/pull/1373", "timeline": [{"oid": "d82570573ee56da11706891d779a9c400d56eb66", "url": "https://github.com/apache/hbase/commit/d82570573ee56da11706891d779a9c400d56eb66", "message": "HBASE-24052 Add debug+fix to TestMasterShutdown\n\nAdd check for stopped server at a few more points in Master startup.\nDefend against NPE in RSProcedureDispatcher; log and retun instead.", "committedDate": "2020-03-27T16:36:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQ2MDMzNQ==", "url": "https://github.com/apache/hbase/pull/1373#discussion_r399460335", "bodyText": "I see what you did here..I'm ok with this to stabilize the test.\nI think internally it is masking the problem of \"shutdown()\" implementation being synchronous on the server side. Ideally, I'd think it should be async on the server side too, meaning the rpc call just sets a flag (startShuttingDown = true) which is read by multiple running threads and they begin the tear down rather than doing it in the context of the rpc thread. We already do this partially but the main driver of master shutdown still runs in the rpc context. Don't think this should block the test, but just something for the future. Thoughts?", "author": "bharathv", "createdAt": "2020-03-27T18:25:06Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -156,48 +151,19 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n-\n-      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n-        // Switching to master registry exacerbated a race in the master bootstrap that can result\n-        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-        // is because the connection creation with ZK registry is so slow that by then the server\n-        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-        // wait() in the test, waiting for the server manager to become available.\n-        final long timeout = TimeUnit.MINUTES.toMillis(10);\n-        assertNotEquals(\"timeout waiting for server manager to become available.\",\n-          -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-            () -> masterThread.getMaster().getServerManager() != null));\n-\n-        // Master has come up far enough that we can terminate it without creating a zombie.\n-        final long result = Waiter.waitFor(htu.getConfiguration(), timeout, 500, () -> {\n-          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n-          LOG.debug(\"Attempting to establish connection.\");\n-          final CompletableFuture<AsyncConnection> connFuture =\n-            ConnectionFactory.createAsyncConnection(conf);\n-          try (final AsyncConnection conn = connFuture.join()) {\n-            LOG.debug(\"Sending shutdown RPC.\");\n-            try {\n-              conn.getAdmin().shutdown().join();\n-              LOG.debug(\"Shutdown RPC sent.\");\n-              return true;\n-            } catch (CompletionException e) {\n-              LOG.debug(\"Failure sending shutdown RPC.\");\n-            }\n-          } catch (IOException|CompletionException e) {\n-            LOG.debug(\"Failed to establish connection.\");\n-          } catch (Throwable e) {\n-            LOG.info(\"Something unexpected happened.\", e);\n-          }\n-          return false;\n-        });\n-        assertNotEquals(\"Failed to issue shutdown RPC after \" + Duration.ofMillis(timeout),\n-          -1, result);\n-      });\n-\n       masterThread.start();\n-      shutdownFuture.join();\n+      // Switching to master registry exacerbated a race in the master bootstrap that can result\n+      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+      // is because the connection creation with ZK registry is so slow that by then the server\n+      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+      // wait() in the test, waiting for the server manager to become available.\n+      final long timeout = TimeUnit.MINUTES.toMillis(10);\n+      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n+        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n+          () -> masterThread.getMaster().getServerManager() != null));\n+      htu.getConnection().getAdmin().shutdown();", "originalCommit": "d82570573ee56da11706891d779a9c400d56eb66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQ5MTg1Ng==", "url": "https://github.com/apache/hbase/pull/1373#discussion_r399491856", "bodyText": "Thanks for taking a look @bharathv .\nYeah, something up. Looking at what is happening inline in the rpc request seems minimal -- setting flags -- but something is not right... I filed HBASE-24070, an issue to look at this, as you suggest above.", "author": "saintstack", "createdAt": "2020-03-27T19:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQ2MDMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTU0Njc1NA==", "url": "https://github.com/apache/hbase/pull/1373#discussion_r399546754", "bodyText": "Thanks for the jira, I'll try to poke around too a little bit to see if I can find something. Fwiw, I think a small comment detailing this finding with a jira reference would be helpful. Thanks.", "author": "bharathv", "createdAt": "2020-03-27T21:28:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQ2MDMzNQ=="}], "type": "inlineReview"}]}