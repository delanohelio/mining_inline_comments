{"pr_number": 1781, "pr_title": "HBASE-24435 Add hedgedReads and hedgedReadWins count metrics", "pr_createdAt": "2020-05-26T11:04:04Z", "pr_url": "https://github.com/apache/hbase/pull/1781", "timeline": [{"oid": "24c737f883b711df8d555e0cf5e49324575cc5e3", "url": "https://github.com/apache/hbase/commit/24c737f883b711df8d555e0cf5e49324575cc5e3", "message": "HBASE-24435 Add hedgedReads and hedgedReadWins count metrics\n\nConflicts:\n\thbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "committedDate": "2020-05-26T11:14:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MDkzMA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430740930", "bodyText": "Unrelated changes. Remove", "author": "apurtell", "createdAt": "2020-05-26T22:24:51Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430741232", "bodyText": "In what Hadoop version were these introduced? If at least 2.7, its definitely fine. If 2.8, probably ok.", "author": "apurtell", "createdAt": "2020-05-26T22:25:41Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -40,7 +41,9 @@\n import org.apache.hadoop.hbase.io.hfile.CacheStats;\n import org.apache.hadoop.hbase.wal.WALProvider;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.hadoop.hdfs.DFSHedgedReadMetrics;", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTk0NQ==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430779945", "bodyText": "This is from 2.4. https://issues.apache.org/jira/browse/HBASE-7509 Should be good.", "author": "clarax", "createdAt": "2020-05-27T00:30:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3MjQ4NQ==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430772485", "bodyText": "Checkstyle conflicts need to be fixed by adding braces.", "author": "clarax", "createdAt": "2020-05-27T00:04:32Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();\n+    if (m == null) return false;", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTEyNA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775124", "bodyText": "Checkstyle conflict. Need braces.", "author": "clarax", "createdAt": "2020-05-27T00:13:28Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException\n+   */\n+  public static DFSHedgedReadMetrics getDFSHedgedReadMetrics(final Configuration c)\n+      throws IOException {\n+    if (!isHDFS(c)) return null;", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTI3NA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775274", "bodyText": "can be removed.", "author": "clarax", "createdAt": "2020-05-27T00:14:02Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTM0Mg==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775342", "bodyText": "can be removed", "author": "clarax", "createdAt": "2020-05-27T00:14:20Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTY1MA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775650", "bodyText": "Not necessary.", "author": "clarax", "createdAt": "2020-05-27T00:15:22Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,146 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   * @throws Exception", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3ODU0MQ==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430778541", "bodyText": "Validated from HBASE15550. LGTM.", "author": "clarax", "createdAt": "2020-05-27T00:25:51Z", "path": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java", "diffHunk": "@@ -504,6 +504,11 @@ public void getMetrics(MetricsCollector metricsCollector, boolean all) {\n               rsWrap.getCompactedCellsSize())\n           .addCounter(Interns.info(MAJOR_COMPACTED_CELLS_SIZE, MAJOR_COMPACTED_CELLS_SIZE_DESC),\n               rsWrap.getMajorCompactedCellsSize())\n+\n+          .addCounter(Interns.info(HEDGED_READS, HEDGED_READS_DESC), rsWrap.getHedgedReadOps())\n+          .addCounter(Interns.info(HEDGED_READ_WINS, HEDGED_READ_WINS_DESC),\n+              rsWrap.getHedgedReadWins())", "originalCommit": "4e9bf66bea31b151ffa43729cdc577f85786509e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "22b3531fe074bc1722ce655d893be3ff80c78abd", "url": "https://github.com/apache/hbase/commit/22b3531fe074bc1722ce655d893be3ff80c78abd", "message": "revert SplitLogManager unrelated changes", "committedDate": "2020-05-27T02:27:19Z", "type": "commit"}, {"oid": "22b3531fe074bc1722ce655d893be3ff80c78abd", "url": "https://github.com/apache/hbase/commit/22b3531fe074bc1722ce655d893be3ff80c78abd", "message": "revert SplitLogManager unrelated changes", "committedDate": "2020-05-27T02:27:19Z", "type": "forcePushed"}, {"oid": "517a0f8574f7fab212a447b925694f15a0d3b061", "url": "https://github.com/apache/hbase/commit/517a0f8574f7fab212a447b925694f15a0d3b061", "message": "remove not needed @param and @throws in headers", "committedDate": "2020-05-27T02:31:22Z", "type": "commit"}, {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5", "url": "https://github.com/apache/hbase/commit/3416a3297b7c07fa15b454bdea5f62bd209184d5", "message": "add braces (style)", "committedDate": "2020-05-27T02:31:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTA5NA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829094", "bodyText": "nit, space between 'null?', '0:'", "author": "Reidddddd", "createdAt": "2020-05-27T02:53:24Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();", "originalCommit": "3416a3297b7c07fa15b454bdea5f62bd209184d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTEyOQ==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829129", "bodyText": "ditto", "author": "Reidddddd", "createdAt": "2020-05-27T02:53:32Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();\n+  }\n+\n+  @Override\n+  public long getHedgedReadWins() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadWins();", "originalCommit": "3416a3297b7c07fa15b454bdea5f62bd209184d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2", "url": "https://github.com/apache/hbase/commit/3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2", "message": "spacing for style", "committedDate": "2020-05-27T03:04:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzM1OA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430833358", "bodyText": "space between 'null?', still missing.", "author": "Reidddddd", "createdAt": "2020-05-27T03:11:19Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0 : this.dfsHedgedReadMetrics.getHedgedReadOps();", "originalCommit": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzgyNg==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430833826", "bodyText": "sorry about that, fixing", "author": "javierluca", "createdAt": "2020-05-27T03:13:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzM1OA=="}], "type": "inlineReview"}, {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512", "url": "https://github.com/apache/hbase/commit/a865e0d51bb6901c8af7980d6434a3cd8f205512", "message": "spacing for style (2)", "committedDate": "2020-05-27T03:13:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDAxNw==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920017", "bodyText": "please pay attention to the space style problems in this unit test.", "author": "Reidddddd", "createdAt": "2020-05-27T07:45:22Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);", "originalCommit": "a865e0d51bb6901c8af7980d6434a3cd8f205512", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDI0OQ==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920249", "bodyText": "there're many.", "author": "Reidddddd", "createdAt": "2020-05-27T07:45:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920890", "bodyText": "Just remove instead of comments if you don't need following codes", "author": "Reidddddd", "createdAt": "2020-05-27T07:46:52Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);\n+    stm.readFully(7*blockSize, actual, 0, 4096);\n+    actual = new byte[3*4096];\n+    stm.readFully(0*blockSize, actual, 0, 3*4096);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 7\");\n+    actual = new byte[8*4096];\n+    stm.readFully(3*blockSize, actual, 0, 8*4096);\n+    checkAndEraseData(actual, 3*blockSize, expected, \"Pread Test 8\");\n+    // read the tail\n+    stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize/2);\n+    IOException res = null;\n+    try { // read beyond the end of the file\n+      stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize);\n+    } catch (IOException e) {\n+      // should throw an exception\n+      res = e;\n+    }\n+    assertTrue(\"Error reading beyond file boundary.\", res != null);\n+\n+    stm.close();\n+  }\n+\n+  private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {\n+    for (int idx = 0; idx < actual.length; idx++) {\n+      assertEquals(message+\" byte \"+(from+idx)+\" differs. expected \"+\n+          expected[from+idx]+\" actual \"+actual[idx],\n+        actual[idx], expected[from+idx]);\n+      actual[idx] = 0;\n+    }\n+  }\n+\n+  private void doPread(FSDataInputStream stm, long position, byte[] buffer,\n+    int offset, int length) throws IOException {\n+    int nread = 0;\n+    // long totalRead = 0;", "originalCommit": "a865e0d51bb6901c8af7980d6434a3cd8f205512", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkzMjUwMA==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430932500", "bodyText": "I see. Agree on these kind of comments should go away. However thought it would be better to keep it as similar as branch-2:\n\n  \n    \n      hbase/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n    \n    \n        Lines 612 to 641\n      in\n      441935a\n    \n    \n    \n    \n\n        \n          \n             private void doPread(FSDataInputStream stm, long position, byte[] buffer, \n        \n\n        \n          \n                 int offset, int length) throws IOException { \n        \n\n        \n          \n               int nread = 0; \n        \n\n        \n          \n               // long totalRead = 0; \n        \n\n        \n          \n               // DFSInputStream dfstm = null; \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (stm.getWrappedStream() instanceof DFSInputStream) { \n        \n\n        \n          \n                 dfstm = (DFSInputStream) (stm.getWrappedStream()); \n        \n\n        \n          \n                 totalRead = dfstm.getReadStatistics().getTotalBytesRead(); \n        \n\n        \n          \n               } */ \n        \n\n        \n          \n            \n        \n\n        \n          \n               while (nread < length) { \n        \n\n        \n          \n                 int nbytes = \n        \n\n        \n          \n                     stm.read(position + nread, buffer, offset + nread, length - nread); \n        \n\n        \n          \n                 assertTrue(\"Error in pread\", nbytes > 0); \n        \n\n        \n          \n                 nread += nbytes; \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (dfstm != null) { \n        \n\n        \n          \n                 if (isHedgedRead) { \n        \n\n        \n          \n                   assertTrue(\"Expected read statistic to be incremented\", \n        \n\n        \n          \n                     length <= dfstm.getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } else { \n        \n\n        \n          \n                   assertEquals(\"Expected read statistic to be incremented\", length, dfstm \n        \n\n        \n          \n                       .getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } \n        \n\n        \n          \n               }*/ \n        \n\n        \n          \n             } \n        \n    \n  \n\n\nOr master:\n\n  \n    \n      hbase/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n    \n    \n        Lines 612 to 641\n      in\n      476cb16\n    \n    \n    \n    \n\n        \n          \n             private void doPread(FSDataInputStream stm, long position, byte[] buffer, \n        \n\n        \n          \n                 int offset, int length) throws IOException { \n        \n\n        \n          \n               int nread = 0; \n        \n\n        \n          \n               // long totalRead = 0; \n        \n\n        \n          \n               // DFSInputStream dfstm = null; \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (stm.getWrappedStream() instanceof DFSInputStream) { \n        \n\n        \n          \n                 dfstm = (DFSInputStream) (stm.getWrappedStream()); \n        \n\n        \n          \n                 totalRead = dfstm.getReadStatistics().getTotalBytesRead(); \n        \n\n        \n          \n               } */ \n        \n\n        \n          \n            \n        \n\n        \n          \n               while (nread < length) { \n        \n\n        \n          \n                 int nbytes = \n        \n\n        \n          \n                     stm.read(position + nread, buffer, offset + nread, length - nread); \n        \n\n        \n          \n                 assertTrue(\"Error in pread\", nbytes > 0); \n        \n\n        \n          \n                 nread += nbytes; \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (dfstm != null) { \n        \n\n        \n          \n                 if (isHedgedRead) { \n        \n\n        \n          \n                   assertTrue(\"Expected read statistic to be incremented\", \n        \n\n        \n          \n                     length <= dfstm.getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } else { \n        \n\n        \n          \n                   assertEquals(\"Expected read statistic to be incremented\", length, dfstm \n        \n\n        \n          \n                       .getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } \n        \n\n        \n          \n               }*/ \n        \n\n        \n          \n             } \n        \n    \n  \n\n\nWas not sure about the policies in this case.\nRemoved for now.", "author": "javierluca", "createdAt": "2020-05-27T08:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0ODE4Nw==", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430948187", "bodyText": "no need to keep as branch-2, there're numerous different already.", "author": "Reidddddd", "createdAt": "2020-05-27T08:33:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA=="}], "type": "inlineReview"}, {"oid": "6640f763f8328d1d8f5b0c1f3f1f01c62c62dcfc", "url": "https://github.com/apache/hbase/commit/6640f763f8328d1d8f5b0c1f3f1f01c62c62dcfc", "message": "more spacing", "committedDate": "2020-05-27T08:02:21Z", "type": "commit"}, {"oid": "58652d4d39d5bf92d429f357e371e36e23baf73f", "url": "https://github.com/apache/hbase/commit/58652d4d39d5bf92d429f357e371e36e23baf73f", "message": "remove commented code", "committedDate": "2020-05-27T08:03:26Z", "type": "commit"}]}