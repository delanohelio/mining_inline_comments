{"pr_number": 1322, "pr_title": "HBASE-24033 Add ut for loading the corrupt recovered hfiles", "pr_createdAt": "2020-03-22T10:36:51Z", "pr_url": "https://github.com/apache/hbase/pull/1322", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA4NDAzOA==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396084038", "bodyText": "Why this name changing? Because for HFile based split we will write HFile to the root fs instead of wal fs?", "author": "Apache9", "createdAt": "2020-03-22T11:44:38Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java", "diffHunk": "@@ -345,16 +345,16 @@ public boolean accept(Path p) {\n \n   /**\n    * Move aside a bad edits file.\n-   * @param walFS WAL FileSystem used to rename bad edits file.\n+   * @param fs the file system used to rename bad edits file.\n    * @param edits Edits file to move aside.\n    * @return The name of the moved aside file.\n    * @throws IOException\n    */\n-  public static Path moveAsideBadEditsFile(final FileSystem walFS, final Path edits)\n+  public static Path moveAsideBadEditsFile(final FileSystem fs, final Path edits)", "originalCommit": "347fe5728f484ae62dbd8e6ab774c8a63b7c6e63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA4NjMwMA==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396086300", "bodyText": "Yes. For hfile, the passed fs is root file system.", "author": "infraio", "createdAt": "2020-03-22T12:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA4NDAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA4NDE5Nw==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396084197", "bodyText": "Better move this line into setUp or tearDown if necessary?", "author": "Apache9", "createdAt": "2020-03-22T11:46:21Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplitToHFile.java", "diffHunk": "@@ -163,24 +175,88 @@ private WAL createWAL(Configuration c, Path hbaseRootDir, String logName) throws\n     return wal;\n   }\n \n-  /**\n-   * Test writing edits into an HRegion, closing it, splitting logs, opening\n-   * Region again.  Verify seqids.\n-   */\n-  @Test\n-  public void testReplayEditsWrittenViaHRegion()\n-      throws IOException, SecurityException, IllegalArgumentException, InterruptedException {\n+  private Pair<TableDescriptor, RegionInfo> setupTableAndRegion() throws IOException {\n     final TableName tableName = TableName.valueOf(TEST_NAME.getMethodName());\n     final TableDescriptor td = createBasic3FamilyTD(tableName);\n     final RegionInfo ri = RegionInfoBuilder.newBuilder(tableName).build();\n     final Path tableDir = FSUtils.getTableDir(this.rootDir, tableName);\n     deleteDir(tableDir);\n     FSTableDescriptors.createTableDescriptorForTableDirectory(fs, tableDir, td, false);\n-    final byte[] rowName = tableName.getName();\n-    final int countPerFamily = 10;\n+    HRegion region = HBaseTestingUtility.createRegionAndWAL(ri, rootDir, this.conf, td);\n+    HBaseTestingUtility.closeRegionAndWAL(region);\n+    return new Pair<>(td, ri);\n+  }\n+\n+  @Test\n+  public void testCorruptRecoveredHFile() throws Exception {\n+    Pair<TableDescriptor, RegionInfo> pair = setupTableAndRegion();\n+    TableDescriptor td = pair.getFirst();\n+    RegionInfo ri = pair.getSecond();\n+\n+    WAL wal = createWAL(this.conf, rootDir, logName);\n+    HRegion region = HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal);\n+    final long timestamp = this.ee.currentTime();\n+    // Write data and flush\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      region.put(new Put(ROW).addColumn(cfd.getName(), Bytes.toBytes(\"x\"), timestamp, VALUE1));\n+    }\n+    region.flush(true);\n+\n+    // Now assert edits made it in.\n+    Result result1 = region.get(new Get(ROW));\n+    assertEquals(td.getColumnFamilies().length, result1.size());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      assertTrue(Bytes.equals(VALUE1, result1.getValue(cfd.getName(), Bytes.toBytes(\"x\"))));\n+    }\n+\n+    // Now close the region\n+    region.close(true);\n+    wal.shutdown();\n+    // split the log\n+    WALSplitter.split(rootDir, logDir, oldLogDir, FileSystem.get(this.conf), this.conf, wals);\n+\n+    // Write a corrupt recovered hfile\n+    Path regionDir =\n+        new Path(CommonFSUtils.getTableDir(rootDir, td.getTableName()), ri.getEncodedName());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      FileStatus[] files =\n+          WALSplitUtil.getRecoveredHFiles(this.fs, regionDir, cfd.getNameAsString());\n+      assertNotNull(files);\n+      assertTrue(files.length > 0);\n+      writeCorruptRecoveredHFile(files[0].getPath());\n+    }\n+\n+    // Failed to reopen the region\n+    WAL wal2 = createWAL(this.conf, rootDir, logName);\n+    try {\n+      HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal2);\n+      fail(\"Should fail to open region\");\n+    } catch (CorruptHFileException che) {\n+      // Expected\n+    }\n+\n+    // Set skip errors to true and reopen the region\n+    this.conf.setBoolean(HConstants.HREGION_EDITS_REPLAY_SKIP_ERRORS, true);\n+    HRegion region2 = HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal2);\n+    Result result2 = region2.get(new Get(ROW));\n+    assertEquals(td.getColumnFamilies().length, result2.size());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      assertTrue(Bytes.equals(VALUE1, result2.getValue(cfd.getName(), Bytes.toBytes(\"x\"))));\n+    }\n+    this.conf.setBoolean(HConstants.HREGION_EDITS_REPLAY_SKIP_ERRORS, false);", "originalCommit": "347fe5728f484ae62dbd8e6ab774c8a63b7c6e63", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA4NDI0NA==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396084244", "bodyText": "Add an assert to confirm that we move the broken HFile to the expected place?", "author": "Apache9", "createdAt": "2020-03-22T11:46:57Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALSplitToHFile.java", "diffHunk": "@@ -163,24 +175,88 @@ private WAL createWAL(Configuration c, Path hbaseRootDir, String logName) throws\n     return wal;\n   }\n \n-  /**\n-   * Test writing edits into an HRegion, closing it, splitting logs, opening\n-   * Region again.  Verify seqids.\n-   */\n-  @Test\n-  public void testReplayEditsWrittenViaHRegion()\n-      throws IOException, SecurityException, IllegalArgumentException, InterruptedException {\n+  private Pair<TableDescriptor, RegionInfo> setupTableAndRegion() throws IOException {\n     final TableName tableName = TableName.valueOf(TEST_NAME.getMethodName());\n     final TableDescriptor td = createBasic3FamilyTD(tableName);\n     final RegionInfo ri = RegionInfoBuilder.newBuilder(tableName).build();\n     final Path tableDir = FSUtils.getTableDir(this.rootDir, tableName);\n     deleteDir(tableDir);\n     FSTableDescriptors.createTableDescriptorForTableDirectory(fs, tableDir, td, false);\n-    final byte[] rowName = tableName.getName();\n-    final int countPerFamily = 10;\n+    HRegion region = HBaseTestingUtility.createRegionAndWAL(ri, rootDir, this.conf, td);\n+    HBaseTestingUtility.closeRegionAndWAL(region);\n+    return new Pair<>(td, ri);\n+  }\n+\n+  @Test\n+  public void testCorruptRecoveredHFile() throws Exception {\n+    Pair<TableDescriptor, RegionInfo> pair = setupTableAndRegion();\n+    TableDescriptor td = pair.getFirst();\n+    RegionInfo ri = pair.getSecond();\n+\n+    WAL wal = createWAL(this.conf, rootDir, logName);\n+    HRegion region = HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal);\n+    final long timestamp = this.ee.currentTime();\n+    // Write data and flush\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      region.put(new Put(ROW).addColumn(cfd.getName(), Bytes.toBytes(\"x\"), timestamp, VALUE1));\n+    }\n+    region.flush(true);\n+\n+    // Now assert edits made it in.\n+    Result result1 = region.get(new Get(ROW));\n+    assertEquals(td.getColumnFamilies().length, result1.size());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      assertTrue(Bytes.equals(VALUE1, result1.getValue(cfd.getName(), Bytes.toBytes(\"x\"))));\n+    }\n+\n+    // Now close the region\n+    region.close(true);\n+    wal.shutdown();\n+    // split the log\n+    WALSplitter.split(rootDir, logDir, oldLogDir, FileSystem.get(this.conf), this.conf, wals);\n+\n+    // Write a corrupt recovered hfile\n+    Path regionDir =\n+        new Path(CommonFSUtils.getTableDir(rootDir, td.getTableName()), ri.getEncodedName());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      FileStatus[] files =\n+          WALSplitUtil.getRecoveredHFiles(this.fs, regionDir, cfd.getNameAsString());\n+      assertNotNull(files);\n+      assertTrue(files.length > 0);\n+      writeCorruptRecoveredHFile(files[0].getPath());\n+    }\n+\n+    // Failed to reopen the region\n+    WAL wal2 = createWAL(this.conf, rootDir, logName);\n+    try {\n+      HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal2);\n+      fail(\"Should fail to open region\");\n+    } catch (CorruptHFileException che) {\n+      // Expected\n+    }\n+\n+    // Set skip errors to true and reopen the region\n+    this.conf.setBoolean(HConstants.HREGION_EDITS_REPLAY_SKIP_ERRORS, true);\n+    HRegion region2 = HRegion.openHRegion(this.conf, this.fs, rootDir, ri, td, wal2);\n+    Result result2 = region2.get(new Get(ROW));\n+    assertEquals(td.getColumnFamilies().length, result2.size());\n+    for (ColumnFamilyDescriptor cfd : td.getColumnFamilies()) {\n+      assertTrue(Bytes.equals(VALUE1, result2.getValue(cfd.getName(), Bytes.toBytes(\"x\"))));\n+    }\n+    this.conf.setBoolean(HConstants.HREGION_EDITS_REPLAY_SKIP_ERRORS, false);\n+  }", "originalCommit": "347fe5728f484ae62dbd8e6ab774c8a63b7c6e63", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5a8698cad64d595d6195e41883ab273cd560752b", "url": "https://github.com/apache/hbase/commit/5a8698cad64d595d6195e41883ab273cd560752b", "message": "HBASE-24033 Add ut for loading the corrupt recovered hfiles", "committedDate": "2020-03-22T12:24:58Z", "type": "commit"}, {"oid": "5a8698cad64d595d6195e41883ab273cd560752b", "url": "https://github.com/apache/hbase/commit/5a8698cad64d595d6195e41883ab273cd560752b", "message": "HBASE-24033 Add ut for loading the corrupt recovered hfiles", "committedDate": "2020-03-22T12:24:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5MTI5NA==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396091294", "bodyText": "This is a bug?", "author": "Apache9", "createdAt": "2020-03-22T13:03:50Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -5449,6 +5449,7 @@ private long loadRecoveredHFilesIfAny(Collection<HStore> stores) throws IOExcept\n             store.assertBulkLoadHFileOk(filePath);\n           } catch (IOException e) {\n             handleException(fs.getFileSystem(), filePath, e);\n+            continue;", "originalCommit": "5a8698cad64d595d6195e41883ab273cd560752b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5MjE0MA==", "url": "https://github.com/apache/hbase/pull/1322#discussion_r396092140", "bodyText": "Yes. Found it when add ut.", "author": "infraio", "createdAt": "2020-03-22T13:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5MTI5NA=="}], "type": "inlineReview"}]}