{"pr_number": 17651, "pr_title": "SQL optimizer design document", "pr_createdAt": "2020-09-30T12:33:45Z", "pr_url": "https://github.com/hazelcast/hazelcast/pull/17651", "timeline": [{"oid": "cf2507a624deccefbeb1e43f51845ff75829c7e2", "url": "https://github.com/hazelcast/hazelcast/commit/cf2507a624deccefbeb1e43f51845ff75829c7e2", "message": "WIP", "committedDate": "2020-09-29T14:02:30Z", "type": "commit"}, {"oid": "50cd8cb68a38df8165663d5622b3ccdc41b2a087", "url": "https://github.com/hazelcast/hazelcast/commit/50cd8cb68a38df8165663d5622b3ccdc41b2a087", "message": "WIP", "committedDate": "2020-09-29T14:02:48Z", "type": "commit"}, {"oid": "50b02d5999021f76aaaf0e5e496d61555b517d5f", "url": "https://github.com/hazelcast/hazelcast/commit/50b02d5999021f76aaaf0e5e496d61555b517d5f", "message": "WIP", "committedDate": "2020-09-29T15:56:22Z", "type": "commit"}, {"oid": "645828e7ffebd637ea292350b4b569e7607ac550", "url": "https://github.com/hazelcast/hazelcast/commit/645828e7ffebd637ea292350b4b569e7607ac550", "message": "Rename to avoid clash with expressions", "committedDate": "2020-09-30T07:37:36Z", "type": "commit"}, {"oid": "e298c3dcb0c4f4a5b5f5bad9585411eae8fc04c9", "url": "https://github.com/hazelcast/hazelcast/commit/e298c3dcb0c4f4a5b5f5bad9585411eae8fc04c9", "message": "Typos", "committedDate": "2020-09-30T07:51:31Z", "type": "commit"}, {"oid": "8a3de41f0797a56b781c7ab7fb8342f69760cee0", "url": "https://github.com/hazelcast/hazelcast/commit/8a3de41f0797a56b781c7ab7fb8342f69760cee0", "message": "WIP", "committedDate": "2020-09-30T08:24:38Z", "type": "commit"}, {"oid": "3e445fdfe2f226f5a8e4a14f9afbce5f758f85cc", "url": "https://github.com/hazelcast/hazelcast/commit/3e445fdfe2f226f5a8e4a14f9afbce5f758f85cc", "message": "The last part left.", "committedDate": "2020-09-30T09:39:18Z", "type": "commit"}, {"oid": "284bc50f24f7698a72e835f0cf5993539fabe251", "url": "https://github.com/hazelcast/hazelcast/commit/284bc50f24f7698a72e835f0cf5993539fabe251", "message": "WIP", "committedDate": "2020-09-30T10:11:15Z", "type": "commit"}, {"oid": "ce769f1325c7bca688f096b1a416ac10a8d4bbee", "url": "https://github.com/hazelcast/hazelcast/commit/ce769f1325c7bca688f096b1a416ac10a8d4bbee", "message": "Logical opto", "committedDate": "2020-09-30T11:03:36Z", "type": "commit"}, {"oid": "71e9dc2abcda536305d79d3b924be05a30d352ed", "url": "https://github.com/hazelcast/hazelcast/commit/71e9dc2abcda536305d79d3b924be05a30d352ed", "message": "Physical", "committedDate": "2020-09-30T12:23:42Z", "type": "commit"}, {"oid": "631d04b4f8f457a993c85884bd331d2b7c56bfef", "url": "https://github.com/hazelcast/hazelcast/commit/631d04b4f8f457a993c85884bd331d2b7c56bfef", "message": "Done", "committedDate": "2020-09-30T12:32:07Z", "type": "commit"}, {"oid": "2d3e890c506be0566c0c2122023d9755f98184a4", "url": "https://github.com/hazelcast/hazelcast/commit/2d3e890c506be0566c0c2122023d9755f98184a4", "message": "Minors", "committedDate": "2020-10-02T10:22:18Z", "type": "commit"}, {"oid": "cf4e39e1be1b78105ad046076b7f0de4b29717b9", "url": "https://github.com/hazelcast/hazelcast/commit/cf4e39e1be1b78105ad046076b7f0de4b29717b9", "message": "Minors", "committedDate": "2020-10-02T10:26:53Z", "type": "commit"}, {"oid": "e74f691940930d82aecb3fa3b1748ce18c05845e", "url": "https://github.com/hazelcast/hazelcast/commit/e74f691940930d82aecb3fa3b1748ce18c05845e", "message": "Typos", "committedDate": "2020-10-02T11:26:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIxMDcwNg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500210706", "bodyText": "Should this memo now contain JOIN_BA?", "author": "mmedenjak", "createdAt": "2020-10-06T11:47:26Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5OTk3Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500299973", "bodyText": "I guess it should contain:\nG2:   [Join_AB, Join_BA]", "author": "viliam-durina", "createdAt": "2020-10-06T13:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIxMDcwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI3MzQ1Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500273452", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n          \n          \n            \n            In this section we describe the theoretical aspects of query optimization that form the basis of the Hazelcast Mustang query", "author": "viliam-durina", "createdAt": "2020-10-06T13:25:28Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI3NDQyOA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500274428", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n          \n          \n            \n            tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators) and produces a", "author": "viliam-durina", "createdAt": "2020-10-06T13:26:47Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI3OTEwMQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500279101", "bodyText": "{1, \"John\"} is IMap<Long, String>", "author": "viliam-durina", "createdAt": "2020-10-06T13:32:34Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI4NDg4Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500284882", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n          \n          \n            \n            to encode the search space efficiently to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used", "author": "viliam-durina", "createdAt": "2020-10-06T13:38:49Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5MTM2MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500291360", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n          \n          \n            \n            Initially, the optimizer accepts an operator tree and a set of transformation rules. For every rule, a pattern", "author": "viliam-durina", "createdAt": "2020-10-06T13:45:30Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5MzI0Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500293242", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n          \n          \n            \n            matching is performed for the available operators. If a rule matches a part of the given operator tree, an", "author": "viliam-durina", "createdAt": "2020-10-06T13:47:32Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5NDQxNQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500294415", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n          \n          \n            \n            When a new operator is created, its cost is estimated. Since it may have better cost than previously known operators", "author": "viliam-durina", "createdAt": "2020-10-06T13:48:46Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5NDg0Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500294843", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n          \n          \n            \n            of the same equivalence group, it is necessary to recalculate the costs of parent operators. This step is called **reanalyzing**,", "author": "viliam-durina", "createdAt": "2020-10-06T13:49:11Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5NjY4NQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500296685", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n          \n          \n            \n            same rule may fire multiple times during reanalyzing/rematching, which makes the engine inefficient.", "author": "viliam-durina", "createdAt": "2020-10-06T13:51:01Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5NzEyMw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500297123", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n          \n          \n            \n            Consider the following operator initial plan and two rules - one changes the join order, and the other attempts", "author": "viliam-durina", "createdAt": "2020-10-06T13:51:28Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI5OTM0MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500299340", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n          \n          \n            \n            Notice how we have to schedule the same rule `SORT_REMOVE` twice to not miss the optimization opportunity.", "author": "viliam-durina", "createdAt": "2020-10-06T13:53:51Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMwMTgzOA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500301838", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n          \n          \n            \n            could be avoided, because reanalyzing/rematching is no longer needed.\n          \n          \n            \n            prune some nodes completely, perform partial optimization of a node, etc. Since the search is guided, many redundant rule calls\n          \n          \n            \n            can be avoided, because reanalyzing/rematching is no longer needed.", "author": "viliam-durina", "createdAt": "2020-10-06T13:56:22Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMwMjc2Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500302762", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n          \n          \n            \n            The guided search can be implemented either as a recursive function calls, or as a queue of tasks. In EXODUS,", "author": "viliam-durina", "createdAt": "2020-10-06T13:57:17Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMwMzI3Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500303272", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Consider the similar query plan, now optimized with the Cascades approach:\n          \n          \n            \n            Consider a similar query plan, now optimized with the Cascades approach:", "author": "viliam-durina", "createdAt": "2020-10-06T13:57:48Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMxMTM2Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500311367", "bodyText": "Didn't you mean \"join commute rule\"?", "author": "viliam-durina", "createdAt": "2020-10-06T14:06:13Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMxMTk3Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500311977", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n          \n          \n            \n            Notice how we avoid the excessive pattern matching and rule execution due to a guided search.", "author": "viliam-durina", "createdAt": "2020-10-06T14:06:51Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMxMjk3Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500312972", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n          \n          \n            \n            transformation rules, input optimizations, and of implementation rules ensures that a single physical plan is found as early as", "author": "viliam-durina", "createdAt": "2020-10-06T14:07:53Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMxNDQ2MQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500314461", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n          \n          \n            \n            possible. Once the first (sub)plan is found, the cost of the group can be calculated. Then this cost can be used to prune", "author": "viliam-durina", "createdAt": "2020-10-06T14:09:22Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMxOTIwNA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500319204", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n          \n          \n            \n            The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs and a set of", "author": "viliam-durina", "createdAt": "2020-10-06T14:14:16Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyMDg4Ng==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500320886", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n          \n          \n            \n            another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n          \n          \n            \n            Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfy one \n          \n          \n            \n            another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not vice versa.", "author": "viliam-durina", "createdAt": "2020-10-06T14:16:04Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyMzQ5Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500323493", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n          \n          \n            \n            for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like\n          \n          \n            \n            this:", "author": "viliam-durina", "createdAt": "2020-10-06T14:18:46Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyNTA5MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500325090", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n          \n          \n            \n            Or we may try to push the filter down to the Cassandra database, thus reducing the number of rows returned to the local process.", "author": "viliam-durina", "createdAt": "2020-10-06T14:20:21Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyNTkwOA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500325908", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n          \n          \n            \n            Many products that integrated Apache Calcite use the `Convention` to distinguish between logical and physical operators.", "author": "viliam-durina", "createdAt": "2020-10-06T14:21:11Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyNzQ0Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500327442", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n          \n          \n            \n                RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n          \n          \n            \n                RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n          \n          \n            \n                RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AxB), LogicalJoin(BxA)\n          \n          \n            \n                RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AxB), HashJoin(BxA)\n          \n          \n            \n                RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AxB)", "author": "viliam-durina", "createdAt": "2020-10-06T14:22:47Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDMyODA2MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500328060", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n          \n          \n            \n            When the plan is submitted for optimization, its operators are copied into the MEMO. Concrete operator inputs are", "author": "viliam-durina", "createdAt": "2020-10-06T14:23:24Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM2NjE4Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500366187", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n          \n          \n            \n            sorted by the group key `a`. Therefore, the rule may enforce the conversion:", "author": "viliam-durina", "createdAt": "2020-10-06T15:02:38Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM2OTcxNg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500369716", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n          \n          \n            \n            1. Metadata is not cached at `RelSet`/`RelSubset` levels, and is re-calculated on every call. This is not optimal,", "author": "viliam-durina", "createdAt": "2020-10-06T15:06:29Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM3MDI2MQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500370261", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n          \n          \n            \n            doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fires them", "author": "viliam-durina", "createdAt": "2020-10-06T15:07:04Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM3Mjc2MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500372760", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n          \n          \n            \n            1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on the operator's signature (`RelNode.explain`)", "author": "viliam-durina", "createdAt": "2020-10-06T15:09:37Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM3MzkyNQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500373925", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n          \n          \n            \n            We now discuss how the query optimization is organized in Hazelcast Mustang. The process is split into the following", "author": "viliam-durina", "createdAt": "2020-10-06T15:10:53Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM3NzkwNg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500377906", "bodyText": "Is it optimized? No optimization is performed yet.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n          \n          \n            \n            The result of this stage is an operator tree (`RelNode`) without subqueries and unused fields.", "author": "viliam-durina", "createdAt": "2020-10-06T15:15:14Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM3OTU0Ng==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500379546", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n          \n          \n            \n            Therefore, we use the cost-based `VolcanoPlanner` as a slower, but safer choice. We may reconsider this in the future, and move", "author": "viliam-durina", "createdAt": "2020-10-06T15:17:01Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4MjcwMw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500382703", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            | `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n          \n          \n            \n            | `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalFilter` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |", "author": "viliam-durina", "createdAt": "2020-10-06T15:20:26Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4Mzg5Mg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500383892", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n          \n          \n            \n            to this step, it could easily blow up the search space, because the EXODUS-like search algorithm does not allow for search", "author": "viliam-durina", "createdAt": "2020-10-06T15:21:42Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4NDcyNQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500384725", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n          \n          \n            \n            The fundamental observation is that splitting optimization into several phases doesn't guarantee the optimal plan", "author": "viliam-durina", "createdAt": "2020-10-06T15:22:37Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4NTk3OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500385978", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n          \n          \n            \n            field trimming, and filter pushdowns produce better plans in almost all cases, this is why we execute them at this", "author": "viliam-durina", "createdAt": "2020-10-06T15:23:52Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4ODM5Ng==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500388396", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n          \n          \n            \n            Since Hazelcast stores data on multiple nodes, we should take the data distribution into account during planning. This is achieved", "author": "viliam-durina", "createdAt": "2020-10-06T15:26:22Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4ODg5NQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500388895", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            | `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n          \n          \n            \n            | `PARTITIONED` | The result set is distributed across members, every row is located on one member only |", "author": "viliam-durina", "createdAt": "2020-10-06T15:26:52Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5MDQzNw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500390437", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n          \n          \n            \n            Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to how `Sort` is the", "author": "viliam-durina", "createdAt": "2020-10-06T15:28:29Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5MTQzNg==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500391436", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Below is the example of the distribution enforcement for the simple logical plan:\n          \n          \n            \n            Below is the example of the distribution enforcement for a simple logical plan:", "author": "viliam-durina", "createdAt": "2020-10-06T15:29:31Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n+enforcer operator for the `RelCollation`.\n+\n+Currently, we support only `RootExchangePhysicalRel` that delivers results to the initiator node. Implementation of joins,\n+aggregations, and sorting will require specialized implementations of the `Exchange` operator. \n+\n+The table below summarizes how the ROOT distribution is enforced.\n+\n+*Table 7: Root Distribution Enforcers*\n+| Input Distribution | Result |\n+|---|---|\n+| `ROOT` | No-op, the child operator is already located on the initiator |\n+| `PARTITIONED` | No-op, if there is only one member in the topology. Otherwise, inject `RootExchangePhysicalRel` on top of the child |\n+| `REPLICATED` | No-op, the child operator is already located on all members, including the initiator |\n+\n+Below is the example of the distribution enforcement for the simple logical plan:", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5NDAwNw==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500394007", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Optimization rules for the intermediate nodes (i.e. not leaves, and not root) follow the similar pattern: get the input's \n          \n          \n            \n            1. Optimization rules for intermediate nodes (i.e. not leaves, and not root) follow a similar pattern: get the input's", "author": "viliam-durina", "createdAt": "2020-10-06T15:32:10Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n+enforcer operator for the `RelCollation`.\n+\n+Currently, we support only `RootExchangePhysicalRel` that delivers results to the initiator node. Implementation of joins,\n+aggregations, and sorting will require specialized implementations of the `Exchange` operator. \n+\n+The table below summarizes how the ROOT distribution is enforced.\n+\n+*Table 7: Root Distribution Enforcers*\n+| Input Distribution | Result |\n+|---|---|\n+| `ROOT` | No-op, the child operator is already located on the initiator |\n+| `PARTITIONED` | No-op, if there is only one member in the topology. Otherwise, inject `RootExchangePhysicalRel` on top of the child |\n+| `REPLICATED` | No-op, the child operator is already located on all members, including the initiator |\n+\n+Below is the example of the distribution enforcement for the simple logical plan:\n+```\n+LOGICAL:\n+RootLogicalRel                      // Return to the user from the initiator\n+  MapScanLogicalRel                 // Scan an IMap\n+\n+PHYSICAL:\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+#### 3.5.2 Optimization Algorithm\n+\n+The goal of the optimizer is to find the cheapest plan that has the `ROOT` distribution at the top operator.  \n+\n+Currently, the optimization is performed **bottom-up**. We start with the leaf nodes, because their distribution is\n+always known. When physical implementations for the leaf node are found, rules are triggered on the parent nodes, and \n+parent distributions are resolved. The process continues until we reach the root node, that always has `ROOT` distribution. \n+Then the root node enforces the `ROOT` distribution on the input, adding the `RootExchangePhysicalRel` if needed.\n+\n+*Table 8: Initial Distributions of the Leaf Nodes*\n+\n+| Node | Distribution | Comment |\n+|---|---|---|\n+| `MapScanLogicalRel` | `PARTITIONED` | Scan of the partitioned `IMap` |\n+| `ValuesLogicalRel` | `REPLICATED` | Constant values, that are delivered to all members as a part of the plan |\n+  \n+The `VolcanoPlanner` is not suitable to work in the bottom-up trait propagation out-of-the-box. Therefore, we adjust our \n+integration with the Apache Calcite as follows:\n+1. During the physical optimization, we gradually convert nodes from the `LOGICAL` convention to `PHYSICAL`. For the \n+`PHYSICAL` convention we override a couple of methods (see `HazelcastConventions.PHYSICAL`) that roughly forces the optimizer\n+to do the following: when a new `PHYSICAL` node is created, force re-optimization of the `LOGICAL` parent. This way, whenever a \n+new `PHYSICAL` node is added to MEMO, the rules for the `LOGICAL` parent node is added to the execution queue. \n+1. Optimization rules for the intermediate nodes (i.e. not leaves, and not root) follow the similar pattern: get the input's ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5NDg1MQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500394851", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            way we ensure that all possibly interesting properties of the current group is propagated to parent groups.\n          \n          \n            \n            way we ensure that all possibly interesting properties of the current group are propagated to parent groups.", "author": "viliam-durina", "createdAt": "2020-10-06T15:33:03Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n+enforcer operator for the `RelCollation`.\n+\n+Currently, we support only `RootExchangePhysicalRel` that delivers results to the initiator node. Implementation of joins,\n+aggregations, and sorting will require specialized implementations of the `Exchange` operator. \n+\n+The table below summarizes how the ROOT distribution is enforced.\n+\n+*Table 7: Root Distribution Enforcers*\n+| Input Distribution | Result |\n+|---|---|\n+| `ROOT` | No-op, the child operator is already located on the initiator |\n+| `PARTITIONED` | No-op, if there is only one member in the topology. Otherwise, inject `RootExchangePhysicalRel` on top of the child |\n+| `REPLICATED` | No-op, the child operator is already located on all members, including the initiator |\n+\n+Below is the example of the distribution enforcement for the simple logical plan:\n+```\n+LOGICAL:\n+RootLogicalRel                      // Return to the user from the initiator\n+  MapScanLogicalRel                 // Scan an IMap\n+\n+PHYSICAL:\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+#### 3.5.2 Optimization Algorithm\n+\n+The goal of the optimizer is to find the cheapest plan that has the `ROOT` distribution at the top operator.  \n+\n+Currently, the optimization is performed **bottom-up**. We start with the leaf nodes, because their distribution is\n+always known. When physical implementations for the leaf node are found, rules are triggered on the parent nodes, and \n+parent distributions are resolved. The process continues until we reach the root node, that always has `ROOT` distribution. \n+Then the root node enforces the `ROOT` distribution on the input, adding the `RootExchangePhysicalRel` if needed.\n+\n+*Table 8: Initial Distributions of the Leaf Nodes*\n+\n+| Node | Distribution | Comment |\n+|---|---|---|\n+| `MapScanLogicalRel` | `PARTITIONED` | Scan of the partitioned `IMap` |\n+| `ValuesLogicalRel` | `REPLICATED` | Constant values, that are delivered to all members as a part of the plan |\n+  \n+The `VolcanoPlanner` is not suitable to work in the bottom-up trait propagation out-of-the-box. Therefore, we adjust our \n+integration with the Apache Calcite as follows:\n+1. During the physical optimization, we gradually convert nodes from the `LOGICAL` convention to `PHYSICAL`. For the \n+`PHYSICAL` convention we override a couple of methods (see `HazelcastConventions.PHYSICAL`) that roughly forces the optimizer\n+to do the following: when a new `PHYSICAL` node is created, force re-optimization of the `LOGICAL` parent. This way, whenever a \n+new `PHYSICAL` node is added to MEMO, the rules for the `LOGICAL` parent node is added to the execution queue. \n+1. Optimization rules for the intermediate nodes (i.e. not leaves, and not root) follow the similar pattern: get the input's \n+`RelSet`, extract all `RelSubset`-s with `PHYSICAL` convention, and create one intermediate physical node per `RelSubset`. This\n+way we ensure that all possibly interesting properties of the current group is propagated to parent groups.", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5NzA3MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500397070", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            After the split it is converted into the plan with two fragments:\n          \n          \n            \n            After the split it is converted into a plan with two fragments:", "author": "viliam-durina", "createdAt": "2020-10-06T15:35:23Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n+enforcer operator for the `RelCollation`.\n+\n+Currently, we support only `RootExchangePhysicalRel` that delivers results to the initiator node. Implementation of joins,\n+aggregations, and sorting will require specialized implementations of the `Exchange` operator. \n+\n+The table below summarizes how the ROOT distribution is enforced.\n+\n+*Table 7: Root Distribution Enforcers*\n+| Input Distribution | Result |\n+|---|---|\n+| `ROOT` | No-op, the child operator is already located on the initiator |\n+| `PARTITIONED` | No-op, if there is only one member in the topology. Otherwise, inject `RootExchangePhysicalRel` on top of the child |\n+| `REPLICATED` | No-op, the child operator is already located on all members, including the initiator |\n+\n+Below is the example of the distribution enforcement for the simple logical plan:\n+```\n+LOGICAL:\n+RootLogicalRel                      // Return to the user from the initiator\n+  MapScanLogicalRel                 // Scan an IMap\n+\n+PHYSICAL:\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+#### 3.5.2 Optimization Algorithm\n+\n+The goal of the optimizer is to find the cheapest plan that has the `ROOT` distribution at the top operator.  \n+\n+Currently, the optimization is performed **bottom-up**. We start with the leaf nodes, because their distribution is\n+always known. When physical implementations for the leaf node are found, rules are triggered on the parent nodes, and \n+parent distributions are resolved. The process continues until we reach the root node, that always has `ROOT` distribution. \n+Then the root node enforces the `ROOT` distribution on the input, adding the `RootExchangePhysicalRel` if needed.\n+\n+*Table 8: Initial Distributions of the Leaf Nodes*\n+\n+| Node | Distribution | Comment |\n+|---|---|---|\n+| `MapScanLogicalRel` | `PARTITIONED` | Scan of the partitioned `IMap` |\n+| `ValuesLogicalRel` | `REPLICATED` | Constant values, that are delivered to all members as a part of the plan |\n+  \n+The `VolcanoPlanner` is not suitable to work in the bottom-up trait propagation out-of-the-box. Therefore, we adjust our \n+integration with the Apache Calcite as follows:\n+1. During the physical optimization, we gradually convert nodes from the `LOGICAL` convention to `PHYSICAL`. For the \n+`PHYSICAL` convention we override a couple of methods (see `HazelcastConventions.PHYSICAL`) that roughly forces the optimizer\n+to do the following: when a new `PHYSICAL` node is created, force re-optimization of the `LOGICAL` parent. This way, whenever a \n+new `PHYSICAL` node is added to MEMO, the rules for the `LOGICAL` parent node is added to the execution queue. \n+1. Optimization rules for the intermediate nodes (i.e. not leaves, and not root) follow the similar pattern: get the input's \n+`RelSet`, extract all `RelSubset`-s with `PHYSICAL` convention, and create one intermediate physical node per `RelSubset`. This\n+way we ensure that all possibly interesting properties of the current group is propagated to parent groups.\n+\n+Note that this algorithm uses the optimistic approach: we create intermediate physical nodes for every possible\n+combination of physical properties, assuming it will help parents find better plans. For complex plans, this may\n+create too many operators. A better approach would be to create only those intermediate operators that are really\n+required by parents, effectively changing the direction of the optimization: top-down instead of bottom-up. To achieve\n+this we may use the solution used in Apache Flink (see `FlinkExpandConversionRule`). It is likely, that we will have\n+to switch to this approach, when joins are implemented.\n+        \n+#### 3.5.3 Optimizations\n+\n+Currently, we have the following physical optimizations.\n+\n+*Table 9: Physical Optimizations*\n+\n+| Name | Produces | Description |\n+|---|---|---|\n+| `RootPhysicalRule` | `RootPhysicalRel` | Convert logical root to physical root, enforcing the `ROOT` distribution on the input |\n+| `ProjectPhysicalRule` | `ProjectPhysicalRel` | Convert logical project to physical project, propagate input properties |\n+| `FilterPhysicalRule` | `FilterPhysicalRel` | Convert logical filter to physical filter, propagate input properties |\n+| `ValuesPhysicalRule` | `ValuesPhysicalRel` | Convert logical values to physical values, assign `REPLICATED` distribution |\n+| `MapScanPhysicalRule` | `MapScanPhysicalRel`, `MapIndexScanPhysicalRel` | Convert logical map scan to physical map scan or physical map index scan based on the filter condition, assign `PARTITIONED` distribution |\n+\n+The result of the physical optimization is an optimized tree of operators with the `HazelcastConvention.PHYSICAL` convention, \n+that has `RootPhysicalRel` at the root, and has zero or more `Exchange` operators that move data within the cluster.\n+\n+### 3.6 Splitting\n+\n+Finally, we split the physical plan into one or more fragments, using `Exchange` operators as fragment boundaries:\n+1. Every node is converted to the `PlanNode` counterpart, that could be serialized and sent over network\n+1. Every `Exchange` node is converted into a pair of `send` and `receive` plan nodes (actual implementation depends on the \n+exchange type)\n+\n+Consider the following physical plan:\n+```\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+After the split it is converted into the plan with two fragments:", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM5NzY4NQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17651#discussion_r500397685", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            on the Apache Calcite, and could be sent to other nodes for execution. \n          \n          \n            \n            on Apache Calcite, and can be sent to other nodes for execution.", "author": "viliam-durina", "createdAt": "2020-10-06T15:36:03Z", "path": "docs/design/sql/09-optimizer.md", "diffHunk": "@@ -0,0 +1,768 @@\n+# SQL Optimizer \n+\n+## Overview\n+\n+When a query string is submitted for execution, it is first parsed, converted to a tree of relational operators, and then \n+passed to the optimizer in order to find the best execution path.\n+\n+In this document, we describe the design of the Hazelcast Mustang query optimizer that is based on Apache Calcite.\n+\n+## 1 Theory\n+\n+In this section we describe the theoretical aspects of query optimization that forms the basis of the Hazelcast Mustang query \n+optimizer.\n+\n+### 1.1 Definitions\n+\n+First, we define several entities from the relational algebra. The definitions are not necessarily precise from the \n+relational algebra standpoint, but are sufficient for the purpose of this document. \n+\n+A **tuple** is an ordered collection of triplets `[name, type, value]`. A **relation** is an unordered collection of \n+tuples. A relational **operator** is a function that takes arbitrary arguments (possibly, other operators), and produces a \n+relation. \n+\n+A **plan** is a tree of relational operators. Two plans are said to be **equivalent** if they produce the same relation for \n+all possible inputs. \n+\n+### 1.2 Cost\n+\n+Typically, a query could be executed in different ways. For example, the query `SELECT * FROM table` could be executed either\n+through a direct table scan, or through a scan of an index on that table. \n+\n+The goal of the query optimizer is to find the **best** plan among one or more alternative equivalent plans. The definition of \n+the **best** is context-dependent. Usually, the goal is to find the plan that consumes the least amount of system resources,\n+that could be CPU, memory, network. The optimization goal could be also defined in terms of performance characteristics, such \n+as \"find the plan with the minimal latency\".\n+\n+To formalize the optimization goal, every operator is assigned a **cost** - an abstract comparable value, that depends on the \n+operator type, and operator arguments. The plan with the least cost is said to be the best plan for the given query.     \n+\n+### 1.3 Properties\n+\n+A **property** is an arbitrary value assigned to an operator, that is used during the optimization process. The query optimizer\n+may **enforce** a certain property on the operator. If the operator satisfies the required property, it is left unchanged. \n+Otherwise, an **enforcer** operator is applied to the original operator to meet the optimizer requirements.\n+\n+In the query optimization literature, the **collation** (aka sort order) is represented as a property. The **Sort** operator is \n+a typical enforcer operator.\n+\n+Consider the class `Person {id:Long, name:String}` and an `IMap<Long, Person>` with two entries `[{1, \"John\"}, {2, \"Jane}]`.\n+Then the query below produces a relation with two tuples:\n+```sql\n+SELECT id, name FROM person ORDER BY id\n+``` \n+```\n+Sort[$0 ASC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+```\n+\n+Since the relation is defined as an **unordered** collection of tuples, the plan of the query below is **equivalent** to the \n+plan of the query above, even though the order of rows is different:  \n+```sql\n+SELECT id, name FROM person ORDER BY id DESC\n+``` \n+```\n+Sort[$0 DESC]\n+  Project[$0=id, $1=name]\n+    Scan[person]\n+```\n+```\n+{id:BIGINT:2, name:VARCHAR:\"Jane\"}\n+{id:BIGINT:1, name:VARCHAR:\"John\"}\n+```\n+\n+While these two plans are equivalent in the relational theory, they produce different results from the user standpoint. \n+The difference comes from the different values of the collation property: the first query has collation `[a ASC]`, while\n+the second query has collation `[a DESC]`.\n+\n+### 1.4 MEMO\n+\n+During the optimization, quite a few alternative plans could be created (thousands, millions, etc). Therefore, it is important\n+to encode the search space efficiently, to prevent out-of-memory conditions. The so-called **MEMO** data structure is often used\n+for this purpose. \n+\n+A **group** is a collection of equivalent operators. When a plan is submitted for optimization, its operators are copied and \n+put into groups. During copying, operator inputs are replaced with groups. Then, the search for equivalent operators is \n+performed. When an equivalent operator is found, it is put into the same equivalence group as the original operator, thus\n+eliminating the need to copy the whole operator tree.\n+\n+Consider a simple query that performs a table scan:\n+```\n+SELECT name FROM person\n+```  \n+If there is an index on the table, then there are two access paths (table scan, index scan), and two equivalent plans. Without\n+the MEMO, we would have to create two separate plans:\n+```\n+Project[name]\n+  TableScan[person]\n+```   \n+```\n+Project[name]\n+  IndexScan[person, index]\n+```\n+With the MEMO, the plan is first copied into the search space:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person]}\n+```\n+Then, when an alternative scan path is found, it is added to the group:\n+```\n+G1: {Project[name]}\n+  G2: {TableScan[person], IndexScan[person, index]}\n+```  \n+\n+### 1.5 Logical and Physical Operators\n+\n+In query optimization literature, there are typically two types of operators. **Logical operators** are operators that define \n+data transformations. **Physical operators** are specific algorithms that implement particular transformations. Below are \n+several examples of logical operators and their physical counterparts.\n+\n+*Table 1: Logical and Physical Operators*\n+\n+| Logical Operator | Physical Operator |\n+|---|---|\n+| `Scan` | `TableScan`, `IndexScan` |\n+| `Join` | `HashJoin`, `MergeJoin` |\n+| `Agg` | `HashAgg`, `StreamingAgg` |\n+\n+The ultimate goal of the optimizer is to find the best physical plan for the initial logical plan.\n+\n+### 1.6 Extensible Query Optimizers\n+\n+Modern query optimizers are extensible. They have a core algorithm, and a set of user-provided interfaces:\n+- Operators\n+- Rules - code that create new operators based on some pattern and other conditions\n+- Metadata - additional information that is used by rules, such as cardinality, column uniqueness, etc\n+\n+Now we discuss two concrete algorithms that are relevant to this document. \n+\n+#### 1.6.1 EXODUS\n+\n+The EXODUS was a research project to design an extensible database system. As a part of this project, an extensible query \n+optimizer was developed [[1]].\n+\n+In the original paper, the logical and physical operators are named **operators** and **methods** respectively. \n+The rules applied to logical operators are named **transformation rules**, while the rules applied to physical operators\n+are named **implementation rules**.\n+\n+Initially, the optimizer accepts the operator tree, and a set of transformation rules. For every rule, a pattern\n+matching is performed for the available operators. If a rule matches the given part of the operator tree, an\n+instance of the rule is placed into a priority queue called **OPEN**. This way the initial queue of rule instances\n+is formed.\n+\n+Then the optimizer takes rule instances from the queue and applies them to the operator tree. The result of rule\n+execution is zero, one or more new logical operators. For every new logical operator, matching implementation rules\n+are executed, possibly producing new physical operators. Newly created operators are stored in a MEMO-like data \n+structure called **MESH**. \n+\n+When a new operator is created, it's cost is estimated. Since it may have better cost than previously known operators\n+of the same equivalence group, it is necessarily to recalculate costs of parent operators. This step is called **reanalyzing**,\n+and is performed by re-execution of the implementation rules on parents. In addition to this, if a new logical operator was \n+created, there might be new possibilities for further transformations. Therefore, the transformation rule instances are \n+scheduled for execution on parent operators (i.e. added to OPEN). This step is called **rematching**.\n+   \n+The algorithm proceeds until OPEN is empty.\n+\n+The EXODUS optimizer doesn't have a strict order of rule execution. One may assign weights to rule instance to make them\n+fire earlier, but generally the search is not guided - every rule instance fires independently of others. As a result, the\n+same rule may fire multiple times during reanalyzing/rematching, what makes the engine inefficient.\n+\n+Consider the following operator initial plan, and two rules - one changes the join order, and the other one attempts \n+to remove the sort operator if the input is already sorted:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+1. JOIN_COMMUTE[Join_AB]\n+2. SORT_REMOVE[Sort, Join_AB]\n+```\n+Once the `JOIN_COMMUTE` rule is fired, a new operator Join_BA operator is created, and a new rule instance is scheduled:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+OPEN (pending):\n+2. SORT_REMOVE[Sort, Join_AB]\n+3. SORT_REMOVE[Sort, Join_BA]\n+\n+OPEN (retired):\n+1. JOIN_COMMUTE[Join_AB]\n+```\n+Notice how we have to schedule the same rule `SORT_REMOVE` twice not to miss the optimization opportunity.\n+\n+#### 1.6.2 Volcano/Cascades\n+\n+The same research group attempted to find more efficient optimization algorithm, what led to the development of Volcano [[2]] \n+and Cascades [[3]] optimizers.\n+\n+The main difference that is that Volcano/Cascades uses a **guided top-down search** strategy. Operators are optimized only\n+if requested explicitly by parents. Therefore, the optimizer is free to define any optimization logic it finds useful - it may \n+prune some nodes completely, perform partial optimization of a node, etc. Since a search is guided, many redundant rule calls\n+could be avoided, because reanalyzing/rematching is no longer needed.\n+\n+The guided search could be implemented either as a recursive function calls, or as a queue of tasks. In the EXODUS,\n+the task is a rule instance, in the Cascades the queue contains optimization tasks, such as \"transform this operator\".\n+\n+Consider the similar query plan, now optimized with the Cascades approach:\n+\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+```\n+\n+Then during optimization of the sort we recognize that we need to optimize the join:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB]\n+\n+STACK:\n+-- OPTIMIZE[G2->JOIN_COMMUTE] \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+During join optimization, the join associate rule is fired:\n+```\n+MEMO:\n+G1: [Sort]\n+G2:   [Join_AB, Join_BA]\n+\n+STACK: \n+-- OPTIMIZE[G1->SORT_REMOVE]\n+``` \n+\n+Last, the sort elimination rule is fired on top of the already optimized `G2`.\n+\n+Notice, how we avoid the excessive pattern matching and rule execution due to a guided search. \n+\n+The Cascades design clearly separates logical optimization (exploration) and physical optimization (implementation).\n+When an unoptimized group is reached, matching transformation rules are scheduled. Then the optimization proceeds\n+to group inputs, and only after that the implementation rules for the group are fired. A careful guided interleaving of \n+transformation rules, input optimization, and implementation rules ensures that a single physical plan is found as early as\n+possible. Once the first (sub)plan is found, the cost of the group could be calculated. Then this cost could be used to prune \n+less efficient alternatives, the technique known as **branch-and-bound** pruning.\n+  \n+The Volcano/Cascades top-down guided search is widely considered superior to EXODUS and earlier bottom-up optimization \n+strategies, because it allows for flexible optimization of only parts of the search space. \n+\n+Cascades-like query optimization is used in SQL Server, Pivotal Greenplum and CockroachDB, to name a few.\n+\n+## 2 Apache Calcite\n+\n+The initial analysis showed that a number of Java-based data management projects use Apache Calcite for query optimization. \n+Apache Calcite is a framework to build data processing engines. It consists of SQL parser, query optimizer, query execution \n+engine, and JDBC driver. \n+\n+During the research phase, we integrated the Apache Calcite, and at the same time researched what will it take us to implement \n+our own optimizer. We revealed a significant problem with Apache Calcite optimization algorithm that could be solved to some\n+extent at the cost of poor optimizer performance (discussed below). At the same time we realized that the implementation of our\n+own optimizer will take enormous time. Therefore, the final decision was to proceed with Apache Calcite as a basis for our \n+optimizer.\n+\n+Apache Calcite has two optimizers - heuristic (`HepPlanner`) and cost-based (`VolcanoPlanner`). Since the heuristic\n+optimizer cannot guarantee the optimal plan, we use cost-based `VolcanoPlanner`. Below we discuss the design of the latter.\n+\n+### 2.1 Operators and Rules\n+\n+The operator abstraction is defined in the `RelNode` interface. The operator may have zero or more inputs, and a set of \n+properties encoded in the `RelTraitSet` data structure:\n+```java\n+interface RelNode {\n+    List<RelNode> getInputs(); // Inputs\n+    RelTraitSet getTraitSet(); // Properties\n+}\n+```\n+The rule abstraction is defined in the `RelOptRule` abstract class. Its constructor accepts the pattern. Users \n+must implement the method `onMatch`, where the actual transformation is performed.\n+```java\n+abstract class RelNode {\n+    void onMatch(RelOptRuleCall call); \n+}\n+```\n+\n+### 2.2 Traits\n+\n+The operator may have a custom property, defined by the `RelTrait` interface. Example property is collation (sort order).\n+Every `RelTrait` has a relevant `RelTraitDef` instance, that defines whether two traits of the same type satisfies one \n+another. For example, `[a ASC, b ASC]` satisfies `[a ASC]`, but not the vice versa.\n+\n+Apache Calcite comes with two built-in traits:\n+- `RelCollation` - collation\n+- `Convention` - an opaque marker, that describes the application-specific type of the node\n+\n+We will use the terms `property` and `trait` interchangeably.\n+\n+#### 2.2.1 Convention\n+\n+Convention is a special trait in Apache Calcite, that describes the type of the operator (`RelNode`).  \n+\n+After the parsing, all operators are assigned the `Convention.NONE`, meaning they are abstract. By default, an operator \n+with `NONE` convention has an infinite cost, and hence cannot be part of a valid plan. One of the goals of the planning process \n+is to find nodes with non-`NONE` conventions. The method `VolcanoPlanner.setNoneConventionHasInfiniteCost` could be used to \n+assign non-infinite costs to `NONE` nodes. \n+\n+Apache Calcite is meant to execute federated queries, such as `SELECT * FROM cassandra.table1 JOIN druid.table2`.\n+In addition, Apache Calcite comes with a couple of execution backends, `Enumerable` (interpreter) and `Bindable` (compiler).\n+Therefore, the original motivation for the `Convention` trait was to define the execution backend for the operator. For example,\n+for the query `SELECT * FROM cassandra.table1 JOIN druid.table2 WHERE table1.field = 1`, the plan after parsing will look like:\n+```\n+Filter[NONE](table1.field = 1)\n+  Join[NONE]\n+    Table[NONE](cassandra.table1)\n+    Table[NONE](druid.table2)\n+```\n+\n+Then we can delegate table scans to the respective backend databases, and then perform the filter and join locally using one the \n+Calcite backends. Assuming we did a filter pushdown, the plan could look like:\n+```\n+Join[BINDABLE]\n+  Filter[BINDABLE](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Or we may try to push the filter down to the Cassandra database, this reducing the number of rows returned to the local process. \n+To do this, we set the `CASSANDRA` convention to the filter node, so that the executor understands that it should be passed to \n+the database, rather than be processed locally: \n+```\n+Join[BINDABLE]\n+  Filter[CASSANDRA](table1.field = 1)\n+    Table[CASSANDRA](table1)\n+  Table[DRUID](table2)\n+```\n+\n+Note that the `Convention` is merely an opaque marker that is used during the planning process. It is up to the execution\n+backend to decide how to deal with the marker.\n+\n+Many products that integrated Apache Calcite, uses the `Convention` to distinguish between logical and physical operators. \n+\n+### 2.3 Memoization\n+\n+The search space is organized in a collection of groups of equivalent operators, called `RelSet`. Within the `RelSet`\n+operators are further grouped by their physical properties into one or more `RelSubset`.  \n+\n+For example, the join equivalence group might look like this:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL, collation=NONE] -> LogicalJoin(AXB), LogicalJoin(BxA)\n+    RelSubset#2: [convention=PHYSICAL, collation=NONE] -> HashJoin(AXB), HashJoin(BxA)\n+    RelSubset#3: [convention=PHYSICAL, collation={A.a ASC}] -> MergeJoin(AXB)\n+}\n+```\n+  \n+When the plan is submitted for optimization, it's operators are copied into the MEMO. Concrete operator inputs are\n+replaced with the relevant RelSubset-s. E.g.:\n+```\n+BEFORE:\n+Sort(Scan)\n+  Scan\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Sort(RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan\n+}\n+```\n+  \n+Every operator has a signature string that uniquely identifies it. Two operators with the same signature are placed into\n+the same equivalence group. For example, for the query `SELECT * FROM t JOIN t`, two scans will end up in the \n+same subset:\n+```\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Join(RelSubset#2, RelSubset#2)\n+}\n+RelSet#2 {\n+    RelSubset#2: [convention=LOGICAL] -> Scan(t)\n+}\n+``` \n+  \n+When a rule is being executed a new operator might be created. This operator is added to the search space through \n+the `RelOptRuleCall.transformTo(RelNode newOperator)` call. The `RelOptRuleCall` knows the original operator it was\n+called for, and hence it adds the `newOperator` to the same `RelSet` as the original operator. For example, when an index\n+scan is applicable for the given table scan, it will be added to the same group:\n+```\n+BEFORE:\n+RelSet#1 {\n+    RelSubset#1: [convention=LOGICAL] -> Scan(t)\n+}\n+\n+AFTER:\n+RelSet#1 {\n+    RelSubset#1: [convention=PHYSICAL] -> Scan(table)\n+    RelSubset#2: [convention=PHYSICAL, collation={a ASC}] -> IndexScan(table, index(a))\n+}\n+```\n+  \n+### 2.4 Enforcers  \n+  \n+It is possible to enforce a certain trait on the operator. This is done through a `RelOptRule.convert` call. When the \n+conversion is requested, the optimizer creates a special `AbstractConverter` operator. When a converter is created, an \n+instance of the `AbstractConverter.ExpandConversionRule` is scheduled to expand it. This rule compares the properties \n+of original operator, and the requested properties. If the original properties do not satisfy the requested ones, the\n+relevant `RelTraitDef` is invoked to enforce the required property. The result of the enforcement could be either a \n+new operator with the desired property, or `null`, which means that the conversion is not possible.\n+\n+For example, consider the following MEMO:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan]\n+```    \n+There could be a rule to produce a non-blocking streaming physical aggregate. Such an aggregate requires the input to be \n+sorted on the group key `a`. Therefore, the rule may enforce the conversion:    \n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC)]\n+```\n+\n+Later, the `AbstractConverter.ExpandConversionRule` instance expands the converter. It calls the `RelTraitDef` of the \n+collation property, that adds a `LogicalSort` operator on top of the scan:\n+```\n+RelSet#1: [LogicalAgg(a)]\n+RelSet#2: [LogicalScan, AbstractConverter(LogicalScan, a ASC), LogicalSort(LogicalScan, a ASC)]\n+``` \n+\n+### 2.5 Metadata\n+\n+Many optimization rules and operator cost functions require access to external metadata, such as column cardinality,\n+column uniqueness, etc. Apache Calcite ships with an extensible framework for metadata management. \n+\n+Every type of metadata is represented as a concrete class extending the `MetadataHandler` interface. For every operator\n+type, a separate method with a predefined name is created in this class.  \n+\n+Then the metadata class is wired up into the `RelMetadataQuery` class by means of Janino-based code generation. An \n+instance of the `RelMetadataQuery` class is passed to the rule invocation and operator cost contexts, thus providing\n+a centralized access point to all required metadata.\n+\n+This approach is convenient and extensible, but has several performance problems:\n+1. Compilation with Janino may take significant time to complete, increasing planning time to seconds on a fresh JVM\n+1. Metadata is not cached at `RelSet`/`RelSubset` levels, and re-calculated on every call. This is not optimal,\n+because metadata calculation typically requires recursive dives into inputs of the operator. \n+\n+### 2.6 Execution\n+\n+The `VolcanoPlanner` employs EXODUS-like approach to query optimization. It uses rules to find alternative plans. However, it \n+doesn't employ the guided top-down search strategy. Instead, the optimizer organizes rule instances in a queue, and fire them\n+until the queue is empty. The word `Volcano` in the name is misleading, because the optimizer doesn't actually follow the \n+main ideas from the Volcano/Cascades papers.\n+\n+First, an instance of the `VolcanoPlanner` is created and initialized with:\n+1. The operator tree to be optimized (`RelNode`)\n+1. The list of rules to use (`RelOptRule`)\n+1. The list of property types that will be considered during optimization (`RelTraitDef`)\n+1. The desired properties of the result\n+\n+Then the optimization process is started through a call to the `VolcanoOptimizer.findBestExp` method. The result is the \n+optimized operator tree (`RelNode`) with the desired properties, or an exception if the desired properties cannot be \n+satisfied through the given set of rules.\n+\n+The optimization process is split into three main phases:\n+1. Copy-in\n+2. Optimization\n+3. Copy-out\n+\n+#### 2.6.1 Copy-in Phase\n+\n+The goal of the **copy-in** phase is to prepare the initial MEMO and rule instance queue. For every operator the following steps \n+are performed:\n+1. A copy of the operator is created using `RelNode.copy` method, with inputs replaced with the relevant `RelSubset` instances\n+1. An operator is added to the relevant `RelSet` and `RelSubset` instances based on an operator's signature (`RelNode.explain`)\n+1. A cost of the operator is calculated and set as the `best` cost of the current `RelSubset`\n+1. For every rule that matches the given operator a rule instance (`VolcanoRuleMatch`) is added to the rule queue \n+(`VolcanoPlanner.ruleQueue`) \n+\n+The copy-in is performed via `VolcanoPlanner.setRoot` method.\n+\n+#### 2.6.2 Optimization Phase\n+\n+The optimization phase proceeds as follows. The next rule instance is taken from the queue and executed. For every new \n+operator created during rule invocation:\n+1. Add the operator to MEMO\n+1. Update the cost of the operator's `RelSubset` and parents, if needed\n+1. Schedule new rule instances for the operator and parents, if needed\n+\n+The process continues until the queue is empty.\n+\n+#### 2.6.3 Copy-out Phase\n+\n+Once the optimization is finished, it is necessary to produce the resulting plan from the MEMO. To do this, a recursive \n+top-bottom dive from the top `RelSubset` is performed. All `RelSubset` instances already have a node with the best cost at \n+this point (aka \"winner\"). To construct the final plan, the optimizer recursively finds winners bottom-up.\n+\n+## 3 Hazelcast Mustang Optimizer\n+\n+We now discuss how the query optimization is organized in the Hazelcast Mustang. The process is split into the following\n+phases:\n+\n+*Table 2: Optimization Phases*\n+\n+| # | Name | Input | Output | Description |\n+|---|---|---|---|---|\n+| 1 | Parsing | `String` | `SqlNode` | Convert the query string into the parse tree |\n+| 2 | Validation | `SqlNode` | `RelNode` | Semantic analysis of the parse tree and conversion to operator tree |\n+| 3 | Rewrite | `RelNode` | `RelNode` | Remove subqueries and trim unused columns |\n+| 4 | Logical Optimization | `RelNode` | `RelNode` | Apply transformation rules |\n+| 5 | Physical Optimization | `RelNode` | `RelNode` | Apply implementation rules |\n+| 6 | Splitting | `RelNode` | `SqlPlan` | Create an executable query plan |\n+\n+### 3.1 Parsing\n+\n+The original query is passed to Calcite's parser and is converted to the parse tree (`SqlNode`). \n+\n+We use the built-in parser, since it is sufficient for the supported feature set. Jet extends the parser with \n+custom commands. \n+\n+### 3.2 Validation\n+\n+The parse tree is validated for semantic correctness. Tables and columns are resolved, return types of every SQL operator is \n+calculated. Since Apache Calcite supports more features than we do, we additionally apply the visitor that attempts to \n+find unsupported operators. When found, an exception is thrown with the precise position in the original query string.\n+\n+Once the `SqlNode` is validated, it is converted to the tree of abstract relational operators (`RelNode`), all with the\n+`Convention.NONE`.\n+\n+We use Calcite's `SqlValidator` and `SqlToRelConverter` with custom extensions for validation and conversion respectively.   \n+\n+### 3.3 Rewrite\n+\n+The initial `RelNode` may contain subqueries that are difficult to deal with. Generally, every subquery (correlated or not) \n+could be replaced with a sequence of join and aggregate operators [[4]]. We use Apache Calcite built-in classes to eliminate\n+subqueries (`SubQueryRemoveRule`, `SqlToRelConverter.decorrelate`). \n+\n+After the subqueries are eliminated, unused fields may remain in the plan. We use `SqlToRelConverter.trimUnusedFields`\n+method to find and remove the unused fields, thus making the plan more efficient.\n+\n+The result of this stage is the optimized operator tree (`RelNode`) without subqueries and unused fields.\n+\n+### 3.4 Logical Optimization\n+\n+Further optimization is split into two independent phases - logical and physical.\n+\n+The logical optimization is concerned with transformations to the operator tree, that simplifies the plan, without \n+considering their physical implementations. Generally, we do the following:\n+- Fuse operators together to make the tree smaller\n+- Removing operators that have no impact on the final result\n+- Removing operators that do not produce any results\n+  \n+In other optimizers, most of these rules are typically part of the rewrite phase, where the heuristic planning is used instead\n+of that cost-based optimization. In Apache Calcite, the `HepPlanner` could be used for this. However, some rules may trigger \n+conflicting actions, causing the heuristic planner to fail. An example is filter \"move around\" rules: often it is important\n+not only to push the filter down, but also to try to push it up, because it may enable further optimizations of the parent node\n+(such as transitive predicate push, partition pruning, etc). The heuristic planner may fail to find these alternatives. \n+Therefore, we use the cost-based `VolcanoPlanner` as slower, but safer choice. We may reconsider this in the future, and move \n+some logical rules to separate stages that use heuristic planner, as it is done in other projects (e.g. Apache Flink).\n+For now this is not important, because we do not support joins, so our queries have small search spaces. \n+  \n+We define the following logical operators and rules:\n+\n+*Table 3: Logical Operators*\n+\n+| Name | Description |\n+|---|---|\n+| `LogicalTableScan` | Scan a table |\n+| `LogicalProject` | Perform a projection (e.g. get an expression `a + b` from the relation `[a, b, c]` |\n+| `LogicalFilter` | Filter rows of the input based on the provided condition |\n+\n+*Table 4: Logical Rules*\n+\n+| Name | Product | Description |\n+|---|---|---|\n+| `FilterMergeRule` | Calcite | Merges two adjacent `LogicalFilter` into one to make the operator tree simpler |\n+| `FilterProjectTransposeRule` | Calcite | Moves `LogicalFilter` past `LogicalProject` (aka \"filter pushdown\") to allow for further optimizations, such as `FilterIntoScanLogicalRule` |\n+| `FilterIntoScanLogicalRule` | Hazelcast | Moves a `LogicalFilter` in the table of the `LogicalScan` to make the operator tree simpler |\n+| `ProjectMergeRule` | Calcite | Merges two adjacent `LogicalProject` into one to make the operator tree simpler |\n+| `ProjectFilterTransposeRule` | Calcite | Moves `LogicalProject` past `LogicalFilter` to allow for further optimizations, such as `ProjectIntoScanLogicalRule` |\n+| `ProjectIntoScanLogicalRule` | Hazelcast | Gets the list of required input fields from the `LogicalProject` and ensures that the child `LogicalScan` doesn't return unused fields |\n+| `ProjectRemoveRule` | Calcite | Removes a `LogicalProject` if it doesn't do anything, e.g. `SELECT a, b FROM (SELECT a, b FROM table)` |\n+| `PruneEmptyRules` | Calcite | Removes `LogicalProject` and `LogicalProject` if their input is known to be empty, e.g. `SELECT a, b FROM table WHERE 1 != 1` |\n+\n+Execution of these rules might create many hundreds and thousands of alternative plans. If we add physical optimization\n+to this step, it could easily blow the search space, because the EXODUS-like search algorithm do not allow for search\n+space pruning, and requires re-execution of the same rules multiple times to guarantee that the optimal plan is found.\n+\n+Consider that we produced `N` different logical plans, and have physical rules that may produce `M` alternatives \n+for every logical plan. As a result, we will have to consider `N * M` plans. Instead, we perform the logical \n+optimization in a separate step, extract only one best plan from the search space, and then apply the physical rules\n+to on the next stage. This way, we have to consider only `N + M` plans, that alleviates the inefficiency of the core\n+search algorithm of `VolcanoPlanner`.\n+\n+The fundamental observation, is that splitting optimization into several phases doesn't guarantee the optimal plan \n+in the general case. That is, if we generated plans `P1, P2 ... Pm ...`, and picked `Pm` as the best one, it\n+doesn't mean that applying additional rules to `Pm` will produce the optimal plan `Pm'`. Another plan `Pn`, \n+such that `cost(Pn) > cost(Pm)`, could yield better plan `Pn'`, such that `cost(Pn') < cost(Pm')`.\n+\n+Therefore, the logical phase should generally include rules, that produce plans that are generally the best starting\n+points for the subsequent physical optimization with high probability. Operator fusion, removal of unused operators, scan \n+field trimming, and filter pushdowns produces better plans in almost all cases, this is why we execute them at this \n+stage.\n+\n+As a part of the logical optimization process, we convert the Calcite operators with the convention `Convention.NONE` \n+to our own logical operators with the convention `HazelcastConvention.LOGICAL`. We do this through a special conversion\n+rules that extend Calcite's `ConverterRule`.  \n+\n+If there is a Calcite operator in the tree that doesn't have a logical counterpart, it indicates that there is either an \n+unsupported operation, that we missed during validation phase, or that we missed some conversion rule. An exception will be \n+thrown in this case.\n+\n+*Table 5: Logical Conversion Rules*\n+\n+| Name | From (Calcite) | To (Hazelcast) |\n+|---|---|---|\n+| `MapScanLogicalRule` | `LogicalTableScan` | `MapScanLogicalRel` |\n+| `FilterLogicalRule` | `LogicalFilter` | `FilterPhysicalRel` |\n+| `ProjectLogicalRule` | `LogicalProject` | `ProjectLogicalRel` |\n+| `ValuesLogicalRule` | `LogicalValues` | `ValuesLogicalRel` |\n+\n+Last, we manually add a special `RootLogicalRel` on top of the result. This operator is an abstraction of a user query\n+cursor that returns results on the initiator member. \n+\n+The result of the logical optimization is an optimized tree of operators with the `HazelcastConvention.LOGICAL` convention, \n+that has `RootLogicalRel` at the root.\n+\n+### 3.5 Physical Optimization\n+\n+The goal of the physical optimization is to find the physical implementations of logical operators. Some physical operators\n+have 1-to-1 mapping to their logical counterparts (e.g. project), while others may have completely different implementations \n+(e.g. index scan created out of logical table scan). \n+\n+#### 3.5.1 Distribution Trait\n+\n+Since Hazelcast stores data on multiple nodes, we should take the data distribution in count during planning. This is achieved\n+through the `DistributionTrait` property that is assigned to physical operators during planning.   \n+\n+*Table 6: Distribution Types*\n+\n+| Name | Description |\n+|---|---|\n+| `ROOT` | The result set is located on the initiator member only |\n+| `PARTITIONED` | The result set is distributed across member, every row is located on one member only |\n+| `REPLICATED` | The whole result set is located on every member |\n+\n+When the optimization starts, we request the `ROOT` distribution from the planner. This literally means \"deliver the final \n+result to the initiator member\". \n+\n+The `DistributionTraitDef` defines what should happen if a parent member requests a certain distribution that cannot\n+be satisfied by the child. If the data movement is needed, a special `Exchange` operator is injected between the \n+parent and the child. The goal of the `Exchange` operator is to move data between members to get the desired distribution.\n+Therefore, the `Exchange` operator is the enforcer operator for the `DistributionTrait`, similarly to `Sort` that is the\n+enforcer operator for the `RelCollation`.\n+\n+Currently, we support only `RootExchangePhysicalRel` that delivers results to the initiator node. Implementation of joins,\n+aggregations, and sorting will require specialized implementations of the `Exchange` operator. \n+\n+The table below summarizes how the ROOT distribution is enforced.\n+\n+*Table 7: Root Distribution Enforcers*\n+| Input Distribution | Result |\n+|---|---|\n+| `ROOT` | No-op, the child operator is already located on the initiator |\n+| `PARTITIONED` | No-op, if there is only one member in the topology. Otherwise, inject `RootExchangePhysicalRel` on top of the child |\n+| `REPLICATED` | No-op, the child operator is already located on all members, including the initiator |\n+\n+Below is the example of the distribution enforcement for the simple logical plan:\n+```\n+LOGICAL:\n+RootLogicalRel                      // Return to the user from the initiator\n+  MapScanLogicalRel                 // Scan an IMap\n+\n+PHYSICAL:\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+#### 3.5.2 Optimization Algorithm\n+\n+The goal of the optimizer is to find the cheapest plan that has the `ROOT` distribution at the top operator.  \n+\n+Currently, the optimization is performed **bottom-up**. We start with the leaf nodes, because their distribution is\n+always known. When physical implementations for the leaf node are found, rules are triggered on the parent nodes, and \n+parent distributions are resolved. The process continues until we reach the root node, that always has `ROOT` distribution. \n+Then the root node enforces the `ROOT` distribution on the input, adding the `RootExchangePhysicalRel` if needed.\n+\n+*Table 8: Initial Distributions of the Leaf Nodes*\n+\n+| Node | Distribution | Comment |\n+|---|---|---|\n+| `MapScanLogicalRel` | `PARTITIONED` | Scan of the partitioned `IMap` |\n+| `ValuesLogicalRel` | `REPLICATED` | Constant values, that are delivered to all members as a part of the plan |\n+  \n+The `VolcanoPlanner` is not suitable to work in the bottom-up trait propagation out-of-the-box. Therefore, we adjust our \n+integration with the Apache Calcite as follows:\n+1. During the physical optimization, we gradually convert nodes from the `LOGICAL` convention to `PHYSICAL`. For the \n+`PHYSICAL` convention we override a couple of methods (see `HazelcastConventions.PHYSICAL`) that roughly forces the optimizer\n+to do the following: when a new `PHYSICAL` node is created, force re-optimization of the `LOGICAL` parent. This way, whenever a \n+new `PHYSICAL` node is added to MEMO, the rules for the `LOGICAL` parent node is added to the execution queue. \n+1. Optimization rules for the intermediate nodes (i.e. not leaves, and not root) follow the similar pattern: get the input's \n+`RelSet`, extract all `RelSubset`-s with `PHYSICAL` convention, and create one intermediate physical node per `RelSubset`. This\n+way we ensure that all possibly interesting properties of the current group is propagated to parent groups.\n+\n+Note that this algorithm uses the optimistic approach: we create intermediate physical nodes for every possible\n+combination of physical properties, assuming it will help parents find better plans. For complex plans, this may\n+create too many operators. A better approach would be to create only those intermediate operators that are really\n+required by parents, effectively changing the direction of the optimization: top-down instead of bottom-up. To achieve\n+this we may use the solution used in Apache Flink (see `FlinkExpandConversionRule`). It is likely, that we will have\n+to switch to this approach, when joins are implemented.\n+        \n+#### 3.5.3 Optimizations\n+\n+Currently, we have the following physical optimizations.\n+\n+*Table 9: Physical Optimizations*\n+\n+| Name | Produces | Description |\n+|---|---|---|\n+| `RootPhysicalRule` | `RootPhysicalRel` | Convert logical root to physical root, enforcing the `ROOT` distribution on the input |\n+| `ProjectPhysicalRule` | `ProjectPhysicalRel` | Convert logical project to physical project, propagate input properties |\n+| `FilterPhysicalRule` | `FilterPhysicalRel` | Convert logical filter to physical filter, propagate input properties |\n+| `ValuesPhysicalRule` | `ValuesPhysicalRel` | Convert logical values to physical values, assign `REPLICATED` distribution |\n+| `MapScanPhysicalRule` | `MapScanPhysicalRel`, `MapIndexScanPhysicalRel` | Convert logical map scan to physical map scan or physical map index scan based on the filter condition, assign `PARTITIONED` distribution |\n+\n+The result of the physical optimization is an optimized tree of operators with the `HazelcastConvention.PHYSICAL` convention, \n+that has `RootPhysicalRel` at the root, and has zero or more `Exchange` operators that move data within the cluster.\n+\n+### 3.6 Splitting\n+\n+Finally, we split the physical plan into one or more fragments, using `Exchange` operators as fragment boundaries:\n+1. Every node is converted to the `PlanNode` counterpart, that could be serialized and sent over network\n+1. Every `Exchange` node is converted into a pair of `send` and `receive` plan nodes (actual implementation depends on the \n+exchange type)\n+\n+Consider the following physical plan:\n+```\n+RootPhysicalRel[ROOT]               // Return to the user from the initiator\n+  RootExchangePhysicalRel[ROOT]     // Send to the initiator\n+    MapScanPhysicalRel[PARTITIONED] // Scan an IMap on all nodes\n+```\n+\n+After the split it is converted into the plan with two fragments:\n+```\n+FRAGMENT 1:\n+RootPlanNode                       // Return to the user from the initiator\n+  ReceivePlanNode[edge=1]          // Receive results from all members\n+\n+FRAGMENT 2:\n+RootSendPlanNode[edge=1]           // Send results to the initiator\n+  MapScanLogicalRel[PARTITIONED]   // Scan an IMap locally\n+```\n+\n+The split logic is located in the `PlanCreateVisitor` class. The result is the `Plan` object, that doesn't depend\n+on the Apache Calcite, and could be sent to other nodes for execution. ", "originalCommit": "e74f691940930d82aecb3fa3b1748ce18c05845e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e3f33938f9edba3718ec33706a496564c4eba6ff", "url": "https://github.com/hazelcast/hazelcast/commit/e3f33938f9edba3718ec33706a496564c4eba6ff", "message": "Merge branch 'master' into sql-optimizer-doc", "committedDate": "2020-10-07T07:45:35Z", "type": "commit"}, {"oid": "b5efda606caf4e12ff617e2acd782eff0060b293", "url": "https://github.com/hazelcast/hazelcast/commit/b5efda606caf4e12ff617e2acd782eff0060b293", "message": "Typos.", "committedDate": "2020-10-07T08:02:22Z", "type": "commit"}]}