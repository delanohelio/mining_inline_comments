{"pr_number": 4630, "pr_title": "DHFPROD-5989: Configure workUnit and endpointState in Spark connector", "pr_createdAt": "2020-09-28T17:51:37Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4630", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE1OTgxMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496159810", "bodyText": "Please remove Sysout", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-28T18:45:22Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+            if(jsonObject.get(\"apiPath\") == null || jsonObject.get(\"apiPath\").toString().length()==0) {\n+                if((jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) ||\n+                    (jsonObject.get(\"workUnit\")!=null && jsonObject.get(\"workUnit\").toString().length()>0))\n+                {\n+                    throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+                }\n+            }\n+            final String apiModulePath = (jsonObject.get(\"apiPath\")!=null && jsonObject.get(\"apiPath\").toString().length()>0) ?\n+                jsonObject.get(\"apiPath\").toString() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+            logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+            try {\n+                this.loader = InputEndpoint.on(\n+                    hubClient.getStagingClient(),\n+                    hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+                ).bulkCaller();\n+            } catch (ResourceNotFoundException ex) {\n+                throw new RuntimeException(\"Endpoint not found.\");\n+            }\n+\n+            if (jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) {\n+                loader.setEndpointState(new ByteArrayInputStream((jsonObject.get(\"endpointState\").toString()).getBytes()));\n+            }\n+            // TODO : remove the below else block after java-client-api 5.3 release\n+            else {\n+                loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \"}\").getBytes()));\n+            }\n+\n+            String uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";\n+\n+            if (jsonObject.get(\"workUnit\") != null && jsonObject.get(\"workUnit\").toString().length()>0) {\n+                JSONObject workUnitJson = new JSONObject(jsonObject.getString(\"workUnit\"));\n+                workUnitJson.putOpt(\"uriprefix\", uriPrefix);\n+                loader.setWorkUnit(new ByteArrayInputStream((workUnitJson.toString()).getBytes()));\n+                System.out.println(workUnitJson.toString());", "originalCommit": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2NDczNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496164735", "bodyText": "We use ObjectMapper/JsonNode from Jackson, so please switch to that instead of this library.", "author": "rjrudin", "createdAt": "2020-09-28T18:54:20Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);", "originalCommit": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2ODIzMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496168230", "bodyText": "There's a lot going on here, and generally, I like to use private/protected methods to break up long methods so that they're easier to read and maintain. I prefer private, though I'll use protected when I want to write unit tests against the logic.\nI see the following steps in this code, where each can become a new private method:\n\nUse the user params to determine the apiPath, the workUnit, and endpointState (and throw a validation error as needed)\nUse the output of #1 to create a BulkInputCaller\n\nI think the signature of the first private method would be \"private JsonNode determineIngestionEndpointParams(Map)\". If the user has defined \"ingestendpointparams\", then use ObjectMapper to parse that into a JsonNode, and then validate it. Else, use objectMapper.createObjectNode() and populate apiPath based on the default API path, and then populate workUnit based on uriPrefix (and then @SameeraPriyathamTadikonda  would add more logic to this for collections/permissions/etc).\nThat first method would likely be protected so that you could easily write unit tests against it without having to involve everything else in this class.\nThe second private method would have a signature of \"private BulkInputCaller buildBulkInputCaller(JsonNode ingestionParams)\".\nEach private method can then have a try/catch as needed to ensure that if an error is thrown, the appropriate context is provided in the error message.", "author": "rjrudin", "createdAt": "2020-09-28T19:00:46Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,61 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n-        this.loader = InputEndpoint.on(\n-            hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n-        ).bulkCaller();\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        try {\n+            JSONObject jsonObject = new JSONObject(ingestEndpointParams);\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+            if(jsonObject.get(\"apiPath\") == null || jsonObject.get(\"apiPath\").toString().length()==0) {\n+                if((jsonObject.get(\"endpointState\") != null && jsonObject.get(\"endpointState\").toString().length()>0) ||\n+                    (jsonObject.get(\"workUnit\")!=null && jsonObject.get(\"workUnit\").toString().length()>0))\n+                {\n+                    throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+                }\n+            }\n+            final String apiModulePath = (jsonObject.get(\"apiPath\")!=null && jsonObject.get(\"apiPath\").toString().length()>0) ?", "originalCommit": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2ODkxMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496168910", "bodyText": "I think that since taskId doesn't matter, let's remove the debug logging above it. It would be meaningless to see it.", "author": "rjrudin", "createdAt": "2020-09-28T19:02:05Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java", "diffHunk": "@@ -53,7 +53,7 @@ public HubDataWriterFactory(Map<String, String> params, StructType schema) {\n         if (logger.isDebugEnabled()) {\n             logger.debug(\"Creating DataWriter with taskId: \" + taskId);\n         }\n-        return new HubDataWriter(hubClient, taskId, schema, params);\n+        return new HubDataWriter(hubClient, schema, params);", "originalCommit": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE3MDI1MQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496170251", "bodyText": "When there are a lot of params in a method, that's generally a sign that a class should be created.\nWhile we don't need this class in the application code, an \"Options\" class would be very helpful here for making it easier to write tests. It would have properties of batchSize, uriPrefix, apiPath, etc. And then it would have a \"Map toMap()\" method that builds a Map of params based on what's been configured.\nThen, each test will just build an Options class based on the scenario that it's testing. That will make it easier to write future tests - i.e. when @SameeraPriyathamTadikonda  needs to test for collections/permissions/etc, we don't have to add those to this method and fix every method that calls it - we just add new properties to our Options class, and the existing tests won't be affected.", "author": "rjrudin", "createdAt": "2020-09-28T19:04:43Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -77,18 +96,16 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n      * @param uriPrefix\n      * @return\n      */\n-    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix) {\n+    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix, String endpointState, String workUnit, String apiPath) {", "originalCommit": "398a0ec14aa6e208e55b552e599b47ecf32d7962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5d9639acaa23af7f891bcd460aa35e69bce914ca", "url": "https://github.com/marklogic/marklogic-data-hub/commit/5d9639acaa23af7f891bcd460aa35e69bce914ca", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-28T23:59:02Z", "type": "forcePushed"}, {"oid": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "url": "https://github.com/marklogic/marklogic-data-hub/commit/7c4ebc32159bc86bedd9f02a73436380038cdff1", "message": "Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-29T00:01:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5MzE0OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496693149", "bodyText": "We always want to provide the original exception here, and it's useful to include the message in the rethrown error's message - e.g. :\nthrow new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n\nAlso, I believe this will fail if ingestendpointparams isn't set, which Ernie doesn't need to set. So we need a test to verify that if ingestendpointparams isn't set, then no error is thrown and the default API is used.", "author": "rjrudin", "createdAt": "2020-09-29T12:55:44Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5MzY1Nw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496693657", "bodyText": "This code should go into determineIngestionEndpointParams. That method should return a JsonNode with all 3 fields populated. Unit tests can then verify that those fields are correct based on a variety of inputs.", "author": "rjrudin", "createdAt": "2020-09-29T12:56:24Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -41,28 +44,26 @@\n     private InputEndpoint.BulkInputCaller loader;\n     private StructType schema;\n     private int batchSize;\n+    private String uriPrefix;\n \n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n+        this.uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        JsonNode endpointParamsJsonNode = determineIngestionEndpointParams(params);\n+        final String apiModulePath = (endpointParamsJsonNode.get(\"apiPath\")!=null &&", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NDIxMw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496694213", "bodyText": "Use node.has(\"apiPath\"), that's a cleaner way to determine if the value is set.", "author": "rjrudin", "createdAt": "2020-09-29T12:57:12Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NTE2Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496695162", "bodyText": "This code can be a lot simpler if the determineParams method always returns a value for endpointState - i.e. it should default to \"{}\".", "author": "rjrudin", "createdAt": "2020-09-29T12:58:41Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {\n+            if((endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length()>0) ||\n+                (endpointParamsJsonNode.get(\"workUnit\")!=null && endpointParamsJsonNode.get(\"workUnit\").asText().length()>0))\n+            {\n+                throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+            }\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiModulePath, JsonNode endpointParamsJsonNode) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");\n+        }\n+        return configureBulkInputCaller(bulkInputCaller, endpointParamsJsonNode);\n+    }\n+\n+    private InputEndpoint.BulkInputCaller configureBulkInputCaller(InputEndpoint.BulkInputCaller loader, JsonNode endpointParamsJsonNode) {\n+\n+        try {\n+            if (endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NTkyNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496695925", "bodyText": "The determineParams method should handle building the workUnit, unless Ernie provides its own. That should be the only method that has knowledge of how to build the workUnit; this method for building the BulkInputCaller then only has to worry about ensuring that the API exists and then building the caller object.", "author": "rjrudin", "createdAt": "2020-09-29T12:59:49Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +115,63 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        String ingestEndpointParams = params.get(\"ingestendpointparams\");\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Exception occurred while processing ingestEndpointParams.\");\n+        }\n+\n+        if(endpointParamsJsonNode.get(\"apiPath\") == null || endpointParamsJsonNode.get(\"apiPath\").asText().length() == 0) {\n+            if((endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length()>0) ||\n+                (endpointParamsJsonNode.get(\"workUnit\")!=null && endpointParamsJsonNode.get(\"workUnit\").asText().length()>0))\n+            {\n+                throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+            }\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiModulePath, JsonNode endpointParamsJsonNode) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiModulePath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");\n+        }\n+        return configureBulkInputCaller(bulkInputCaller, endpointParamsJsonNode);\n+    }\n+\n+    private InputEndpoint.BulkInputCaller configureBulkInputCaller(InputEndpoint.BulkInputCaller loader, JsonNode endpointParamsJsonNode) {\n+\n+        try {\n+            if (endpointParamsJsonNode.get(\"endpointState\") != null && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+                loader.setEndpointState(new ByteArrayInputStream((endpointParamsJsonNode.get(\"endpointState\").toString()).getBytes()));\n+            }\n+            // TODO : remove the below else block after java-client-api 5.3 release\n+            else {\n+                loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \"}\").getBytes()));\n+            }\n+\n+            if (endpointParamsJsonNode.get(\"workUnit\") != null && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+                ObjectMapper objectMapper = new ObjectMapper();\n+\n+                JsonNode workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+                ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5OTY0MQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496699641", "bodyText": "Pinging @SameeraPriyathamTadikonda  about this - for each workUnit input that the endpoint supports, we'll want to verify that it works correctly in the BulkIngestTest class, as that class has no knowledge of Spark.\nThe tests for our Spark connector should then focus on converting the params Map into the proper workUnit object. Those tests don't need to connect to ML - they can be very fast unit tests that live in a separate class that doesn't extend AbstractHubCoreTest. I'll put together a PR to demonstrate this and submit it against this branch.\nWe'll still want one test in this project that writes data with every workUnit field populated, just to make sure that we're setting the correct workUnit fields.", "author": "rjrudin", "createdAt": "2020-09-29T13:05:24Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -30,7 +34,9 @@\n \n     @Test\n     void ingestThreeFruitsWithBatchSizeOfTwo() throws IOException {\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(\"2\", \"/testFruit\");\n+        Map<String, String> params = getHubPropertiesAsMap();", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjcwMDQzNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r496700435", "bodyText": "Can this just be \"{}\", since we don't care about \"next\"? It's supposed to go away once we shift to the newer Java client, but I think it'd be better to make it \"{}\" in the meantime.", "author": "rjrudin", "createdAt": "2020-09-29T13:06:32Z", "path": "marklogic-data-hub/src/test/java/com/marklogic/hub/dataservices/ingestion/BulkIngestTest.java", "diffHunk": "@@ -32,8 +32,8 @@ public void setupTest() {\n     public void testBulkIngest() {\n \n         String prefix = \"/bulkIngesterTest\";\n-        String endpointState = \"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";\n-        String workUnit      = \"{\\\"taskId\\\":\"+1+\"}\";\n+        String endpointState =  \"{\\\"next\\\":\"+0+\"}\";", "originalCommit": "7c4ebc32159bc86bedd9f02a73436380038cdff1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "url": "https://github.com/marklogic/marklogic-data-hub/commit/6e0ccb36e5926380cee293321a4b62ab2472e4b4", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-29T20:19:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMDY4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497030688", "bodyText": "I am wondering if the ResourceNotFoundException needs to be caught here. If we do throw a new exception, we definitely need to include the original exception so that Ernie doesn't lose any context.\nI did a quick test, and this is what the exception shows:\ncom.marklogic.client.ResourceNotFoundException: Local message: Could not read non-existent document. Server Message: RESTAPI-NODOCUMENT: (err:FOER0000) Resource or document does not exist:  category: content message: /data-hub/5/data-services/ingestion/bulkIngesterrrr.api\n\nThat is likely helpful enough for Ernie - it shows the module he referenced, and the Java Client is giving a good indication of the error. So I think this try/catch can be removed.\nAnd because of that, we don't really need this method anymore, since it's just a few lines of code now. If we do need some special error handling here, I think it's worth the separate method. But that doesn't appear to be the case.", "author": "rjrudin", "createdAt": "2020-09-29T20:37:13Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\").asText());\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", workUnitNode.toString());\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + uriPrefix + \"\\\"}\");\n+        }\n+        return endpointParamsJsonNode;\n+    }\n+\n+    private InputEndpoint.BulkInputCaller buildBulkInputCaller(HubClient hubClient, String apiPath, String endpointState,\n+                                                               String workUnit) {\n+        InputEndpoint.BulkInputCaller bulkInputCaller;\n+        try {\n+            bulkInputCaller= InputEndpoint.on(\n+                hubClient.getStagingClient(),\n+                hubClient.getModulesClient().newJSONDocumentManager().read(apiPath, new StringHandle())\n+            ).bulkCaller();\n+        } catch (ResourceNotFoundException ex) {\n+            throw new RuntimeException(\"Endpoint not found.\");", "originalCommit": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzMwMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497033300", "bodyText": "Exception messages are crucial here to explain to Ernie what to do if he made a mistake. We want to be more precise about how to fix the problem - e.g.\nCannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well", "author": "rjrudin", "createdAt": "2020-09-29T20:39:45Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");", "originalCommit": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNDQ3MQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497034471", "bodyText": "No need to capture this here - batchSize needs to be captured, but not this. Just need to read this in the determineIngestionEndpointParams method.", "author": "rjrudin", "createdAt": "2020-09-29T20:40:52Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -41,28 +44,24 @@\n     private InputEndpoint.BulkInputCaller loader;\n     private StructType schema;\n     private int batchSize;\n+    private String uriPrefix;\n \n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n+        this.uriPrefix = params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\";", "originalCommit": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNjM5MQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497036391", "bodyText": "Use set(\"workUnit\", workUnitNode) instead. That returns the JSON structure instead of converting it into a string. You can then do setWorkUnit(new JacksonHandle(theNode)) and don't have to fiddle with converting things into strings. Same goes for endpointState.", "author": "rjrudin", "createdAt": "2020-09-29T20:42:45Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +113,68 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot override default endpointState and/or workUnit. Please provide custom endpoint.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\").asText());\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", uriPrefix);\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", workUnitNode.toString());", "originalCommit": "6e0ccb36e5926380cee293321a4b62ab2472e4b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "660d87d7616098274a5b97ff3d2005b0a07461fb", "url": "https://github.com/marklogic/marklogic-data-hub/commit/660d87d7616098274a5b97ff3d2005b0a07461fb", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-29T21:34:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MDYxMw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497090613", "bodyText": "I think this needs to be a JsonNode as well instead of a string for consistency. You can do mapper.createObjectNode() and then node.put(\"uriprefix\", params.get(\"uriprefix\")) .\nI think it's good to handle these as JsonNode's instead of as strings. When Ernie provides us with a custom workUnit and/or endpointState, we should convert those ASAP to JsonNode. That way, if Ernie's JSON is malformed, it'll fail as quickly as possible - i.e. in the connector code and not in ML.\nAlso this is where @SameeraPriyathamTadikonda will add his code for collections/permissions/etc.", "author": "rjrudin", "createdAt": "2020-09-29T22:11:55Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +116,52 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).set(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\"));\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", params.get(\"uriprefix\"));\n+            ((ObjectNode)endpointParamsJsonNode).set(\"workUnit\", workUnitNode);\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\");", "originalCommit": "660d87d7616098274a5b97ff3d2005b0a07461fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MjQyMw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497092423", "bodyText": "I'll test this locally, but I didn't think \"asText\" does what we want here - it's just getting the text of this node, it's not the same as toString. I think we want to use JacksonHandle here instead, but I'm not sure.\nWhich makes me realize - we really need a test that uses a custom API path. That way, we know for sure that our custom workUnit and endpointState are passed to our custom endpoint correctly.\nLet me handle that test - I want to think a bit about where the modules for this should go. I think they can go under ./marklogic-data-hub/src/test/ml-modules, I just need to confirm that. I'll try to submit that tonight or first thing tomorrow morning.\nIn the meantime though, this can go forward so that @SameeraPriyathamTadikonda  can make use of it.", "author": "rjrudin", "createdAt": "2020-09-29T22:16:19Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -45,24 +48,23 @@\n     /**\n      *\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n      * @param params contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        JsonNode endpointParamsJsonNode = determineIngestionEndpointParams(params);\n+\n         this.loader = InputEndpoint.on(\n             hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n+            hubClient.getModulesClient().newJSONDocumentManager().read(endpointParamsJsonNode.get(\"apiPath\").asText(), new StringHandle())\n         ).bulkCaller();\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+        this.loader.setEndpointState(new ByteArrayInputStream((endpointParamsJsonNode.get(\"endpointState\").asText()).getBytes()));", "originalCommit": "660d87d7616098274a5b97ff3d2005b0a07461fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497111057", "bodyText": "what's the purpose of userDefinedValue?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-29T22:52:00Z", "path": "marklogic-data-hub/src/test/java/com/marklogic/hub/dataservices/ingestion/BulkIngestTest.java", "diffHunk": "@@ -32,8 +32,8 @@ public void setupTest() {\n     public void testBulkIngest() {\n \n         String prefix = \"/bulkIngesterTest\";\n-        String endpointState = \"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";\n-        String workUnit      = \"{\\\"taskId\\\":\"+1+\"}\";\n+        String endpointState =  \"{}\";\n+        String workUnit      = \"{\\\"userDefinedValue\\\":\" + 1 + \", \\\"uriprefix\\\":\\\"\"+prefix+\"\\\"}\";", "originalCommit": "660d87d7616098274a5b97ff3d2005b0a07461fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyNzAxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497127017", "bodyText": "\"userDefinedValue\" is just a variable added by the user. We can also have something like the below -\nString workUnit      = \"{\"testValue\":\" + 1 + \", \"uriprefix\":\"\"+prefix+\"\"}\";\nAdding the \"userDefinedValue\" confirms that no exception is thrown even when we have extra values defined by the user.", "author": "anu3990", "createdAt": "2020-09-29T23:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzNzI1Nw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497137257", "bodyText": "How will the user pass this option from glue/spark? I'm still confused where are we using this?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-29T23:27:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzE1NDAxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497154017", "bodyText": "This test is not in the marklogic-data-hub-spark-connector folder. It is testing only the bulk caller in the java-client-api and uses the bulkIngester api and sjs. The uriPrefix appended in the workUnit is used by the sjs file here - https://github.com/anu3990/marklogic-data-hub/blob/660d87d7616098274a5b97ff3d2005b0a07461fb/marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs#L39\nIn the Glue environment, uriprefix will be passed in as an option(i have defined in the next comment) and appended in the workUnit by the HubDataWriter class before calling bulkIngester.api.", "author": "anu3990", "createdAt": "2020-09-29T23:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzE2NTIyOQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497165229", "bodyText": "I understand that. I'm confused on the userDefinedValue. How it is passed and what's the use of it?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-30T00:07:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NjUzNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497996536", "bodyText": "We don't need the UserDefinedValue here.", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-01T05:47:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMTA1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497112549", "bodyText": "Do we need to add this even if user did not add the option URIPrefix?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-29T22:54:05Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +116,52 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        String ingestEndpointParams = params.containsKey(\"ingestendpointparams\")?\n+            params.get(\"ingestendpointparams\"):objectMapper.createObjectNode().toString();\n+\n+        JsonNode endpointParamsJsonNode;\n+        try {\n+            endpointParamsJsonNode = objectMapper.readValue(ingestEndpointParams, JsonNode.class);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+        }\n+\n+        boolean doesNotHaveApiPath = !endpointParamsJsonNode.has(\"apiPath\");\n+        boolean hasWorkUnitOrEndpointState = endpointParamsJsonNode.has(\"workUnit\") || endpointParamsJsonNode.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        final String apiModulePath = (endpointParamsJsonNode.hasNonNull(\"apiPath\") &&\n+            endpointParamsJsonNode.get(\"apiPath\").asText().length() > 0) ?\n+            endpointParamsJsonNode.get(\"apiPath\").asText() : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        ((ObjectNode)endpointParamsJsonNode).put(\"apiPath\", apiModulePath);\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"endpointState\") && endpointParamsJsonNode.get(\"endpointState\").asText().length() > 0) {\n+            ((ObjectNode)endpointParamsJsonNode).set(\"endpointState\", endpointParamsJsonNode.get(\"endpointState\"));\n+        }\n+        // TODO : remove the below else block after java-client-api 5.3 release\n+        else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"endpointState\", \"{}\");\n+        }\n+\n+        if (endpointParamsJsonNode.hasNonNull(\"workUnit\") && endpointParamsJsonNode.get(\"workUnit\").asText().length() > 0) {\n+\n+            JsonNode workUnitNode;\n+            try {\n+                workUnitNode = objectMapper.readValue(endpointParamsJsonNode.get(\"workUnit\").asText(), JsonNode.class);\n+            } catch (JsonProcessingException e) {\n+                throw new RuntimeException(\"Unable to parse workUnit, cause: \" + e.getMessage(), e);\n+            }\n+            ((ObjectNode) workUnitNode).put(\"uriprefix\", params.get(\"uriprefix\"));\n+            ((ObjectNode)endpointParamsJsonNode).set(\"workUnit\", workUnitNode);\n+        } else {\n+            ((ObjectNode)endpointParamsJsonNode).put(\"workUnit\", \"{\\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\");", "originalCommit": "660d87d7616098274a5b97ff3d2005b0a07461fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyNzEzMQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497127131", "bodyText": "Yes we need this even when the user did not add URIPrefix because our sjs file looks for this value. Since, the uriPrefix value does not change, we moved it out from endpointState to workUnit", "author": "anu3990", "createdAt": "2020-09-29T23:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzNjMwMQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497136301", "bodyText": "Are we going to add null in that case?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-29T23:26:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzE1Mzk4MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497153980", "bodyText": "No we will not be adding null to the uris. Our Glue script will be something like this -\nval prefix = \"\"\nval writer = dataframe.write.format(\"com.marklogic.client.spark.Writer.MarkLogicWriteDataSource\")\n.option(\"host\", host)\n.option(\"port\", port)\n.option(\"user\", username)\n.option(\"password\",password)\n.option(\"uriprefix\", prefix).....(other options);\nBy default the value of variable prefix is an empty string. The user can add a custom value.", "author": "anu3990", "createdAt": "2020-09-29T23:51:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzE2MTIzNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497161234", "bodyText": "But I don't think Ernie has to specify '.option(\"uriprefix\", prefix)', right? In which case the option will not exist in the options map? And so we'd only want to add it to the workUnit if it's not null (unless the endpoint is careful to not use it if it's null).", "author": "rjrudin", "createdAt": "2020-09-30T00:02:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzE2NDM4MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497164380", "bodyText": "Yes, that's exactly my point. I don't think that is handled in the endpoint.\nAlso the endpoint throws an exception if workUnit is empty when no options are set by the user.", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-09-30T00:06:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzExMjU0OQ=="}], "type": "inlineReview"}, {"oid": "d26929ecc037f084836d4c103d8415ba558ce840", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d26929ecc037f084836d4c103d8415ba558ce840", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-30T16:39:14Z", "type": "forcePushed"}, {"oid": "c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "url": "https://github.com/marklogic/marklogic-data-hub/commit/c3b1ffc5f65dfd94d5999748db28c0a386e5cdf6", "message": "DHFPROD-6027: Add intro text to the top of each tile\n\nIncludes tests of intro text display.", "committedDate": "2020-09-30T18:58:38Z", "type": "forcePushed"}, {"oid": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "url": "https://github.com/marklogic/marklogic-data-hub/commit/a444f2452ba4d9ef24707d03ddf575cf91f84935", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-09-30T22:37:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mjk2Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497992963", "bodyText": "endpointParams.get(\"apiPath\").asText() will throw a NPE", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-01T05:34:13Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));", "originalCommit": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3NzU4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498177588", "bodyText": "An NPE won't occur because if apiPath were null or did not exist, then the first part of the conditional would be true and then Java won't evaluate the second part.", "author": "rjrudin", "createdAt": "2020-10-01T11:38:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mjk2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5MzY2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497993665", "bodyText": "we can use the variable \"doesNotHaveApiPath\" instead of writing the logic again", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-01T05:36:36Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasWorkUnitOrEndpointState = endpointParams.has(\"workUnit\") || endpointParams.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        if (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText())) {", "originalCommit": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mzg4NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497993885", "bodyText": "else block missing", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-01T05:37:23Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -114,4 +121,41 @@ private String convertRowToJSONString(InternalRow record) {\n         return jsonObjectWriter.toString();\n     }\n \n+    protected JsonNode determineIngestionEndpointParams(Map<String, String> params) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointParams;\n+        if (params.containsKey(\"ingestendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode)objectMapper.readTree(params.get(\"ingestendpointparams\"));\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasWorkUnitOrEndpointState = endpointParams.has(\"workUnit\") || endpointParams.has(\"endpointState\");\n+        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n+            throw new RuntimeException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n+        }\n+\n+        if (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText())) {\n+            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+        }\n+\n+        // TODO : remove the below else block after java-client-api 5.3 release", "originalCommit": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3Nzk5MQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498177991", "bodyText": "There's no else needed - this is just saying \"If Ernie didn't provide a non-null endpointState, then toss in an empty object node\".", "author": "rjrudin", "createdAt": "2020-10-01T11:39:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5Mzg4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NzAyNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r497997027", "bodyText": "provide a meaningful name for this test", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-01T05:48:59Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataViaCustomEndpointTest.java", "diffHunk": "@@ -0,0 +1,67 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.FileHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.jupiter.api.Test;\n+import org.springframework.core.io.ClassPathResource;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataViaCustomEndpointTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void test() throws IOException {", "originalCommit": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4MTI2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498181265", "bodyText": "This is actually a common pattern we have in existing tests - since the class name already identifies the scope of the test, the method is just \"test\". If a second test method is added, then this would need to be updated.", "author": "rjrudin", "createdAt": "2020-10-01T11:45:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NzAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4MTQ3Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4630#discussion_r498181476", "bodyText": "I think you can drop this test because WriteDataViaCustomEndpointTest already covers this with an actual custom endpoint, as opposed to using the default endpoint.", "author": "rjrudin", "createdAt": "2020-10-01T11:46:17Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -68,45 +52,68 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n         assertFalse(uriQueryResult.hasNext());\n     }\n \n-    /**\n-     * Spark will do all of this in the real world - i.e. a user will specify the entry class and the set of options.\n-     * But in a test, we need to do that ourselves. So we create the DataSource class, build up the params, and then\n-     * call the factory/writer methods ourselves.\n-     *\n-     * @param batchSize\n-     * @param uriPrefix\n-     * @return\n-     */\n-    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix) {\n-        HubDataSource dataSource = new HubDataSource();\n-        final String writeUUID = \"doesntMatter\";\n-        final SaveMode saveModeDoesntMatter = SaveMode.Overwrite;\n-\n-        // Get the set of DHF properties used to connect to ML as a map, and then add connector-specific params\n-        Map<String, String> params = getHubPropertiesAsMap();\n-        params.put(\"batchsize\", batchSize);\n-\n-        if(uriPrefix!=null && uriPrefix.length()!=0) {\n-            params.put(\"uriprefix\", uriPrefix);\n-        }\n+    @Test\n+    public void ingestWithoutCustomApiWithCustomWorkunit(){\n+        ObjectNode customWorkUnit = objectMapper.createObjectNode();\n+        customWorkUnit.put(\"userDefinedValue\", 0);\n+\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestWorkUnit(customWorkUnit)),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        assertEquals(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\", ex.getMessage());\n+    }\n+\n+    @Test\n+    public void ingestWithIncorrectApi(){\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestApiPath(\"/incorrect.api\")),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        System.out.println(ex.getMessage());\n+        assertTrue( ex.getMessage().contains(\"Could not read non-existent document.\"));\n+    }\n+\n+    @Test\n+    public void ingestWithCustomApiWithCustomWorkunit() throws IOException {", "originalCommit": "a444f2452ba4d9ef24707d03ddf575cf91f84935", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "url": "https://github.com/marklogic/marklogic-data-hub/commit/fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-10-01T16:16:32Z", "type": "commit"}, {"oid": "fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "url": "https://github.com/marklogic/marklogic-data-hub/commit/fe41fcb5826b4f0fb1a6fe5b6aaef4bdc1cd8182", "message": "DHFPROD-5989 : Configure workUnit and endpointState in Spark connector (https://project.marklogic.com/jira/browse/DHFPROD-5989)", "committedDate": "2020-10-01T16:16:32Z", "type": "forcePushed"}]}