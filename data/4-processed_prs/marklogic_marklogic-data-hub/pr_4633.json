{"pr_number": 4633, "pr_title": "DHFPROD-5944: Configure Document Ingestion via connector", "pr_createdAt": "2020-09-28T20:10:33Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4633", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNTg2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496205865", "bodyText": "I don't think it's safe to write all of the params as a JSON object, as that could contain the user password and other sensitive information.\nWe should be much more precise about what goes into workUnit. So we should have a method of e.g. \"ObjectNode buildWorkUnitFromParams(Map)\" that builds an ObjectNode based on the params we know may exist in the Map, and nothing more than that.", "author": "rjrudin", "createdAt": "2020-09-28T20:14:12Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -61,7 +62,15 @@ public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<St\n             hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n         ).bulkCaller();\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n+        String workUnit = \"{\\\"taskId\\\":\" + taskId + \"}\";\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            workUnit = workUnit.concat(objectMapper.writeValueAsString(params));", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNjM3OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496206378", "bodyText": "See my comment to @anu3990  about an Options class that will simplify writing tests, as opposed to building a Map. We will likely have many tests that populate different sets of options. The Options class will handle that for us.", "author": "rjrudin", "createdAt": "2020-09-28T20:15:14Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -86,9 +86,13 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n         Map<String, String> params = getHubPropertiesAsMap();\n         params.put(\"batchsize\", batchSize);\n \n-        if(uriPrefix!=null && uriPrefix.length()!=0) {\n+        if (uriPrefix != null && uriPrefix.length() != 0) {\n             params.put(\"uriprefix\", uriPrefix);\n         }\n+        params.put(\"collections\", \"fruits,gluefruits\");", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNjY2Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496206663", "bodyText": "No need for xdmp.log here, the stacktrace from the fn.error will be visible in the logs", "author": "rjrudin", "createdAt": "2020-09-28T20:15:48Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -29,6 +29,29 @@ const consts = require('/data-hub/5/impl/consts.sjs');\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n+let collections=[];\n+if(work.collections !=null){\n+  collections=work.collections.split(',');\n+}\n+\n+const permissionsArray = []\n+\n+if(!work.permissions){\n+  work.permissions='data-hub-common,read,data-hub-common,update'\n+}\n+\n+const permissions = work.permissions.split(',');\n+\n+if((permissions.length%2)!= 0){\n+  xdmp.log(\"Error: Unable to parse Permissions\")", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNjg5Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496206896", "bodyText": "Use ds-utils.sjs - that has helper functions in it to simplify this - e.g. throwBadRequest would be appropriate here.", "author": "rjrudin", "createdAt": "2020-09-28T20:16:21Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -29,6 +29,29 @@ const consts = require('/data-hub/5/impl/consts.sjs');\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n+let collections=[];\n+if(work.collections !=null){\n+  collections=work.collections.split(',');\n+}\n+\n+const permissionsArray = []\n+\n+if(!work.permissions){\n+  work.permissions='data-hub-common,read,data-hub-common,update'\n+}\n+\n+const permissions = work.permissions.split(',');\n+\n+if((permissions.length%2)!= 0){\n+  xdmp.log(\"Error: Unable to parse Permissions\")\n+  fn.error(null, \"RESTAPI-SRVEXERR\", Sequence.from([400, \"Illegal Argument Exception\", \"Unable to parse permissions string, which must be a comma-separated list of role names and capabilities - i.e. role1,read,role2,update,role3,execute; string:\" + permissions]));", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNzE0NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496207145", "bodyText": "Use hub-utils.parsePermissions", "author": "rjrudin", "createdAt": "2020-09-28T20:16:54Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -29,6 +29,29 @@ const consts = require('/data-hub/5/impl/consts.sjs');\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n+let collections=[];\n+if(work.collections !=null){\n+  collections=work.collections.split(',');\n+}\n+\n+const permissionsArray = []\n+\n+if(!work.permissions){\n+  work.permissions='data-hub-common,read,data-hub-common,update'\n+}\n+\n+const permissions = work.permissions.split(',');\n+\n+if((permissions.length%2)!= 0){\n+  xdmp.log(\"Error: Unable to parse Permissions\")\n+  fn.error(null, \"RESTAPI-SRVEXERR\", Sequence.from([400, \"Illegal Argument Exception\", \"Unable to parse permissions string, which must be a comma-separated list of role names and capabilities - i.e. role1,read,role2,update,role3,execute; string:\" + permissions]));\n+}\n+\n+for(let i=0;i<permissions.length;i+=2){", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNzQ3Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496207473", "bodyText": "We only want to add a source object if at least one of these is set. So you should add a test that verifies that when neither sourcename nor sourcetype is provided, then the sources array does not exist.", "author": "rjrudin", "createdAt": "2020-09-28T20:17:35Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -38,16 +61,23 @@ inputs.forEach(record => {\n   state.next = state.next + 1;\n   const uri = (state.uriprefix) +  sem.uuidString() + '.json';\n   record = ingest.main({uri: uri, value: record}, {\n-    outputFormat: consts.JSON, headers: {createdOn: consts.CURRENT_DATE_TIME, createdBy: consts.CURRENT_USER}\n+    outputFormat: consts.JSON,\n+                  headers: {\n+                      createdOn: consts.CURRENT_DATE_TIME,\n+                      createdBy: consts.CURRENT_USER,\n+                      sources: [\n+                        { name: work.sourcename,", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNzk4NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496207984", "bodyText": "We need a test verifying that it's okay if permissions is not, then the documents will be added with no permissions. You'll need to use \"runAsAdmin\" after ingesting the data to verify that the documents exist.\nWe can't force Ernie to specify permissions, he's free to insert documents without any permissions.", "author": "rjrudin", "createdAt": "2020-09-28T20:18:41Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -29,6 +29,29 @@ const consts = require('/data-hub/5/impl/consts.sjs');\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n+let collections=[];\n+if(work.collections !=null){\n+  collections=work.collections.split(',');\n+}\n+\n+const permissionsArray = []\n+\n+if(!work.permissions){\n+  work.permissions='data-hub-common,read,data-hub-common,update'\n+}\n+\n+const permissions = work.permissions.split(',');", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "url": "https://github.com/marklogic/marklogic-data-hub/commit/6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T18:50:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016284", "bodyText": "uriprefix should go into buildWorkUnitFromParams. Also, I'd just call that \"buildDefaultWorkUnit\" - it's clear that it's built from params because that's the sole argument.", "author": "rjrudin", "createdAt": "2020-10-02T19:40:58Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMjk3Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499032976", "bodyText": "Added uriprefix to buildDefaultWorkUnit", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016973", "bodyText": "A more succinct approach with Jackson is to use \"has\":\nif (params.has(\"collections\"))\n\nAlso, all 4 conditionals are the same, so you could simplify this like so:\nStream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n  if (params.containsKey(key)) {\n    defaultWorkUnit.put(key, params.get(key));\n  }\n}\n\nThe one downside is we have to use the lower-casing that Spark requires. I think we can live with that for now though.", "author": "rjrudin", "createdAt": "2020-10-02T19:42:41Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");\n+            buildWorkUnitFromParams(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n \n         return endpointParams;\n     }\n+\n+    protected void buildWorkUnitFromParams(Map<String, String> params) {\n+        if (params.get(\"collections\") != null) {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzA1NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033054", "bodyText": "Updated the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499018126", "bodyText": "Put \"Test\" as a suffix on this class name, that's our convention for every test class.", "author": "rjrudin", "createdAt": "2020-10-02T19:45:31Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptions extends AbstractSparkConnectorTest {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzE3NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033175", "bodyText": "Renamed the class", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:22:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg=="}], "type": "inlineReview"}, {"oid": "410088ee47dbd98710c389893448d1f1b40e33bb", "url": "https://github.com/marklogic/marklogic-data-hub/commit/410088ee47dbd98710c389893448d1f1b40e33bb", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T20:19:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637417", "bodyText": "The endpoint should do this, not the connector. The endpoint should say - if uriprefix is in the workUnit, then I'll use that, even if it's null. It's up to the client to pass in the correct value for uriprefix - if the client passes in \"uriprefix\": null, then the endpoint says - Well I guess you want null as the prefix, so I'll use that.", "author": "rjrudin", "createdAt": "2020-10-05T14:24:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,22 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            if (params.get(\"uriprefix\") == null) {", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDM5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804390", "bodyText": "Made the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637835", "bodyText": "This test should verify that two collections work, so e.g. \"fruits,stuff\".", "author": "rjrudin", "createdAt": "2020-10-05T14:24:32Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDU5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804592", "bodyText": "Addressed via https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499643716", "bodyText": "Let's not specify a uriprefix here, as the scope of this test is to verify the collections.\nSo instead of doing a uriMatch query, do a cts.uris(null, null, cts.collectionQuery('fruits')). And I recommend getting the value back as a string, and then splitting that into an array based on the newline symbol:\nString[] uris = getHubClient().getStagingClient().newServerEval().javascript(\"cts.uris(null, null, cts.collectionQuery('fruits'))\").evalAs(String.class).split(\"\\n\");\n        assertEquals(1, uris.length);\n        DocumentMetadataHandle metadata = getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n        assertEquals(2, metadata.getCollections().size());\n        assertTrue(metadata.getCollections().contains(\"fruits\"));\n        assertTrue(metadata.getCollections().contains(\"stuff\"));", "author": "rjrudin", "createdAt": "2020-10-05T14:32:12Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDY5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804690", "bodyText": "Same https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499654740", "bodyText": "There's a lot of duplication across these 4 tests - I'm going to submit a PR to yours to resolve that.", "author": "rjrudin", "createdAt": "2020-10-05T14:46:45Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n+\n+        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n+        String uri = uriQueryResult.next().getString();\n+\n+        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n+        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n+\n+        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+    }\n+\n+    @Test\n+    void ingestDocsWithSourceName() throws IOException {\n+        String sourceName = \"spark\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1OTI0Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499659242", "bodyText": "See https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "rjrudin", "createdAt": "2020-10-05T14:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NzEwNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499657106", "bodyText": "\"const\" is preferable to \"let\", as it makes it clear that the value will not be changing. So do this instead:\nconst collections = work.collections != null ? work.collections.split(',') : [];", "author": "rjrudin", "createdAt": "2020-10-05T14:49:48Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -25,11 +25,38 @@ var input;         // jsonDocument*\n declareUpdate();\n \n const ingest = require(\"/data-hub/5/builtins/steps/ingestion/default/main.sjs\");\n-const consts = require('/data-hub/5/impl/consts.sjs');\n+const consts = require(\"/data-hub/5/impl/consts.sjs\");\n+const HubUtils = require(\"/data-hub/5/impl/hub-utils.sjs\");\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n \n+let collections = [];", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDc5Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804793", "bodyText": "Made the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:55:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NzEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NzMxOA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499657318", "bodyText": "Same thing here - can adapt this to use \"const\" instead of \"let\".", "author": "rjrudin", "createdAt": "2020-10-05T14:50:05Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -25,11 +25,38 @@ var input;         // jsonDocument*\n declareUpdate();\n \n const ingest = require(\"/data-hub/5/builtins/steps/ingestion/default/main.sjs\");\n-const consts = require('/data-hub/5/impl/consts.sjs');\n+const consts = require(\"/data-hub/5/impl/consts.sjs\");\n+const HubUtils = require(\"/data-hub/5/impl/hub-utils.sjs\");\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n \n+let collections = [];\n+if(work.collections != null){\n+  collections = work.collections.split(',');\n+}\n+\n+let permissionsArray = []", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1ODAwOQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499658009", "bodyText": "Pinging @rahulvudutala  and @fsnow  and @ryanjdew  about this - this is the property name we're proposing for capturing the source type. Note that this is going on the develop branch. Nothing in DHF will care about it yet though.", "author": "rjrudin", "createdAt": "2020-10-05T14:50:59Z", "path": "marklogic-data-hub/src/main/resources/ml-modules/root/data-hub/5/data-services/ingestion/bulkIngester.sjs", "diffHunk": "@@ -25,11 +25,38 @@ var input;         // jsonDocument*\n declareUpdate();\n \n const ingest = require(\"/data-hub/5/builtins/steps/ingestion/default/main.sjs\");\n-const consts = require('/data-hub/5/impl/consts.sjs');\n+const consts = require(\"/data-hub/5/impl/consts.sjs\");\n+const HubUtils = require(\"/data-hub/5/impl/hub-utils.sjs\");\n const state = fn.head(xdmp.fromJSON(endpointState));\n \n const work = fn.head(xdmp.fromJSON(workUnit));\n \n+let collections = [];\n+if(work.collections != null){\n+  collections = work.collections.split(',');\n+}\n+\n+let permissionsArray = []\n+\n+if(work.permissions == null){\n+  work.permissions = 'data-hub-common,read,data-hub-common,update'\n+}\n+\n+permissionsArray = new HubUtils().parsePermissions(work.permissions);\n+\n+const headers = {};\n+headers.createdOn = consts.CURRENT_DATE_TIME\n+headers.createdBy = consts.CURRENT_USER\n+\n+if(work.sourcename != null || work.sourcetype != null){\n+  const sources = [];\n+  const source = {};\n+  source.name = work.sourcename\n+  source.datahubSourceType = work.sourcetype", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "commit"}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "forcePushed"}]}