{"pr_number": 4755, "pr_title": "DHFPROD-6142:Add test program for streaming data into ML", "pr_createdAt": "2020-10-23T07:01:30Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4755", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzQ2MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853460", "bodyText": "Nice - I was wondering how to do this!", "author": "rjrudin", "createdAt": "2020-10-23T12:37:35Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/SparkTest.java", "diffHunk": "@@ -52,7 +52,7 @@ private static void setConnectionProperties(String[] args) {\n         SQLContext sqlContext = new SQLContext(javaSparkContext);\n         final String filePath = getTestFilePath();\n         logger.info(\"Loading from file: \" + filePath);\n-        Dataset<org.apache.spark.sql.Row> rows = sqlContext.load(filePath, \"csv\");\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.read().option(\"header\", true).csv(getTestFilePath());", "originalCommit": "9cfb1618188ca3edf15849610a64236119702bc8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510853819", "bodyText": "Do you know if this line is needed? I added it here when I was hacking around, but I don't know if it's needed. We want this to be as slim as possible, we want the bare minimum amount of code to enable streaming.", "author": "rjrudin", "createdAt": "2020-10-23T12:38:15Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");", "originalCommit": "9cfb1618188ca3edf15849610a64236119702bc8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2MDg3OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510960879", "bodyText": "Not sure about that Rob. However i did notice that SQLContext(inside loadRowsFromTestFile) is deprecated, so we may want to shift towards SparkSession.", "author": "anu3990", "createdAt": "2020-10-23T15:24:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY1ODE3Nw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511658177", "bodyText": "Changed the code to use SparkSession.", "author": "anu3990", "createdAt": "2020-10-25T22:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1MzgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDA4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854088", "bodyText": "Just a note, this duplication is totally fine right now. I'll submit a PR later to refactor it.", "author": "rjrudin", "createdAt": "2020-10-23T12:38:44Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {", "originalCommit": "9cfb1618188ca3edf15849610a64236119702bc8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDQ0Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510854446", "bodyText": "Nice - so the key is to read the header first and determine the schema from it, and then pass the schema in when calling readStream?", "author": "rjrudin", "createdAt": "2020-10-23T12:39:20Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating JavaSparkContext\");\n+\n+        SparkConf sparkConf = new SparkConf()\n+            .setAppName(\"TestAppName\")\n+            .setMaster(\"local[1]\")\n+            .set(\"spark.driver.allowMultipleContexts\", \"true\");\n+\n+        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(1));\n+        JavaSparkContext javaSparkContext = streamingContext.sparkContext();\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(javaSparkContext);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing JavaSparkContext\");\n+            javaSparkContext.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(JavaSparkContext javaSparkContext) {\n+        SQLContext sqlContext = new SQLContext(javaSparkContext);\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sqlContext.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sqlContext.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");", "originalCommit": "9cfb1618188ca3edf15849610a64236119702bc8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2MTcwNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510961706", "bodyText": "Yes i believe so.", "author": "anu3990", "createdAt": "2020-10-23T15:25:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDg1NDQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk2ODgyNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r510968827", "bodyText": "Is there any reason why  \"test-data-hub-operator\" isn't used here (like SparkTest) ?", "author": "srinathgit", "createdAt": "2020-10-23T15:33:56Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(SparkTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"pari\";", "originalCommit": "9cfb1618188ca3edf15849610a64236119702bc8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "751a217e26213c385857370ba445b1ba638af48f", "url": "https://github.com/marklogic/marklogic-data-hub/commit/751a217e26213c385857370ba445b1ba638af48f", "message": "DHFPROD-6142:Add test program for streaming data into ML", "committedDate": "2020-10-25T22:33:59Z", "type": "forcePushed"}, {"oid": "3482fa0b427d7aace4f2052ea8e386726e783c0d", "url": "https://github.com/marklogic/marklogic-data-hub/commit/3482fa0b427d7aace4f2052ea8e386726e783c0d", "message": "DHFPROD-6142:Add test program for streaming data into ML", "committedDate": "2020-10-25T22:37:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMDg5Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511930896", "bodyText": "At least for this test program, I think we need to pass in e.g. 3000l here to force a timeout. Our connector doesn't otherwise know when to stop.", "author": "rjrudin", "createdAt": "2020-10-26T12:43:57Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);\n+            writeRowsToDataHub(rows);\n+        } finally {\n+            logger.info(\"Closing SparkSession\");\n+            sparkSession.close();\n+        }\n+    }\n+\n+    private static void setConnectionProperties(String[] args) {\n+        if (args.length != 3) {\n+            logger.info(\"Defaulting to host=localhost and username=test-data-hub-operator\");\n+            username=\"test-data-hub-operator\";\n+        } else {\n+            host = args[0];\n+            username = args[1];\n+            password = args[2];\n+            logger.info(String.format(\"Will write to '%s' as user '%s'\", host, username));\n+        }\n+    }\n+\n+    private static Dataset<Row> loadRowsFromTestFile(SparkSession sparkSession) {\n+        final String filePath = getTestFilePath();\n+        logger.info(\"Loading from file: \" + filePath);\n+        StructType structType = sparkSession.read().option(\"header\", true).csv(getTestFilePath()).schema();\n+        logger.info(\"Schema of the input file is \"+structType);\n+        Dataset<org.apache.spark.sql.Row> rows = sparkSession.readStream().schema(structType).format(\"csv\").option(\"header\", true).csv(\"src/main/resources/data\");\n+\n+        return rows;\n+    }\n+\n+    /**\n+     * Depending on how this program is run - e.g. via Gradle or an IDE - the path will resolve to either this project\n+     * directory or the root DHF project directory. So gotta support both.\n+     *\n+     * @return\n+     */\n+    private static String getTestFilePath() {\n+        String filePath = \"src/main/resources/data/databook.csv\";\n+        if (new File(filePath).exists()) {\n+            return new File(filePath).getAbsolutePath();\n+        }\n+        return new File(\"marklogic-data-hub-spark-connector/spark-test-project/\" + filePath).getAbsolutePath();\n+    }\n+\n+    /**\n+     * Customize the options in here as needed for ad hoc testing.\n+     *\n+     * @param rows\n+     */\n+    private static void writeRowsToDataHub(Dataset<Row> rows) throws StreamingQueryException {\n+        rows.writeStream()\n+            .format(\"com.marklogic.hub.spark.sql.sources.v2\")\n+            .option(\"mlHost\", host)\n+            .option(\"mlUsername\", username)\n+            .option(\"mlPassword\", password)\n+            .option(\"collections\", \"sparkTestOne,sparkTestTwo\")\n+            .option(\"hubDhs\", \"false\")\n+            .option(\"hubSsl\", \"false\")\n+            .option(\"batchSize\", \"3\")\n+            .option(\"checkpointLocation\", \"src/main/resources/logs\")\n+            .start()\n+            .awaitTermination();", "originalCommit": "3482fa0b427d7aace4f2052ea8e386726e783c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzMTEyMw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4755#discussion_r511931123", "bodyText": "Can you remove this line from both test programs? I think I included it via copy/paste, but it doesn't seem necessary, and we want as few lines of code here as possible.", "author": "rjrudin", "createdAt": "2020-10-26T12:44:21Z", "path": "marklogic-data-hub-spark-connector/spark-test-project/src/main/java/test/StreamTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+package test;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.streaming.StreamingQueryException;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+\n+public class StreamTest {\n+\n+    private static Logger logger = LoggerFactory.getLogger(StreamTest.class);\n+\n+    private static String host = \"localhost\";\n+    private static String username = \"test-data-hub-operator\";\n+    private static String password = \"password\";\n+\n+    public static void main(String[] args) throws Exception {\n+        setConnectionProperties(args);\n+\n+        logger.info(\"Creating SparkSession\");\n+\n+        SparkSession sparkSession = SparkSession.builder()\n+            .master(\"local\")\n+            .getOrCreate();\n+\n+\n+        try {\n+            Dataset<Row> rows = loadRowsFromTestFile(sparkSession);\n+            rows.join(rows);", "originalCommit": "3482fa0b427d7aace4f2052ea8e386726e783c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "69759a8823244ddb82c2996ed81dbb1a4d603809", "url": "https://github.com/marklogic/marklogic-data-hub/commit/69759a8823244ddb82c2996ed81dbb1a4d603809", "message": "DHFPROD-6142:Add test program for streaming data into ML", "committedDate": "2020-10-27T17:48:43Z", "type": "forcePushed"}, {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "url": "https://github.com/marklogic/marklogic-data-hub/commit/ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "message": "DHFPROD-6142:Add test program for streaming data into ML.", "committedDate": "2020-10-27T19:24:16Z", "type": "commit"}, {"oid": "ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "url": "https://github.com/marklogic/marklogic-data-hub/commit/ead58fdff0e11bdf9fdc3bc2a5a31a40afa2dda7", "message": "DHFPROD-6142:Add test program for streaming data into ML.", "committedDate": "2020-10-27T19:24:16Z", "type": "forcePushed"}]}