{"pr_number": 389, "pr_title": "Avro: add compression", "pr_createdAt": "2020-06-19T01:55:21Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/389", "timeline": [{"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "url": "https://github.com/greenplum-db/pxf/commit/5f8ed4f1eeb016af64d603c84c31dc408918f515", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). For XZ and BZIP2 we cannot currently\nsupport as it requires adding the Apache Commons Compression jar[0].\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-19T02:34:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDM4OA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600388", "bodyText": "not sure what this does, but we don't want to write to stderr from the app", "author": "denalex", "createdAt": "2020-06-19T02:38:15Z", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);\n+\n+        try (PrintStream printStream = new PrintStream(new FileOutputStream(new File(pathToMetadata)))) {\n+            tool.run(null, printStream, System.err, args);", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NTcxNw==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443055717", "bodyText": "Thanks for the review. This is used in automation test for Writable Avro tables, in which we write the contents of PXF-generated Avro data/metadata into files to make sure that what ends up in external is what we think (avoids potential garbage in/garbage out problem when testing with writable/readable tables).\nThe API for these Avro tools takes a stream for stdout and stderr, but mostly we care about stdout in this case which will go to a file.", "author": "oliverralbertini", "createdAt": "2020-06-19T21:47:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDM4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDU1NQ==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600555", "bodyText": "is Tool thread safe and stateless ? If yes, we should be using a single (static) instance of it", "author": "denalex", "createdAt": "2020-06-19T02:39:02Z", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjA4Nw==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056087", "bodyText": "Since it's not being used in the app, we can ignore that.", "author": "oliverralbertini", "createdAt": "2020-06-19T21:48:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDU1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDkwNg==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600906", "bodyText": "List<String> args = Arrays.asList(pathToFile)", "author": "denalex", "createdAt": "2020-06-19T02:40:33Z", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjY3NQ==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056675", "bodyText": "with editor help, ended up with\nList<String> args = Collections.singletonList(pathToFile);", "author": "oliverralbertini", "createdAt": "2020-06-19T21:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDkwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTUyOA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601528", "bodyText": "what's the point, it will throw NPE in either case (before and after the change) ? This is mostly used for checking input params to the functions are not nulls.", "author": "denalex", "createdAt": "2020-06-19T02:43:03Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -60,7 +61,7 @@\n     public void beforeClass() throws Exception {\n         // path for storing data on HDFS (for processing by PXF)\n         hdfsPath = hdfs.getWorkingDirectory() + \"/writableAvro/\";\n-        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        String absolutePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"data\")).getPath();", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjE5Ng==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056196", "bodyText": "yeah, this was suggested by the editor for some reason.", "author": "oliverralbertini", "createdAt": "2020-06-19T21:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTg1Ng==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601856", "bodyText": "should we use the value uncompressed (like in other profiles, I believe) instead of null ?", "author": "denalex", "createdAt": "2020-06-19T02:44:26Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -96,12 +97,43 @@ public void generateSchemaPrimitive() throws Exception {\n         publicStage += \"generateSchemaPrimitive/\";\n         // fetch all the segment-generated avro files and make them into json records\n         // confirm that the lines generated by the segments match what we expect\n-        fetchAndVerifyAvroHcfsFiles(\"primitives.json\");\n+        fetchAndVerifyAvroHcfsFiles(\"primitives.json\", \"deflate\");\n \n         // check using GPDB readable external table that what went into HCFS is correct\n         runTincTest(\"pxf.features.hdfs.writable.avro.primitives_generate_schema.runTest\");\n     }\n \n+    @Test(groups = {\"features\", \"gpdb\", \"hcfs\", \"security\"})\n+    public void generateSchemaPrimitive_withNoCompression() throws Exception {\n+        gpdbTable = \"writable_avro_primitive_no_compression\";\n+        fullTestPath = hdfsPath + \"generate_schema_primitive_types_with_no_compression\";\n+        exTable = new WritableExternalTable(gpdbTable + \"_writable\", avroPrimitiveTableCols, fullTestPath, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_export\");\n+        exTable.setProfile(protocol.value() + \":avro\");\n+        exTable.setUserParameters(new String[]{\"COMPRESSION_CODEC=null\"});", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMjY0OQ==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442602649", "bodyText": "I suggest to make all these as constants", "author": "denalex", "createdAt": "2020-06-19T02:47:43Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzAwMQ==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603001", "bodyText": "the default case should handle all unsupported ones and throw exception, while no-compression case should be one of the explicit choices above", "author": "denalex", "createdAt": "2020-06-19T02:49:06Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n+                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n+                // break;\n+            case \"bzip2\":\n+                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n+                // writer.setCodec(CodecFactory.bzip2Codec());\n+                // break;\n+            case \"snappy\":\n+                writer.setCodec(CodecFactory.snappyCodec());\n+                break;\n+            default:", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603457", "bodyText": "I don't like going back to profile and relying on its name, let's think if this can be done in a better way", "author": "denalex", "createdAt": "2020-06-19T02:51:03Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTk4Mw==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845983", "bodyText": "yeah, this needs some more thought. But I don't see where else the information might come from, without accessing the file. I believe this happens at the beginning of the bridge call to determine if the access will need locking", "author": "frankgh", "createdAt": "2020-06-19T13:38:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2NDczNA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443064734", "bodyText": "Simplest approach is just to make sure that COMPRESSION_CODEC != \"bzip2\", this works regardless of the type of data (Avro, Parquet, or TEXT).", "author": "oliverralbertini", "createdAt": "2020-06-19T22:24:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzAxNg==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442843016", "bodyText": "why are these not supported? sometimes some compressions are supported only when you have hadoop native tools in your ld_library path, which is a likely scenario in customer environments", "author": "frankgh", "createdAt": "2020-06-19T13:33:24Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTcwMA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845700", "bodyText": "Let's avoid using regex. This will fail for legacy profiles ie Avro. This will also fail if the user creates the table as\ncreate external table avro ()\nlocation('pxf://foo?PROFILE=HDFS:aVrO')\nformat ....\n\nIt's not a case sensitive comparison", "author": "frankgh", "createdAt": "2020-06-19T13:38:13Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {\n+            return isParquetCompressionThreadSafe(compCodec);\n+        }\n+        if (context.getProfile().matches(\"^.*:avro$\")) {", "originalCommit": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5Njg0Ng==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096846", "bodyText": "I am not sure this logic is equivalent (or a superset) or what was removed, but it might as well be. Actually, I prefer the way comparison was done before: BZip2Codec.class.isAssignableFrom(..) instead of comparing class names. There was also getting Hadoop mapping compressionCodecName.getHadoopCompressionCodecClass() which is no longer done here.", "author": "denalex", "createdAt": "2020-06-20T03:25:40Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        // bzip2 for Avro, and BZip2Codec for Parquet\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(\"org.apache.hadoop.io.compress.BZip2Codec\");", "originalCommit": "5f0181f7d17d38d69ce10e362f5661585d5bf948", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE0MjMxNw==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443142317", "bodyText": "That brings us right back around to the Parquet vs Avro issue. How do we know which type of compression we are dealing with?", "author": "oliverralbertini", "createdAt": "2020-06-20T16:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5Njg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5NjkxMA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096910", "bodyText": "what if datasource points to directory full of bz2 files (and not an individual file), will this detect it ?", "author": "denalex", "createdAt": "2020-06-20T03:27:06Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());", "originalCommit": "5f0181f7d17d38d69ce10e362f5661585d5bf948", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE0MjE3MA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443142170", "bodyText": "Good catch, no it will not. This isn't an issue with my change, however, this was always the case. On the other hand, I think that the Accessor stage in PXF should only see files, not directories. If this were happening during fragmentation of the external resource, it would be more concerning.", "author": "oliverralbertini", "createdAt": "2020-06-20T16:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5NjkxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3NjAzNA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443076034", "bodyText": "should we call it NO_CODEC?", "author": "frankgh", "createdAt": "2020-06-19T23:23:31Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -142,26 +147,20 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n-        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, DEFLATE_CODEC).toLowerCase();\n         int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n         switch (codec) {\n-            case \"deflate\":\n+            case DEFLATE_CODEC:\n                 writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n                 break;\n-            case \"xz\":\n-                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n-                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n-                // break;\n-            case \"bzip2\":\n-                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n-                // writer.setCodec(CodecFactory.bzip2Codec());\n-                // break;\n-            case \"snappy\":\n+            case SNAPPY_CODEC:\n                 writer.setCodec(CodecFactory.snappyCodec());\n                 break;\n-            default:\n+            case NULL_CODEC:", "originalCommit": "5f0181f7d17d38d69ce10e362f5661585d5bf948", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "322f966be914b6d30c62a335e97cac4b20346aba", "url": "https://github.com/greenplum-db/pxf/commit/322f966be914b6d30c62a335e97cac4b20346aba", "message": "Make tests pass after rebasing master\n\n(smallint changes were merged to master)\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-22T19:01:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzOTUyMg==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446339522", "bodyText": "I was suggesting to collapse into 1 return statement, like this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                        return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n          \n          \n            \n                    }\n          \n          \n            \n                    return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());\n          \n          \n            \n                    Class<? extends CompressionCodec> codecClass = null;\n          \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                    }\n          \n          \n            \n                    return !( \"bzip2\".equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   BZip2Codec.class.getName().equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   (codecClass != null && BZip2Codec.class.isAssignableFrom(codecClass))\n          \n          \n            \n                                  );", "author": "denalex", "createdAt": "2020-06-26T18:17:42Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/CodecFactory.java", "diffHunk": "@@ -80,6 +81,24 @@ public CompressionCodec getCodec(String name, Configuration conf) {\n         return codecClass;\n     }\n \n+    /**\n+     * Determine whether a given compression codec is safe for multiple concurrent threads\n+     *\n+     * @param compCodec     the user-given COMPRESSION_CODEC, may be null\n+     * @param dataSource    the file that we are accessing\n+     * @param configuration HDFS config\n+     * @return true only if it's thread safe\n+     */\n+    public boolean isCodecThreadSafe(String compCodec, String dataSource, Configuration configuration) {\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Text only)\n+            // currently doesn't check for bzip2 in .avro files\n+            Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());", "originalCommit": "5e20e30348703dabfd80788ed668f1caac8558aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1NjExOA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446356118", "bodyText": "This looks equivalent, I will give it a try.", "author": "oliverralbertini", "createdAt": "2020-06-26T18:55:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzOTUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzA1Ng==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446343056", "bodyText": "should we have context.getCompressionCodec() method not to deal with strings here ? That method can also deal with short-name vs. fully qualified class (if needed).", "author": "denalex", "createdAt": "2020-06-26T18:25:45Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        return codecFactory.isCodecThreadSafe(context.getOption(\"COMPRESSION_CODEC\"), context.getDataSource(), configuration);", "originalCommit": "5e20e30348703dabfd80788ed668f1caac8558aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1MzMyNA==", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446353324", "bodyText": "This would be a great thing to tackle in #393 . That's where we start to allow short names in the text profiles.", "author": "oliverralbertini", "createdAt": "2020-06-26T18:48:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzA1Ng=="}], "type": "inlineReview"}, {"oid": "fcbe7d2be740e019407317417aa45b7b9615639d", "url": "https://github.com/greenplum-db/pxf/commit/fcbe7d2be740e019407317417aa45b7b9615639d", "message": "Push thread safe check down to CodecFactory class\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-26T18:59:30Z", "type": "forcePushed"}, {"oid": "cd506be15ad5f333981afe2add5f06745a0084a0", "url": "https://github.com/greenplum-db/pxf/commit/cd506be15ad5f333981afe2add5f06745a0084a0", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-26T22:09:02Z", "type": "forcePushed"}, {"oid": "bb3931557f185a157233378587bbc585a6b77c5c", "url": "https://github.com/greenplum-db/pxf/commit/bb3931557f185a157233378587bbc585a6b77c5c", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-26T22:11:04Z", "type": "forcePushed"}, {"oid": "749291163e8b7f42463dcc6ad61eaef6a8f40d98", "url": "https://github.com/greenplum-db/pxf/commit/749291163e8b7f42463dcc6ad61eaef6a8f40d98", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-27T01:16:09Z", "type": "commit"}, {"oid": "749291163e8b7f42463dcc6ad61eaef6a8f40d98", "url": "https://github.com/greenplum-db/pxf/commit/749291163e8b7f42463dcc6ad61eaef6a8f40d98", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>", "committedDate": "2020-06-27T01:16:09Z", "type": "forcePushed"}]}