{"pr_number": 483, "pr_title": "docs - Hive profile supports column projection and pushdown", "pr_createdAt": "2020-11-05T22:38:29Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/483", "timeline": [{"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077", "url": "https://github.com/greenplum-db/pxf/commit/6131acaccb2dc0a10917370a29df4c3ffb56c077", "message": "docs - Hive profile supports column projection and pushdown", "committedDate": "2020-11-05T21:51:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNjA0Nw==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518426047", "bodyText": "I think line should be merged with line 59,  but let's confirm with Alex.", "author": "frankgh", "createdAt": "2020-11-05T23:11:13Z", "path": "docs/content/filter_push.html.md.erb", "diffHunk": "@@ -57,8 +57,10 @@ PXF accesses data sources using profiles exposed by different connectors, and fi\n | s3:parquet and s3:text with S3-Select | Y |  N | Y | Y | Y | Y | Y |\n | HBase | Y | N | Y | N | Y | Y | N |\n | Hive | Y<sup>2</sup> | N | N | N | Y<sup>2</sup> | Y<sup>2</sup> | N |\n+| Hive (accessing stored as Parquet) | Y, Y<sup>2</sup> | N | N | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |\n+| Hive (accessing stored as RCFile or ORC) | Y, Y<sup>2</sup> | N | Y | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |", "originalCommit": "6131acaccb2dc0a10917370a29df4c3ffb56c077", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NjUxMA==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518466510", "bodyText": "this is tricky -- Hive itself does not make sense, the pushdown is different based on the file storage format for a table partition, each partition can have different pushdown eve when accessed by the same Hive profile. I'd remove Hive line altogether. Also HiveText does not support any pushdown (other than partition pruning). And I think VectorizedORC does not support pushdown at all.", "author": "denalex", "createdAt": "2020-11-06T01:16:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNjA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ3MzI0OA==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518473248", "bodyText": "I checked vectorized ORC and we don't do predicate pushdown, just partition pruning. We should be able to use the sarg for vectorized ORC.", "author": "frankgh", "createdAt": "2020-11-06T01:38:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNjA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNzkxMw==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518427913", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The data resides in a Hive table, and you do not know the type of data up front.\n          \n          \n            \n            - The data resides in a Hive table, and you do not know the underlying file type of the table up front.", "author": "frankgh", "createdAt": "2020-11-05T23:15:52Z", "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the type of data up front.", "originalCommit": "6131acaccb2dc0a10917370a29df4c3ffb56c077", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2", "url": "https://github.com/greenplum-db/pxf/commit/c9b07319da168e25cb1fa2722c664ea28e0417e2", "message": "underlying file type of the table", "committedDate": "2020-11-05T23:36:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzUzMQ==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518463531", "bodyText": "also for tables stored as text files", "author": "denalex", "createdAt": "2020-11-06T01:06:14Z", "path": "docs/content/col_project.html.md.erb", "diffHunk": "@@ -11,7 +11,7 @@ Column projection is automatically enabled for the `pxf` external table protocol\n | Data Source | Connector | Profile(s) |\n |-------------|---------------|---------|\n | External SQL database | JDBC Connector | Jdbc |\n-| Hive | Hive Connector | Hive, HiveRC, HiveORC, HiveVectorizedORC |\n+| Hive | Hive Connector | Hive (accessing tables stored as Parquet, RCFile, and ORC), HiveRC, HiveORC, HiveVectorizedORC |", "originalCommit": "c9b07319da168e25cb1fa2722c664ea28e0417e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgxOTk0OA==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518819948", "bodyText": "Mixing both of these in the intro sentences was a little confusing to me.  Doesn't the first bullet always go with hdfs:text and the second one with hdfss:parquet?  If so I'd just make each a separate paragraph \"Choose the hdfs:text profile when you know the file type and you know the location of the file in the HDFS file system.\" \"Choose the hdfs:parquet profile when the file e is Parquet, you know ..., \"", "author": "dyozie", "createdAt": "2020-11-06T15:20:15Z", "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the underlying file type of the table up front.\n+- The data resides in a Hive table, and the Hive table is partitioned.\n+\n+Choose the `hdfs:text` or `hdfs:parquet` profile when:", "originalCommit": "c9b07319da168e25cb1fa2722c664ea28e0417e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgyMTUxMg==", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518821512", "bodyText": "on a -> in a\n, or HBase or Hive table -> , HBase table, or Hive table", "author": "dyozie", "createdAt": "2020-11-06T15:22:28Z", "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the underlying file type of the table up front.\n+- The data resides in a Hive table, and the Hive table is partitioned.\n+\n+Choose the `hdfs:text` or `hdfs:parquet` profile when:\n+\n+- You know the file type, and you know the location of the file in the HDFS file system.\n+- The file is Parquet, you know the location of the file in the HDFS file system, and you want to take advantage of extended filter pushdown support for additional data types and operators.\n+\n+\n+### <a id=\"specify_profile\"></a>Specifying the Profile\n+\n+You must provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file or directory, or HBase or Hive table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text` to access the HDFS file `/data/pxf_examples/pxf_hdfs_simple.txt`:", "originalCommit": "c9b07319da168e25cb1fa2722c664ea28e0417e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3bcd7996b6312f782c9691b0118bd0628a894bef", "url": "https://github.com/greenplum-db/pxf/commit/3bcd7996b6312f782c9691b0118bd0628a894bef", "message": "Hive profile accessing text supports column projection", "committedDate": "2020-11-06T17:50:04Z", "type": "commit"}, {"oid": "c5a1d283b442704dd7f63e9456615b76ad979233", "url": "https://github.com/greenplum-db/pxf/commit/c5a1d283b442704dd7f63e9456615b76ad979233", "message": "edits requested by david", "committedDate": "2020-11-06T17:54:47Z", "type": "commit"}, {"oid": "e15a53914fc23b5dce5bcaa1c19db2a2e833ce10", "url": "https://github.com/greenplum-db/pxf/commit/e15a53914fc23b5dce5bcaa1c19db2a2e833ce10", "message": "(hopefully) correctly address comments on filter pushdown page", "committedDate": "2020-11-06T18:08:20Z", "type": "commit"}, {"oid": "1090e2b8ccc842d55fd944a8581597b3b3d22140", "url": "https://github.com/greenplum-db/pxf/commit/1090e2b8ccc842d55fd944a8581597b3b3d22140", "message": "use partition pruning terminology in topic heading", "committedDate": "2020-11-06T18:10:51Z", "type": "commit"}]}