{"pr_number": 480, "pr_title": "Enable predicate pushdown for Hive profile when accessing Parquet backed tables", "pr_createdAt": "2020-11-03T23:15:21Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/480", "timeline": [{"oid": "d7c9d0126bf60241871f298fdf33b0a4724a359b", "url": "https://github.com/greenplum-db/pxf/commit/d7c9d0126bf60241871f298fdf33b0a4724a359b", "message": "Enable predicate pushdown for Parquet files read via Hive profile", "committedDate": "2020-11-03T16:44:01Z", "type": "commit"}, {"oid": "4092992ecd0a9914445181647719ea43a800567f", "url": "https://github.com/greenplum-db/pxf/commit/4092992ecd0a9914445181647719ea43a800567f", "message": "moved all setup to openForRead()", "committedDate": "2020-11-03T16:44:02Z", "type": "commit"}, {"oid": "5f2991decb3a6a0852003d38f1d74f24ca44ab0d", "url": "https://github.com/greenplum-db/pxf/commit/5f2991decb3a6a0852003d38f1d74f24ca44ab0d", "message": "unit tests, type pruner, ppd property", "committedDate": "2020-11-03T17:01:59Z", "type": "commit"}, {"oid": "0cf0fd3479bdb9112f7bcaaef787c1b8f535a191", "url": "https://github.com/greenplum-db/pxf/commit/0cf0fd3479bdb9112f7bcaaef787c1b8f535a191", "message": "added unit test for pruner", "committedDate": "2020-11-03T17:01:59Z", "type": "commit"}, {"oid": "a810fb2539b6efc7e370c7e0e864167581fe1afe", "url": "https://github.com/greenplum-db/pxf/commit/a810fb2539b6efc7e370c7e0e864167581fe1afe", "message": "cleanup after rebase", "committedDate": "2020-11-03T18:43:44Z", "type": "commit"}, {"oid": "70fd6512a06c10df133070addfb0eb05f2d417ee", "url": "https://github.com/greenplum-db/pxf/commit/70fd6512a06c10df133070addfb0eb05f2d417ee", "message": "added HiveAccessor unit tests", "committedDate": "2020-11-03T23:13:53Z", "type": "commit"}, {"oid": "8901a0302a5f95c185e11cb861183832c3196b68", "url": "https://github.com/greenplum-db/pxf/commit/8901a0302a5f95c185e11cb861183832c3196b68", "message": "cleaned unused imports", "committedDate": "2020-11-03T23:18:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM2NDI0MQ==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517364241", "bodyText": "I realize I wrote this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            // AND need at least two children. If the operator has a\n          \n          \n            \n                            // AND needs at least two children. If the operator has a", "author": "frankgh", "createdAt": "2020-11-04T14:02:45Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/BaseTreePruner.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Optional;\n+\n+import static org.greenplum.pxf.api.filter.Operator.AND;\n+import static org.greenplum.pxf.api.filter.Operator.NOT;\n+import static org.greenplum.pxf.api.filter.Operator.OR;\n+\n+/**\n+ * Base tree pruner that maintains tree correctness by analyzing the current node and its children\n+ * after both the node and the children have been visited and potentially pruned.\n+ */\n+public abstract class BaseTreePruner implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public abstract Node visit(Node node, int level);\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            int childCount = operatorNode.childCount();\n+            if (AND == operator && childCount == 1) {\n+                Node promoted = Optional.ofNullable(operatorNode.getLeft()).orElse(operatorNode.getRight());\n+                LOG.debug(\"Child {} was promoted higher in the tree\", promoted);\n+                // AND need at least two children. If the operator has a", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM2NTkxMQ==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517365911", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            // OR need two or more children\n          \n          \n            \n                            // OR needs two or more children", "author": "frankgh", "createdAt": "2020-11-04T14:05:06Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/BaseTreePruner.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Optional;\n+\n+import static org.greenplum.pxf.api.filter.Operator.AND;\n+import static org.greenplum.pxf.api.filter.Operator.NOT;\n+import static org.greenplum.pxf.api.filter.Operator.OR;\n+\n+/**\n+ * Base tree pruner that maintains tree correctness by analyzing the current node and its children\n+ * after both the node and the children have been visited and potentially pruned.\n+ */\n+public abstract class BaseTreePruner implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public abstract Node visit(Node node, int level);\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            int childCount = operatorNode.childCount();\n+            if (AND == operator && childCount == 1) {\n+                Node promoted = Optional.ofNullable(operatorNode.getLeft()).orElse(operatorNode.getRight());\n+                LOG.debug(\"Child {} was promoted higher in the tree\", promoted);\n+                // AND need at least two children. If the operator has a\n+                // single child node left, we promote the child one level up\n+                // the tree\n+                return promoted;\n+            } else if (OR == operator && childCount <= 1) {\n+                LOG.debug(\"Child with operator {} will be pruned because it has {} children\", operator, childCount);\n+                operatorNode.setLeft(null);\n+                operatorNode.setRight(null);\n+                // OR need two or more children", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM2Nzc3Mg==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517367772", "bodyText": "these are Greenplum data types\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * A tree pruner that removes operator nodes for non-supported column data types.\n          \n          \n            \n             * A tree pruner that removes operator nodes for non-supported Greenplum column data types.", "author": "frankgh", "createdAt": "2020-11-04T14:07:39Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/SupportedDataTypePruner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.EnumSet;\n+import java.util.List;\n+\n+/**\n+ * A tree pruner that removes operator nodes for non-supported column data types.", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM2ODY5Nw==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517368697", "bodyText": "I wonder if we need this constructor. Why have a pruner that won't prune any types. It seems like if we have such pruner, it will do unnecessary work", "author": "frankgh", "createdAt": "2020-11-04T14:08:54Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/SupportedDataTypePruner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.EnumSet;\n+import java.util.List;\n+\n+/**\n+ * A tree pruner that removes operator nodes for non-supported column data types.\n+ */\n+public class SupportedDataTypePruner extends BaseTreePruner {\n+\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final EnumSet<DataType> supportedDataTypes;\n+\n+    /**\n+     * Constructor, assumes all possible predicate pushdown datatypes will be supported\n+     *\n+     * @param columnDescriptors  the list of column descriptors for the table\n+     */\n+    public SupportedDataTypePruner(List<ColumnDescriptor> columnDescriptors) {", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM2OTc5Ng==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517369796", "bodyText": "can we also log the name here? mostly for convenience when debugging", "author": "frankgh", "createdAt": "2020-11-04T14:10:18Z", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/SupportedDataTypePruner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.EnumSet;\n+import java.util.List;\n+\n+/**\n+ * A tree pruner that removes operator nodes for non-supported column data types.\n+ */\n+public class SupportedDataTypePruner extends BaseTreePruner {\n+\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final EnumSet<DataType> supportedDataTypes;\n+\n+    /**\n+     * Constructor, assumes all possible predicate pushdown datatypes will be supported\n+     *\n+     * @param columnDescriptors  the list of column descriptors for the table\n+     */\n+    public SupportedDataTypePruner(List<ColumnDescriptor> columnDescriptors) {\n+        this(columnDescriptors, FilterParser.SUPPORTED_DATA_TYPES);\n+    }\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors  the list of column descriptors for the table\n+     * @param supportedDataTypes the EnumSet of supported data types\n+     */\n+    public SupportedDataTypePruner(List<ColumnDescriptor> columnDescriptors,\n+                                   EnumSet<DataType> supportedDataTypes) {\n+        this.columnDescriptors = columnDescriptors;\n+        this.supportedDataTypes = supportedDataTypes;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            if (!operatorNode.getOperator().isLogical()) {\n+                DataType datatype = getColumnType(operatorNode);\n+                if (!supportedDataTypes.contains(datatype)) {\n+                    // prune the operator node if its operand is a column of unsupported type\n+                    LOG.debug(\"DataType oid={} is not supported\", datatype.getOID());", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM3MzAyMg==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517373022", "bodyText": "Should we remove these? I personally like keeping them here commented out because we can quickly see what's supported and what's not.", "author": "frankgh", "createdAt": "2020-11-04T14:14:34Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveAccessor.java", "diffHunk": "@@ -84,13 +92,91 @@\n public class HiveAccessor extends HdfsSplittableDataAccessor {\n \n     private static final Logger LOG = LoggerFactory.getLogger(HiveAccessor.class);\n+    private static final String PXF_PPD_HIVE = \"pxf.ppd.hive\";\n+    private static final String HIVE_DEFAULT_PARTITION = \"__HIVE_DEFAULT_PARTITION__\";\n \n     private List<HivePartition> partitions;\n-    private static final String HIVE_DEFAULT_PARTITION = \"__HIVE_DEFAULT_PARTITION__\";\n     private int skipHeaderCount;\n     protected List<Integer> hiveIndexes;\n     private String hiveColumnsString;\n     private String hiveColumnTypesString;\n+    private boolean isPredicatePushdownAllowed;\n+\n+    // ----- members for predicate pushdown handling -----\n+    private static final int KRYO_BUFFER_SIZE = 4 * 1024;\n+    private static final int KRYO_MAX_BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+    static final EnumSet<Operator> PARQUET_SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    //Operator.IS_NULL,\n+                    //Operator.IS_NOT_NULL,", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU3Mjc0NQ==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517572745", "bodyText": "I left them on purpose to see which ones are not supported from the overall set, will leave as is.", "author": "denalex", "createdAt": "2020-11-04T19:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM3MzAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM3ODA1NA==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517378054", "bodyText": "Should we explicitly set the flag to prevent any second guessing here?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (configuration.get(PXF_PPD_HIVE, \"true\").equalsIgnoreCase(\"true\")) {\n          \n          \n            \n                        isPredicatePushdownAllowed = true;\n          \n          \n            \n                    isPredicatePushdownAllowed = configuration.get(PXF_PPD_HIVE, \"true\").equalsIgnoreCase(\"true\");\n          \n          \n            \n                    if (isPredicatePushdownAllowed) {", "author": "frankgh", "createdAt": "2020-11-04T14:21:26Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveAccessor.java", "diffHunk": "@@ -123,8 +209,29 @@ public HiveAccessor() {\n      */\n     @Override\n     public void initialize(RequestContext context) {\n+\n         super.initialize(context);\n-        HiveMetadata metadata;\n+\n+        // determine if predicate pushdown is allowed by configuration\n+        if (configuration.get(PXF_PPD_HIVE, \"true\").equalsIgnoreCase(\"true\")) {\n+            isPredicatePushdownAllowed = true;", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM4MDAyOQ==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517380029", "bodyText": "it looks like parquet is our lower bound in terms of supported datatype and operators. I wonder if we can do something better in the future to determine the underlying file types (or maybe we can choose based on the input format). We don't need to address it here.", "author": "frankgh", "createdAt": "2020-11-04T14:24:06Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveAccessor.java", "diffHunk": "@@ -498,6 +609,61 @@ protected void addColumns() {\n         jobConf.set(READ_COLUMN_NAMES_CONF_STR, StringUtils.join(colNames, \",\"));\n     }\n \n+    /**\n+     * Uses {@link HiveSearchArgumentBuilder} to translate a filter string into a\n+     * Hive {@link SearchArgument} object. The result is added as a filter to\n+     * JobConf object\n+     */\n+    private void addFilters() throws Exception {\n+        if (!isPredicatePushdownAllowed || !context.hasFilter()) {\n+            return;\n+        }\n+\n+        /* Predicate push-down configuration */\n+        HiveSearchArgumentBuilder searchArgumentBuilder =\n+                new HiveSearchArgumentBuilder(context.getTupleDescription(), configuration);\n+\n+        // Parse the filter string into a expression tree Node\n+        String filterStr = context.getFilterString();\n+        Node root = new FilterParser().parse(filterStr);\n+\n+        // Prune the parsed tree with valid supported datatypes and operators and then\n+        // traverse the pruned tree with the searchArgumentBuilder to produce a SearchArgument\n+        TRAVERSER.traverse(\n+                root,\n+                new SupportedDataTypePruner(context.getTupleDescription(), getSupportedDatatypesForPushdown()),", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU4OTM4NA==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517589384", "bodyText": "yes, makes sense", "author": "denalex", "createdAt": "2020-11-04T19:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM4MDAyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM4ODEzMw==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517388133", "bodyText": "I wonder if we should generalize HiveUtilities#serializeProperties and then use that method to get the byte[] resulting from serialization. We would need a better name for serializeProperties\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String kryoString = toKryo(searchArgumentBuilder.getFilterBuilder().build());\n          \n          \n            \n                    String kryoString = Base64.encodeBase64String(HiveUtilities.serializeProperties(searchArgumentBuilder.getFilterBuilder().build()));", "author": "frankgh", "createdAt": "2020-11-04T14:35:15Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveAccessor.java", "diffHunk": "@@ -498,6 +609,61 @@ protected void addColumns() {\n         jobConf.set(READ_COLUMN_NAMES_CONF_STR, StringUtils.join(colNames, \",\"));\n     }\n \n+    /**\n+     * Uses {@link HiveSearchArgumentBuilder} to translate a filter string into a\n+     * Hive {@link SearchArgument} object. The result is added as a filter to\n+     * JobConf object\n+     */\n+    private void addFilters() throws Exception {\n+        if (!isPredicatePushdownAllowed || !context.hasFilter()) {\n+            return;\n+        }\n+\n+        /* Predicate push-down configuration */\n+        HiveSearchArgumentBuilder searchArgumentBuilder =\n+                new HiveSearchArgumentBuilder(context.getTupleDescription(), configuration);\n+\n+        // Parse the filter string into a expression tree Node\n+        String filterStr = context.getFilterString();\n+        Node root = new FilterParser().parse(filterStr);\n+\n+        // Prune the parsed tree with valid supported datatypes and operators and then\n+        // traverse the pruned tree with the searchArgumentBuilder to produce a SearchArgument\n+        TRAVERSER.traverse(\n+                root,\n+                new SupportedDataTypePruner(context.getTupleDescription(), getSupportedDatatypesForPushdown()),\n+                new SupportedOperatorPruner(getSupportedOperatorsForPushdown()),\n+                searchArgumentBuilder);\n+\n+        String kryoString = toKryo(searchArgumentBuilder.getFilterBuilder().build());", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM5MDk1Mw==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517390953", "bodyText": "this will change when ORC lands", "author": "frankgh", "createdAt": "2020-11-04T14:39:04Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveSearchArgumentBuilder.java", "diffHunk": "@@ -46,14 +46,14 @@\n  * ..endNot\n  * endAnd\n  */\n-public class HiveORCSearchArgumentBuilder implements TreeVisitor {", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM5NzUwMg==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517397502", "bodyText": "good set of test cases!", "author": "frankgh", "createdAt": "2020-11-04T14:47:56Z", "path": "server/pxf-api/src/test/java/org/greenplum/pxf/api/filter/SupportedDataTypePrunerTest.java", "diffHunk": "@@ -0,0 +1,520 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.EnumSet;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class SupportedDataTypePrunerTest {", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQwMTU5NQ==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517401595", "bodyText": "this seems to be unused", "author": "frankgh", "createdAt": "2020-11-04T14:53:33Z", "path": "server/pxf-hive/src/test/java/org/greenplum/pxf/plugins/hive/HiveParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,919 @@\n+package org.greenplum.pxf.plugins.hive;\n+\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.greenplum.pxf.plugins.hive.utilities.HiveUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HiveParquetFilterPushDownTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final String[] COL14 = {\"1.23456\", \"1.23456\", \"-1.23456\", \"123456789.1\", \"1E-12\", \"1234.889\", \"0.0001\", \"45678.00002\", \"23457.1\", \"45678.00002\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", null, \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\"};\n+    private static final String[] COL15 = {\"0\", \"123.45\", \"-1.45\", \"0.25\", \"-0.25\", \"999.99\", \"-999.99\", \"1\", \"-1\", \"789\", \"-789\", \"0.99\", \"-0.99\", \"1.99\", null, \"-1.99\", \"15.99\", \"-15.99\", \"-299.99\", \"299.99\", \"555.55\", \"0.15\", \"3.89\", \"3.14\", \"8\"};\n+    private static final String[] COL16 = {\"0.12345\", \"-0.12345\", \"12345678.90123\", \"-12345678.90123\", \"99999999\", \"-99999999\", \"-99999999.99999\", \"99999999.99999\", \"0\", \"1\", \"-1\", \"0.9\", \"-0.9\", \"45\", null, \"-45\", \"3.14159\", \"-3.14159\", \"2.71828\", \"-2.71828\", \"45.99999\", \"-45.99999\", \"450.45001\", \"0.00001\", \"-0.00001\"};\n+    private static final Integer[] COL17 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, null, 11, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11};\n+    private static final int[] ALL = COL1;\n+    private static final int[] NONE = new int[]{};\n+\n+    // HiveUserData\n+    private static final String INPUT_FORMAT_NAME = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\";\n+    private static final String SERDE_CLASS_NAME = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\";\n+    private static final String COLUMN_NAMES = \"id,name,cdate,amt,grade,b,tm,bg,bin,sml,r,vc1,c1,dec1,dec2,dec3,num1\";\n+    private static final String COLUMN_TYPES = \"int:string:date:double:string:boolean:timestamp:bigint:binary:smallint:float:varchar(5):char(3):decimal(38,18):decimal(5,2):decimal(13,5):int\";\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        columnDescriptors = new ArrayList<>();\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 1, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 2, \"date\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 3, \"float8\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"grade\", DataType.TEXT.getOID(), 4, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 6, \"timestamp\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 7, \"bigint\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 8, \"bytea\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 9, \"int2\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 10, \"real\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 11, \"varchar\", new Integer[]{5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"c1\", DataType.BPCHAR.getOID(), 12, \"char\", new Integer[]{3}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec1\", DataType.NUMERIC.getOID(), 13, \"numeric\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec2\", DataType.NUMERIC.getOID(), 14, \"numeric\", new Integer[]{5, 2}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec3\", DataType.NUMERIC.getOID(), 15, \"numeric\", new Integer[]{13, 5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"num1\", DataType.INTEGER.getOID(), 16, \"int\", null));\n+\n+        MessageType schema = MessageTypeParser.parseMessageType(\"message hive_schema {\\n\" +\n+                \"  optional int32 id;\\n\" +\n+                \"  optional binary name (UTF8);\\n\" +\n+                \"  optional int32 cdate (DATE);\\n\" +\n+                \"  optional double amt;\\n\" +\n+                \"  optional binary grade (UTF8);\\n\" +\n+                \"  optional boolean b;\\n\" +\n+                \"  optional int96 tm;\\n\" +\n+                \"  optional int64 bg;\\n\" +\n+                \"  optional binary bin;\\n\" +\n+                \"  optional int32 sml (INT_16);\\n\" +\n+                \"  optional float r;\\n\" +\n+                \"  optional binary vc1 (UTF8);\\n\" +\n+                \"  optional binary c1 (UTF8);\\n\" +\n+                \"  optional fixed_len_byte_array(16) dec1 (DECIMAL(38,18));\\n\" +\n+                \"  optional fixed_len_byte_array(3) dec2 (DECIMAL(5,2));\\n\" +\n+                \"  optional fixed_len_byte_array(6) dec3 (DECIMAL(13,5));\\n\" +\n+                \"  optional int32 num1;\\n\" +\n+                \"}\");\n+\n+        // Hive User Data\n+        Properties props = new Properties();\n+        props.put(\"file.inputformat\", INPUT_FORMAT_NAME);\n+        props.put(\"serialization.lib\", SERDE_CLASS_NAME);\n+        props.put(serdeConstants.LIST_COLUMNS, COLUMN_NAMES);\n+        props.put(serdeConstants.LIST_COLUMN_TYPES, COLUMN_TYPES);\n+\n+        accessor = new HiveAccessor();\n+        resolver = new HiveResolver();\n+        context = new RequestContext();\n+\n+        String path = Objects.requireNonNull(getClass().getClassLoader().getResource(\"parquet_types.parquet\")).getPath();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setRequestType(RequestContext.RequestType.READ_BRIDGE);\n+        context.setDataSource(path);\n+        context.setFragmentMetadata(HdfsUtilities.prepareFragmentMetadata(0, 4196, Fragment.HOSTS));\n+        context.setFragmentUserData(HiveUtilities.serializeProperties(props));\n+        context.setTupleDescription(columnDescriptors);\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+    }\n+\n+    @Test\n+    public void testNoFilter() throws Exception {\n+        // all rows are expected\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testIdPushDown() throws Exception {\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            assertRowsReturned(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testIdPushDownWithProjectedColumns() throws Exception {\n+        List<ColumnDescriptor> columnDescriptors = context.getTupleDescription();\n+        columnDescriptors.forEach(d -> d.setProjected(false));\n+        columnDescriptors.get(0).setProjected(true);\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            assertRowsReturned(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testBooleanPushDown() throws Exception {\n+        int[] expectedRows = {2, 4, 6, 8, 10};\n+        // a5 == true\n+        context.setFilterString(\"a5c16s4dtrueo0\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a5 <> true\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a5c16s4dtrueo6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a5 == false\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a5c16s5dfalseo0\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a5 <> false\n+        expectedRows = new int[]{2, 4, 6, 8, 10, 19};\n+        context.setFilterString(\"a5c16s5dfalseo6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a5 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a5o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a5 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a5o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testNotBooleanPushDown() throws Exception {\n+        int[] expectedRows = {1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        // NOT (a5 == true)\n+        context.setFilterString(\"a5c16s4dtrueo0l2\");\n+        assertRowsReturned(expectedRows);\n+    }\n+\n+    @Test\n+    public void testIntPushDown() throws Exception {\n+        // a16 = 11\n+        int[] expectedRows = {11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+\n+        context.setFilterString(\"a16c23s2d11o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 < 11\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n+        context.setFilterString(\"a16c23s2d11o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 > 11\n+        expectedRows = new int[]{15};\n+        context.setFilterString(\"a16c23s2d11o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 <= 2\n+        expectedRows = new int[]{1, 2};\n+        context.setFilterString(\"a16c23s1d2o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 >= 11\n+        expectedRows = new int[]{11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a16c23s2d11o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 <> 11\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 15};\n+        context.setFilterString(\"a16c23s2d11o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a16 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a16o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a16 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a16o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testLongPushDown() throws Exception {\n+        // a7 = 2147483655\n+        int[] expectedRows = {9};\n+        context.setFilterString(\"a7c20s10d2147483655o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 < 0\n+        expectedRows = new int[]{19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a7c23s1d0o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 > 2147483655\n+        expectedRows = new int[]{10, 11, 12, 13, 14, 15, 16, 17};\n+        context.setFilterString(\"a7c20s10d2147483655o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 <= -2147483643\n+        expectedRows = new int[]{20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a7c23s11d-2147483643o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 >= 2147483655\n+        expectedRows = new int[]{9, 10, 11, 12, 13, 14, 15, 16, 17};\n+        context.setFilterString(\"a7c20s10d2147483655o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 <> -1\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a7c23s2d-1o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a7 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a7o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a7 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a7o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testRealPushDown() throws Exception {\n+        // a10 = 8.7\n+        int[] expectedRows = {2};\n+        context.setFilterString(\"a10c701s3d8.7o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 < 8.7\n+        expectedRows = new int[]{1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a10c701s3d8.7o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 > 8.7\n+        expectedRows = new int[]{3, 4, 5, 6};\n+        context.setFilterString(\"a10c701s3d8.7o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 <= 8.7\n+        expectedRows = new int[]{1, 2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a10c701s3d8.7o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 >= 8.7\n+        expectedRows = new int[]{2, 3, 4, 5, 6};\n+        context.setFilterString(\"a10c701s3d8.7o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 <> 8.7\n+        expectedRows = new int[]{1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a10c701s3d8.7o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a10 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a10o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a10 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a10o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testTextPushDown() throws Exception {\n+        // a4 = 'excellent'\n+        int[] expectedRows = {2, 4, 9, 14, 20, 22};\n+        context.setFilterString(\"a4c25s9dexcellento5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 < 'excellent'\n+        expectedRows = new int[]{6, 8, 10, 16, 18, 24};\n+        context.setFilterString(\"a4c25s9dexcellento1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 > 'excellent'\n+        expectedRows = new int[]{1, 3, 5, 7, 11, 13, 15, 17, 19, 21, 23, 25};\n+        context.setFilterString(\"a4c25s9dexcellento2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 <= 'excellent'\n+        expectedRows = new int[]{2, 4, 6, 8, 9, 10, 14, 16, 18, 20, 22, 24};\n+        context.setFilterString(\"a4c25s9dexcellento3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 >= 'excellent'\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 7, 9, 11, 13, 14, 15, 17, 19, 20, 21, 22, 23, 25};\n+        context.setFilterString(\"a4c25s9dexcellento4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 <> 'excellent'\n+        expectedRows = new int[]{1, 3, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 23, 24, 25};\n+        context.setFilterString(\"a4c25s9dexcellento6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a4 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a4o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a4 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a4o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testVarCharPushDown() throws Exception {\n+        // a11 = 's_16'\n+        int[] expectedRows = {11, 12, 13, 14};\n+        context.setFilterString(\"a11c25s4ds_16o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 < 's_10'\n+        expectedRows = new int[]{};\n+        context.setFilterString(\"a11c25s4ds_10o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 > 's_168'\n+        expectedRows = new int[]{1, 2, 3, 4, 15, 25};\n+        context.setFilterString(\"a11c25s5ds_168o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 <= 's_10'\n+        expectedRows = new int[]{5};\n+        context.setFilterString(\"a11c25s4ds_10o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 >= 's_168'\n+        expectedRows = new int[]{1, 2, 3, 4, 15, 24, 25};\n+        context.setFilterString(\"a11c25s5ds_168o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 <> 's_16'\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a11c25s4ds_16o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a11 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a11o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a11 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a11o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testCharPushDown() throws Exception {\n+        // a12 = 'EUR'\n+        int[] expectedRows = {8, 12, 17, 22, 23};\n+        context.setFilterString(\"a12c1042s3dEURo5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 < 'USD'\n+        expectedRows = new int[]{8, 9, 11, 12, 14, 17, 20, 22, 23};\n+        context.setFilterString(\"a12c1042s3dUSDo1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 > 'EUR'\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 25};\n+        context.setFilterString(\"a12c1042s3dEURo2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 <= 'EUR'\n+        expectedRows = new int[]{8, 12, 17, 22, 23};\n+        context.setFilterString(\"a12c1042s3dEURo3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 >= 'USD'\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 10, 13, 15, 16, 18, 19, 21, 25};\n+        context.setFilterString(\"a12c1042s3dUSDo4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 <> 'USD'\n+        expectedRows = new int[]{8, 9, 11, 12, 14, 17, 20, 22, 23, 24};\n+        context.setFilterString(\"a12c1042s3dUSDo6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a12o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a12 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a12o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedCharPushDownWithWhitespaces() throws Exception {\n+        // a12 = 'EUR '\n+        context.setFilterString(\"a12c1042s4dEUR o5\");\n+        assertRowsReturned(NONE);\n+\n+        // a12 > 'EUR '\n+        int[] expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 25};\n+        context.setFilterString(\"a12c1042s4dEUR o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 <= 'EUR '\n+        expectedRows = new int[]{8, 12, 17, 22, 23};\n+        context.setFilterString(\"a12c1042s4dEUR o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a12 >= 'USD '\n+        context.setFilterString(\"a12c1042s4dUSD o4\");\n+        assertRowsReturned(NONE);\n+\n+        // a12 <> 'USD '\n+        context.setFilterString(\"a12c1042s4dUSD o6\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testSmallIntPushDown() throws Exception {\n+        // a9 = 1000\n+        int[] expectedRows = {16};\n+        context.setFilterString(\"a9c23s4d1000o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 < -1000\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6};\n+        context.setFilterString(\"a9c23s5d-1000o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 > 31000\n+        expectedRows = new int[]{22, 23, 24, 25};\n+        context.setFilterString(\"a9c23s5d31000o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 <= 0\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13};\n+        context.setFilterString(\"a9c23s1d0o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 >= 0\n+        expectedRows = new int[]{13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25};\n+        context.setFilterString(\"a9c23s1d0o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 <> 0\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a9c23s1d0o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a9 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a9o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a9 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a9o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testDatePushDown() throws Exception {\n+        // DATE pushdown is not supported for Hive Parquet, expect to receive all rows back\n+        // a2 = '2019-12-04'\n+        context.setFilterString(\"a2c1082s10d2019-12-04o5\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 < '2019-12-04'\n+        context.setFilterString(\"a2c1082s10d2019-12-04o1\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 > '2019-12-20'\n+        context.setFilterString(\"a2c1082s10d2019-12-20o2\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 <= '2019-12-06'\n+        context.setFilterString(\"a2c1082s10d2019-12-06o3\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 >= '2019-12-15'\n+        context.setFilterString(\"a2c1082s10d2019-12-15o4\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 <> '2019-12-15'\n+        context.setFilterString(\"a2c1082s10d2019-12-15o6\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 IS NULL\n+        context.setFilterString(\"a2o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a2 IS NOT NULL\n+        context.setFilterString(\"a2o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testDoublePushDown() throws Exception {\n+        // a3 = 1200\n+        int[] expectedRows = {1};\n+        context.setFilterString(\"a3c701s4d1200o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 < 1500\n+        expectedRows = new int[]{1, 2, 3};\n+        context.setFilterString(\"a3c701s4d1500o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 > 2500\n+        expectedRows = new int[]{16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a3c701s4d2500o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 <= 1500\n+        expectedRows = new int[]{1, 2, 3, 4};\n+        context.setFilterString(\"a3c701s4d1500o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 >= 2550\n+        expectedRows = new int[]{16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a3c701s4d2550o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 <> 1200\n+        expectedRows = new int[]{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a3c701s4d1200o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // a3 IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a3o8\");\n+        assertRowsReturned(ALL);\n+\n+        // a3 IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a3o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testByteAFilter() throws Exception {\n+        // bin = '1'\n+        int[] expectedRows = {1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24};\n+        context.setFilterString(\"a8c25s1d1o5\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin < '1'\n+        expectedRows = new int[]{10};\n+        context.setFilterString(\"a8c25s1d1o1\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin > '1'\n+        expectedRows = new int[]{2, 3, 4, 5, 6, 7, 8, 9};\n+        context.setFilterString(\"a8c25s1d1o2\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin <= '1'\n+        expectedRows = new int[]{1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24};\n+        context.setFilterString(\"a8c25s1d1o3\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin >= '1'\n+        expectedRows = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24};\n+        context.setFilterString(\"a8c25s1d1o4\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin <> '1'\n+        expectedRows = new int[]{2, 3, 4, 5, 6, 7, 8, 9, 10, 25};\n+        context.setFilterString(\"a8c25s1d1o6\");\n+        assertRowsReturned(expectedRows);\n+\n+        // bin IS NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a8o8\");\n+        assertRowsReturned(ALL);\n+\n+        // bin IS NOT NULL -- not supported in HIVE Parquet PPD\n+        context.setFilterString(\"a8o9\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedDateAndAmtFilter() throws Exception {\n+        // cdate > '2019-12-02' and cdate < '2019-12-12' and amt > 1500\n+        // DATE is not supported, only amt > 1500 will be effective\n+        int[] expectedRows = new int[]{5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a2c1082s10d2019-12-02o2a2c1082s10d2019-12-12o1a3c701s4d1500o2l0l0\");\n+        assertRowsReturned(expectedRows);\n+    }\n+\n+    @Test\n+    public void testUnsupportedDateWithOrAndAmtFilter() throws Exception {\n+        // cdate > '2019-12-19' OR ( cdate <= '2019-12-15' and amt > 2000)\n+        context.setFilterString(\"a2c1082s10d2019-12-19o2a2c1082s10d2019-12-15o3a3c701s4d2000o2l0l1\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedDateWithOrAndAmtFilterWithProjectedColumns() throws Exception {\n+\n+        List<ColumnDescriptor> columnDescriptors = context.getTupleDescription();\n+        columnDescriptors.forEach(d -> d.setProjected(false));\n+        columnDescriptors.get(0).setProjected(true);\n+        columnDescriptors.get(2).setProjected(true);\n+        columnDescriptors.get(3).setProjected(true);\n+        columnDescriptors.get(5).setProjected(true);\n+\n+        // cdate > '2019-12-19' OR ( cdate <= '2019-12-15' and amt > 2000)\n+        context.setFilterString(\"a2c1082s10d2019-12-19o2a2c1082s10d2019-12-15o3a3c701s4d2000o2l0l1\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedDateOrAmtFilter() throws Exception {\n+        // cdate > '2019-12-20' OR amt < 1500\n+        context.setFilterString(\"a2c1082s10d2019-12-20o2a3c701s4d1500o1l1\");\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedINT96Filter() throws Exception {\n+        // tm = '2013-07-23 21:00:00'\n+        context.setFilterString(\"a6c1114s19d2013-07-23 21:00:00o5\");\n+        // all rows are expected\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testUnsupportedFixedLenByteArrayFilter() throws Exception {\n+        // dec2 = 0\n+        context.setFilterString(\"a14c23s1d0o5\");\n+        // all rows are expected\n+        assertRowsReturned(ALL);\n+    }\n+\n+    @Test\n+    public void testInOperationFilter() throws Exception {\n+        // a16 in (11, 12)\n+        int[] expectedRows = {11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a16m1007s2d11s2d12o10\");\n+        assertRowsReturned(expectedRows);\n+    }\n+\n+    private Map<String, Type> getOriginalFieldsMap(MessageType originalSchema) {", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQwMzk0OA==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517403948", "bodyText": "In the native Parquet reader, we expect Double, it looks like hive resolves to string for these columns?", "author": "frankgh", "createdAt": "2020-11-04T14:56:36Z", "path": "server/pxf-hive/src/test/java/org/greenplum/pxf/plugins/hive/HiveParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,919 @@\n+package org.greenplum.pxf.plugins.hive;\n+\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.greenplum.pxf.plugins.hive.utilities.HiveUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HiveParquetFilterPushDownTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final String[] COL14 = {\"1.23456\", \"1.23456\", \"-1.23456\", \"123456789.1\", \"1E-12\", \"1234.889\", \"0.0001\", \"45678.00002\", \"23457.1\", \"45678.00002\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", null, \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\", \"0.123456789\"};\n+    private static final String[] COL15 = {\"0\", \"123.45\", \"-1.45\", \"0.25\", \"-0.25\", \"999.99\", \"-999.99\", \"1\", \"-1\", \"789\", \"-789\", \"0.99\", \"-0.99\", \"1.99\", null, \"-1.99\", \"15.99\", \"-15.99\", \"-299.99\", \"299.99\", \"555.55\", \"0.15\", \"3.89\", \"3.14\", \"8\"};\n+    private static final String[] COL16 = {\"0.12345\", \"-0.12345\", \"12345678.90123\", \"-12345678.90123\", \"99999999\", \"-99999999\", \"-99999999.99999\", \"99999999.99999\", \"0\", \"1\", \"-1\", \"0.9\", \"-0.9\", \"45\", null, \"-45\", \"3.14159\", \"-3.14159\", \"2.71828\", \"-2.71828\", \"45.99999\", \"-45.99999\", \"450.45001\", \"0.00001\", \"-0.00001\"};", "originalCommit": "8901a0302a5f95c185e11cb861183832c3196b68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgxMTQ1MA==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517811450", "bodyText": "yes, those are numeric / decimals so are represented by a String. It is not correct to treat them as Double as that would cause precision loss, maybe we should review native Parquet for this ?", "author": "denalex", "createdAt": "2020-11-05T06:07:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQwMzk0OA=="}], "type": "inlineReview"}, {"oid": "13ddb0a24dcaef898e6b6c231f543b79a2c193f6", "url": "https://github.com/greenplum-db/pxf/commit/13ddb0a24dcaef898e6b6c231f543b79a2c193f6", "message": "addressed PR feedback", "committedDate": "2020-11-04T19:58:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU5ODA0MA==", "url": "https://github.com/greenplum-db/pxf/pull/480#discussion_r517598040", "bodyText": "we no longer need lines 106-107", "author": "frankgh", "createdAt": "2020-11-04T20:01:08Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveAccessor.java", "diffHunk": "@@ -657,13 +659,6 @@ JobConf getJobConf() {\n         return jobConf;\n     }\n \n-    private String toKryo(SearchArgument sarg) {\n-        Output out = new Output(KRYO_BUFFER_SIZE, KRYO_MAX_BUFFER_SIZE);", "originalCommit": "13ddb0a24dcaef898e6b6c231f543b79a2c193f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "91bf0b0903473f7c6cdfd4274b9ab8f9785158ec", "url": "https://github.com/greenplum-db/pxf/commit/91bf0b0903473f7c6cdfd4274b9ab8f9785158ec", "message": "fixed test Fragmenter", "committedDate": "2020-11-04T21:59:17Z", "type": "commit"}, {"oid": "25015c9d23b124b72fa763ecfc3267a37e0d72a1", "url": "https://github.com/greenplum-db/pxf/commit/25015c9d23b124b72fa763ecfc3267a37e0d72a1", "message": "clean unused code", "committedDate": "2020-11-04T23:10:03Z", "type": "commit"}, {"oid": "65742935bfa7a18820bae3359e36b3263d2b71c5", "url": "https://github.com/greenplum-db/pxf/commit/65742935bfa7a18820bae3359e36b3263d2b71c5", "message": "removed unused method", "committedDate": "2020-11-05T06:08:06Z", "type": "commit"}]}