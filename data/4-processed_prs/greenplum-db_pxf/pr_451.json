{"pr_number": 451, "pr_title": "Support dropping columns in PXF external tables", "pr_createdAt": "2020-09-15T15:39:15Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/451", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NjY3MQ==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490586671", "bodyText": "why do you need sleep?", "author": "ashuka24", "createdAt": "2020-09-17T22:01:27Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/general/AlterTableTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.greenplum.pxf.automation.features.general;\n+\n+import org.greenplum.pxf.automation.features.BaseFeature;\n+import org.greenplum.pxf.automation.structures.tables.basic.Table;\n+import org.greenplum.pxf.automation.structures.tables.pxf.ReadableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.pxf.WritableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.utils.TableFactory;\n+import org.greenplum.pxf.automation.utils.system.ProtocolEnum;\n+import org.greenplum.pxf.automation.utils.system.ProtocolUtils;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+\n+import static java.lang.Thread.sleep;", "originalCommit": "904003217a18f82003afbc540c1c01398c5bce5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzgwOA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490603808", "bodyText": "good catch, the imports need cleaning", "author": "frankgh", "createdAt": "2020-09-17T22:47:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NjY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NzIzMw==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490587233", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Attempt to query data on a table that has a column that does not exist.\n          \n          \n            \n                 * Attempt to query data on a table with column that does not exist.", "author": "ashuka24", "createdAt": "2020-09-17T22:02:55Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/general/AlterTableTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.greenplum.pxf.automation.features.general;\n+\n+import org.greenplum.pxf.automation.features.BaseFeature;\n+import org.greenplum.pxf.automation.structures.tables.basic.Table;\n+import org.greenplum.pxf.automation.structures.tables.pxf.ReadableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.pxf.WritableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.utils.TableFactory;\n+import org.greenplum.pxf.automation.utils.system.ProtocolEnum;\n+import org.greenplum.pxf.automation.utils.system.ProtocolUtils;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+\n+import static java.lang.Thread.sleep;\n+\n+public class AlterTableTest extends BaseFeature {\n+\n+    private static final String AVRO_TYPES_FILE_NAME = \"supported_primitive_types\";\n+    private static final String FILE_SCHEME = \"file://\";\n+    private static final String PXF_ALTER_AVRO_TABLE = \"pxf_alter_avro_table\";\n+    private static final String PXF_ALTER_CSV_TABLE = \"pxf_alter_csv_table\";\n+    private static final String PXF_PARQUET_TABLE_SOURCE = \"pxf_alter_parquet_primitive_types\";\n+    private static final String PXF_ALTER_PARQUET_TABLE = \"pxf_alter_parquet_table\";\n+    private static final String PXF_ALTER_WRITE_PARQUET_TABLE = \"pxf_alter_write_parquet_table\";\n+    private static final String PARQUET_PRIMITIVE_TYPES = \"parquet_primitive_types\";\n+    private static final String PARQUET_WRITE_PRIMITIVES = \"parquet_write_primitives\";\n+    private static final String SUFFIX_JSON = \".json\";\n+    private static final String SUFFIX_AVRO = \".avro\";\n+    private static final String SUFFIX_AVSC = \".avsc\";\n+\n+    private static final String[] PARQUET_TABLE_COLUMNS = new String[]{\n+            \"s1    TEXT\",\n+            \"s2    TEXT\",\n+            \"n1    INTEGER\",\n+            \"d1    DOUBLE PRECISION\",\n+            \"dc1   NUMERIC\",\n+            \"tm    TIMESTAMP\",\n+            \"f     REAL\",\n+            \"bg    BIGINT\",\n+            \"b     BOOLEAN\",\n+            \"tn    SMALLINT\",\n+            \"vc1   VARCHAR(5)\",\n+            \"sml   SMALLINT\",\n+            \"c1    CHAR(3)\",\n+            \"bin   BYTEA\"\n+    };\n+\n+    private static final String[] PARQUET_TABLE_SUBSET_COLUMNS = new String[]{\n+            \"s1    TEXT\",\n+            \"s2    TEXT\",\n+            \"n1    INTEGER\",\n+            \"d1    DOUBLE PRECISION\",\n+            \"dc1   NUMERIC\",\n+            \"f     REAL\",\n+            \"bg    BIGINT\",\n+            \"b     BOOLEAN\",\n+            \"tn    SMALLINT\",\n+            \"vc1   VARCHAR(5)\",\n+            \"sml   SMALLINT\",\n+            \"c1    CHAR(3)\"\n+    };\n+\n+    private String hdfsPath;\n+\n+    @Override\n+    public void beforeClass() throws Exception {\n+        // path for storing data on HDFS (for processing by PXF)\n+        hdfsPath = hdfs.getWorkingDirectory() + \"/alter-tests\";\n+\n+        String resourcePath = localDataResourcesFolder + \"/parquet/\";\n+        hdfs.copyFromLocal(resourcePath + PARQUET_PRIMITIVE_TYPES, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES);\n+\n+        // Create Data and write it to HDFS\n+        Table dataTable = getSmallData();\n+        hdfs.writeTableToFile(hdfsPath + \"/csv/\" + fileName, dataTable, \",\");\n+\n+        // Avro\n+        // location of schema and data files\n+        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        resourcePath = absolutePath + \"/avro/\";\n+        hdfs.writeAvroFileFromJson(hdfsPath + \"/avro/\" + AVRO_TYPES_FILE_NAME + SUFFIX_AVRO,\n+                FILE_SCHEME + resourcePath + AVRO_TYPES_FILE_NAME + SUFFIX_AVSC,\n+                FILE_SCHEME + resourcePath + AVRO_TYPES_FILE_NAME + SUFFIX_JSON, null);\n+    }\n+\n+    /**\n+     * Query data on a table, then drops column(s), then queries again.\n+     * Finally, the test adds back one of the dropped columns to the table,\n+     * and a new query is performed. The query uses parquet which supports\n+     * column projection, and uses the pxfwritable_import formatter.\n+     *\n+     * @throws Exception when the test execution fails\n+     */\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropAndAddColumnsPxfWritableImportWithColumnProjectionSupport() throws Exception {\n+\n+        exTable = new ReadableExternalTable(PXF_ALTER_PARQUET_TABLE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+\n+        gpdb.createTableAndVerify(exTable);\n+\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_import.with_column_projection.runTest\");\n+    }\n+\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropColumnsPxfWritableExport() throws Exception {\n+\n+        // Create source table\n+        exTable = new ReadableExternalTable(PXF_PARQUET_TABLE_SOURCE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Create writable table\n+        exTable = new WritableExternalTable(PXF_ALTER_WRITE_PARQUET_TABLE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet-write/\" + PARQUET_WRITE_PRIMITIVES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_export\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Create validation table\n+        exTable = new ReadableExternalTable(PXF_ALTER_WRITE_PARQUET_TABLE + \"_r\",\n+                PARQUET_TABLE_SUBSET_COLUMNS, hdfsPath + \"/parquet-write/\" + PARQUET_WRITE_PRIMITIVES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_export.parquet.runTest\");\n+    }\n+\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropAndAddColumnsPxfWritableImportWithoutColumnProjectionSupport() throws Exception {\n+        // default external table with common settings\n+        exTable = new ReadableExternalTable(PXF_ALTER_AVRO_TABLE, new String[]{\n+                \"type_int int\",\n+                \"type_double float8\",\n+                \"type_string text\",\n+                \"type_float real\",\n+                \"col_does_not_exist text\",\n+                \"type_long bigint\",\n+                \"type_bytes bytea\",\n+                \"type_boolean bool\"}, hdfsPath + \"/avro/\" + AVRO_TYPES_FILE_NAME + SUFFIX_AVRO, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":avro\");\n+\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Verify results\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_import.without_column_projection.runTest\");\n+    }\n+\n+    /**\n+     * Attempt to query data on a table that has a column that does not exist.", "originalCommit": "904003217a18f82003afbc540c1c01398c5bce5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4ODU3OA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490588578", "bodyText": "or maybe\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Attempt to query data on a table that has a column that does not exist.\n          \n          \n            \n                 * Attempt to query data on a column from a table where the column does not exist.", "author": "ashuka24", "createdAt": "2020-09-17T22:06:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NzIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NzczMA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490587730", "bodyText": "could you please clarify this? if it tries to query data on a non-existent column, then what exactly is being dropped?", "author": "ashuka24", "createdAt": "2020-09-17T22:04:08Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/general/AlterTableTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+package org.greenplum.pxf.automation.features.general;\n+\n+import org.greenplum.pxf.automation.features.BaseFeature;\n+import org.greenplum.pxf.automation.structures.tables.basic.Table;\n+import org.greenplum.pxf.automation.structures.tables.pxf.ReadableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.pxf.WritableExternalTable;\n+import org.greenplum.pxf.automation.structures.tables.utils.TableFactory;\n+import org.greenplum.pxf.automation.utils.system.ProtocolEnum;\n+import org.greenplum.pxf.automation.utils.system.ProtocolUtils;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+\n+import static java.lang.Thread.sleep;\n+\n+public class AlterTableTest extends BaseFeature {\n+\n+    private static final String AVRO_TYPES_FILE_NAME = \"supported_primitive_types\";\n+    private static final String FILE_SCHEME = \"file://\";\n+    private static final String PXF_ALTER_AVRO_TABLE = \"pxf_alter_avro_table\";\n+    private static final String PXF_ALTER_CSV_TABLE = \"pxf_alter_csv_table\";\n+    private static final String PXF_PARQUET_TABLE_SOURCE = \"pxf_alter_parquet_primitive_types\";\n+    private static final String PXF_ALTER_PARQUET_TABLE = \"pxf_alter_parquet_table\";\n+    private static final String PXF_ALTER_WRITE_PARQUET_TABLE = \"pxf_alter_write_parquet_table\";\n+    private static final String PARQUET_PRIMITIVE_TYPES = \"parquet_primitive_types\";\n+    private static final String PARQUET_WRITE_PRIMITIVES = \"parquet_write_primitives\";\n+    private static final String SUFFIX_JSON = \".json\";\n+    private static final String SUFFIX_AVRO = \".avro\";\n+    private static final String SUFFIX_AVSC = \".avsc\";\n+\n+    private static final String[] PARQUET_TABLE_COLUMNS = new String[]{\n+            \"s1    TEXT\",\n+            \"s2    TEXT\",\n+            \"n1    INTEGER\",\n+            \"d1    DOUBLE PRECISION\",\n+            \"dc1   NUMERIC\",\n+            \"tm    TIMESTAMP\",\n+            \"f     REAL\",\n+            \"bg    BIGINT\",\n+            \"b     BOOLEAN\",\n+            \"tn    SMALLINT\",\n+            \"vc1   VARCHAR(5)\",\n+            \"sml   SMALLINT\",\n+            \"c1    CHAR(3)\",\n+            \"bin   BYTEA\"\n+    };\n+\n+    private static final String[] PARQUET_TABLE_SUBSET_COLUMNS = new String[]{\n+            \"s1    TEXT\",\n+            \"s2    TEXT\",\n+            \"n1    INTEGER\",\n+            \"d1    DOUBLE PRECISION\",\n+            \"dc1   NUMERIC\",\n+            \"f     REAL\",\n+            \"bg    BIGINT\",\n+            \"b     BOOLEAN\",\n+            \"tn    SMALLINT\",\n+            \"vc1   VARCHAR(5)\",\n+            \"sml   SMALLINT\",\n+            \"c1    CHAR(3)\"\n+    };\n+\n+    private String hdfsPath;\n+\n+    @Override\n+    public void beforeClass() throws Exception {\n+        // path for storing data on HDFS (for processing by PXF)\n+        hdfsPath = hdfs.getWorkingDirectory() + \"/alter-tests\";\n+\n+        String resourcePath = localDataResourcesFolder + \"/parquet/\";\n+        hdfs.copyFromLocal(resourcePath + PARQUET_PRIMITIVE_TYPES, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES);\n+\n+        // Create Data and write it to HDFS\n+        Table dataTable = getSmallData();\n+        hdfs.writeTableToFile(hdfsPath + \"/csv/\" + fileName, dataTable, \",\");\n+\n+        // Avro\n+        // location of schema and data files\n+        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        resourcePath = absolutePath + \"/avro/\";\n+        hdfs.writeAvroFileFromJson(hdfsPath + \"/avro/\" + AVRO_TYPES_FILE_NAME + SUFFIX_AVRO,\n+                FILE_SCHEME + resourcePath + AVRO_TYPES_FILE_NAME + SUFFIX_AVSC,\n+                FILE_SCHEME + resourcePath + AVRO_TYPES_FILE_NAME + SUFFIX_JSON, null);\n+    }\n+\n+    /**\n+     * Query data on a table, then drops column(s), then queries again.\n+     * Finally, the test adds back one of the dropped columns to the table,\n+     * and a new query is performed. The query uses parquet which supports\n+     * column projection, and uses the pxfwritable_import formatter.\n+     *\n+     * @throws Exception when the test execution fails\n+     */\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropAndAddColumnsPxfWritableImportWithColumnProjectionSupport() throws Exception {\n+\n+        exTable = new ReadableExternalTable(PXF_ALTER_PARQUET_TABLE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+\n+        gpdb.createTableAndVerify(exTable);\n+\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_import.with_column_projection.runTest\");\n+    }\n+\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropColumnsPxfWritableExport() throws Exception {\n+\n+        // Create source table\n+        exTable = new ReadableExternalTable(PXF_PARQUET_TABLE_SOURCE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet/\" + PARQUET_PRIMITIVE_TYPES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Create writable table\n+        exTable = new WritableExternalTable(PXF_ALTER_WRITE_PARQUET_TABLE,\n+                PARQUET_TABLE_COLUMNS, hdfsPath + \"/parquet-write/\" + PARQUET_WRITE_PRIMITIVES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_export\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Create validation table\n+        exTable = new ReadableExternalTable(PXF_ALTER_WRITE_PARQUET_TABLE + \"_r\",\n+                PARQUET_TABLE_SUBSET_COLUMNS, hdfsPath + \"/parquet-write/\" + PARQUET_WRITE_PRIMITIVES, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":parquet\");\n+        gpdb.createTableAndVerify(exTable);\n+\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_export.parquet.runTest\");\n+    }\n+\n+    @Test(groups = {\"features\", \"gpdb\", \"security\"})\n+    public void dropAndAddColumnsPxfWritableImportWithoutColumnProjectionSupport() throws Exception {\n+        // default external table with common settings\n+        exTable = new ReadableExternalTable(PXF_ALTER_AVRO_TABLE, new String[]{\n+                \"type_int int\",\n+                \"type_double float8\",\n+                \"type_string text\",\n+                \"type_float real\",\n+                \"col_does_not_exist text\",\n+                \"type_long bigint\",\n+                \"type_bytes bytea\",\n+                \"type_boolean bool\"}, hdfsPath + \"/avro/\" + AVRO_TYPES_FILE_NAME + SUFFIX_AVRO, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_import\");\n+        exTable.setProfile(ProtocolUtils.getProtocol().value() + \":avro\");\n+\n+        gpdb.createTableAndVerify(exTable);\n+\n+        // Verify results\n+        runTincTest(\"pxf.features.general.alter.pxfwritable_import.without_column_projection.runTest\");\n+    }\n+\n+    /**\n+     * Attempt to query data on a table that has a column that does not exist.\n+     * It then drops the column and performs the query successfully.", "originalCommit": "904003217a18f82003afbc540c1c01398c5bce5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNTA0NA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r490605044", "bodyText": "done, hopefully it makes more sense now.", "author": "frankgh", "createdAt": "2020-09-17T22:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NzczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4MDIxMw==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492280213", "bodyText": "why not inline this on line 361 since attr is not used anywhere else ?", "author": "denalex", "createdAt": "2020-09-21T19:00:05Z", "path": "external-table/src/gpdbwritableformatter.c", "diffHunk": "@@ -323,16 +329,41 @@ boolArrayToByteArray(bool *data, int len, int *outlen)\n /*\n  * Helper routine to convert byte array to boolean array\n  * It'll write the output to booldata.\n+ *\n+ * This routine supports dropped columns, in the case of tables with dropped\n+ * columns booldata's size will match the number of original columns\n+ * (including dropped columns), and the source `data` will match the number\n+ * of columns provided by the PXF server, so if there are dropped columns,\n+ * PXF server will only provide the subset of columns. Below a graphical\n+ * representation of the mapping.\n+ *\n+ *  --------------------------------------------\n+ * |  col1  |  col2  |  col3  |  col5  |  col6  |  input: *data\n+ *  --------------------------------------------\n+ *     |        |        |         |        \u2514----------------\u2b0e\n+ *     \u2193        \u2193        \u2193         \u2514----------------\u2b0e        \u2193\n+ *  -------------------------------------------------------------\n+ * |  col1  |  col2  |  col3  |  col4 (dropped)  | col5  | col6  | output: **booldata\n+ *  -------------------------------------------------------------\n  */\n static void\n-byteArrayToBoolArray(bits8 *data, int len, bool **booldata, int boollen)\n+byteArrayToBoolArray(bits8 *data, int len, bool **booldata, int boollen, TupleDesc tupdesc)\n {\n \tint\t\t\ti,\n \t\t\t\tj,\n \t\t\t\tk;\n \n \tfor (i = 0, j = 0, k = 7; i < boollen; i++)\n \t{\n+\t\tForm_pg_attribute attr = tupdesc->attrs[i];", "originalCommit": "09ac7cfbdde52f0a9daf4223a8c1797835c9b752", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMxMDY1Nw==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492310657", "bodyText": "sure", "author": "frankgh", "createdAt": "2020-09-21T19:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4MDIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4ODU5MA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492288590", "bodyText": "why not init at declaration ?", "author": "denalex", "createdAt": "2020-09-21T19:14:36Z", "path": "external-table/src/gpdbwritableformatter.c", "diffHunk": "@@ -637,8 +695,14 @@ gpdbwritableformatter_import(PG_FUNCTION_ARGS)\n \n \t/* Get our internal description of the formatter */\n \tncolumns = tupdesc->natts;\n+\tnvalidcolumns = 0;", "originalCommit": "09ac7cfbdde52f0a9daf4223a8c1797835c9b752", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMxMTAwMg==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492311002", "bodyText": "yeah, let me do that", "author": "frankgh", "createdAt": "2020-09-21T19:57:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4ODU5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4OTY3MQ==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492289671", "bodyText": "wouldn't ncolumns_remote and nvalidcolumns be the same ?", "author": "denalex", "createdAt": "2020-09-21T19:16:34Z", "path": "external-table/src/gpdbwritableformatter.c", "diffHunk": "@@ -757,27 +820,27 @@ gpdbwritableformatter_import(PG_FUNCTION_ARGS)\n \n \t/* Verify once on the first row */\n \tif (FIRST_LINE_NUM == myData->lineno++)\n-\t{\n-\t\tverifyExternalTableDefinition(ncolumns_remote, ncolumns, tupdesc, data_buf, &bufidx);\n-\t}\n-\t/* Skipping the columns' enum types */\n-\telse\n-\t{\n-\t\tbufidx += ncolumns;\n-\t}\n+\t\tverifyExternalTableDefinition(ncolumns_remote, nvalidcolumns, ncolumns, tupdesc, data_buf, &bufidx);", "originalCommit": "09ac7cfbdde52f0a9daf4223a8c1797835c9b752", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMwOTQ5MQ==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492309491", "bodyText": "correct, but verifyExternalTableDefinition makes sure nvalidcolumns == ncolumns: https://github.com/greenplum-db/pxf/blob/fix-dropped-col/external-table/src/gpdbwritableformatter.c#L388", "author": "frankgh", "createdAt": "2020-09-21T19:54:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI4OTY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMwMTE4NA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492301184", "bodyText": "not sure what this FirstLowInvalidHeapAttributeNumber is and where it is coming from. Also, do we need to cleanup allocated BMS attrs_used at the end ?", "author": "denalex", "createdAt": "2020-09-21T19:38:21Z", "path": "external-table/src/pxfheaders.c", "diffHunk": "@@ -376,8 +412,9 @@ add_projection_desc_httpheader(CHURL_HEADERS headers,\n \tfor (i = 0; varNumbers && i < numSimpleVars; i++)\n #endif\n \t{\n-\t\tadd_projection_index_header(headers,\n-\t\t\t\t\t\t\t\t\tformatter, varNumbers[i] - 1, long_number);\n+\t\tattrs_used =\n+\t\t\tbms_add_member(attrs_used,\n+\t\t\t\t\t\t varNumbers[i] - FirstLowInvalidHeapAttributeNumber);", "originalCommit": "09ac7cfbdde52f0a9daf4223a8c1797835c9b752", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMxMzYxOA==", "url": "https://github.com/greenplum-db/pxf/pull/451#discussion_r492313618", "bodyText": "That seems to be the way to use bms_add_member, here is a snippet from copy.c. It's also in postgres_fdw.c and pxf_fdw.c (based on postgres_fdw).\nint\t\t\tattno = lfirst_int(cur) -\n\t\t\tFirstLowInvalidHeapAttributeNumber;\n\n\t\t\tif (is_from)\n\t\t\t\trte->modifiedCols = bms_add_member(rte->modifiedCols, attno);\n\t\t\telse\n\t\t\t\trte->selectedCols = bms_add_member(rte->selectedCols, attno);\nI will free bms after that method call.", "author": "frankgh", "createdAt": "2020-09-21T20:02:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMwMTE4NA=="}], "type": "inlineReview"}, {"oid": "c83c8630b623b13e5934cc1f0eb97dd8f07c4dac", "url": "https://github.com/greenplum-db/pxf/commit/c83c8630b623b13e5934cc1f0eb97dd8f07c4dac", "message": "Support dropping columns in PXF readable external tables\n\nPXF does not support dropping columns from readable external tables.\nThis commit adds support for dropping columns, especially when the\nformat is CUSTOM and the formatter is `pxfwritable_import`. This also\napplies for TEXT/CSV format, but the complexity lies in the\n`pxfwritable_import` formatter. Add automation tests for altering PXF\nreadable external tables.", "committedDate": "2020-09-23T00:30:30Z", "type": "commit"}, {"oid": "ba4105151e844d016383f0968ad7ae556b3a6e16", "url": "https://github.com/greenplum-db/pxf/commit/ba4105151e844d016383f0968ad7ae556b3a6e16", "message": "Support dropping columns in PXF writable external tables\n\nPXF does not support dropping columns from writable external tables.\nThis commit adds support for dropping columns in PXF writable external\ntables. The complexity of the implementation lies in the\n`pxfwritable_export` formatter. Add automation tests for altering PXF\nwritable external tables.", "committedDate": "2020-09-23T00:30:37Z", "type": "commit"}, {"oid": "ba4105151e844d016383f0968ad7ae556b3a6e16", "url": "https://github.com/greenplum-db/pxf/commit/ba4105151e844d016383f0968ad7ae556b3a6e16", "message": "Support dropping columns in PXF writable external tables\n\nPXF does not support dropping columns from writable external tables.\nThis commit adds support for dropping columns in PXF writable external\ntables. The complexity of the implementation lies in the\n`pxfwritable_export` formatter. Add automation tests for altering PXF\nwritable external tables.", "committedDate": "2020-09-23T00:30:37Z", "type": "forcePushed"}]}