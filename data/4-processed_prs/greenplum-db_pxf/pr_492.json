{"pr_number": 492, "pr_title": "docs - remove deprecated-in-v5 config props and vars", "pr_createdAt": "2020-11-16T18:19:23Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/492", "timeline": [{"oid": "f036fe24a9a35c7eccdb179563bc3544a6884ae2", "url": "https://github.com/greenplum-db/pxf/commit/f036fe24a9a35c7eccdb179563bc3544a6884ae2", "message": "docs - remove deprecated-in-v5 config props and vars", "committedDate": "2020-11-16T18:11:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDk2MA==", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r524564960", "bodyText": "product question: should we rename these as hive, hive:orc, hive:rc with vectorized being a user option ? @frankgh ?", "author": "denalex", "createdAt": "2020-11-16T20:52:43Z", "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -103,21 +103,21 @@ The PXF Hadoop connectors provide built-in profiles to support the following dat\n \n The PXF Hadoop connectors expose the following profiles to read, and in many cases write, these supported data formats:\n \n-| Data Source | Data Format | Profile Name(s) | Deprecated Profile Name |\n-|-----|------|---------|------------|\n-| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text | HdfsTextSimple |\n-| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) | hdfs:text:multi | HdfsTextMulti |\n-| HDFS | [Avro](hdfs_avro.html) | hdfs:avro | Avro |\n-| HDFS | [JSON](hdfs_json.html) | hdfs:json | Json |\n-| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet | Parquet |\n-| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile | n/a |\n-| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile | SequenceWritable |\n-| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) | n/a |\n-| [Hive](hive_pxf.html) | stored as SequenceFile | Hive | n/a |\n-| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) | n/a |\n-| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC | n/a |\n-| [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n-| [HBase](hbase_pxf.html) | Any | HBase | n/a |\n+| Data Source | Data Format | Profile Name(s) |\n+|-------------|------|---------|\n+| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text |\n+| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) |\n+| HDFS | [Avro](hdfs_avro.html) | hdfs:avro |\n+| HDFS | [JSON](hdfs_json.html) | hdfs:json |\n+| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet |\n+| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile |\n+| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile |\n+| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) |\n+| [Hive](hive_pxf.html) | stored as SequenceFile | Hive |\n+| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) |\n+| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC |", "originalCommit": "f036fe24a9a35c7eccdb179563bc3544a6884ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU3MDYyNQ==", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r524570625", "bodyText": "that makes sense, we wanted to do that at some point, so it makes sense to do it now", "author": "frankgh", "createdAt": "2020-11-16T20:57:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQzODU1Mg==", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r525438552", "bodyText": "are -> were\n?  Maybe, to work for later releases.", "author": "dyozie", "createdAt": "2020-11-17T19:38:18Z", "path": "docs/content/upgrade_5_to_6.html.md.erb", "diffHunk": "@@ -106,7 +106,16 @@ After you install the new version of PXF, perform the following procedure:\n \n 1. **If you are upgrading to PXF version 6.0**:\n \n-    1. step 1\n+    1. Ensure that you no longer reference previously deprecated features that are removed in PXF 6.0:", "originalCommit": "f036fe24a9a35c7eccdb179563bc3544a6884ae2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQ0MDcyNg==", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r525440726", "bodyText": "Not sure why the all-caps property names aren't in code format here.  But if that's the convention used throughout it's fine.", "author": "dyozie", "createdAt": "2020-11-17T19:40:02Z", "path": "docs/content/upgrade_5_to_6.html.md.erb", "diffHunk": "@@ -106,7 +106,16 @@ After you install the new version of PXF, perform the following procedure:\n \n 1. **If you are upgrading to PXF version 6.0**:\n \n-    1. step 1\n+    1. Ensure that you no longer reference previously deprecated features that are removed in PXF 6.0:\n+\n+        | Deprecated Feature | Use Instead |\n+        | -------------------|-------------|\n+        | Hadoop profile names | hdfs:\\<profile> as noted [here](access_hdfs.html#hadoop_connectors) |\n+        | `jdbc.user.impersonation` property | `pxf.service.user.impersonation` property in the [jdbc&#8209;site.xml](jdbc_cfg.html#jdbcimpers) server configuration file |\n+        | PXF_KEYTAB configuration property | `pxf.service.kerberos.keytab` property in the [pxf&#8209;site.xml](cfg_server.html#pxf-site) server configuration file |", "originalCommit": "f036fe24a9a35c7eccdb179563bc3544a6884ae2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "414476ae829e8ae5fa182333652f2ae9d1512e80", "url": "https://github.com/greenplum-db/pxf/commit/414476ae829e8ae5fa182333652f2ae9d1512e80", "message": "use code font for props, are->were", "committedDate": "2020-11-17T19:48:04Z", "type": "commit"}]}