{"pr_number": 6881, "pr_title": "Moving and testing ingest scripts from variantstore", "pr_createdAt": "2020-10-09T19:40:58Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6881", "timeline": [{"oid": "7800601ef8262673e111f76d83d6729c76c7ba2f", "url": "https://github.com/broadinstitute/gatk/commit/7800601ef8262673e111f76d83d6729c76c7ba2f", "message": "Copying files over from variantstore", "committedDate": "2020-10-06T18:00:33Z", "type": "commit"}, {"oid": "6345769746452a63b38f708c8e9d3abaa9957daf", "url": "https://github.com/broadinstitute/gatk/commit/6345769746452a63b38f708c8e9d3abaa9957daf", "message": "some changes", "committedDate": "2020-10-09T19:38:22Z", "type": "commit"}, {"oid": "ab476e37c0d2e4474c79520e4abf49cacb45005b", "url": "https://github.com/broadinstitute/gatk/commit/ab476e37c0d2e4474c79520e4abf49cacb45005b", "message": "forgot to turn the test back on", "committedDate": "2020-10-09T19:41:26Z", "type": "commit"}, {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78", "url": "https://github.com/broadinstitute/gatk/commit/f9d4f77bf68040a568c938e1a976ac4b3e050a78", "message": "fixing test", "committedDate": "2020-10-13T13:37:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUwMjY1MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508502651", "bodyText": "i changed the name of the file from metadata_.tsv to sample_.tsv. it might make it more clear to update this output param. (if you do, also change it in the inputs to LoadArrays)", "author": "ahaessly", "createdAt": "2020-10-20T13:29:25Z", "path": "scripts/variantstore_wdl/ImportArrays.wdl", "diffHunk": "@@ -0,0 +1,222 @@\n+version 1.0\n+\n+workflow ImportArrays {\n+\n+  input {\n+    Array[File] input_vcfs\n+    Array[File]? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+    String project_id\n+    String dataset_name\n+    File raw_schema\n+    File sample_list_schema\n+    #TODO: determine table_id from input sample_map (including looping over multiple table_ids)\n+    Int table_id\n+\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+\n+  scatter (i in range(length(input_vcfs))) {\n+    if (defined(input_metrics)) {\n+      File input_metric = select_first([input_metrics])[i]\n+    }\n+\n+    call CreateImportTsvs {\n+      input:\n+        input_vcf = input_vcfs[i],\n+        input_metrics = input_metric,\n+        probe_info_table = probe_info_table,\n+        probe_info_file = probe_info_file,\n+        sample_map = sample_map,\n+        output_directory = output_directory,\n+        gatk_override = gatk_override,\n+        docker = docker_final,\n+        preemptible_tries = preemptible_tries\n+    }\n+  }\n+\n+  call LoadArrays {\n+    input:\n+      metadata_tsvs = CreateImportTsvs.metadata_tsv,\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      storage_location = output_directory,\n+      table_id = table_id,\n+      raw_schema = raw_schema,\n+      sample_list_schema = sample_list_schema,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+}\n+\n+\n+task CreateImportTsvs {\n+  input {\n+    File input_vcf\n+    File? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+\n+    # runtime\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String docker\n+\n+    String? for_testing_only\n+  }\n+\n+  Int disk_size = ceil(size(input_vcf, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    input_vcf: {\n+      localization_optional: true\n+    }\n+  }\n+  command <<<\n+      set -e\n+\n+      #workaround for https://github.com/broadinstitute/cromwell/issues/3647\n+      export TMPDIR=/tmp\n+      export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+      ~{for_testing_only}\n+\n+      gatk --java-options \"-Xmx2500m\" CreateArrayIngestFiles \\\n+        -V ~{input_vcf} \\\n+        ~{\"-QCF \" + input_metrics} \\\n+        ~{\"--probe-info-file \" + probe_info_file} \\\n+        ~{\"--probe-info-table \" + probe_info_table} \\\n+        -SNM ~{sample_map} \\\n+        --ref-version 37\n+        \n+      gsutil cp sample_*.tsv ~{output_directory}/sample_tsvs/\n+      gsutil cp raw_*.tsv ~{output_directory}/raw_tsvs/\n+  >>>\n+  runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + disk_size + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+  output {\n+      File metadata_tsv = glob(\"sample_*.tsv\")[0]", "originalCommit": "f9d4f77bf68040a568c938e1a976ac4b3e050a78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUxNzU0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508517544", "bodyText": "is false the right default?", "author": "ahaessly", "createdAt": "2020-10-20T13:46:00Z", "path": "scripts/variantstore_wdl/ImportArrays.wdl", "diffHunk": "@@ -0,0 +1,222 @@\n+version 1.0\n+\n+workflow ImportArrays {\n+\n+  input {\n+    Array[File] input_vcfs\n+    Array[File]? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+    String project_id\n+    String dataset_name\n+    File raw_schema\n+    File sample_list_schema\n+    #TODO: determine table_id from input sample_map (including looping over multiple table_ids)\n+    Int table_id\n+\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+\n+  scatter (i in range(length(input_vcfs))) {\n+    if (defined(input_metrics)) {\n+      File input_metric = select_first([input_metrics])[i]\n+    }\n+\n+    call CreateImportTsvs {\n+      input:\n+        input_vcf = input_vcfs[i],\n+        input_metrics = input_metric,\n+        probe_info_table = probe_info_table,\n+        probe_info_file = probe_info_file,\n+        sample_map = sample_map,\n+        output_directory = output_directory,\n+        gatk_override = gatk_override,\n+        docker = docker_final,\n+        preemptible_tries = preemptible_tries\n+    }\n+  }\n+\n+  call LoadArrays {\n+    input:\n+      metadata_tsvs = CreateImportTsvs.metadata_tsv,\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      storage_location = output_directory,\n+      table_id = table_id,\n+      raw_schema = raw_schema,\n+      sample_list_schema = sample_list_schema,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+}\n+\n+\n+task CreateImportTsvs {\n+  input {\n+    File input_vcf\n+    File? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+\n+    # runtime\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String docker\n+\n+    String? for_testing_only\n+  }\n+\n+  Int disk_size = ceil(size(input_vcf, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    input_vcf: {\n+      localization_optional: true\n+    }\n+  }\n+  command <<<\n+      set -e\n+\n+      #workaround for https://github.com/broadinstitute/cromwell/issues/3647\n+      export TMPDIR=/tmp\n+      export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+      ~{for_testing_only}\n+\n+      gatk --java-options \"-Xmx2500m\" CreateArrayIngestFiles \\\n+        -V ~{input_vcf} \\\n+        ~{\"-QCF \" + input_metrics} \\\n+        ~{\"--probe-info-file \" + probe_info_file} \\\n+        ~{\"--probe-info-table \" + probe_info_table} \\\n+        -SNM ~{sample_map} \\\n+        --ref-version 37\n+        \n+      gsutil cp sample_*.tsv ~{output_directory}/sample_tsvs/\n+      gsutil cp raw_*.tsv ~{output_directory}/raw_tsvs/\n+  >>>\n+  runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + disk_size + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+  output {\n+      File metadata_tsv = glob(\"sample_*.tsv\")[0]\n+      File arraydata_tsv = glob(\"raw_*.tsv\")[0] \n+  }\n+}\n+\n+task LoadArrays {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String storage_location\n+    Int table_id\n+    File raw_schema\n+    File sample_list_schema\n+    String load = \"false\"", "originalCommit": "f9d4f77bf68040a568c938e1a976ac4b3e050a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYxNjAxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508616018", "bodyText": "I'll change it to true.", "author": "meganshand", "createdAt": "2020-10-20T15:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUxNzU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3OTQ5OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508579499", "bodyText": "when do you not want to load?", "author": "ahaessly", "createdAt": "2020-10-20T14:54:05Z", "path": "scripts/variantstore_wdl/bq_ingest_arrays.sh", "diffHunk": "@@ -0,0 +1,97 @@\n+#!/usr/bin/env bash\n+set -e\n+\n+if [ $# -lt 5 ]; then\n+  echo \"usage: $0 <project-id> <dataset-name> <storage-location> <table-id> <load> <uuid>\"\n+  exit 1\n+fi\n+\n+PROJECT_ID=$1\n+DATASET_NAME=$2\n+STORAGE_LOCATION=$3\n+TABLE_ID=$4\n+if [ $5 == \"true\" ]; then\n+  LOAD=true\n+else\n+  LOAD=false\n+fi", "originalCommit": "f9d4f77bf68040a568c938e1a976ac4b3e050a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYxNTc4OA==", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508615788", "bodyText": "My thought was that if we end up using Google Data Transfer we need a script that will create the tables but not actually load the data. Ideally we'd add the generation of the Transfers to this script too, but I didn't get around to doing that.\nNow that I look at this though, I think the code for this script is all contained within the WDL and I shouldn't have committed this extra file. I'll add a comment about the Google Data Transfer to the WDL and delete this bash script.", "author": "meganshand", "createdAt": "2020-10-20T15:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3OTQ5OQ=="}], "type": "inlineReview"}, {"oid": "c94004c31ffc356895fa46420ed36b243f3db6b7", "url": "https://github.com/broadinstitute/gatk/commit/c94004c31ffc356895fa46420ed36b243f3db6b7", "message": "addressing comments", "committedDate": "2020-10-20T15:35:17Z", "type": "commit"}]}