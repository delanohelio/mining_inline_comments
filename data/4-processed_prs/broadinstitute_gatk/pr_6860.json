{"pr_number": 6860, "pr_title": "Moving the WDL for importing array manifest to BQ", "pr_createdAt": "2020-10-02T12:39:17Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6860", "timeline": [{"oid": "2256c037384dfc26783834008154bed5e72c5d1f", "url": "https://github.com/broadinstitute/gatk/commit/2256c037384dfc26783834008154bed5e72c5d1f", "message": "Copying wdl from variantstore repo", "committedDate": "2020-10-01T12:35:24Z", "type": "commit"}, {"oid": "feeaa1cb1dee552ad6966455524aee222a98cf71", "url": "https://github.com/broadinstitute/gatk/commit/feeaa1cb1dee552ad6966455524aee222a98cf71", "message": "Adding tests and changes to WDL", "committedDate": "2020-10-02T00:09:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzIyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r498917226", "bodyText": "id++ should be changed to ++id\nthis was in a recent PR of mine, but the rest looks correct.\nEventually we should add a test that verifies 1. there is no probe 0 and 2. that 2 different runs to this code yield the same probe->id map (these were both bugs in the past. and don't feel like you have to do that for this PR)", "author": "ahaessly", "createdAt": "2020-10-02T16:12:51Z", "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+ \n+  call CreateManifestCsv {\n+    input:\n+      extended_manifest_csv = extended_manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+\n+  call LoadManifest {\n+    input:\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      table_name = table_name,\n+      manifest_schema_json = manifest_schema_json,\n+      manifest_csv = CreateManifestCsv.manifest_csv,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+  output {\n+    File manifest_csv = CreateManifestCsv.manifest_csv\n+    File manifest_ingest_csv = CreateManifestCsv.manifest_ingest_csv\n+    File manifest_sub_csv = CreateManifestCsv.manifest_sub_csv\n+    File manifest_proc_csv = CreateManifestCsv.manifest_proc_csv\n+  }\n+}\n+\n+task LoadManifest {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+    File manifest_csv\n+    File manifest_schema_json\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+    # String to add command for testing only. Can be ignored otherwise.\n+    String? for_testing_only\n+  }\n+\n+  String ingest_table = dataset_name + \".\" + select_first([table_name, \"probe_info\"])\n+\n+  parameter_meta {\n+    manifest_schema_json: {\n+      localization_optional: false\n+    }\n+  }\n+   \n+    command <<<\n+      set +e\n+      ~{for_testing_only}\n+      bq ls --project_id ~{project_id} ~{dataset_name} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making dataset ~{project_id}.~{dataset_name}\"\n+        bq mk --project_id=~{project_id} ~{dataset_name}\n+      fi\n+      bq show --project_id ~{project_id} ~{ingest_table} > /dev/null\n+      if [ $? -ne 0 ]; then\n+        echo \"making table ~{ingest_table}\"\n+        # create a site info table and load - schema and TSV header need to be the same order\n+        bq --location=US mk --project_id=~{project_id} ~{ingest_table} ~{manifest_schema_json}\n+      fi\n+      set -e\n+\n+      bq load --location=US --project_id=~{project_id} --null_marker \"null\" --source_format=CSV ~{ingest_table} ~{manifest_csv} ~{manifest_schema_json}\n+    >>>\n+    runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + 20 + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+\n+}\n+\n+task CreateManifestCsv {\n+  input {\n+    File extended_manifest_csv\n+\n+    # runtime\n+    Int? preemptible_tries\n+    String docker\n+  }\n+\n+  Int disk_size = ceil(size(extended_manifest_csv, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    extended_manifest_csv: {\n+      localization_optional: false\n+    }\n+  }\n+  command <<<\n+    set -e\n+\n+    TMP_SORTED=\"manifest_ingest_sorted.csv\"\n+    TMP_SUB=\"manifest_ingest_sub.csv\"\n+    TMP_PROC=\"manifest_ingest_processed.csv\"\n+    TMP=\"manifest_ingest.csv\"\n+\n+    # put integers in front of the chromosomes that are not numbered so that they end up ordered by X, Y and MT\n+    sed 's/,X,/,23X,/g; s/,Y,/,24Y,/g; s/,MT,/,25MT,/g' ~{extended_manifest_csv} > $TMP_SUB\n+\n+    # sort the probes by chrom, position and then name so there is a specific ordering when we assign integers\n+    sort -t , -k23n,23 -k24n,24 -k2,2 $TMP_SUB > $TMP_SORTED\n+\n+    # checking for != \"build37Flag\" skips the header row (we don't want that numbered)\n+    # only process rows with 29 fields - this skips some header info fields\n+    # also skip entries that are flagged, not matched or have index conflict\n+    awk -F ',' 'NF==29 && ($29!=\"ILLUMINA_FLAGGED\" && $29!=\"INDEL_NOT_MATCHED\" && $29!=\"INDEL_CONFLICT\" && $29!=\"build37Flag\") { flag=$29; if ($29==\"PASS\") flag=\"null\"; print id++\",\"$2\",\"$9\",\"$23\",\"$24\",\"$25\",\"$26\",\"$27\",\"flag }' $TMP_SORTED > $TMP_PROC", "originalCommit": "feeaa1cb1dee552ad6966455524aee222a98cf71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk1Mzg3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r498953870", "bodyText": "Definitely considering it a TODO to add tests that actually make sure the data that was imported is correct. I haven't figured out the best way to do this though. Maybe I need a GATK tool that pulls down a table and compares to an expected file? Or maybe by looking at the extracted VCF from the full \"end-to-end\" test we'll have enough coverage? I'm not sure if that part of the pipeline pulls down information in the manifest.", "author": "meganshand", "createdAt": "2020-10-02T17:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzIyNg=="}], "type": "inlineReview"}, {"oid": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a", "url": "https://github.com/broadinstitute/gatk/commit/df29cd95aeb3322ff7dbe78d29f28529217f4b0a", "message": "addressing comments", "committedDate": "2020-10-02T17:23:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyNjIxNA==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499626214", "bodyText": "Is this just copied from elsewhere?  Seems like there should be a gatk-test google project...", "author": "kcibul", "createdAt": "2020-10-05T14:08:33Z", "path": "scripts/variantstore_cromwell_tests/import_array_manifest_test.json", "diffHunk": "@@ -0,0 +1,8 @@\n+{\n+  \"ImportArrayManifest.extended_manifest_csv\":\"/home/travis/build/broadinstitute/gatk/src/test/resources/org/broadinstitute/hellbender/tools/variantdb/arrays/tiny_manifest.csv\",\n+  \"ImportArrayManifest.manifest_schema_json\":\"/home/travis/build/broadinstitute/gatk/scripts/variantstore_wdl/schemas/manifest_schema.json\",\n+  \"ImportArrayManifest.project_id\":\"broad-dsde-dev\",", "originalCommit": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMjU5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499632592", "bodyText": "This is the GATK test project AFAIK. It's used in the BigQueryUtils tests (in GATK).", "author": "meganshand", "createdAt": "2020-10-05T14:17:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyNjIxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMDg2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499630866", "bodyText": "Are you overriding this?  Or are we always testing with this static GATK version?", "author": "kcibul", "createdAt": "2020-10-05T14:15:11Z", "path": "scripts/variantstore_wdl/ImportArrayManifest.wdl", "diffHunk": "@@ -0,0 +1,150 @@\n+version 1.0\n+\n+workflow ImportArrayManifest {\n+\n+  input {\n+    File extended_manifest_csv\n+    File manifest_schema_json\n+    String project_id\n+    String dataset_name\n+    String? table_name\n+ \n+    Int? preemptible_tries\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])", "originalCommit": "df29cd95aeb3322ff7dbe78d29f28529217f4b0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMzk0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6860#discussion_r499633946", "bodyText": "I'm not overriding it in this case because we're not using any GATK tools in this WDL. But maybe we should still be testing the current branch regardless? I know we'll need to for future WDLs (Ingest, calculate metrics, and extract will all use GATK tools that will need to be in the docker from the current branch)", "author": "meganshand", "createdAt": "2020-10-05T14:19:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzMDg2Ng=="}], "type": "inlineReview"}, {"oid": "4359bdab65a45349d6c468c0f090678f620cf50c", "url": "https://github.com/broadinstitute/gatk/commit/4359bdab65a45349d6c468c0f090678f620cf50c", "message": "adding readme", "committedDate": "2020-10-05T17:25:37Z", "type": "commit"}]}