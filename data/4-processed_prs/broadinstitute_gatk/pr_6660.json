{"pr_number": 6660, "pr_title": "Update for funcotator data sources", "pr_createdAt": "2020-06-16T14:13:48Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6660", "timeline": [{"oid": "c769d2193f94c5803e7b9f6d59896b93ee599763", "url": "https://github.com/broadinstitute/gatk/commit/c769d2193f94c5803e7b9f6d59896b93ee599763", "message": "Fixed issue with dbSNP source data for hg38.\n\nCode updates:\n- Now both hg19 and hg38 have the contig names translated to `chr__`\n- Added 'lncRNA' to GeneTranscriptType.\n- Added \"TAGENE\" gene tag.\n- Added the MANE_SELECT tag to FeatureTag.\n- Added the STOP_CODON_READTHROUGH tag to FeatureTag.\n- Updated the GTF versions that are parseable.\n- Fixed a parsing error with new versions of gencode and the remap\npositions (for liftover files).\n- Added test for indexing new lifted over gencode GTF.\n- Added Gencode_34 entries to MAF output map.\n- Minor changes to FuncotatorIntegrationTest.java for code syntax.\n- Pointed data source downloader at new data sources URL.\n- Minor updates to workflows to point at new data sources.\n\nScript updates:\n- Updated retrieval scripts for dbSNP and Gencode.\n- Added required field to gencode config file generation.\n- Now gencode retrieval script enforces double hash comments at\ntop of gencode GTF files.\n\nBug Fixes:\nRemoving erroneous trailing tab in MAF file output.\n\n- Fixes #6693", "committedDate": "2020-07-16T19:34:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDg4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459650889", "bodyText": "Couldn't you download the file without modification, verify the MD5, and then change the contig names after verification?", "author": "droazen", "createdAt": "2020-07-23T18:37:12Z", "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -154,57 +154,60 @@ function downloadAndVerifyVcfFiles() {\n \n     curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n \n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n+    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n \n     echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n     wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n \n     # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n     echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n+    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n+\n+\t\t# We can no longer verify the MD5sum because we have to change the file contents.\n+\t\t# Uncomment the following block when we can once again verify contents:", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE0Mjc5Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467142797", "bodyText": "Yeah - that's fair.", "author": "jonn-smith", "createdAt": "2020-08-07T16:25:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE0NTE2Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467145163", "bodyText": "Though on second thought, it will only work if the person running this script has twice the space needed for dbSNP - one for the original copy and one for the transformed copy.\nThese files are pretty big (15gb each) so if you wanted to get both hg19 and hg38 you'd need 60gb free to do this.\nIt might be a moot point since we refresh the files infrequently, but that's a lot of space.", "author": "jonn-smith", "createdAt": "2020-08-07T16:30:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE1Njk5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481156990", "bodyText": "OK - added in md5sum validation.", "author": "jonn-smith", "createdAt": "2020-09-01T13:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MTczMA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459651730", "bodyText": "Is sed really the best mechanism to do this transformation? Is there a more robust option we could consider (like a liftover file)?", "author": "droazen", "createdAt": "2020-07-23T18:38:50Z", "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -154,57 +154,60 @@ function downloadAndVerifyVcfFiles() {\n \n     curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n \n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n+    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n \n     echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n     wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n \n     # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n     echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n+    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE0MjcyNw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467142727", "bodyText": "A liftover seemed like overkill to me.  The only difference here is the contig names, rather than any actual content, so I think using sed is appropriate.", "author": "jonn-smith", "createdAt": "2020-08-07T16:25:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MTczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzYzOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459653639", "bodyText": "This script has a lot of arguments -- should ideally document them in a comment at the top.", "author": "droazen", "createdAt": "2020-07-23T18:42:09Z", "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -89,9 +88,10 @@ function createConfigFile() {\n \n     local dataSourceName=$1\n     local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-    local fastaPath=$5\n+    local refVersion=$3\n+    local srcFile=$4\n+    local originLocation=$5\n+    local fastaPath=$6", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1MjMzNw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467152337", "bodyText": "No problem.  I'll add in comments.", "author": "jonn-smith", "createdAt": "2020-08-07T16:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NDkyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459654926", "bodyText": "Your indentation is a bit off in several places -- are you mixing spaces and tabs perhaps?", "author": "droazen", "createdAt": "2020-07-23T18:44:26Z", "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE0MzI5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467143292", "bodyText": "Yeah - I made changes in a few different programs that have their own settings for tabs / spaces.", "author": "jonn-smith", "createdAt": "2020-08-07T16:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NDkyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjA5OA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656098", "bodyText": "Are you guaranteed to have exactly 5 lines of header?", "author": "droazen", "createdAt": "2020-07-23T18:46:24Z", "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n+\t\telse\n+\t\t\tfileRefVersion=\"${version}lift37\"\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl}\n+\t\t\twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n+\t\tfi\n+\t\t\n+    gunzip gencode.v${fileRefVersion}.annotation.gtf.gz\n+    gunzip gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n \n     # We must fix the information in the gencode gtf file:\n     echo \"Reordering Gencode GTF data ...\"\n-    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${version}.annotation.gtf > gencode.v${version}.annotation.REORDERED.gtf\n+    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${fileRefVersion}.annotation.gtf | sed -e '1,5s$^#\\([a-z]\\)$##\\1$' > gencode.v${fileRefVersion}.annotation.REORDERED.gtf", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1Mzc2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467153764", "bodyText": "By the spec no, but in practice this is what I've always seen.\nThis is really just applying that sed expression over the first 5 lines anyway and if those lines have single and not double comments it adds a second # in there.  I'll expand the range on the sed command to be safe (and it won't affect lines that aren't commented).", "author": "jonn-smith", "createdAt": "2020-08-07T16:47:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjU3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656579", "bodyText": "Do you create the .dict file as well?", "author": "droazen", "createdAt": "2020-07-23T18:47:18Z", "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n+\t\telse\n+\t\t\tfileRefVersion=\"${version}lift37\"\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl}\n+\t\t\twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n+\t\tfi\n+\t\t\n+    gunzip gencode.v${fileRefVersion}.annotation.gtf.gz\n+    gunzip gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n \n     # We must fix the information in the gencode gtf file:\n     echo \"Reordering Gencode GTF data ...\"\n-    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${version}.annotation.gtf > gencode.v${version}.annotation.REORDERED.gtf\n+    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${fileRefVersion}.annotation.gtf | sed -e '1,5s$^#\\([a-z]\\)$##\\1$' > gencode.v${fileRefVersion}.annotation.REORDERED.gtf\n \n     # Clean up original file:\n-    rm gencode.v${version}.annotation.gtf\n+    rm gencode.v${fileRefVersion}.annotation.gtf\n \n     echo \"Creating config file ...\"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" \"gencode.v${version}.annotation.REORDERED.gtf\" \"ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\" \"gencode.v${version}.pc_transcripts.fa\" > gencode.config\n+    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" \"${refVersion}\" \"gencode.v${fileRefVersion}.annotation.REORDERED.gtf\" \"${sourceUrl}\" \"gencode.v${fileRefVersion}.pc_transcripts.fa\" > gencode.config\n \n     if $HAS_SAMTOOLS ; then\n-        echo \"Indexing Fasta File: gencode.v${version}.pc_transcripts.fa\"\n-        samtools faidx gencode.v${version}.pc_transcripts.fa\n+        echo \"Indexing Fasta File: gencode.v${fileRefVersion}.pc_transcripts.fa\"\n+        samtools faidx gencode.v${fileRefVersion}.pc_transcripts.fa", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1NDQ2Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467154467", "bodyText": "No.  I print to the user at the end that they need to make their own dict and fasta index file (lines 252-274).\nI don't have a way of guaranteeing that the GATK is on their path to create the sequence dictionary (or samtools for that matter).", "author": "jonn-smith", "createdAt": "2020-08-07T16:48:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656964", "bodyText": "Perhaps the base directory should be a script argument instead of hardcoded to your home directory? Is this meant to be used by anyone other than yourself?", "author": "droazen", "createdAt": "2020-07-23T18:47:57Z", "path": "scripts/funcotator/testing/testFuncotator.sh", "diffHunk": "@@ -56,6 +57,7 @@ MANUAL_MODE=false\n ################################################################################\n \n # Change this to point to your funcotator data sources folder:\n+DATA_SOURCES_PATH_16=/Users/jonn/Development/funcotator_dataSources.v1.6.20190124s\n DATA_SOURCES_PATH=/Users/jonn/Development/funcotator_dataSources_latest\n DATA_SOURCES_PATH_GERMLINE=/Users/jonn/Development/funcotator_dataSources_germline_latest", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1NTYwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467155609", "bodyText": "This is in here so that I can have a consistent script to test with.  Way back when I originally checked it in, Lee suggested that I add in the comment above.  The datasource paths I have in here are long and it's a little painful to type everything in correctly each time.\nNo one other than myself has ever run this, but it is a useful script.\nI can add in warning messages if the paths / files don't exist.  @droazen What do you think?", "author": "jonn-smith", "createdAt": "2020-08-07T16:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMjkwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471712905", "bodyText": "At least declare /Users/jonn/Development/ as a named constant at the top of the script, so that someone else could potentially make the script work by editing only a single line.", "author": "droazen", "createdAt": "2020-08-17T18:58:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEyMTIwNA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481121204", "bodyText": "OK - fixed!", "author": "jonn-smith", "createdAt": "2020-09-01T13:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTg5OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459661899", "bodyText": "What's the motivation behind this IndexFeatureFile test to index this specific gencode file? Can you add a code comment with an explanation?", "author": "droazen", "createdAt": "2020-07-23T18:56:52Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/IndexFeatureFileIntegrationTest.java", "diffHunk": "@@ -490,4 +492,19 @@ public void testEnsemblGtfIndexQuery(final SimpleInterval interval,\n             }\n         }\n     }\n+\n+    @Test\n+    public void testNewGencodeLiftoverGtfFile() {\n+        // First ensure that we can index the file:\n+        // Required Args:\n+        final ArgumentsBuilder arguments = new ArgumentsBuilder();\n+\n+        final File output = createTempFile(GENCODE_NEW_LIFTOVER_FILE.getName(), \".idx\");\n+\n+        arguments.addInput(GENCODE_NEW_LIFTOVER_FILE);\n+        arguments.addOutput(output.getAbsolutePath());\n+\n+        // Run the beast:\n+        runCommandLine(arguments);\n+    }", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1ODE2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467158162", "bodyText": "The latest gencode files for hg19 have slightly different header information and formatting\nthan previous versions (first noticed for gencode v34).  This is because they are lifting over the hg38 version to hg19.  This test is designed to ensure that such a file can still be indexed with the gencode codec.\nI'll add this info to the test.", "author": "jonn-smith", "createdAt": "2020-08-07T16:56:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTg5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzAxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459663016", "bodyText": "Can you add new Funcotator tests to cover the fixes in this branch (the hg38 dbsnp contig names fix and the MAF tab fix)?", "author": "droazen", "createdAt": "2020-07-23T18:59:00Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorIntegrationTest.java", "diffHunk": "@@ -1689,7 +1691,6 @@ public void testEColiFuncotations() {\n         arguments.add(FuncotatorArgumentDefinitions.TRANSCRIPT_SELECTION_MODE_LONG_NAME, TranscriptSelectionMode.CANONICAL.toString());\n         runCommandLine(arguments);\n         assertEqualVariantFiles(outputFile, E_COLI_EXPECTED_OUT);\n-\n     }", "originalCommit": "54ff67a5555d4b0ca9074071c8c10658edfb3a46", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MTE2OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467161169", "bodyText": "I'll add one in for the MAF trailing tab fix.\nI can add in a trivial test for hg38 dbsnp with the updated contig names, but I'm not sure it adds much value - I'd just be using an excerpt of that file to create funcotations.  The issue for that was really in the datasource as opposed to the code, so while adding in the test would show that the latest lifted over file works, it doesn't actually reflect the data source in the package we provide.  That is, this test would need to be updated every time we update the dbSNP data source so we have an excerpt from it here.  I think what makes more sense is to refresh the test datasource for dbSNP with the latest version - then the large scale tests can make sure that it works properly.  In fact, this should be done every time we update the data sources, but I think it's beyond the scope of this fix.\n@droazen What do you think?", "author": "jonn-smith", "createdAt": "2020-08-07T17:02:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzAxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3NDEwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r467174107", "bodyText": "Actually, rather than adding in a test for the trailing tab thing, I'm adding a new overload for IntegrationTestSpec::assertEqualTextFiles to allow for whitespace to not be ignored.  Were it not for this, this trailing tab issue would have been found a long time ago.", "author": "jonn-smith", "createdAt": "2020-08-07T17:28:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwNzU3Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471707572", "bodyText": "This comment appears to be obsolete, since as far as I can tell you are now always verifying the checksum, aren't you?", "author": "droazen", "createdAt": "2020-08-17T18:47:47Z", "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:", "originalCommit": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEyMjAxNA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481122014", "bodyText": "Correct - this was a leftover from before.\nFixed!", "author": "jonn-smith", "createdAt": "2020-09-01T13:05:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwNzU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwODEwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471708105", "bodyText": "You should consider doing the sed operation in-place (sed -i) rather than writing to a temp file, to cut down on disk usage.", "author": "droazen", "createdAt": "2020-08-17T18:48:56Z", "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n+\techo \"${indentSpace}Verifying VCF checksum ...\"\n+\tif [[ \"$(uname)\" == \"Darwin\" ]] ; then\n+\t\twhich md5sum-lite &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n+\t\tfi\n+\telse\n+\t\twhich md5sum &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n+\t\tfi\n+\tfi\n+\n+\t# Now change the contigs in the file:\n+\tcat ${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${tmpVcfFile} ", "originalCommit": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwOTM1Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471709357", "bodyText": "Also, I reiterate my concern that the sed approach overall is a bit dangerous/brittle. What if there happened to be alt contigs that began with a digit, or the letter X or Y? Have you checked that there are no such contigs in either hg19 or hg38?", "author": "droazen", "createdAt": "2020-08-17T18:51:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwODEwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEyMzIxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481123212", "bodyText": "sed -i behaves differently on linux and OSX so I try to avoid using it in scripts.  In this case, I need to gunzip the compressed VCF, so I can't really edit it in place anyway.\nYeah - I misunderstood what you meant before.  I'll add in anchors to the regex to ensure that only the correct contigs are changed.", "author": "jonn-smith", "createdAt": "2020-09-01T13:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwODEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMDAzMA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471710030", "bodyText": "Why only delete the md5 file when we're not on hg19?", "author": "droazen", "createdAt": "2020-08-17T18:52:34Z", "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n+\techo \"${indentSpace}Verifying VCF checksum ...\"\n+\tif [[ \"$(uname)\" == \"Darwin\" ]] ; then\n+\t\twhich md5sum-lite &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n+\t\tfi\n+\telse\n+\t\twhich md5sum &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n+\t\tfi\n+\tfi\n+\n+\t# Now change the contigs in the file:\n+\tcat ${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${tmpVcfFile} \n+\tmv ${tmpVcfFile} ${vcfFile}\n+\n+\t# Now put it in the right place and clean up:\n+\techo \"${indentSpace}Creating output directory ...\"\n+\tmkdir -p ${outputFolder}\n+\n+\techo \"${indentSpace}Moving files to output directory ...\"\n+\tmv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n+\tif [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n+\t\tmv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n+\t\trm ${md5File}", "originalCommit": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEyODY3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481128670", "bodyText": "Good catch.\nBefore it was because the md5 file wouldn't be there since we didn't verify the hashes.  Now it should be removed for both.", "author": "jonn-smith", "createdAt": "2020-09-01T13:16:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMDAzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNDQzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471714432", "bodyText": "Add a comment about not ignoring trailing whitespace here.", "author": "droazen", "createdAt": "2020-08-17T19:00:58Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/mafOutput/MafOutputRendererUnitTest.java", "diffHunk": "@@ -1163,8 +1163,9 @@ public void testWrite(final List<VariantContext> variants, final List<List<Funco\n             }\n         }\n \n+        // Make sure our files are as we expect them to be:\n         try {\n-            IntegrationTestSpec.assertEqualTextFiles(outFile, expectedFile, \"#\");\n+            IntegrationTestSpec.assertEqualTextFiles(outFile, expectedFile, MafOutputRendererConstants.COMMENT_STRING, false);", "originalCommit": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEyOTcyNA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481129724", "bodyText": "\ud83d\udc4d", "author": "jonn-smith", "createdAt": "2020-09-01T13:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNDQzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxODU3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471718574", "bodyText": "You still need to add some kind of Funcotator test to cover the fix for dbsnp. Anything reasonable that fails without the fix and passes with it is fine, provided that it covers the actual files published in the latest datasource release. How about a cloud test (@Test(groups={\"cloud\"})) that does 4 small queries on the actual hosted dbsnp VCF from the latest Funcotator datasources in GCS: a tiny interval on each of chr1, chrM, chrX, and chrY. The test could just assert that you get a certain expected number of records from each query. You can do this trivially using a FeatureDataSource.", "author": "droazen", "createdAt": "2020-08-17T19:08:53Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorIntegrationTest.java", "diffHunk": "@@ -498,7 +498,7 @@ private ArgumentsBuilder createBaselineArgumentsForFuncotator(final String varia\n     // DO NOT ADD THIS TO ANY TEST GROUPS!", "originalCommit": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEzMDIzNg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r481130236", "bodyText": "Sounds good to me.", "author": "jonn-smith", "createdAt": "2020-09-01T13:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxODU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTU5NA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485819594", "bodyText": "These can't be final? Also, generally when something is @VisibleForTesting, the test class is in the same package and so default/package access is sufficient rather than public access.", "author": "droazen", "createdAt": "2020-09-09T18:14:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/funcotator/BaseFuncotatorArgumentCollection.java", "diffHunk": "@@ -22,9 +23,11 @@\n      * This variable is necessary to resolve the differences between b37 and hg19 when\n      * dealing with Homo Sapiens samples.\n      */\n-    protected static String FuncotatorReferenceVersionHg19 = \"hg19\";\n+    @VisibleForTesting\n+    public static String FuncotatorReferenceVersionHg19 = \"hg19\";\n     /** String representing the hg38 version of the homo sapiens reference. */\n-    protected static String FuncotatorReferenceVersionHg38 = \"hg38\";\n+    @VisibleForTesting\n+    public static String FuncotatorReferenceVersionHg38 = \"hg38\";", "originalCommit": "b92ecb99e79116699512e3f34daf7585145965dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgzMjYyNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485832625", "bodyText": "Oops.  They definitely should be final.  Should have done that when I originally implemented this.\nUnfortunately they cannot be package-private.  DbSnpIntegrationTest (org.broadinstitute.hellbender.tools.funcotator.dataSource) is not in the same package as BaseFuncotatorArgumentCollection (org.broadinstitute.hellbender.tools.funcotator) so needs the extra visibility.", "author": "jonn-smith", "createdAt": "2020-09-09T18:37:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTkxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485819916", "bodyText": "Same question: can these be final and package access instead of public?", "author": "droazen", "createdAt": "2020-09-09T18:14:35Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorDataSourceDownloader.java", "diffHunk": "@@ -71,17 +71,17 @@\n     //==================================================================================================================\n     // Private Static Members:\n \n-    private static String BASE_URL = \"gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124\";\n+    private static String BASE_URL = \"gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521\";\n \n     private static String GERMLINE_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"g\";\n     @VisibleForTesting\n     static Path   GERMLINE_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(GERMLINE_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");\n     @VisibleForTesting\n     static Path   GERMLINE_GCLOUD_DATASOURCES_SHA256_PATH = IOUtils.getPath(GERMLINE_GCLOUD_DATASOURCES_BASEURL + \".sha256\");\n \n-    private static String SOMATIC_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"s\";\n-    @VisibleForTesting\n-    static Path   SOMATIC_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(SOMATIC_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");\n+    public static String SOMATIC_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"s\";\n+\n+    public static Path   SOMATIC_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(SOMATIC_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");", "originalCommit": "b92ecb99e79116699512e3f34daf7585145965dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg0MzQwNA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485843404", "bodyText": "A lot of fields here can be made final, but the same package issue exists here as in the above case.", "author": "jonn-smith", "createdAt": "2020-09-09T18:57:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTkxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNjg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485826881", "bodyText": "Open this FeatureDataSource in a try-with-resources block to ensure that it gets closed.", "author": "droazen", "createdAt": "2020-09-09T18:27:09Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());", "originalCommit": "b92ecb99e79116699512e3f34daf7585145965dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgzNDI5NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485834295", "bodyText": "Yup.  Fixed!", "author": "jonn-smith", "createdAt": "2020-09-09T18:40:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNjg4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzM4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485827386", "bodyText": "New test looks good otherwise \ud83d\udc4d", "author": "droazen", "createdAt": "2020-09-09T18:27:56Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());\n+\n+        // Do a dummy check here:\n+        Assert.assertNotNull(dbSnpDataSource);\n+\n+        // 3 - Attempt to read sites and features from the FeatureDataSource:\n+        final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+        Assert.assertEquals(features.size(), 1);\n+\n+        final VariantContext dbSnpVariant = features.get(0);\n+        Assert.assertEquals(dbSnpVariant.getContig(), interval.getContig());\n+        Assert.assertEquals(dbSnpVariant.getStart(), interval.getStart());\n+        Assert.assertEquals(dbSnpVariant.getEnd(), interval.getEnd());\n+        Assert.assertEquals(dbSnpVariant.getID(), expectedID);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().size(), 2);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(0), expectedRefAllele, \"Variant has incorrect ref allele: \" + dbSnpVariant.getAlleles().get(0)  + \" != \" + expectedRefAllele + \" [\" + interval + \" in \" + dbSnpFile + \"]\");\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(1), expectedAltAllele);\n+    }", "originalCommit": "b92ecb99e79116699512e3f34daf7585145965dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgzNDM4Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485834383", "bodyText": "Cool.  Thanks!", "author": "jonn-smith", "createdAt": "2020-09-09T18:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzcyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485827723", "bodyText": "Remove commented-out code", "author": "droazen", "createdAt": "2020-09-09T18:28:33Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());\n+\n+        // Do a dummy check here:\n+        Assert.assertNotNull(dbSnpDataSource);\n+\n+        // 3 - Attempt to read sites and features from the FeatureDataSource:\n+        final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+        Assert.assertEquals(features.size(), 1);\n+\n+        final VariantContext dbSnpVariant = features.get(0);\n+        Assert.assertEquals(dbSnpVariant.getContig(), interval.getContig());\n+        Assert.assertEquals(dbSnpVariant.getStart(), interval.getStart());\n+        Assert.assertEquals(dbSnpVariant.getEnd(), interval.getEnd());\n+        Assert.assertEquals(dbSnpVariant.getID(), expectedID);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().size(), 2);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(0), expectedRefAllele, \"Variant has incorrect ref allele: \" + dbSnpVariant.getAlleles().get(0)  + \" != \" + expectedRefAllele + \" [\" + interval + \" in \" + dbSnpFile + \"]\");\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(1), expectedAltAllele);\n+    }\n+\n+\n+////    @Test(groups={\"cloud\"})\n+//    @Test\n+//    public void testDbSnpDataSourceParsing() {\n+//        // 1 - Extract the dbSNP file from the current datasources for Funcotator:\n+//        logger.info(\"Creating input stream from gcloud file:\");\n+//        try (final InputStream dataSourcesInputStream = new BufferedInputStream(Files.newInputStream(DB_SNP_FILE_NAME))) {\n+//            final Path dbSnpPath = extractAndReturnDbSnpPath(dataSourcesInputStream);\n+//\n+//            // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+//            final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpPath.toUri().toString());\n+//\n+//            // Do a dummy check here:\n+//            Assert.assertNotNull(dbSnpDataSource);\n+//\n+//            // 3 - Attempt to read sites and features from the FeatureDataSource that would fail with the old code:\n+//            final List<Locatable> intervalsToQuery = Arrays.asList(\n+//                    new SimpleInterval(\"chr1\", 84349784, 84349786), // rs17131617 T/C\n+//                    new SimpleInterval(\"chrX\", 80688069, 80688071), // rs3122407 T/C\n+//                    new SimpleInterval(\"chrY\", 13355943, 13355945), // rs2032654 A/G\n+//                    new SimpleInterval(\"chrM\", 5131, 5134)    // rs199476116 2-BP DEL, 5132AA\n+//            );\n+//\n+//            for (int i = 0; i < intervalsToQuery.size(); ++i) {\n+//                final Locatable interval = intervalsToQuery.get(i);\n+//                final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+//\n+//                Assert.assertEquals(features.size(), 1);\n+//            }\n+//        }\n+//        catch (final IOException ex) {\n+//            throw new UserException(\"Unable to open data sources from gcloud!\", ex);\n+//        }\n+//    }", "originalCommit": "b92ecb99e79116699512e3f34daf7585145965dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgzNDY3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485834674", "bodyText": "Oops.  Good catch.\nFixed!", "author": "jonn-smith", "createdAt": "2020-09-09T18:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzcyMw=="}], "type": "inlineReview"}, {"oid": "f910b260717c8b8a1d2da6092bff7e1aede78961", "url": "https://github.com/broadinstitute/gatk/commit/f910b260717c8b8a1d2da6092bff7e1aede78961", "message": "Fixed issue with dbSNP source data for hg38.\n\nCode updates:\n- Now both hg19 and hg38 have the contig names translated to `chr__`\n- Added 'lncRNA' to GeneTranscriptType.\n- Added \"TAGENE\" gene tag.\n- Added the MANE_SELECT tag to FeatureTag.\n- Added the STOP_CODON_READTHROUGH tag to FeatureTag.\n- Updated the GTF versions that are parseable.\n- Fixed a parsing error with new versions of gencode and the remap\npositions (for liftover files).\n- Added test for indexing new lifted over gencode GTF.\n- Added Gencode_34 entries to MAF output map.\n- Minor changes to FuncotatorIntegrationTest.java for code syntax.\n- Pointed data source downloader at new data sources URL.\n- Minor updates to workflows to point at new data sources.\n\nScript updates:\n- Updated retrieval scripts for dbSNP and Gencode.\n- Added required field to gencode config file generation.\n- Now gencode retrieval script enforces double hash comments at\ntop of gencode GTF files.\n\nBug Fixes:\nRemoving erroneous trailing tab in MAF file output.\n\n- Fixes #6693", "committedDate": "2020-09-10T14:53:14Z", "type": "commit"}, {"oid": "33b1e51397ab7e4400a2056acda5172bd6965be7", "url": "https://github.com/broadinstitute/gatk/commit/33b1e51397ab7e4400a2056acda5172bd6965be7", "message": "Adding missing resource file.", "committedDate": "2020-09-10T14:53:14Z", "type": "commit"}, {"oid": "de266c79f528014909a296b651913c71489483a8", "url": "https://github.com/broadinstitute/gatk/commit/de266c79f528014909a296b651913c71489483a8", "message": "Addressing code review comments.", "committedDate": "2020-09-10T14:53:15Z", "type": "commit"}, {"oid": "087c6c8e35fad15a6c8f7e62047b1ef321b4628b", "url": "https://github.com/broadinstitute/gatk/commit/087c6c8e35fad15a6c8f7e62047b1ef321b4628b", "message": "Addressing code review comments.  Need to fix test.", "committedDate": "2020-09-10T14:53:15Z", "type": "commit"}, {"oid": "6b0b144c9badf44389a0521a878b753fb9ea5d01", "url": "https://github.com/broadinstitute/gatk/commit/6b0b144c9badf44389a0521a878b753fb9ea5d01", "message": "Added tests for accessing / reading from hg38 dbSNP.", "committedDate": "2020-09-10T14:53:16Z", "type": "commit"}, {"oid": "1906b06fb57e169a43dbbc18d1248eba0bc18ca5", "url": "https://github.com/broadinstitute/gatk/commit/1906b06fb57e169a43dbbc18d1248eba0bc18ca5", "message": "Adding final tests for dbSNP.", "committedDate": "2020-09-10T14:53:16Z", "type": "commit"}, {"oid": "fe7f0c4470501f8e0437d94ee9669f997ef40e2d", "url": "https://github.com/broadinstitute/gatk/commit/fe7f0c4470501f8e0437d94ee9669f997ef40e2d", "message": "Minor test fixes.", "committedDate": "2020-09-10T14:53:17Z", "type": "commit"}, {"oid": "99711688bb1a6c7e7bfbac7a4fa3ccd1fb904386", "url": "https://github.com/broadinstitute/gatk/commit/99711688bb1a6c7e7bfbac7a4fa3ccd1fb904386", "message": "Finishing the last round of comment responses.", "committedDate": "2020-09-10T14:53:17Z", "type": "commit"}, {"oid": "be7daea9872500bec7decd7abb14abfc850a8425", "url": "https://github.com/broadinstitute/gatk/commit/be7daea9872500bec7decd7abb14abfc850a8425", "message": "Missed a comment.  Actually last commit.", "committedDate": "2020-09-10T14:53:17Z", "type": "commit"}, {"oid": "be7daea9872500bec7decd7abb14abfc850a8425", "url": "https://github.com/broadinstitute/gatk/commit/be7daea9872500bec7decd7abb14abfc850a8425", "message": "Missed a comment.  Actually last commit.", "committedDate": "2020-09-10T14:53:17Z", "type": "forcePushed"}]}