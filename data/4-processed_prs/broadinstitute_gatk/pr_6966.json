{"pr_number": 6966, "pr_title": "WDL to run feature extract, VQSR, and upload", "pr_createdAt": "2020-11-18T01:20:25Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6966", "timeline": [{"oid": "faa554aa8a39d5cca58298018d5300cb8340f946", "url": "https://github.com/broadinstitute/gatk/commit/faa554aa8a39d5cca58298018d5300cb8340f946", "message": "WDL to run feature extract, VQSR, and upload", "committedDate": "2020-11-18T01:19:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NDMzNw==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526494337", "bodyText": "that's weird that you need a : after the project. in our other scripts, we specify the project with --project_id=~{project_id}  and then we don't include it in the dataset.table. also, when i was doing a bq query, the : was needed when i was using legacy mode and the . otherwise.", "author": "ahaessly", "createdAt": "2020-11-18T23:38:00Z", "path": "scripts/variantstore/wdl/ngs_filter_extract.wdl", "diffHunk": "@@ -0,0 +1,497 @@\n+version 1.0\n+\n+workflow NgsFilterExtract {\n+   input {\n+        Int max_chrom_id = 24\n+        \n+        # bug in cromwell, can't support large integers...\n+        # https://github.com/broadinstitute/cromwell/issues/2685\n+        String chrom_offset = \"1000000000000\"\n+       \n+        File reference\n+        File reference_index\n+        File reference_dict\n+        \n+        String fq_sample_table\n+        String fq_alt_allele_table\n+        String query_project\n+    \n+        String filter_set_name\n+        String fq_filter_set_info_table\n+        String fq_filter_set_tranches_table\n+\n+        String output_file_base_name\n+        File? gatk_override\n+\n+        File dbsnp_vcf\n+        File dbsnp_vcf_index\n+\n+        Array[String] snp_recalibration_tranche_values\n+        Array[String] snp_recalibration_annotation_values\n+        Array[String] indel_recalibration_tranche_values\n+        Array[String] indel_recalibration_annotation_values\n+\n+        File hapmap_resource_vcf\n+        File hapmap_resource_vcf_index\n+        File omni_resource_vcf\n+        File omni_resource_vcf_index\n+        File one_thousand_genomes_resource_vcf\n+        File one_thousand_genomes_resource_vcf_index\n+        File mills_resource_vcf\n+        File mills_resource_vcf_index\n+        File axiomPoly_resource_vcf\n+        File axiomPoly_resource_vcf_index\n+        File dbsnp_resource_vcf = dbsnp_vcf\n+        File dbsnp_resource_vcf_index = dbsnp_vcf_index\n+\n+        # Runtime attributes\n+        Int? small_disk_override\n+        Int small_disk = select_first([small_disk_override, \"100\"])\n+        Int? medium_disk_override\n+        Int medium_disk = select_first([medium_disk_override, \"200\"])\n+        Int? large_disk_override\n+        Int large_disk = select_first([large_disk_override, \"300\"])\n+        Int? huge_disk_override\n+        Int huge_disk = select_first([huge_disk_override, \"400\"])\n+\n+        String? preemptible_tries_override\n+        Int preemptible_tries = select_first([preemptible_tries_override, \"3\"])\n+\n+    }\n+    \n+    scatter(i in range(max_chrom_id)) {\n+        call ExtractFilterTask {\n+            input:\n+                gatk_override            = gatk_override,\n+                reference                = reference,\n+                reference_index          = reference_index,\n+                reference_dict           = reference_dict,\n+                fq_sample_table          = fq_sample_table,\n+                chrom_offset             = chrom_offset,\n+                chrom_id                 = i+1,\n+                fq_alt_allele_table      = fq_alt_allele_table,\n+                read_project_id          = query_project,\n+                output_file              = \"${output_file_base_name}_${i}.vcf.gz\"\n+        }\n+    }\n+\n+    call MergeVCFs { \n+       input:\n+           input_vcfs = ExtractFilterTask.output_vcf,\n+           input_vcfs_indexes = ExtractFilterTask.output_vcf_index,\n+           output_vcf_name = \"${output_file_base_name}.vcf.gz\",\n+           preemptible_tries = 3\n+    }\n+    \n+    call IndelsVariantRecalibrator {\n+        input:\n+        sites_only_variant_filtered_vcf = MergeVCFs.output_vcf,\n+        sites_only_variant_filtered_vcf_index = MergeVCFs.output_vcf_index,\n+        recalibration_filename = filter_set_name + \".indels.recal\",\n+        tranches_filename = filter_set_name + \".indels.tranches\",\n+        recalibration_tranche_values = indel_recalibration_tranche_values,\n+        recalibration_annotation_values = indel_recalibration_annotation_values,\n+        mills_resource_vcf = mills_resource_vcf,\n+        mills_resource_vcf_index = mills_resource_vcf_index,\n+        axiomPoly_resource_vcf = axiomPoly_resource_vcf,\n+        axiomPoly_resource_vcf_index = axiomPoly_resource_vcf_index,\n+        dbsnp_resource_vcf = dbsnp_resource_vcf,\n+        dbsnp_resource_vcf_index = dbsnp_resource_vcf_index,\n+        use_allele_specific_annotations = true,\n+        disk_size = large_disk\n+    }\n+\n+    call SNPsVariantRecalibrator as SNPsVariantRecalibratorClassic {\n+      input:\n+          sites_only_variant_filtered_vcf = MergeVCFs.output_vcf,\n+          sites_only_variant_filtered_vcf_index = MergeVCFs.output_vcf_index,\n+          recalibration_filename = filter_set_name + \".snps.recal\",\n+          tranches_filename = filter_set_name + \".snps.tranches\",\n+          recalibration_tranche_values = snp_recalibration_tranche_values,\n+          recalibration_annotation_values = snp_recalibration_annotation_values,\n+          hapmap_resource_vcf = hapmap_resource_vcf,\n+          hapmap_resource_vcf_index = hapmap_resource_vcf_index,\n+          omni_resource_vcf = omni_resource_vcf,\n+          omni_resource_vcf_index = omni_resource_vcf_index,\n+          one_thousand_genomes_resource_vcf = one_thousand_genomes_resource_vcf,\n+          one_thousand_genomes_resource_vcf_index = one_thousand_genomes_resource_vcf_index,\n+          dbsnp_resource_vcf = dbsnp_resource_vcf,\n+          dbsnp_resource_vcf_index = dbsnp_resource_vcf_index,\n+          use_allele_specific_annotations = true,\n+          disk_size = large_disk\n+    }\n+\n+   call UploadFilterSetToBQ {\n+     input:\n+        gatk_override = gatk_override,\n+        filter_set_name = filter_set_name,\n+        snp_recal_file = SNPsVariantRecalibratorClassic.recalibration,\n+        snp_recal_file_index = SNPsVariantRecalibratorClassic.recalibration_index,\n+        snp_recal_tranches = SNPsVariantRecalibratorClassic.tranches,\n+\n+        indel_recal_file = IndelsVariantRecalibrator.recalibration,\n+        indel_recal_file_index = IndelsVariantRecalibrator.recalibration_index,\n+        indel_recal_tranches = IndelsVariantRecalibrator.tranches,\n+\n+        fq_info_destination_table = fq_filter_set_info_table,\n+        fq_tranches_destination_table = fq_filter_set_tranches_table\n+   }\n+\n+    output {\n+        File output_vcf = MergeVCFs.output_vcf\n+        File output_vcf_idx = MergeVCFs.output_vcf_index\n+    }\n+}\n+\n+################################################################################\n+task ExtractFilterTask {\n+    # indicates that this task should NOT be call cached\n+    \n+    # TODO: should this be marked as volatile???\n+    #meta {\n+    #   volatile: true\n+    #}\n+\n+    input {\n+        # ------------------------------------------------\n+        # Input args:\n+        File reference\n+        File reference_index\n+        File reference_dict\n+    \n+        String fq_sample_table\n+\n+        # bug in cromwell, can't support large integers...\n+        # https://github.com/broadinstitute/cromwell/issues/2685\n+        String chrom_offset\n+        Int chrom_id\n+\n+        String fq_alt_allele_table\n+        String read_project_id\n+        String output_file\n+        \n+        # Runtime Options:\n+        File? gatk_override\n+        \n+        Int? local_sort_max_records_in_ram = 1000000\n+    }\n+\n+\n+    # ------------------------------------------------\n+    # Run our command:\n+    command <<<\n+        set -e\n+        export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+\n+        df -h\n+        min_location=$(echo \"~{chrom_id} * ~{chrom_offset}\" | bc)\n+        max_location=$(echo \"( ~{chrom_id} + 1 ) * ~{chrom_offset}\" | bc)\n+\n+        gatk --java-options \"-Xmx4g\" \\\n+            ExtractFeatures \\\n+                --ref-version 38  \\\n+                -R \"~{reference}\" \\\n+                -O \"~{output_file}\" \\\n+                --local-sort-max-records-in-ram ~{local_sort_max_records_in_ram} \\\n+                --sample-table ~{fq_sample_table} \\\n+                --alt-allele-table ~{fq_alt_allele_table} \\\n+                --min-location ${min_location} --max-location ${max_location} \\\n+                --project-id ~{read_project_id}\n+    >>>\n+\n+    # ------------------------------------------------\n+    # Runtime settings:\n+    runtime {\n+        docker: \"us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:varstore_d8a72b825eab2d979c8877448c0ca948fd9b34c7_change_to_hwe\"\n+        memory: \"7 GB\"\n+        disks: \"local-disk 10 HDD\"\n+        bootDiskSizeGb: 15\n+        preemptible: 3\n+        cpu: 2\n+    }\n+\n+    # ------------------------------------------------\n+    # Outputs:\n+    output {\n+        File output_vcf = \"~{output_file}\"\n+        File output_vcf_index = \"~{output_file}.tbi\"\n+    }\n+ }\n+\n+task UploadFilterSetToBQ {\n+    # indicates that this task should NOT be call cached\n+    \n+    # TODO: should this be marked as volatile???\n+    #meta {\n+    #   volatile: true\n+    #}\n+\n+    input {\n+        # ------------------------------------------------\n+        # Input args:\n+\n+        # Runtime Options:\n+        File? gatk_override\n+\n+        String filter_set_name\n+\n+        File snp_recal_file\n+        File snp_recal_file_index\n+        File snp_recal_tranches\n+\n+        File indel_recal_file\n+        File indel_recal_file_index\n+        File indel_recal_tranches\n+\n+        String fq_info_destination_table\n+        String fq_tranches_destination_table\n+    }\n+\n+\n+    # ------------------------------------------------\n+    # Run our command:\n+    command <<<\n+        set -e\n+        set -o pipefail\n+\n+        export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+\n+        gatk --java-options \"-Xmx4g\" \\\n+            CreateFilteringFiles \\\n+            --ref-version 38 \\\n+            --filter-set-name ~{filter_set_name} \\\n+            -mode SNP \\\n+            -V ~{snp_recal_file} \\\n+            -O ~{filter_set_name}.snps.recal.tsv\n+\n+        gatk --java-options \"-Xmx4g\" \\\n+            CreateFilteringFiles \\\n+            --ref-version 38 \\\n+            --filter-set-name ~{filter_set_name} \\\n+            -mode INDEL \\\n+            -V ~{indel_recal_file} \\\n+            -O ~{filter_set_name}.indels.recal.tsv\n+\n+        # merge into a single file\n+        echo \"Merging SNP + INDELs\"\n+        cat ~{filter_set_name}.snps.recal.tsv ~{filter_set_name}.indels.recal.tsv | grep -v filter_set_name | grep -v \"#\"  > filter_set_load.tsv\n+\n+        # BQ load likes a : instead of a . after the project \n+        bq_info_table=$(echo ~{fq_info_destination_table} | sed s/\\\\./:/)", "originalCommit": "faa554aa8a39d5cca58298018d5300cb8340f946", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMzExMA==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526503110", "bodyText": "yeah you can supply the default project with --project_id OR can you supply a fully qualified name.  Since for all the other scripts we tend to take in a FQ table name (project.dataset.table) I kept this the same from an input perspective for user consistency, but that means we have to fiddle it here to get it in the right format for bq load", "author": "kcibul", "createdAt": "2020-11-19T00:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NDMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NTIwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526495207", "bodyText": "nit - we could remove the as here (i'm guessing we used to have a classic and non classic mode)", "author": "ahaessly", "createdAt": "2020-11-18T23:40:13Z", "path": "scripts/variantstore/wdl/ngs_filter_extract.wdl", "diffHunk": "@@ -0,0 +1,497 @@\n+version 1.0\n+\n+workflow NgsFilterExtract {\n+   input {\n+        Int max_chrom_id = 24\n+        \n+        # bug in cromwell, can't support large integers...\n+        # https://github.com/broadinstitute/cromwell/issues/2685\n+        String chrom_offset = \"1000000000000\"\n+       \n+        File reference\n+        File reference_index\n+        File reference_dict\n+        \n+        String fq_sample_table\n+        String fq_alt_allele_table\n+        String query_project\n+    \n+        String filter_set_name\n+        String fq_filter_set_info_table\n+        String fq_filter_set_tranches_table\n+\n+        String output_file_base_name\n+        File? gatk_override\n+\n+        File dbsnp_vcf\n+        File dbsnp_vcf_index\n+\n+        Array[String] snp_recalibration_tranche_values\n+        Array[String] snp_recalibration_annotation_values\n+        Array[String] indel_recalibration_tranche_values\n+        Array[String] indel_recalibration_annotation_values\n+\n+        File hapmap_resource_vcf\n+        File hapmap_resource_vcf_index\n+        File omni_resource_vcf\n+        File omni_resource_vcf_index\n+        File one_thousand_genomes_resource_vcf\n+        File one_thousand_genomes_resource_vcf_index\n+        File mills_resource_vcf\n+        File mills_resource_vcf_index\n+        File axiomPoly_resource_vcf\n+        File axiomPoly_resource_vcf_index\n+        File dbsnp_resource_vcf = dbsnp_vcf\n+        File dbsnp_resource_vcf_index = dbsnp_vcf_index\n+\n+        # Runtime attributes\n+        Int? small_disk_override\n+        Int small_disk = select_first([small_disk_override, \"100\"])\n+        Int? medium_disk_override\n+        Int medium_disk = select_first([medium_disk_override, \"200\"])\n+        Int? large_disk_override\n+        Int large_disk = select_first([large_disk_override, \"300\"])\n+        Int? huge_disk_override\n+        Int huge_disk = select_first([huge_disk_override, \"400\"])\n+\n+        String? preemptible_tries_override\n+        Int preemptible_tries = select_first([preemptible_tries_override, \"3\"])\n+\n+    }\n+    \n+    scatter(i in range(max_chrom_id)) {\n+        call ExtractFilterTask {\n+            input:\n+                gatk_override            = gatk_override,\n+                reference                = reference,\n+                reference_index          = reference_index,\n+                reference_dict           = reference_dict,\n+                fq_sample_table          = fq_sample_table,\n+                chrom_offset             = chrom_offset,\n+                chrom_id                 = i+1,\n+                fq_alt_allele_table      = fq_alt_allele_table,\n+                read_project_id          = query_project,\n+                output_file              = \"${output_file_base_name}_${i}.vcf.gz\"\n+        }\n+    }\n+\n+    call MergeVCFs { \n+       input:\n+           input_vcfs = ExtractFilterTask.output_vcf,\n+           input_vcfs_indexes = ExtractFilterTask.output_vcf_index,\n+           output_vcf_name = \"${output_file_base_name}.vcf.gz\",\n+           preemptible_tries = 3\n+    }\n+    \n+    call IndelsVariantRecalibrator {\n+        input:\n+        sites_only_variant_filtered_vcf = MergeVCFs.output_vcf,\n+        sites_only_variant_filtered_vcf_index = MergeVCFs.output_vcf_index,\n+        recalibration_filename = filter_set_name + \".indels.recal\",\n+        tranches_filename = filter_set_name + \".indels.tranches\",\n+        recalibration_tranche_values = indel_recalibration_tranche_values,\n+        recalibration_annotation_values = indel_recalibration_annotation_values,\n+        mills_resource_vcf = mills_resource_vcf,\n+        mills_resource_vcf_index = mills_resource_vcf_index,\n+        axiomPoly_resource_vcf = axiomPoly_resource_vcf,\n+        axiomPoly_resource_vcf_index = axiomPoly_resource_vcf_index,\n+        dbsnp_resource_vcf = dbsnp_resource_vcf,\n+        dbsnp_resource_vcf_index = dbsnp_resource_vcf_index,\n+        use_allele_specific_annotations = true,\n+        disk_size = large_disk\n+    }\n+\n+    call SNPsVariantRecalibrator as SNPsVariantRecalibratorClassic {", "originalCommit": "faa554aa8a39d5cca58298018d5300cb8340f946", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMzQyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526503426", "bodyText": "yeah -- I think we're going to have a \"non-classic\" mode as well once we start scale testing.  The WARP pipeline has it for > 10k wgs samples... but it's a bunch of logic.  Happy to remove the \"as ...\" for now though", "author": "kcibul", "createdAt": "2020-11-19T00:02:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NTIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NzEzNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526497135", "bodyText": "bc - TIL", "author": "ahaessly", "createdAt": "2020-11-18T23:45:10Z", "path": "scripts/variantstore/wdl/ngs_filter_extract.wdl", "diffHunk": "@@ -0,0 +1,497 @@\n+version 1.0\n+\n+workflow NgsFilterExtract {\n+   input {\n+        Int max_chrom_id = 24\n+        \n+        # bug in cromwell, can't support large integers...\n+        # https://github.com/broadinstitute/cromwell/issues/2685\n+        String chrom_offset = \"1000000000000\"\n+       \n+        File reference\n+        File reference_index\n+        File reference_dict\n+        \n+        String fq_sample_table\n+        String fq_alt_allele_table\n+        String query_project\n+    \n+        String filter_set_name\n+        String fq_filter_set_info_table\n+        String fq_filter_set_tranches_table\n+\n+        String output_file_base_name\n+        File? gatk_override\n+\n+        File dbsnp_vcf\n+        File dbsnp_vcf_index\n+\n+        Array[String] snp_recalibration_tranche_values\n+        Array[String] snp_recalibration_annotation_values\n+        Array[String] indel_recalibration_tranche_values\n+        Array[String] indel_recalibration_annotation_values\n+\n+        File hapmap_resource_vcf\n+        File hapmap_resource_vcf_index\n+        File omni_resource_vcf\n+        File omni_resource_vcf_index\n+        File one_thousand_genomes_resource_vcf\n+        File one_thousand_genomes_resource_vcf_index\n+        File mills_resource_vcf\n+        File mills_resource_vcf_index\n+        File axiomPoly_resource_vcf\n+        File axiomPoly_resource_vcf_index\n+        File dbsnp_resource_vcf = dbsnp_vcf\n+        File dbsnp_resource_vcf_index = dbsnp_vcf_index\n+\n+        # Runtime attributes\n+        Int? small_disk_override\n+        Int small_disk = select_first([small_disk_override, \"100\"])\n+        Int? medium_disk_override\n+        Int medium_disk = select_first([medium_disk_override, \"200\"])\n+        Int? large_disk_override\n+        Int large_disk = select_first([large_disk_override, \"300\"])\n+        Int? huge_disk_override\n+        Int huge_disk = select_first([huge_disk_override, \"400\"])\n+\n+        String? preemptible_tries_override\n+        Int preemptible_tries = select_first([preemptible_tries_override, \"3\"])\n+\n+    }\n+    \n+    scatter(i in range(max_chrom_id)) {\n+        call ExtractFilterTask {\n+            input:\n+                gatk_override            = gatk_override,\n+                reference                = reference,\n+                reference_index          = reference_index,\n+                reference_dict           = reference_dict,\n+                fq_sample_table          = fq_sample_table,\n+                chrom_offset             = chrom_offset,\n+                chrom_id                 = i+1,\n+                fq_alt_allele_table      = fq_alt_allele_table,\n+                read_project_id          = query_project,\n+                output_file              = \"${output_file_base_name}_${i}.vcf.gz\"\n+        }\n+    }\n+\n+    call MergeVCFs { \n+       input:\n+           input_vcfs = ExtractFilterTask.output_vcf,\n+           input_vcfs_indexes = ExtractFilterTask.output_vcf_index,\n+           output_vcf_name = \"${output_file_base_name}.vcf.gz\",\n+           preemptible_tries = 3\n+    }\n+    \n+    call IndelsVariantRecalibrator {\n+        input:\n+        sites_only_variant_filtered_vcf = MergeVCFs.output_vcf,\n+        sites_only_variant_filtered_vcf_index = MergeVCFs.output_vcf_index,\n+        recalibration_filename = filter_set_name + \".indels.recal\",\n+        tranches_filename = filter_set_name + \".indels.tranches\",\n+        recalibration_tranche_values = indel_recalibration_tranche_values,\n+        recalibration_annotation_values = indel_recalibration_annotation_values,\n+        mills_resource_vcf = mills_resource_vcf,\n+        mills_resource_vcf_index = mills_resource_vcf_index,\n+        axiomPoly_resource_vcf = axiomPoly_resource_vcf,\n+        axiomPoly_resource_vcf_index = axiomPoly_resource_vcf_index,\n+        dbsnp_resource_vcf = dbsnp_resource_vcf,\n+        dbsnp_resource_vcf_index = dbsnp_resource_vcf_index,\n+        use_allele_specific_annotations = true,\n+        disk_size = large_disk\n+    }\n+\n+    call SNPsVariantRecalibrator as SNPsVariantRecalibratorClassic {\n+      input:\n+          sites_only_variant_filtered_vcf = MergeVCFs.output_vcf,\n+          sites_only_variant_filtered_vcf_index = MergeVCFs.output_vcf_index,\n+          recalibration_filename = filter_set_name + \".snps.recal\",\n+          tranches_filename = filter_set_name + \".snps.tranches\",\n+          recalibration_tranche_values = snp_recalibration_tranche_values,\n+          recalibration_annotation_values = snp_recalibration_annotation_values,\n+          hapmap_resource_vcf = hapmap_resource_vcf,\n+          hapmap_resource_vcf_index = hapmap_resource_vcf_index,\n+          omni_resource_vcf = omni_resource_vcf,\n+          omni_resource_vcf_index = omni_resource_vcf_index,\n+          one_thousand_genomes_resource_vcf = one_thousand_genomes_resource_vcf,\n+          one_thousand_genomes_resource_vcf_index = one_thousand_genomes_resource_vcf_index,\n+          dbsnp_resource_vcf = dbsnp_resource_vcf,\n+          dbsnp_resource_vcf_index = dbsnp_resource_vcf_index,\n+          use_allele_specific_annotations = true,\n+          disk_size = large_disk\n+    }\n+\n+   call UploadFilterSetToBQ {\n+     input:\n+        gatk_override = gatk_override,\n+        filter_set_name = filter_set_name,\n+        snp_recal_file = SNPsVariantRecalibratorClassic.recalibration,\n+        snp_recal_file_index = SNPsVariantRecalibratorClassic.recalibration_index,\n+        snp_recal_tranches = SNPsVariantRecalibratorClassic.tranches,\n+\n+        indel_recal_file = IndelsVariantRecalibrator.recalibration,\n+        indel_recal_file_index = IndelsVariantRecalibrator.recalibration_index,\n+        indel_recal_tranches = IndelsVariantRecalibrator.tranches,\n+\n+        fq_info_destination_table = fq_filter_set_info_table,\n+        fq_tranches_destination_table = fq_filter_set_tranches_table\n+   }\n+\n+    output {\n+        File output_vcf = MergeVCFs.output_vcf\n+        File output_vcf_idx = MergeVCFs.output_vcf_index\n+    }\n+}\n+\n+################################################################################\n+task ExtractFilterTask {\n+    # indicates that this task should NOT be call cached\n+    \n+    # TODO: should this be marked as volatile???\n+    #meta {\n+    #   volatile: true\n+    #}\n+\n+    input {\n+        # ------------------------------------------------\n+        # Input args:\n+        File reference\n+        File reference_index\n+        File reference_dict\n+    \n+        String fq_sample_table\n+\n+        # bug in cromwell, can't support large integers...\n+        # https://github.com/broadinstitute/cromwell/issues/2685\n+        String chrom_offset\n+        Int chrom_id\n+\n+        String fq_alt_allele_table\n+        String read_project_id\n+        String output_file\n+        \n+        # Runtime Options:\n+        File? gatk_override\n+        \n+        Int? local_sort_max_records_in_ram = 1000000\n+    }\n+\n+\n+    # ------------------------------------------------\n+    # Run our command:\n+    command <<<\n+        set -e\n+        export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+\n+        df -h\n+        min_location=$(echo \"~{chrom_id} * ~{chrom_offset}\" | bc)\n+        max_location=$(echo \"( ~{chrom_id} + 1 ) * ~{chrom_offset}\" | bc)", "originalCommit": "faa554aa8a39d5cca58298018d5300cb8340f946", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMzYwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6966#discussion_r526503603", "bodyText": "\ud83e\uddf0", "author": "kcibul", "createdAt": "2020-11-19T00:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NzEzNQ=="}], "type": "inlineReview"}, {"oid": "3dedace5ce6829ed2c33e55f8492b7fdb64fd46f", "url": "https://github.com/broadinstitute/gatk/commit/3dedace5ce6829ed2c33e55f8492b7fdb64fd46f", "message": "PR feedback", "committedDate": "2020-11-19T00:04:38Z", "type": "commit"}]}