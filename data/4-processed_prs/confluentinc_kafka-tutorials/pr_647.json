{"pr_number": 647, "pr_title": "DEVX-2183: Convert \"Event Time Processing\" recipe to KT", "pr_createdAt": "2020-11-30T17:01:33Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/647", "timeline": [{"oid": "2cf1408e0f59dc2f3d7bc79bf7db480c461f77dd", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2cf1408e0f59dc2f3d7bc79bf7db480c461f77dd", "message": "DEVX-2183/GH-265: Time concepts: event time, ingestion time", "committedDate": "2020-11-18T01:28:52Z", "type": "commit"}, {"oid": "89339028d4e6c30703635d13b163897e3976237f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/89339028d4e6c30703635d13b163897e3976237f", "message": "Checkpoint", "committedDate": "2020-11-18T01:49:52Z", "type": "commit"}, {"oid": "01dcfae7c19b502536f6345d659d57b488f202a8", "url": "https://github.com/confluentinc/kafka-tutorials/commit/01dcfae7c19b502536f6345d659d57b488f202a8", "message": "Add new steps", "committedDate": "2020-11-18T02:03:05Z", "type": "commit"}, {"oid": "8f07dc82bc8f3c49736e6386e1ac491a11f63158", "url": "https://github.com/confluentinc/kafka-tutorials/commit/8f07dc82bc8f3c49736e6386e1ac491a11f63158", "message": "Create avsc schema files and build", "committedDate": "2020-11-23T14:31:58Z", "type": "commit"}, {"oid": "cc2b92ae1848b2a21c1888351d3fb0f708009679", "url": "https://github.com/confluentinc/kafka-tutorials/commit/cc2b92ae1848b2a21c1888351d3fb0f708009679", "message": "Checkpoint", "committedDate": "2020-11-23T14:55:32Z", "type": "commit"}, {"oid": "fb01b726b0847d988f9c888b40ed29d647214f4f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/fb01b726b0847d988f9c888b40ed29d647214f4f", "message": "Print timestamps", "committedDate": "2020-11-23T17:27:40Z", "type": "commit"}, {"oid": "6776933aeefe4ecba092cd673a96a5f3b63de87f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6776933aeefe4ecba092cd673a96a5f3b63de87f", "message": "Checkpoint 2", "committedDate": "2020-11-23T19:48:48Z", "type": "commit"}, {"oid": "6967d6402d7f2c354a367369ad900a55fb17f075", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6967d6402d7f2c354a367369ad900a55fb17f075", "message": "Remove extraneous files", "committedDate": "2020-11-23T19:52:43Z", "type": "commit"}, {"oid": "158bfa5bc9ca743d9b4fd25526ee0748a93b7998", "url": "https://github.com/confluentinc/kafka-tutorials/commit/158bfa5bc9ca743d9b4fd25526ee0748a93b7998", "message": "DEVX-2183: cleanup", "committedDate": "2020-11-30T15:48:57Z", "type": "commit"}, {"oid": "08080b6e82c4e793e4bad4221334ef7a9c3f312c", "url": "https://github.com/confluentinc/kafka-tutorials/commit/08080b6e82c4e793e4bad4221334ef7a9c3f312c", "message": "Prettier format; tweak", "committedDate": "2020-11-30T17:42:19Z", "type": "commit"}, {"oid": "6dff1eec918ec1da995ca854d1c3d7d18fd00247", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6dff1eec918ec1da995ca854d1c3d7d18fd00247", "message": "Updated Makefile", "committedDate": "2020-11-30T19:25:20Z", "type": "commit"}, {"oid": "dd530c96f92218c1a2c2e4934e8584011550f61b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/dd530c96f92218c1a2c2e4934e8584011550f61b", "message": "Update short answer", "committedDate": "2020-11-30T19:28:00Z", "type": "commit"}, {"oid": "1b09c0ffeff30a99853e1180ca75d0debcc211bc", "url": "https://github.com/confluentinc/kafka-tutorials/commit/1b09c0ffeff30a99853e1180ca75d0debcc211bc", "message": "Merge branch 'master' into DEVX-2183", "committedDate": "2020-11-30T19:30:28Z", "type": "commit"}, {"oid": "55c96d9a1f3767adb0068f194c879cdb3dcf76b2", "url": "https://github.com/confluentinc/kafka-tutorials/commit/55c96d9a1f3767adb0068f194c879cdb3dcf76b2", "message": "Update introduction", "committedDate": "2020-11-30T19:33:37Z", "type": "commit"}, {"oid": "546d72d8aa2f4e4cd5207ec00cf3f7ffc9af17db", "url": "https://github.com/confluentinc/kafka-tutorials/commit/546d72d8aa2f4e4cd5207ec00cf3f7ffc9af17db", "message": "Add missing status", "committedDate": "2020-11-30T19:38:50Z", "type": "commit"}, {"oid": "825d42a0ce9364c76db6f07a7fb8d604eccab7f4", "url": "https://github.com/confluentinc/kafka-tutorials/commit/825d42a0ce9364c76db6f07a7fb8d604eccab7f4", "message": "Tweak short answer", "committedDate": "2020-11-30T19:40:02Z", "type": "commit"}, {"oid": "faec12b5b2a7a1da6e73e21bb394a21c408352b7", "url": "https://github.com/confluentinc/kafka-tutorials/commit/faec12b5b2a7a1da6e73e21bb394a21c408352b7", "message": "Add link to ksqlDB documentation", "committedDate": "2020-11-30T19:48:10Z", "type": "commit"}, {"oid": "4b293edf686402061cac1edac5f06d2fe604d33a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4b293edf686402061cac1edac5f06d2fe604d33a", "message": "Tweak", "committedDate": "2020-11-30T20:04:10Z", "type": "commit"}, {"oid": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "url": "https://github.com/confluentinc/kafka-tutorials/commit/9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "message": "Add explanation for Kafka producer", "committedDate": "2020-11-30T20:59:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNjM1OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532926358", "bodyText": "I know you generally stick to and point out \"event driven language\" any reason why you \"data record\" here instead of \"event\"?", "author": "awalther28", "createdAt": "2020-11-30T21:50:33Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Achieving event-time semantics typically requires embedding timestamps into the data record at the time it is produced.", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3ODI4Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532978282", "bodyText": "@awalther28 the reasoning was that event is a touch more abstract/conceptual, whereas record refers more to the actual data/fields/metadata/etc...and because this KT is talking about the actual data and fields within the payload, there are places where record seemed to fit better than event.  However, in writing his book, @bbejeck has probably stronger leanings on when to use which term -- any thoughts @bbejeck ?", "author": "ybyzek", "createdAt": "2020-11-30T23:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNjM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQ3MzY0Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r533473643", "bodyText": "Gotcha, with those definitions I think record makes sense here. I am curious to hear @bbejeck 's opinion.", "author": "awalther28", "createdAt": "2020-12-01T14:53:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNjM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4MDUzOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r533580538", "bodyText": "Yeah, in this case, I think using the term data record,  or just record makes sense. I would do the same.  IMO the term event is more about something that happens and the details captured in a physical record.\nWhile the terms event record, data record, or record are somewhat synonymous,  my personal preference is to talk about events as the \"thing that happened\" and then refer to the results as a record, as it's implied they are event records.  To me, the main point is to emphasize that Kafka captures the event when it occurs then makes it available immediately.  I may have gone off-topic here a little, but that's my  2 cents.", "author": "bbejeck", "createdAt": "2020-12-01T17:11:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNjM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4MzAwOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r533583009", "bodyText": "@bbejeck \ud83d\udc4d thanks for the insight.  Makes sense", "author": "ybyzek", "createdAt": "2020-12-01T17:15:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNjM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNzUwMA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532927500", "bodyText": "I appreciate that you use the same terminology throughout this article. One thing I did notice is that sometimes you put wall-clock in quotes and sometimes you don't. Maybe this is for a specific reason that I didn't pick up on..? If there isn't a reasoning, would you mind making sure that either all mentions of wall-clock are in quotes or not in quotes?", "author": "awalther28", "createdAt": "2020-11-30T21:52:41Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/make-event-schema.adoc", "diffHunk": "@@ -0,0 +1,6 @@\n+Then create the following Avro schema file at `src/main/avro/DeviceEvent.avsc` for the event.\n+This schema has two fields, one of which is called `eventTime` that represents the event time, or wall-clock time.", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2MzE2OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532963169", "bodyText": "Thanks for spotting the inconsistency.  But after conversation with @bbejeck , wall clock has now been struck completely from the KT ;)", "author": "ybyzek", "createdAt": "2020-11-30T23:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNzUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyODgzNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532928835", "bodyText": "I'm not sure this should be committed, should it?", "author": "awalther28", "createdAt": "2020-11-30T21:55:17Z", "path": "_includes/tutorials/kafka-producer-application/kafka/code/gradlew.bat", "diffHunk": "@@ -1,100 +1,104 @@\n-@rem\n-@rem Copyright 2015 the original author or authors.\n-@rem\n-@rem Licensed under the Apache License, Version 2.0 (the \"License\");\n-@rem you may not use this file except in compliance with the License.\n-@rem You may obtain a copy of the License at\n-@rem\n-@rem      https://www.apache.org/licenses/LICENSE-2.0\n-@rem\n-@rem Unless required by applicable law or agreed to in writing, software\n-@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n-@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-@rem See the License for the specific language governing permissions and\n-@rem limitations under the License.\n-@rem\n-\n-@if \"%DEBUG%\" == \"\" @echo off\n-@rem ##########################################################################\n-@rem\n-@rem  Gradle startup script for Windows\n-@rem\n-@rem ##########################################################################\n-\n-@rem Set local scope for the variables with windows NT shell\n-if \"%OS%\"==\"Windows_NT\" setlocal\n-\n-set DIRNAME=%~dp0\n-if \"%DIRNAME%\" == \"\" set DIRNAME=.\n-set APP_BASE_NAME=%~n0\n-set APP_HOME=%DIRNAME%\n-\n-@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\n-set DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\n-\n-@rem Find java.exe\n-if defined JAVA_HOME goto findJavaFromJavaHome\n-\n-set JAVA_EXE=java.exe\n-%JAVA_EXE% -version >NUL 2>&1\n-if \"%ERRORLEVEL%\" == \"0\" goto init\n-\n-echo.\n-echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\n-echo.\n-echo Please set the JAVA_HOME variable in your environment to match the\n-echo location of your Java installation.\n-\n-goto fail\n-\n-:findJavaFromJavaHome\n-set JAVA_HOME=%JAVA_HOME:\"=%\n-set JAVA_EXE=%JAVA_HOME%/bin/java.exe\n-\n-if exist \"%JAVA_EXE%\" goto init\n-\n-echo.\n-echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\n-echo.\n-echo Please set the JAVA_HOME variable in your environment to match the\n-echo location of your Java installation.\n-\n-goto fail\n-\n-:init\n-@rem Get command-line arguments, handling Windows variants\n-\n-if not \"%OS%\" == \"Windows_NT\" goto win9xME_args\n-\n-:win9xME_args\n-@rem Slurp the command line arguments.\n-set CMD_LINE_ARGS=\n-set _SKIP=2\n-\n-:win9xME_args_slurp\n-if \"x%~1\" == \"x\" goto execute\n-\n-set CMD_LINE_ARGS=%*\n-\n-:execute\n-@rem Setup the command line\n-\n-set CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\n-\n-@rem Execute Gradle\n-\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%\n-\n-:end\n-@rem End local scope for the variables with windows NT shell\n-if \"%ERRORLEVEL%\"==\"0\" goto mainEnd\n-\n-:fail\n-rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\n-rem the _cmd.exe /c_ return code!\n-if  not \"\" == \"%GRADLE_EXIT_CONSOLE%\" exit 1\n-exit /b 1\n-\n-:mainEnd\n-if \"%OS%\"==\"Windows_NT\" endlocal\n-\n-:omega\n+@rem\r\n+@rem Copyright 2015 the original author or authors.\r\n+@rem\r\n+@rem Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+@rem you may not use this file except in compliance with the License.\r\n+@rem You may obtain a copy of the License at\r\n+@rem\r\n+@rem      https://www.apache.org/licenses/LICENSE-2.0\r\n+@rem\r\n+@rem Unless required by applicable law or agreed to in writing, software\r\n+@rem distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+@rem See the License for the specific language governing permissions and\r\n+@rem limitations under the License.\r\n+@rem\r\n+\r\n+@if \"%DEBUG%\" == \"\" @echo off\r\n+@rem ##########################################################################\r\n+@rem\r\n+@rem  Gradle startup script for Windows\r\n+@rem\r\n+@rem ##########################################################################\r\n+\r\n+@rem Set local scope for the variables with windows NT shell\r\n+if \"%OS%\"==\"Windows_NT\" setlocal\r\n+\r\n+set DIRNAME=%~dp0\r\n+if \"%DIRNAME%\" == \"\" set DIRNAME=.\r\n+set APP_BASE_NAME=%~n0\r\n+set APP_HOME=%DIRNAME%\r\n+\r\n+@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\r\n+for %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\r\n+\r\n+@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\r\n+set DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\r\n+\r\n+@rem Find java.exe\r\n+if defined JAVA_HOME goto findJavaFromJavaHome\r\n+\r\n+set JAVA_EXE=java.exe\r\n+%JAVA_EXE% -version >NUL 2>&1\r\n+if \"%ERRORLEVEL%\" == \"0\" goto init\r\n+\r\n+echo.\r\n+echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\r\n+echo.\r\n+echo Please set the JAVA_HOME variable in your environment to match the\r\n+echo location of your Java installation.\r\n+\r\n+goto fail\r\n+\r\n+:findJavaFromJavaHome\r\n+set JAVA_HOME=%JAVA_HOME:\"=%\r\n+set JAVA_EXE=%JAVA_HOME%/bin/java.exe\r\n+\r\n+if exist \"%JAVA_EXE%\" goto init\r\n+\r\n+echo.\r\n+echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\r\n+echo.\r\n+echo Please set the JAVA_HOME variable in your environment to match the\r\n+echo location of your Java installation.\r\n+\r\n+goto fail\r\n+\r\n+:init\r\n+@rem Get command-line arguments, handling Windows variants\r\n+\r\n+if not \"%OS%\" == \"Windows_NT\" goto win9xME_args\r\n+\r\n+:win9xME_args\r\n+@rem Slurp the command line arguments.\r\n+set CMD_LINE_ARGS=\r\n+set _SKIP=2\r\n+\r\n+:win9xME_args_slurp\r\n+if \"x%~1\" == \"x\" goto execute\r\n+\r\n+set CMD_LINE_ARGS=%*\r\n+\r\n+:execute\r\n+@rem Setup the command line\r\n+\r\n+set CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\r\n+\r\n+\r\n+@rem Execute Gradle\r\n+\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%\r\n+\r\n+:end\r\n+@rem End local scope for the variables with windows NT shell\r\n+if \"%ERRORLEVEL%\"==\"0\" goto mainEnd\r\n+\r\n+:fail\r\n+rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\r\n+rem the _cmd.exe /c_ return code!\r\n+if  not \"\" == \"%GRADLE_EXIT_CONSOLE%\" exit 1\r\n+exit /b 1\r\n+\r\n+:mainEnd\r\n+if \"%OS%\"==\"Windows_NT\" endlocal\r\n+\r\n+:omega\r", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2MzY3Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532963676", "bodyText": "@awalther28 is there no .gitignore covering gradlew.bat?  Should there be one?", "author": "ybyzek", "createdAt": "2020-11-30T23:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyODgzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTA1Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532979053", "bodyText": "@ybyzek I was asking about this one in particular because it's for the kafka-producer-application tutorial and not for the time-concepts tutorial.", "author": "awalther28", "createdAt": "2020-11-30T23:49:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyODgzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4NzU3OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532987578", "bodyText": "Ack and resolved.", "author": "ybyzek", "createdAt": "2020-12-01T00:13:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyODgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMjUyNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532932524", "bodyText": "This file isn't used in the harness yaml for time-concepts. Can we remove this?", "author": "awalther28", "createdAt": "2020-11-30T22:02:32Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/persistent-query.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Since the output looks right, the next step is to make the query persistent. This looks exactly like the push query, except we have added a `CREATE TABLE AS` statement to the beginning of it. This statement returns to the CLI prompt right away, having created a persistent stream processing program running in the ksqlDB engine, continuously processing input records and updating the resulting `MOVIE_TICKETS_SOLD` table. Moreover, we don\u2019t see the results of the query displayed in the CLI, because they are updating the newly-created table itself. That table is available to other ksqlDB queries for further processing, and by default all its records are produced to a topic having the same name (MOVIE_TICKETS_SOLD).", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2Mzg1Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532963852", "bodyText": "@awalther28 good catch.  Removing...", "author": "ybyzek", "createdAt": "2020-11-30T23:11:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMjUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzAzMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532933031", "bodyText": "We don't use this either. Can we remove it?", "author": "awalther28", "createdAt": "2020-11-30T22:03:40Z", "path": "_includes/tutorials/time-concepts/ksql/markup/test/make-test-input.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+The Confluent ksqlDB CLI Docker image contains a program called the `ksql-test-runner`. We can pass this program a JSON file describing our desired input data, a JSON file containing the intended output results, and a file of ksqlDB queries to run, and it will tell us whether our queries successfully turn the input into the output. To get started, create a file at `test/input.json` with the inputs for testing", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3MjEwMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532972101", "bodyText": "Removed", "author": "ybyzek", "createdAt": "2020-11-30T23:32:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzA3Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532933072", "bodyText": "We don't use this either. Can we remove it?", "author": "awalther28", "createdAt": "2020-11-30T22:03:46Z", "path": "_includes/tutorials/time-concepts/ksql/markup/test/make-test-output.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Next, create a file at `test/output.json` with the expected outputs:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3MjE4MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532972180", "bodyText": "Removed", "author": "ybyzek", "createdAt": "2020-11-30T23:32:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzE0MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532933140", "bodyText": "We don't use this either. Can we remove it?", "author": "awalther28", "createdAt": "2020-11-30T22:03:52Z", "path": "_includes/tutorials/time-concepts/ksql/markup/test/run-tests.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Finally, invoke the tests using the test runner and the statements file that you created earlier:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3MjI4MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532972280", "bodyText": "Removed", "author": "ybyzek", "createdAt": "2020-11-30T23:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzE0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzI3Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532933276", "bodyText": "We don't use this either. Can we remove it?", "author": "awalther28", "createdAt": "2020-11-30T22:04:10Z", "path": "_includes/tutorials/time-concepts/ksql/markup/prod/submit-to-api.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Launch your statements into production by sending them to the ksqlDB server REST endpoint with the following command:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3MjAxMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532972011", "bodyText": "Removed", "author": "ybyzek", "createdAt": "2020-11-30T23:31:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMzI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNDY0Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532934647", "bodyText": "This file isn't used in the harness yaml for time-concepts. Can we remove this?", "author": "awalther28", "createdAt": "2020-11-30T22:06:53Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/make-src-file.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Now that you have a series of statements that's doing the right thing, the last step is to put them into a file so that they can be used outside the CLI session. Create a file at `src/statements.sql` with the following content:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3MTg4Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532971887", "bodyText": "Removed", "author": "ybyzek", "createdAt": "2020-11-30T23:31:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNDY0Nw=="}], "type": "inlineReview"}, {"oid": "8869c84846a5b5ada02ac00810fc683c41cb2a2c", "url": "https://github.com/confluentinc/kafka-tutorials/commit/8869c84846a5b5ada02ac00810fc683c41cb2a2c", "message": "Remove wall-clock time from KT", "committedDate": "2020-11-30T23:04:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1NTAwNg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532955006", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"Ingestion time is the time at which the Kafka broker appends the record to its log file, versus event time which is the time at which the source creates the record. By default, time-based aggregations in Kafka Streams and ksqlDB operate on the ingestion timestamp, but there may be use cases where they need to operate on the actual time at which the event occurred, in which case the event time should be carried in the message payload . This tutorial will show you how to extract and use the ingestion timestamp and the event timestamp.\"\n          \n          \n            \n              introduction: \"By default, time-based aggregations in Kafka Streams and ksqlDB operate on the record's timestamp, which could be either https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#time-semantics[CreateTime or LogAppendTime] depending on the configuration of the topic. But there may be use cases where you need to operate on a timestamp contained inside the message payload. This tutorial will show you how to extract and use either the record's timestamp or the timestamp from a field in the record's value. \"", "author": "bbejeck", "createdAt": "2020-11-30T22:50:50Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -526,6 +526,17 @@ anomaly-detection:\n     scala: disabled\n     swift: disabled\n \n+time-concepts:\n+  title: \"Ingestion time and event time\"\n+  meta-description: \"Ingestion time and event time\"\n+  slug: \"/time-concepts\"\n+  question: \"What is the difference between ingestion time and event time, and how can I choose which one to use for stream processing?\"\n+  introduction: \"Ingestion time is the time at which the Kafka broker appends the record to its log file, versus event time which is the time at which the source creates the record. By default, time-based aggregations in Kafka Streams and ksqlDB operate on the ingestion timestamp, but there may be use cases where they need to operate on the actual time at which the event occurred, in which case the event time should be carried in the message payload . This tutorial will show you how to extract and use the ingestion timestamp and the event timestamp.\"", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4NTk1Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532985956", "bodyText": "@bbejeck I took the spirit of this suggestion and applied it in latest commit. WDYT?", "author": "ybyzek", "createdAt": "2020-12-01T00:09:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1NTAwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1OTYyMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532959623", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To process data based on *ingestion time*, also known as *log time*, which uses the Kafka record timestamp:\n          \n          \n            \n            To process data based on the Kafka record timestamp, `ROWTIME` :", "author": "bbejeck", "createdAt": "2020-11-30T23:01:37Z", "path": "_includes/tutorials/time-concepts/ksql/markup/answer/short-answer.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+To process data based on *ingestion time*, also known as *log time*, which uses the Kafka record timestamp:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5MjY3NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532992674", "bodyText": "@bbejeck I took the spirit of this suggestion and applied it in latest commit. WDYT?", "author": "ybyzek", "createdAt": "2020-12-01T00:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1OTYyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2MTU5OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532961599", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To process data based on *event time*, also known as *wall-clock time*, which uses a timestamp from within the record payload:\n          \n          \n            \n            To process data using a timestamp from a field within the record payload:", "author": "bbejeck", "createdAt": "2020-11-30T23:06:39Z", "path": "_includes/tutorials/time-concepts/ksql/markup/answer/short-answer.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+To process data based on *ingestion time*, also known as *log time*, which uses the Kafka record timestamp:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"groovy\">{% include_raw tutorials/time-concepts/ksql/code/tutorial-steps/dev/create-stream-logtime.sql  %}</code></pre>\n++++++\n+\n+To process data based on *event time*, also known as *wall-clock time*, which uses a timestamp from within the record payload:", "originalCommit": "9934a76c9b9aa8a7f8954c82078cdcbfe473a785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5MjcxOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532992719", "bodyText": "@bbejeck I took the spirit of this suggestion and applied it in latest commit. WDYT?", "author": "ybyzek", "createdAt": "2020-12-01T00:27:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2MTU5OQ=="}], "type": "inlineReview"}, {"oid": "b8b0fb8e010749564c83dba906bd13dd6bfb9dd3", "url": "https://github.com/confluentinc/kafka-tutorials/commit/b8b0fb8e010749564c83dba906bd13dd6bfb9dd3", "message": "Remove ./ksql/markup/dev/persistent-query.adoc", "committedDate": "2020-11-30T23:12:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2ODE0OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532968149", "bodyText": "Might be good to mention here that ROWTIME is a pseudo column and is the Kafka record timestamp", "author": "bbejeck", "createdAt": "2020-11-30T23:22:37Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/transient-query-logtime.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Let\u2019s inspect the events in this newly created `TEMPERATURE_READINGS_LOGTIME` stream by running a `SELECT` statement with an `EMIT CHANGES` clause, limited to 10.\n+It shows the payload fields `TEMPERATURE` and `EVENTTIME`, plus the `ROWTIME`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/time-concepts/ksql/code/tutorial-steps/dev/transient-query-logtime.sql %}</code></pre>\n++++++\n+\n+This should yield the following output:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/time-concepts/ksql/code/tutorial-steps/dev/expected-transient-query-logtime.log %}</code></pre>\n++++++\n+\n+Notice that for each row:\n+\n+- The `EVENTTIME` value in ksqlDB corresponds exactly to the `payload eventTime` set by the producer\n+- The `ROWTIME` value in ksqlDB corresponds exactly to the `log timestamp` printed by the producer's callback (i.e., ingestion time, when the record was written into the broker's log file)\n+\n+All processing on this stream is based on `ROWTIME`, consequently this results in processing based on ingestion time.", "originalCommit": "b8b0fb8e010749564c83dba906bd13dd6bfb9dd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5MzY4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532993684", "bodyText": "Agree and resolved", "author": "ybyzek", "createdAt": "2020-12-01T00:30:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2ODE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2ODY4NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532968685", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            All processing on this stream is based on `ROWTIME`, consequently this results in processing based on ingestion time.\n          \n          \n            \n            All processing on this stream is based on `ROWTIME`, consequently this results in processing based on the timestamp of the Kafka record\n          \n      \n    \n    \n  \n\nor something similar", "author": "bbejeck", "createdAt": "2020-11-30T23:23:59Z", "path": "_includes/tutorials/time-concepts/ksql/markup/dev/transient-query-logtime.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Let\u2019s inspect the events in this newly created `TEMPERATURE_READINGS_LOGTIME` stream by running a `SELECT` statement with an `EMIT CHANGES` clause, limited to 10.\n+It shows the payload fields `TEMPERATURE` and `EVENTTIME`, plus the `ROWTIME`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/time-concepts/ksql/code/tutorial-steps/dev/transient-query-logtime.sql %}</code></pre>\n++++++\n+\n+This should yield the following output:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/time-concepts/ksql/code/tutorial-steps/dev/expected-transient-query-logtime.log %}</code></pre>\n++++++\n+\n+Notice that for each row:\n+\n+- The `EVENTTIME` value in ksqlDB corresponds exactly to the `payload eventTime` set by the producer\n+- The `ROWTIME` value in ksqlDB corresponds exactly to the `log timestamp` printed by the producer's callback (i.e., ingestion time, when the record was written into the broker's log file)\n+\n+All processing on this stream is based on `ROWTIME`, consequently this results in processing based on ingestion time.", "originalCommit": "b8b0fb8e010749564c83dba906bd13dd6bfb9dd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5Mzk1Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/647#discussion_r532993952", "bodyText": "Made it similar.", "author": "ybyzek", "createdAt": "2020-12-01T00:31:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk2ODY4NQ=="}], "type": "inlineReview"}, {"oid": "7cdd2f6e83f5b6463f134e1e0d2b9701e4c24619", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7cdd2f6e83f5b6463f134e1e0d2b9701e4c24619", "message": "Removed unused files (copied via clone)", "committedDate": "2020-11-30T23:32:56Z", "type": "commit"}, {"oid": "17b45677b5948ccb74cb3c7a1b70adead94f33b8", "url": "https://github.com/confluentinc/kafka-tutorials/commit/17b45677b5948ccb74cb3c7a1b70adead94f33b8", "message": "Merge branch 'master' into DEVX-2183", "committedDate": "2020-11-30T23:37:51Z", "type": "commit"}, {"oid": "f975ce345ea7fc8df039a6339cca86f5a5229973", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f975ce345ea7fc8df039a6339cca86f5a5229973", "message": "Update introduction", "committedDate": "2020-12-01T00:08:20Z", "type": "commit"}, {"oid": "623bcbee8ce6af37b033bc0ca999c64775fdd4b2", "url": "https://github.com/confluentinc/kafka-tutorials/commit/623bcbee8ce6af37b033bc0ca999c64775fdd4b2", "message": "Update  _includes/tutorials/kafka-producer-application/kafka/code/gradlew.bat from master", "committedDate": "2020-12-01T00:13:16Z", "type": "commit"}, {"oid": "a441422f7943aeedc4823f22138460d6a3e7d47f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/a441422f7943aeedc4823f22138460d6a3e7d47f", "message": "Update short answer", "committedDate": "2020-12-01T00:27:23Z", "type": "commit"}, {"oid": "4899bb349eaf995a5da4743719c2ac67c1136e5e", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4899bb349eaf995a5da4743719c2ac67c1136e5e", "message": "Tweak based on feedback from Bill", "committedDate": "2020-12-01T00:31:55Z", "type": "commit"}, {"oid": "44f4d674dd905583898cada704380db35361a134", "url": "https://github.com/confluentinc/kafka-tutorials/commit/44f4d674dd905583898cada704380db35361a134", "message": "Make language consistent", "committedDate": "2020-12-01T00:57:11Z", "type": "commit"}, {"oid": "dab9f2c79bd49a6070bbdaeea82447f19afa9486", "url": "https://github.com/confluentinc/kafka-tutorials/commit/dab9f2c79bd49a6070bbdaeea82447f19afa9486", "message": "Tweak", "committedDate": "2020-12-01T00:59:59Z", "type": "commit"}, {"oid": "5f21343fe247b5ace2bb47b1dca65415594d36be", "url": "https://github.com/confluentinc/kafka-tutorials/commit/5f21343fe247b5ace2bb47b1dca65415594d36be", "message": "Move sleeps to exaggerate time difference", "committedDate": "2020-12-01T01:24:35Z", "type": "commit"}, {"oid": "f0b413330f984c68efe014a7310c94afe99fe6c4", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f0b413330f984c68efe014a7310c94afe99fe6c4", "message": "Update .semaphore/semaphore.yml", "committedDate": "2020-12-01T01:27:53Z", "type": "commit"}, {"oid": "64b0576efd0f82a9c95b862996dbd2abab8a5217", "url": "https://github.com/confluentinc/kafka-tutorials/commit/64b0576efd0f82a9c95b862996dbd2abab8a5217", "message": "Update .semaphore/semaphore.yml", "committedDate": "2020-12-01T01:29:19Z", "type": "commit"}, {"oid": "7e2846c3df65a5a09f5a2e4e7708377b84e7ad15", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7e2846c3df65a5a09f5a2e4e7708377b84e7ad15", "message": "Checkout kafka-producer-application files from master", "committedDate": "2020-12-01T01:40:17Z", "type": "commit"}, {"oid": "ce4ba22a1e148d10473ea2b52274f18a3f1f7392", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ce4ba22a1e148d10473ea2b52274f18a3f1f7392", "message": "Modify Makefile", "committedDate": "2020-12-01T01:46:36Z", "type": "commit"}, {"oid": "e65048d1bd8c8ad54cca86a6a5f0151f47d86b19", "url": "https://github.com/confluentinc/kafka-tutorials/commit/e65048d1bd8c8ad54cca86a6a5f0151f47d86b19", "message": "Move KT to different card", "committedDate": "2020-12-01T01:50:37Z", "type": "commit"}, {"oid": "56c41ff0aed7abf7f90f23bd65cc2f412a6f2b9a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/56c41ff0aed7abf7f90f23bd65cc2f412a6f2b9a", "message": "Simplify KT title", "committedDate": "2020-12-01T01:52:14Z", "type": "commit"}, {"oid": "86109da971fc3280a9ba541cd3627f0c90cac33f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/86109da971fc3280a9ba541cd3627f0c90cac33f", "message": "Simplify answer; simplify stream creation without fields; tweak introduction", "committedDate": "2020-12-01T14:38:01Z", "type": "commit"}, {"oid": "694d83447304cac3fec8bb44c699b7f807caf93a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/694d83447304cac3fec8bb44c699b7f807caf93a", "message": "Change pseudo to system; tweak short answer", "committedDate": "2020-12-01T14:45:48Z", "type": "commit"}, {"oid": "2b946119c49ca4ff633bce7512438291152609cf", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2b946119c49ca4ff633bce7512438291152609cf", "message": "Tweak short answer", "committedDate": "2020-12-01T14:48:50Z", "type": "commit"}, {"oid": "2f71abe20630641743df5df8deb7d032594fed05", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2f71abe20630641743df5df8deb7d032594fed05", "message": "Spelling", "committedDate": "2020-12-01T15:09:55Z", "type": "commit"}]}