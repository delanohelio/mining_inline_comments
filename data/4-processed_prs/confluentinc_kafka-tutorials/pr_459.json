{"pr_number": 459, "pr_title": "Produce consume tutorial in scala", "pr_createdAt": "2020-06-29T04:23:52Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/459", "timeline": [{"oid": "82bbe4bad5bd97ca9b26a078cf98621e22f535ae", "url": "https://github.com/confluentinc/kafka-tutorials/commit/82bbe4bad5bd97ca9b26a078cf98621e22f535ae", "message": "Add produce/consume in scala code (#152)", "committedDate": "2020-06-28T21:18:08Z", "type": "commit"}, {"oid": "92e54ff6060335e968d272f63c00738e630f545d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/92e54ff6060335e968d272f63c00738e630f545d", "message": "Add produce/consume in scala narrative (#152)", "committedDate": "2020-06-28T21:18:15Z", "type": "commit"}, {"oid": "bccc1a2338f5faec93c0950249121b25c9e8ba3a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/bccc1a2338f5faec93c0950249121b25c9e8ba3a", "message": "Add produce/consume in scala setup (#152)", "committedDate": "2020-06-28T21:18:21Z", "type": "commit"}, {"oid": "6f604bf01e529bd0ee96f1b3e3e31adb61856fcc", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6f604bf01e529bd0ee96f1b3e3e31adb61856fcc", "message": "Add produce/consume in scala harness runner test (#152)", "committedDate": "2020-06-28T21:18:29Z", "type": "commit"}, {"oid": "249f37d71700eb7508eb35b44cbe787166d69d96", "url": "https://github.com/confluentinc/kafka-tutorials/commit/249f37d71700eb7508eb35b44cbe787166d69d96", "message": "Add support for produce/consume tutorials (#189)", "committedDate": "2020-06-28T21:18:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc2NjkwMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r446766902", "bodyText": "If the tutorial display is approved, this should be turned off.", "author": "DivLoic", "createdAt": "2020-06-29T04:34:59Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -302,3 +302,16 @@ console-consumer-read-specific-offsets-partition:\n     ksql: disabled\n     kstreams: disabled\n     kafka: enabled\n+\n+produce-consume-lang:\n+  title: \"Produce and Consume Records in multiple languages\"\n+  meta-description: \"Produce and Consume Records in multiple languages\"\n+  slug: \"/produce-and-consume-lang/\"\n+  problem: \"you have a scala application in your favorite programming language and you'd like to consume/produce events from/to Kafka\"\n+  introduction: \"You want to enrich and expose a list of books from a library. You have to produce an event for each book acquisition (title, editor, release ...), and consume back the same events to serve the book collection over HTTP.\"\n+  status:\n+    scala: enabled\n+    swift: enabled\n+    go: enabled\n+    python: enabled\n+    c: enabled", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNTYxNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448535615", "bodyText": "Yeah, let's go ahead and set these to disabled until we get examples in other languages.", "author": "bbejeck", "createdAt": "2020-07-01T18:12:37Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -302,3 +302,16 @@ console-consumer-read-specific-offsets-partition:\n     ksql: disabled\n     kstreams: disabled\n     kafka: enabled\n+\n+produce-consume-lang:\n+  title: \"Produce and Consume Records in multiple languages\"\n+  meta-description: \"Produce and Consume Records in multiple languages\"\n+  slug: \"/produce-and-consume-lang/\"\n+  problem: \"you have a scala application in your favorite programming language and you'd like to consume/produce events from/to Kafka\"\n+  introduction: \"You want to enrich and expose a list of books from a library. You have to produce an event for each book acquisition (title, editor, release ...), and consume back the same events to serve the book collection over HTTP.\"\n+  status:\n+    scala: enabled\n+    swift: enabled\n+    go: enabled\n+    python: enabled\n+    c: enabled", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2Nzc1Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448567752", "bodyText": "For your consideration: This is a great description of what's going on with the serdes. But there's a lot of information to absorb in the Create a schema for the events section. Maybe break the serde part into a separate section in the scala.yml file?", "author": "bbejeck", "createdAt": "2020-07-01T19:19:07Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2ODM3MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448568370", "bodyText": "Missing the path and name for creating the ScalaReflectionSerde.scala file in the text.", "author": "bbejeck", "createdAt": "2020-07-01T19:20:28Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n+To do so, we will need the appropriate serializers/deserializers.\n+We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NTYwMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448575601", "bodyText": "Great details on the code. We've recently added some additional asciidoctor support\n.\nCould you put the Producer.produce  method in a code block with callouts?\nYou probably don't need this but here's an example", "author": "bbejeck", "createdAt": "2020-07-01T19:36:23Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,47 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+- Reminder: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+- `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+- `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n+\n+- `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker acknowledgement.\n+\n+- `Producer#produce` use a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n+to confirm the future completion and the record write.", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3Nzc1Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448577753", "bodyText": "Same thing here put the Consumer.consume in a code block with callouts in the narrative.", "author": "bbejeck", "createdAt": "2020-07-01T19:40:55Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,46 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+Reminder: we use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n+\n+A `Consumer#conusme` function covers most of the record consumption.\n+\n+- `Consumer#conusme` takes a\n+`https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer]`\n+and gives back a collection of `Book`.\n+\n+- `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n+`fetch.max.bytes` Consumer Config allows.\n+\n+- `Consumer#consume` extracts the value from each records. The consumed books will filled `bookMap`, our book collection\n+of type `mutable.Map[String, Book]`.\n+\n+- We also have two functions `getCount` and `getBooks` just to define two HTTP routes.", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3OTI2Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448579263", "bodyText": "Maybe just add the scala section and we'll add the other language blocks when we get tutorials.  I don't have a strong opinion on this though.", "author": "bbejeck", "createdAt": "2020-07-01T19:44:25Z", "path": "_layouts/tutorial.html", "diffHunk": "@@ -83,6 +113,36 @@ <h3 class=\"title\">Code example:</h3>\n             Basic Kafka\n           </a>\n           {% endif %}\n+          {% if site.data.tutorials[page.static_data].status.scala == 'enabled' %}\n+          <a class=\"button {% if page.stack == 'scala' %} is-selected {% endif %}\" href=\"{{ site.data.tutorials[page.static_data].slug | relative_url }}/scala.html\">\n+            Scala\n+          </a>", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU4MTU0OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448581548", "bodyText": "Same thing here as above.\nWe'll need to update the tools scripts to accommodate different languages.  I know that's beyond the scope of this PR, I'm just putting a note here.", "author": "bbejeck", "createdAt": "2020-07-01T19:49:27Z", "path": "templates/tutorial-description-template.yml", "diffHunk": "@@ -9,3 +9,9 @@\n     ksql: <KSQL-ENABLED>\n     kstreams: <KSTREAMS-ENABLED>\n     kafka: disabled\n+    c: disabled", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU4MjE1NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448582155", "bodyText": "suggestion: Produce and consume messages with clients in multiple languages", "author": "bbejeck", "createdAt": "2020-07-01T19:50:37Z", "path": "index.html", "diffHunk": "@@ -39,6 +39,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"kafka-console-consumer-producer-basics/kafka.html\">Console producer and consumer basics</a></li>\n             <li><a href=\"kafka-console-consumer-primitive-keys-values/kafka.html\">Console consumer with primitive keys and values</a></li>\n             <li><a href=\"kafka-console-consumer-read-specific-offsets-partitions/kafka.html\">Console consumer reads from a specific offset and partition</a></li>\n+            <li><a href=\"produce-consume-lang/scala.html\">Produce and consume messages with low level clients in multiple languages</a></li>", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU4Mjc1Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448582752", "bodyText": "The same comment as below, only add scala and update when we add additional tutorials.", "author": "bbejeck", "createdAt": "2020-07-01T19:51:53Z", "path": "_layouts/tutorial.html", "diffHunk": "@@ -64,6 +64,36 @@ <h3 class=\"title\">Code example:</h3>\n             Basic Kafka\n           </option>\n           {% endif %}\n+          {% if site.data.tutorials[page.static_data].status.scala == 'enabled' %}\n+          <option value=\"{{ site.data.tutorials[page.static_data].slug | relative_url }}/scala.html\" {% if page.stack == 'scala' %} selected {% endif %}>\n+          Scala\n+          </option>", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU4MzYwMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r448583602", "bodyText": "Remove the prompt in the command\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <pre class=\"snippet\"><code class=\"bash\">produce-a-book> Loic D.,How to sharpen a knife,Other,10,2020-06-09</code></pre>\n          \n          \n            \n            <pre class=\"snippet\"><code class=\"bash\"> Loic D.,How to sharpen a knife,Other,10,2020-06-09</code></pre>", "author": "bbejeck", "createdAt": "2020-07-01T19:53:54Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_1-produce-example.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+To be as interactive as possible le tutorial suggest to type books properties in form of a CSV line.\n+\n+We can create a book by starting the Produce app:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">./sbtx \"-Djline.terminal=none\" --error produce</code></pre>\n++++++\n+\n+And then typing the book to the standard entry (type \"exit\" to stop the program)\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">produce-a-book> Loic D.,How to sharpen a knife,Other,10,2020-06-09</code></pre>", "originalCommit": "249f37d71700eb7508eb35b44cbe787166d69d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "55312dd3eea0b74739f856693e8385f51964f086", "url": "https://github.com/confluentinc/kafka-tutorials/commit/55312dd3eea0b74739f856693e8385f51964f086", "message": "Add the callouts and desable the unused stacks (#152)", "committedDate": "2020-07-03T20:10:56Z", "type": "commit"}, {"oid": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "message": "Add the scala highlight support (#189)", "committedDate": "2020-07-03T23:11:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NTY0Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r453865647", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                cient.id = scala-tutorial\n          \n          \n            \n                client.id = scala-tutorial", "author": "DivLoic", "createdAt": "2020-07-13T19:00:29Z", "path": "_includes/tutorials/produce-consume-lang/scala/code/src/main/resources/application.conf", "diffHunk": "@@ -0,0 +1,61 @@\n+bootstrap.servers = \"localhost:29092\"\n+bootstrap.servers = ${?BOOTSTRAP_SERVERS}\n+\n+schema.registry.url = \"http://localhost:8081\"\n+schema.registry.url = ${?SCHEMA_REGISTRY_URL}\n+\n+producer {\n+  client-config {\n+    acks = all\n+    cient.id = scala-tutorial", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NjA0Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r453866043", "bodyText": "pretty sure it's client.id :/ https://docs.confluent.io/current/installation/configuration/producer-configs.html#client.id", "author": "DivLoic", "createdAt": "2020-07-13T19:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NTY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgxODI5Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458818293", "bodyText": "Yep! please commit this one @DivLoic", "author": "rspurgeon", "createdAt": "2020-07-22T14:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NTY0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc5NzA5OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458797098", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              problem: \"you have a scala application in your favorite programming language and you'd like to consume/produce events from/to Kafka\"\n          \n          \n            \n              problem: \"you want to produce and consume events from Kafka using a programming language other than Java\"", "author": "rspurgeon", "createdAt": "2020-07-22T13:37:03Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -302,3 +302,21 @@ console-consumer-read-specific-offsets-partition:\n     ksql: disabled\n     kstreams: disabled\n     kafka: enabled\n+\n+produce-consume-lang:\n+  title: \"Produce and Consume Records in multiple languages\"\n+  meta-description: \"Produce and Consume Records in multiple languages\"\n+  slug: \"/produce-and-consume-lang/\"\n+  problem: \"you have a scala application in your favorite programming language and you'd like to consume/produce events from/to Kafka\"", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgwMTc5MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458801790", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Lets's describe the keys pieces of this program.\n          \n          \n            \n            Lets's describe the key sections of the producer code.", "author": "rspurgeon", "createdAt": "2020-07-22T13:43:41Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgwNjI5OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458806299", "bodyText": "\"at the root of the project folder\" might be confusing for the student as there is a project subfolder.  At this stage the users terminal is at produce-consume-scala, so I suggest we just give them the file name:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create the following Sbt build file, named `build.sbt` at the root of the project folder:\n          \n          \n            \n            Create the following Sbt build file, named `build.sbt` for the project:", "author": "rspurgeon", "createdAt": "2020-07-22T13:49:50Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/2_0-create-build-file.adoc", "diffHunk": "@@ -0,0 +1,7 @@\n+Create the following Sbt build file, named `build.sbt` at the root of the project folder:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgxMDI5OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458810299", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It will be useful to package our application.\n          \n          \n            \n            Jib will be useful to package our application into a Docker image.", "author": "rspurgeon", "createdAt": "2020-07-22T13:54:50Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/2_3-create-plugins-file.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+Add in the _project_ folder the file `project/plugins.sbt` with all the Sbt plugins required.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"scala\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/project/plugins.sbt\n+%}</code></pre>\n++++++\n+`sbt-jib` is an sbt alternative to the gradle/maven plugin for https://github.com/GoogleContainerTools/jib[Jib].\n+It will be useful to package our application.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgxNTMyMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458815323", "bodyText": "I suggest we remove this section as you go into deeper details on serialization later in the tutorial.  IMO It will be easier for the student to understand the schema/serde technique when explained in one section.", "author": "rspurgeon", "createdAt": "2020-07-22T14:01:46Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/2_3-create-plugins-file.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+Add in the _project_ folder the file `project/plugins.sbt` with all the Sbt plugins required.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"scala\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/project/plugins.sbt\n+%}</code></pre>\n++++++\n+`sbt-jib` is an sbt alternative to the gradle/maven plugin for https://github.com/GoogleContainerTools/jib[Jib].\n+It will be useful to package our application.\n+\n+\n+Note: Most tutorials will include a schema code generation plugin.\n+Since code generation is not a common practice in scala we will deal with the schema by using automatic\n+derivation instead. If you still want to use code generation with sbt plugins,\n+the classic https://github.com/sbt/sbt-avro[Java class generation plugin] can be used in our scala application.\n+But plugins like https://github.com/julianpeeters/sbt-avrohugger[avro-hugger]\n+will be a better fit for our scala application since it brings scala case class generation.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgyMjEyNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458822125", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Add the corresponding conf classes `/src/main/scala/io/confluent/developer/Configuration.scala`:\n          \n          \n            \n            Add the corresponding configuration classes `src/main/scala/io/confluent/developer/Configuration.scala`:", "author": "rspurgeon", "createdAt": "2020-07-22T14:11:10Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/2_7-add-config-parser.adoc", "diffHunk": "@@ -0,0 +1,7 @@\n+Add the corresponding conf classes `/src/main/scala/io/confluent/developer/Configuration.scala`:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgyMjg2NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458822864", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Add in your project resources the file `src/main/resources/logback.xml`:\n          \n          \n            \n            Add logging configuration to your project resources in the file `src/main/resources/logback.xml`:", "author": "rspurgeon", "createdAt": "2020-07-22T14:12:17Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/2_8-add-logging-conf.adoc", "diffHunk": "@@ -0,0 +1,7 @@\n+Add in your project resources the file `src/main/resources/logback.xml`:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgyNDkyOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458824928", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As discussed earlier, our schema will be represented by case classes.\n          \n          \n            \n            We will use case classes to represent Book records in our Scala code.", "author": "rspurgeon", "createdAt": "2020-07-22T14:15:12Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_1-schema-case-class.adoc", "diffHunk": "@@ -0,0 +1,9 @@\n+As discussed earlier, our schema will be represented by case classes.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgyNjY0Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458826646", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `src/main/scala/io/confluent/developer/schema/BookType.scala`:\n          \n          \n            \n            And the following Enum type to represent the book type.\n          \n          \n            \n            \n          \n          \n            \n            `src/main/scala/io/confluent/developer/schema/BookType.scala`:", "author": "rspurgeon", "createdAt": "2020-07-22T14:17:26Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_3-schema-case-class.adoc", "diffHunk": "@@ -0,0 +1,6 @@\n+`src/main/scala/io/confluent/developer/schema/BookType.scala`:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgzMjU3Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458832573", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n          \n          \n            \n            In this tutorial, events will be will be serialized in https://avro.apache.org/[Avro] format.", "author": "rspurgeon", "createdAt": "2020-07-22T14:25:14Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgzMjk1Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458832957", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To do so, we will need the appropriate serializers/deserializers.\n          \n          \n            \n            To produce and consume events we need the appropriate serializers/deserializers.", "author": "rspurgeon", "createdAt": "2020-07-22T14:25:45Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n+To do so, we will need the appropriate serializers/deserializers.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgzNDY3MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458834670", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value\n          \n          \n            \n            We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value we'd like to serialize.", "author": "rspurgeon", "createdAt": "2020-07-22T14:28:03Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n+To do so, we will need the appropriate serializers/deserializers.\n+We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgzNDcxNw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458834717", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            we'd like to serialise. Add the class: `src/main/scala/io/confluent/developer/schema/ScalaReflectionSerde.scala`\n          \n          \n            \n            Add the following class: `src/main/scala/io/confluent/developer/schema/ScalaReflectionSerde.scala`", "author": "rspurgeon", "createdAt": "2020-07-22T14:28:06Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n+To do so, we will need the appropriate serializers/deserializers.\n+We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value\n+we'd like to serialise. Add the class: `src/main/scala/io/confluent/developer/schema/ScalaReflectionSerde.scala`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NTY3Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458845672", "bodyText": "Since there are many opinons on techniques with this, I recommend we rework this slightly by just focusing on the method we are using.  The blurb about the ReflectionAvroSerde resulting in \"a incomplete type management\" is a little vague leaving the reader wondering about the issue is.\nI suggest we remove that and simplify the section.  I'll make an attempt at that here but please feel free to rework as you see fit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            While the specific method (where code is generated from schemas) for creating record is more common,\n          \n          \n            \n            in scala you might want to use the reflection method (where schemas is generated from code)\n          \n          \n            \n            to preserve your case class flexibility. Since Confluent 5.4.0, a\n          \n          \n            \n            `https://docs.confluent.io/current/streams/developer-guide/datatypes.html#reflection-avro[ReflectionAvroSerde]` class\n          \n          \n            \n            let you do that, but using it in scala might result in a incomplete type management. The previous file shows\n          \n          \n            \n            a quick way to cover this need in scala by using https://github.com/sksamuel/avro4s[Avro4s].\n          \n          \n            \n            Avro4s is a scala library generation of Apache Avro objects. It will create records or instances in two steps:\n          \n          \n            \n            \n          \n          \n            \n            - Serialization:    Raw bytes <- Generic Records <- Case Class Instances\n          \n          \n            \n            - Deserialization:  Raw bytes -> Generic Records -> Case Class Instances\n          \n          \n            \n            In this tutorial we are going to use a reflection based serialization method using https://github.com/sksamuel/avro4s[Avro4s], which is a serializing/deserializing library for Avro written in Scala.  \n          \n          \n            \n            \n          \n          \n            \n            Avro4s performs it's de/serialization in multiple stages:\n          \n          \n            \n            - For Serialization:      `Case Class Instance   ->  Generic Record -> Avro Formatted Bytes`\n          \n          \n            \n            - For Deserialization:  `Avro Formatted bytes -> Generic Record -> Case Class Instance`", "author": "rspurgeon", "createdAt": "2020-07-22T14:42:43Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_4-schema-serde.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+The case class instances in our code will be serialized in an https://avro.apache.org/[Avro] format.\n+To do so, we will need the appropriate serializers/deserializers.\n+We add the following function to easily create a `Serializer[T]` where `T` in the type of the key or the value\n+we'd like to serialise. Add the class: `src/main/scala/io/confluent/developer/schema/ScalaReflectionSerde.scala`\n+\n++++++\n+<pre class=\"snippet\"><code class=\"scala\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/schema/ScalaReflectionSerde.scala\n+%}</code></pre>\n++++++\n+\n+While the specific method (where code is generated from schemas) for creating record is more common,\n+in scala you might want to use the reflection method (where schemas is generated from code)\n+to preserve your case class flexibility. Since Confluent 5.4.0, a\n+`https://docs.confluent.io/current/streams/developer-guide/datatypes.html#reflection-avro[ReflectionAvroSerde]` class\n+let you do that, but using it in scala might result in a incomplete type management. The previous file shows\n+a quick way to cover this need in scala by using https://github.com/sksamuel/avro4s[Avro4s].\n+Avro4s is a scala library generation of Apache Avro objects. It will create records or instances in two steps:\n+\n+- Serialization:    Raw bytes <- Generic Records <- Case Class Instances\n+- Deserialization:  Raw bytes -> Generic Records -> Case Class Instances", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3NTM1Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r464575353", "bodyText": "Hi @rspurgeon,\nHonestly, I just tried to give everything I've got here (thinking: the more, the better)\nBut if this version says enough to you I think it's perfectly fine!!", "author": "DivLoic", "createdAt": "2020-08-03T18:01:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NTY3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0ODM3MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458848371", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In consequence, we will also add a package object to hold all the necessary functions (CSV conversion, JSON Conversion ...)\n          \n          \n            \n            that are not directly related to our goal (produce and consume). Theses functions will increase the interactive aspect\n          \n          \n            \n            of the tutorial.\n          \n          \n            \n            Let's add some helper functions to the code making our application more interactive.", "author": "rspurgeon", "createdAt": "2020-07-22T14:46:03Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/3_5-utils-implicits.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+In consequence, we will also add a package object to hold all the necessary functions (CSV conversion, JSON Conversion ...)\n+that are not directly related to our goal (produce and consume). Theses functions will increase the interactive aspect\n+of the tutorial.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3OTkyOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r464579928", "bodyText": "I tried to insist on not focusing too much on this part. Maybe I'm the one focusing too much \ud83d\ude06!", "author": "DivLoic", "createdAt": "2020-08-03T18:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0ODM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0OTk1NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458849955", "bodyText": "This is well authored!  Very clear and concise explanation of the purpose of the sbt packages and how these functions are usually performed outside the application.", "author": "rspurgeon", "createdAt": "2020-07-22T14:48:01Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/4_0-helper-package.adoc", "diffHunk": "@@ -0,0 +1,10 @@\n+Topic creation and avro schema declaration are often part of an external process. For the sake of clarity in this\n+tutorial, we won't include these steps as part of the main application, but isolate theme in a dedicated package.\n+", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MzkwMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458853902", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Check the `build.sbt` again. You will find the tasks declared as `TaskKey[Unit]` with a main class corresponding to the\n          \n          \n            \n            two last tasks.\n          \n          \n            \n            These sbt tasks are defined in the `build.sbt` file and are declared as `TaskKey[Unit]` with the corresponding main class .", "author": "rspurgeon", "createdAt": "2020-07-22T14:52:59Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/4_3-sbt-execution.adoc", "diffHunk": "@@ -0,0 +1,10 @@\n+Run the following command to create the topic and then the schemas:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+include_raw tutorials/produce-consume-lang/scala/code/tutorial-steps/dev/run-helpers.sh\n+%}</code></pre>\n++++++\n+\n+Check the `build.sbt` again. You will find the tasks declared as `TaskKey[Unit]` with a main class corresponding to the\n+two last tasks.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1NDk0MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458854940", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Add the following class in the `src/main/scala/io/confluent/developer/helper/SchemaPublication.scala`\n          \n          \n            \n            Add the following class in the file `src/main/scala/io/confluent/developer/helper/SchemaPublication.scala`", "author": "rspurgeon", "createdAt": "2020-07-22T14:54:28Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/4_2-schema-task.adoc", "diffHunk": "@@ -0,0 +1,7 @@\n+Add the following class in the `src/main/scala/io/confluent/developer/helper/SchemaPublication.scala`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1NTM5NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458855395", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The following Scala class will be the entry point of the producer application.\n          \n          \n            \n            At `src/main/scala/io/confluent/developer/Producer.scala` add:\n          \n          \n            \n            The following Scala class will be the entry point of the producer application.\n          \n          \n            \n            \n          \n          \n            \n            Add the following class in the file `src/main/scala/io/confluent/developer/Producer.scala`", "author": "rspurgeon", "createdAt": "2020-07-22T14:55:05Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1NjQyNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458856425", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n          \n          \n            \n            of the `application.conf`\n          \n          \n            \n            NOTE: The producer application loads its configuration from the _producer_ block\n          \n          \n            \n            of the `application.conf`", "author": "rspurgeon", "createdAt": "2020-07-22T14:56:31Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1NzYyNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458857624", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <1> `Producer#produce` takes a\n          \n          \n            \n            `https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n          \n          \n            \n            a topic name, and an instance of book to send into a Kafka topic.\n          \n          \n            \n            <1> The function accepts a\n          \n          \n            \n            `https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n          \n          \n            \n            a topic name, and an instance of book to send into a Kafka topic.", "author": "rspurgeon", "createdAt": "2020-07-22T14:58:05Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1ODMwNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458858304", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n          \n          \n            \n            <2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where we attach the topic name to the book.", "author": "rspurgeon", "createdAt": "2020-07-22T14:59:04Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+<2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1OTg1MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458859850", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <3> `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker\n          \n          \n            \n            acknowledgement.\n          \n          \n            \n            <3> The `KafkaProducer#send` method  is called on the `producer` instance, returning a Java future which will contain the broker response.", "author": "rspurgeon", "createdAt": "2020-07-22T15:01:11Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+<2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n+\n+<3> `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker\n+acknowledgement.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg2MDg1OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458860858", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <4> `Producer#produce` use a\n          \n          \n            \n            `https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n          \n          \n            \n            to confirm the future completion and the record write.\n          \n          \n            \n            <4> `Producer#produce` uses a\n          \n          \n            \n            `https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n          \n          \n            \n            to respond to success or failure for the record production.", "author": "rspurgeon", "createdAt": "2020-07-22T15:02:30Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+<2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n+\n+<3> `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker\n+acknowledgement.\n+\n+<4> `Producer#produce` use a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n+to confirm the future completion and the record write.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3MDE2MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458870161", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \ud83e\udd14 \ud83e\udd14 Now what do we need to call our new `Producer#produce` function?\n          \n          \n            \n            \n          \n          \n            \n            - The `KafkaProducer` has types parameter corresponding to its key and value types. Its constructor takes\n          \n          \n            \n            serializer / deserializers with the same types.\n          \n          \n            \n            \n          \n          \n            \n            - `reflectionSerializer4S[T]` is introduced to lighten the tutorial and quickly get a `Serializer[Book]` (See dev part n\u00b03).\n          \n          \n            \n            \n          \n          \n            \n            - We carefully configure, the serializer with a map containing the schema-registry url.\n          \n          \n            \n            \n          \n          \n            \n            - The keys will always be null in our example, so we choose an arbitrary type such `Bytes`\n          \n          \n            \n            from _kafka.common.utils_ package.\n          \n          \n            \n            \n          \n          \n            \n            - Then we instantiate the `KafkaProducer` by passing a Java Properties from the _producer.client-config_ block of\n          \n          \n            \n            `application.conf`\n          \n          \n            \n            In order to utilize the `Producer#produce` function, we construct an instance of our `KafkaProducer` to pass to it.\n          \n          \n            \n            \n          \n          \n            \n            - The `KafkaProducer` has types parameter corresponding to its key and value types. Its constructor takes\n          \n          \n            \n            serializer / deserializers with the same types.\n          \n          \n            \n            \n          \n          \n            \n            - As described above, we utilze the `reflectionSerializer4S[T]` for our `Serializer[Book]` which is configured to connect to the Schema Registry.\n          \n          \n            \n            \n          \n          \n            \n            - Our Kafka records do not have keys and will always be null, so we choose an arbitrary type such `Bytes`\n          \n          \n            \n            from the _kafka.common.utils_ package.\n          \n          \n            \n            \n          \n          \n            \n            - We instantiate the `KafkaProducer` by passing a Java Properties from the _producer.client-config_ block of\n          \n          \n            \n            `application.conf`", "author": "rspurgeon", "createdAt": "2020-07-22T15:15:05Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+<2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n+\n+<3> `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker\n+acknowledgement.\n+\n+<4> `Producer#produce` use a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n+to confirm the future completion and the record write.\n+\n+\ud83e\udd14 \ud83e\udd14 Now what do we need to call our new `Producer#produce` function?\n+\n+- The `KafkaProducer` has types parameter corresponding to its key and value types. Its constructor takes\n+serializer / deserializers with the same types.\n+\n+- `reflectionSerializer4S[T]` is introduced to lighten the tutorial and quickly get a `Serializer[Book]` (See dev part n\u00b03).\n+\n+- We carefully configure, the serializer with a map containing the schema-registry url.\n+\n+- The keys will always be null in our example, so we choose an arbitrary type such `Bytes`\n+from _kafka.common.utils_ package.\n+\n+- Then we instantiate the `KafkaProducer` by passing a Java Properties from the _producer.client-config_ block of\n+`application.conf`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3MTUzOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458871539", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: At the end of the programme we use `KafkaProducer#flush` to check\n          \n          \n            \n            NOTE: At the end of the program we use `KafkaProducer#flush` to check", "author": "rspurgeon", "createdAt": "2020-07-22T15:17:01Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_0-produce-narrative.adoc", "diffHunk": "@@ -0,0 +1,61 @@\n+The following Scala class will be the entry point of the producer application.\n+At `src/main/scala/io/confluent/developer/Producer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Producer.scala\n+%}</code></pre>\n++++++\n+\n+Lets's describe the keys pieces of this program.\n+\n+NOTE: we use an externalised config and this app loads its config from the _producer_ bloc\n+of the `application.conf`\n+\n+A `Producer#produce` function covers most of the record production.\n+\n+[source,scala]\n+----\n+def produce(producer: KafkaProducer[Bytes, Book], topic: String, book: Book): Future[RecordMetadata] = { //<1>\n+    val record: ProducerRecord[Bytes, Book] = new ProducerRecord(topic, book) //<2>\n+\n+    producer.send(record, new Callback { //<3>\n+      override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = Option(exception) //<4>\n+        .map(ex => logger error s\"fail to produce record due to: ${ex.getMessage}\")\n+        .getOrElse(logger info s\"successfully produced - ${printMetaData(metadata)}\")\n+    })\n+}\n+----\n+\n+<1> `Producer#produce` takes a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[KafkaProducer]`,\n+a topic name, and an instance of book to send into a Kafka topic.\n+\n+<2> `Producer#produce` wraps our book with a `ProducerRecord[K, V]`, that's where attache the topic name to the book.\n+\n+<3> `Producer#produce` call the `KafkaProducer#send` method and gets back Java future tight to the broker\n+acknowledgement.\n+\n+<4> `Producer#produce` use a\n+`https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html[CallBack]`\n+to confirm the future completion and the record write.\n+\n+\ud83e\udd14 \ud83e\udd14 Now what do we need to call our new `Producer#produce` function?\n+\n+- The `KafkaProducer` has types parameter corresponding to its key and value types. Its constructor takes\n+serializer / deserializers with the same types.\n+\n+- `reflectionSerializer4S[T]` is introduced to lighten the tutorial and quickly get a `Serializer[Book]` (See dev part n\u00b03).\n+\n+- We carefully configure, the serializer with a map containing the schema-registry url.\n+\n+- The keys will always be null in our example, so we choose an arbitrary type such `Bytes`\n+from _kafka.common.utils_ package.\n+\n+- Then we instantiate the `KafkaProducer` by passing a Java Properties from the _producer.client-config_ block of\n+`application.conf`\n+\n+That's it! we are ready to produce records.\n+\n+NOTE: At the end of the programme we use `KafkaProducer#flush` to check", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4NjA1NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r464586054", "bodyText": "Sorry \ud83d\ude4b\u200d\u2642\ufe0f\ud83c\uddeb\ud83c\uddf7\ud83c\uddeb\ud83c\uddf7\ud83c\uddeb\ud83c\uddf7\ud83e\udd56\ud83e\uddc0", "author": "DivLoic", "createdAt": "2020-08-03T18:22:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3MTUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3MjgzNg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458872836", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To be as interactive as possible le tutorial suggest to type books properties in form of a CSV line.\n          \n          \n            \n            To make producing records easy, the producer application can accept book data in the form of a CSV line.", "author": "rspurgeon", "createdAt": "2020-07-22T15:18:46Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_1-produce-example.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+To be as interactive as possible le tutorial suggest to type books properties in form of a CSV line.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3Mzk2Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458873967", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And then typing the book to the standard entry (type \"exit\" to stop the program)\n          \n          \n            \n            And then typing the book data at the prompt (type \"exit\" to stop the program)", "author": "rspurgeon", "createdAt": "2020-07-22T15:20:17Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_1-produce-example.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+To be as interactive as possible le tutorial suggest to type books properties in form of a CSV line.\n+\n+We can create a book by starting the Produce app:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">./sbtx \"-Djline.terminal=none\" --error produce</code></pre>\n++++++\n+\n+And then typing the book to the standard entry (type \"exit\" to stop the program)", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3NTc5Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458875796", "bodyText": "IMO this section isn't necessary\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The CSV file contains the following lines without headers\n          \n          \n            \n            \n          \n          \n            \n            |===\n          \n          \n            \n            |Author |Title |Type |Pages |Release Date\n          \n          \n            \n            \n          \n          \n            \n            |Franz Kafka\n          \n          \n            \n            |Der Prozess\n          \n          \n            \n            |Novel\n          \n          \n            \n            |239\n          \n          \n            \n            |1925-09-01\n          \n          \n            \n            \n          \n          \n            \n            |Franz Kafka\n          \n          \n            \n            |Die Verwandlung\n          \n          \n            \n            |Novel\n          \n          \n            \n            |144\n          \n          \n            \n            |1915-01-01\n          \n          \n            \n            \n          \n          \n            \n            |Paul Chiusano\n          \n          \n            \n            |Functional Programming in Scala\n          \n          \n            \n            |Tech\n          \n          \n            \n            |320\n          \n          \n            \n            |2014-09-01\n          \n          \n            \n            \n          \n          \n            \n            |Stendhal\n          \n          \n            \n            |Le Rouge et le Noir\n          \n          \n            \n            |Novel\n          \n          \n            \n            |640\n          \n          \n            \n            |1830-09-01\n          \n          \n            \n            \n          \n          \n            \n            |\u00c9mile Zola\n          \n          \n            \n            |Au Bonheur des Dames\n          \n          \n            \n            |Novel\n          \n          \n            \n            |542\n          \n          \n            \n            |1883-11-01\n          \n          \n            \n            \n          \n          \n            \n            |Loic D.\n          \n          \n            \n            |Not the worst ramen recipe\n          \n          \n            \n            |Other\n          \n          \n            \n            |3\n          \n          \n            \n            |2020-06-01\n          \n          \n            \n            \n          \n          \n            \n            |Neha Narkhede\n          \n          \n            \n            |Kafka: The Definitive Guide\n          \n          \n            \n            |Tech\n          \n          \n            \n            |322\n          \n          \n            \n            |2017-07-07\n          \n          \n            \n            |===", "author": "rspurgeon", "createdAt": "2020-07-22T15:22:50Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_2-produce-dataset.adoc", "diffHunk": "@@ -0,0 +1,55 @@\n+Add the following file as an example dataset: `data.csv`\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/data.csv\n+%}</code></pre>\n++++++\n+\n+The CSV file contains the following lines without headers\n+\n+|===\n+|Author |Title |Type |Pages |Release Date\n+\n+|Franz Kafka\n+|Der Prozess\n+|Novel\n+|239\n+|1925-09-01\n+\n+|Franz Kafka\n+|Die Verwandlung\n+|Novel\n+|144\n+|1915-01-01\n+\n+|Paul Chiusano\n+|Functional Programming in Scala\n+|Tech\n+|320\n+|2014-09-01\n+\n+|Stendhal\n+|Le Rouge et le Noir\n+|Novel\n+|640\n+|1830-09-01\n+\n+|\u00c9mile Zola\n+|Au Bonheur des Dames\n+|Novel\n+|542\n+|1883-11-01\n+\n+|Loic D.\n+|Not the worst ramen recipe\n+|Other\n+|3\n+|2020-06-01\n+\n+|Neha Narkhede\n+|Kafka: The Definitive Guide\n+|Tech\n+|322\n+|2017-07-07\n+|===", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3NTk2Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458875963", "bodyText": "LOL!", "author": "rspurgeon", "createdAt": "2020-07-22T15:23:03Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/5_2-produce-dataset.adoc", "diffHunk": "@@ -0,0 +1,55 @@\n+Add the following file as an example dataset: `data.csv`\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/data.csv\n+%}</code></pre>\n++++++\n+\n+The CSV file contains the following lines without headers\n+\n+|===\n+|Author |Title |Type |Pages |Release Date\n+\n+|Franz Kafka\n+|Der Prozess\n+|Novel\n+|239\n+|1925-09-01\n+\n+|Franz Kafka\n+|Die Verwandlung\n+|Novel\n+|144\n+|1915-01-01\n+\n+|Paul Chiusano\n+|Functional Programming in Scala\n+|Tech\n+|320\n+|2014-09-01\n+\n+|Stendhal\n+|Le Rouge et le Noir\n+|Novel\n+|640\n+|1830-09-01\n+\n+|\u00c9mile Zola\n+|Au Bonheur des Dames\n+|Novel\n+|542\n+|1883-11-01\n+\n+|Loic D.\n+|Not the worst ramen recipe", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3NzU4MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458877581", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The following Scala class will be the entry point of the consumer application.\n          \n          \n            \n            At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n          \n          \n            \n            The following Scala class defines our Consumer application.\n          \n          \n            \n            \n          \n          \n            \n            Add the following class to the file `src/main/scala/io/confluent/developer/Consumer.scala`", "author": "rspurgeon", "createdAt": "2020-07-22T15:25:12Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg3ODIwNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458878204", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n          \n          \n            \n            NOTE: The Consumer application loads its configuration from the _consumer_ bloc of the `application.conf`", "author": "rspurgeon", "createdAt": "2020-07-22T15:26:04Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg4MDM0MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458880341", "bodyText": "IMO remove the dialog about fetch.max.bytes; slightly advanced and also doesn't tell the full story about how many records will be in the return set.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <2> `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n          \n          \n            \n            `fetch.max.bytes` Consumer Config allows.\n          \n          \n            \n            <2> We call the `KafkaConsumer#poll` function on the `KafkaConsumer` which returns a `ConsumerRecords` containing Book instances.", "author": "rspurgeon", "createdAt": "2020-07-22T15:29:03Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n+\n+A `Consumer#conusme` function covers most of the record consumption.\n+\n+[source,scala]\n+----\n+def consume(consumer: KafkaConsumer[Bytes, Book]): Vector[Book] = { //<1>\n+\n+  val books: ConsumerRecords[Bytes, Book] = consumer.poll((1 second) toJava) //<2>\n+\n+  books.asScala.toVector.map(_.value()) //<3>\n+}\n+----\n+\n+<1> `Consumer#conusme` takes a\n+`https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer]`\n+and gives back a collection of `Book`.\n+\n+<2> `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n+`fetch.max.bytes` Consumer Config allows.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg4MzAxMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458883013", "bodyText": "Considering the addition to the class level map isn't done here, IMO it's slightly confusing to the reader.  I would remove that bit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <3> `Consumer#consume` extracts the value from each records. The consumed books will filled `bookMap`, our book collection\n          \n          \n            \n            of type `mutable.Map[String, Book]`.\n          \n          \n            \n            <3> For each returned value in the `ConsumerRecords` instance, the record value is extracted and passed back in a `Vector`.", "author": "rspurgeon", "createdAt": "2020-07-22T15:32:32Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n+\n+A `Consumer#conusme` function covers most of the record consumption.\n+\n+[source,scala]\n+----\n+def consume(consumer: KafkaConsumer[Bytes, Book]): Vector[Book] = { //<1>\n+\n+  val books: ConsumerRecords[Bytes, Book] = consumer.poll((1 second) toJava) //<2>\n+\n+  books.asScala.toVector.map(_.value()) //<3>\n+}\n+----\n+\n+<1> `Consumer#conusme` takes a\n+`https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer]`\n+and gives back a collection of `Book`.\n+\n+<2> `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n+`fetch.max.bytes` Consumer Config allows.\n+\n+<3> `Consumer#consume` extracts the value from each records. The consumed books will filled `bookMap`, our book collection\n+of type `mutable.Map[String, Book]`.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg4NDM4MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458884380", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We also have two functions `getCount` and `getBooks` just to define two HTTP routes.\n          \n          \n            \n            Our Consumer application functions as an HTTP service defining two routes, `/count` and `/books`.  These routes are mapped to the functions `getCount` and `getBooks` in our code.", "author": "rspurgeon", "createdAt": "2020-07-22T15:34:20Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n+\n+A `Consumer#conusme` function covers most of the record consumption.\n+\n+[source,scala]\n+----\n+def consume(consumer: KafkaConsumer[Bytes, Book]): Vector[Book] = { //<1>\n+\n+  val books: ConsumerRecords[Bytes, Book] = consumer.poll((1 second) toJava) //<2>\n+\n+  books.asScala.toVector.map(_.value()) //<3>\n+}\n+----\n+\n+<1> `Consumer#conusme` takes a\n+`https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer]`\n+and gives back a collection of `Book`.\n+\n+<2> `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n+`fetch.max.bytes` Consumer Config allows.\n+\n+<3> `Consumer#consume` extracts the value from each records. The consumed books will filled `bookMap`, our book collection\n+of type `mutable.Map[String, Book]`.\n+\n+We also have two functions `getCount` and `getBooks` just to define two HTTP routes.\n+", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg4NzE3NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458887175", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \ud83e\udd14 \ud83e\udd14 Now what do we need to call our new `Consumer#consume` function?\n          \n          \n            \n            \n          \n          \n            \n            - `reflectionDeserializer4S[T]` is introduced to lighten the tutorial and quickly get a `Deserializer[Book]`\n          \n          \n            \n            (See dev part n\u00b03).\n          \n          \n            \n            \n          \n          \n            \n            - We carefully configure, the deserializer with a map containing the schema-registry url.\n          \n          \n            \n            \n          \n          \n            \n            - Then we instantiate the `KafkaConsumer` by passing a Java Properties from the _consumer.client-config_ block of\n          \n          \n            \n            `application.conf`.\n          \n          \n            \n            \n          \n          \n            \n            - We subscribe to the book's topics by calling `KafkaConsumer#suscribe` with a topic name collection.\n          \n          \n            \n            \n          \n          \n            \n            That's it! we are ready to poll records in a infinite while loop to keep our Map of book updated.\n          \n          \n            \n            Similar to the Producer code, we utilize our `Consumer#consume` function by constructing an instance of the `KafkaConsumer`.  \n          \n          \n            \n            \n          \n          \n            \n            - The `KafkaConsumer` is constructed with the `reflectionDeserializer4S[T]` which is configured to connect to the Schema Registry.\n          \n          \n            \n            \n          \n          \n            \n            - The `KafkaConsumer` reads it's configuration from the _consumer.client-config_ block of\n          \n          \n            \n            `application.conf`.\n          \n          \n            \n            \n          \n          \n            \n            - Finally, we subscribe to the `BOOK` topic by calling `KafkaConsumer#suscribe`.\n          \n          \n            \n            \n          \n          \n            \n            That's it! we are ready to poll records in a loop, updating our map of books.  The map of books is used to respond to queries on the HTTP routes.", "author": "rspurgeon", "createdAt": "2020-07-22T15:38:15Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/dev/6_0-consume-narrative.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+The following Scala class will be the entry point of the consumer application.\n+At `src/main/scala/io/confluent/developer/Consumer.scala` add:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala\n+%}</code></pre>\n++++++\n+\n+Lets\u2019s describe the keys pieces of this program.\n+\n+NOTE: We use an externalised config and this app loads its config from the _consumer_ bloc of the `application.conf`\n+\n+A `Consumer#conusme` function covers most of the record consumption.\n+\n+[source,scala]\n+----\n+def consume(consumer: KafkaConsumer[Bytes, Book]): Vector[Book] = { //<1>\n+\n+  val books: ConsumerRecords[Bytes, Book] = consumer.poll((1 second) toJava) //<2>\n+\n+  books.asScala.toVector.map(_.value()) //<3>\n+}\n+----\n+\n+<1> `Consumer#conusme` takes a\n+`https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer]`\n+and gives back a collection of `Book`.\n+\n+<2> `Consumer#consume` call the `KafkaConsumer#poll` and gets a `ConsumerRecords` filed with as much books as the\n+`fetch.max.bytes` Consumer Config allows.\n+\n+<3> `Consumer#consume` extracts the value from each records. The consumed books will filled `bookMap`, our book collection\n+of type `mutable.Map[String, Book]`.\n+\n+We also have two functions `getCount` and `getBooks` just to define two HTTP routes.\n+\n+\ud83e\udd14 \ud83e\udd14 Now what do we need to call our new `Consumer#consume` function?\n+\n+- `reflectionDeserializer4S[T]` is introduced to lighten the tutorial and quickly get a `Deserializer[Book]`\n+(See dev part n\u00b03).\n+\n+- We carefully configure, the deserializer with a map containing the schema-registry url.\n+\n+- Then we instantiate the `KafkaConsumer` by passing a Java Properties from the _consumer.client-config_ block of\n+`application.conf`.\n+\n+- We subscribe to the book's topics by calling `KafkaConsumer#suscribe` with a topic name collection.\n+\n+That's it! we are ready to poll records in a infinite while loop to keep our Map of book updated.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkxNzA5NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458917094", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            *What* to test? We don't want to test Kafka clients them self's. Instead, we'd like to test functions with\n          \n          \n            \n            business behavior, even if they have side effects due to the call to Kafka producers / consumers.\n          \n          \n            \n            *What* to test? We don't want to test Kafka clients themselves. Instead, we'd like to test functions with\n          \n          \n            \n            business behavior, even if they have side effects due to the call to Kafka producers / consumers.", "author": "rspurgeon", "createdAt": "2020-07-22T16:21:29Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/test/2_0-make-test-sources-dir.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+Let's create a directory for the tests to live in:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/tutorial-steps/test/make-test-sources-dir.sh\n+%}</code></pre>\n++++++\n+\n+Before we go further we need to discuss *what* and *how* to test in these applications.\n+\n+*What* to test? We don't want to test Kafka clients them self's. Instead, we'd like to test functions with\n+business behavior, even if they have side effects due to the call to Kafka producers / consumers.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkxODk5Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458918997", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            *How* to test? On the opposite of most tutorials on this site, we are not testing streaming applications here.\n          \n          \n            \n            Which means we can not extract a streaming topology and test it separately.\n          \n          \n            \n            We have to spawn a real Kafka broker to test our functions. And to do this we will use the\n          \n          \n            \n            https://https://www.testcontainers.org/modules/kafka/[testcontainers] Kafka module.\n          \n          \n            \n            *How* to test? We are going to use a real Kafka broker to test our functions, and to do this we will use Test Containers and the https://https://www.testcontainers.org/modules/kafka/[testcontainers Kafka module].", "author": "rspurgeon", "createdAt": "2020-07-22T16:23:23Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/test/2_0-make-test-sources-dir.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+Let's create a directory for the tests to live in:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/tutorial-steps/test/make-test-sources-dir.sh\n+%}</code></pre>\n++++++\n+\n+Before we go further we need to discuss *what* and *how* to test in these applications.\n+\n+*What* to test? We don't want to test Kafka clients them self's. Instead, we'd like to test functions with\n+business behavior, even if they have side effects due to the call to Kafka producers / consumers.\n+\n+*How* to test? On the opposite of most tutorials on this site, we are not testing streaming applications here.\n+Which means we can not extract a streaming topology and test it separately.\n+We have to spawn a real Kafka broker to test our functions. And to do this we will use the\n+https://https://www.testcontainers.org/modules/kafka/[testcontainers] Kafka module.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODc3OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r464858778", "bodyText": "I really thought the comparison with the Kafka Streams Test Utils was important for someone already familiar with other tutorials.", "author": "DivLoic", "createdAt": "2020-08-04T07:38:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkxODk5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAyNzcyOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r465027729", "bodyText": "If you feel strongly that it's good for the student, please feel free to leave it!", "author": "rspurgeon", "createdAt": "2020-08-04T12:54:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkxODk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkyMTUyMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458921523", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Because theses tests might be asynchronous we will use the scalatest functions from `Eventually`.\n          \n          \n            \n            And because all Kafka Clients related tests will extends this trait let's call it:\n          \n          \n            \n            `src/test/scala/io/confluent/developer/KafkaFlatSpec.scala`\n          \n          \n            \n            Because theses tests might be asynchronous, we will use the scalatest functions from `Eventually`. \n          \n          \n            \n            \n          \n          \n            \n            All Kafka Clients related tests will extends this trait, so let's call it:\n          \n          \n            \n            `src/test/scala/io/confluent/developer/KafkaFlatSpec.scala`", "author": "rspurgeon", "createdAt": "2020-07-22T16:25:49Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/test/2_1-write-test-trait.adoc", "diffHunk": "@@ -0,0 +1,10 @@\n+First we create a `trait` that will extend `AnyFlatSpec`, `Matchers` and other tests traits.\n+Because theses tests might be asynchronous we will use the scalatest functions from `Eventually`.\n+And because all Kafka Clients related tests will extends this trait let's call it:\n+`src/test/scala/io/confluent/developer/KafkaFlatSpec.scala`", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkyMzgyNg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458923826", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Find the sbt task configuration of `packageConsumer` in the _build.sbt_ file.\n          \n          \n            \n            Reference the sbt task configuration of `packageConsumer` in the _build.sbt_ file for details.", "author": "rspurgeon", "createdAt": "2020-07-22T16:28:42Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/prod/1_0-consumer-docker-image.adoc", "diffHunk": "@@ -0,0 +1,9 @@\n+In your terminal, execute the following command to invoke the Jib plugin and build a docker image for the consumer app:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/tutorial-steps/prod/build-consumer.sh\n+%}</code></pre>\n++++++\n+\n+Find the sbt task configuration of `packageConsumer` in the _build.sbt_ file.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkyNDE1Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458924157", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Find the sbt task configuration of `packageProducer` in the _build.sbt_ file.\n          \n          \n            \n            Reference the sbt task configuration of `packageProducer` in the _build.sbt_ file for details.", "author": "rspurgeon", "createdAt": "2020-07-22T16:29:08Z", "path": "_includes/tutorials/produce-consume-lang/scala/markup/prod/2_0-producer-docker-image.adoc", "diffHunk": "@@ -0,0 +1,9 @@\n+In your terminal, execute the following command to invoke the Jib plugin and build a docker image for the producer app:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"bash\">{%\n+    include_raw tutorials/produce-consume-lang/scala/code/tutorial-steps/prod/build-producer.sh\n+%}</code></pre>\n++++++\n+\n+Find the sbt task configuration of `packageProducer` in the _build.sbt_ file.", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk4Nzc5Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r458987792", "bodyText": "I circulated a question to the DevRel team about serialisation vs serialization and the consesus was to stick w/ the \"z\" spelling.  I'm going to make suggested edits for these throughout.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - title: Setup the serialisation method\n          \n          \n            \n                - title: Setup the serialization method", "author": "rspurgeon", "createdAt": "2020-07-22T18:11:29Z", "path": "_data/harnesses/produce-consume-lang/scala.yml", "diffHunk": "@@ -0,0 +1,261 @@\n+dev:\n+  steps:\n+    - title: Get Confluent Platform\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/project-dirs.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/1_0-create-project-dirs.adoc\n+\n+        - change_directory: produce-consume-scala\n+          action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/1_1-make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/1_2-start-docker-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Initialize the project\n+      content:\n+        - action: make_file\n+          file: build.sbt\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_0-create-build-file.adoc\n+\n+        - action: make_file\n+          file: project/build.properties\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_1-create-properties-file.adoc\n+\n+        - action: make_file\n+          file: project/Dependencies.scala\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_2-create-deps-file.adoc\n+\n+        - action: make_file\n+          file: project/plugins.sbt\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_3-create-plugins-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/sbt-extras.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_4-sbt-extras.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-resources-dir.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_5-resources-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/resources/application.conf\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_6-add-config-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-sources-dir.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_5-sources-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/scala/io/confluent/developer/Configuration.scala\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_7-add-config-parser.adoc\n+\n+        - action: make_file\n+          file: src/main/resources/logback.xml\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/2_8-add-logging-conf.adoc\n+\n+    - title: Create a schema for the events\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-schema-package.sh\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/3_0-schema-package.adoc\n+\n+        - action: make_file\n+          file: src/main/scala/io/confluent/developer/schema/Book.scala\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/3_1-schema-case-class.adoc\n+\n+        - action: make_file\n+          file: src/main/scala/io/confluent/developer/schema/BookType.scala\n+          render:\n+            file: tutorials/produce-consume-lang/scala/markup/dev/3_3-schema-case-class.adoc\n+\n+    - title: Setup the serialisation method", "originalCommit": "f8ec06d3e4f72b7e1815f5e2aba39602c99e1689", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4871113bab7be391b437dd0ee7150cc3a7763c08", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4871113bab7be391b437dd0ee7150cc3a7763c08", "message": "Merge remote-tracking branch 'upstream/master' into ldivad-produce-consume-scala", "committedDate": "2020-08-04T08:10:50Z", "type": "commit"}, {"oid": "677db0888e3a44efe985ecdfbf35dc38537e8261", "url": "https://github.com/confluentinc/kafka-tutorials/commit/677db0888e3a44efe985ecdfbf35dc38537e8261", "message": "Accept narrative suggestions from PR459 (#152)", "committedDate": "2020-08-04T08:28:52Z", "type": "commit"}, {"oid": "4788ae78b0fe20137930508a8237f447d90dee48", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4788ae78b0fe20137930508a8237f447d90dee48", "message": "Accept narrative suggestions from PR459 (#152)", "committedDate": "2020-08-04T08:45:08Z", "type": "commit"}, {"oid": "39b27697272a8bcc0a98518dd812c3cc81ff4885", "url": "https://github.com/confluentinc/kafka-tutorials/commit/39b27697272a8bcc0a98518dd812c3cc81ff4885", "message": "Accept narrative suggestions from PR459 (#152)", "committedDate": "2020-08-04T09:08:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMDYxMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r466220613", "bodyText": "Hi @bbejeck , Hi @rspurgeon\nI have a suggestion here. The KafkaConsumer#close inside the shutdown hook has inconsistent behaviours. It randomly raises a ConcurrentModificationException.  To solve this we can use a Scheduler and close the consumer inside the same thread, once we leave the while loop.\nval scheduler: ScheduledExecutorService = Executors.newSingleThreadScheduledExecutor()\n  scheduler.schedule(new Runnable {\n\n    override def run(): Unit = {\n      logger debug \"creating the deserializers and configuring\"\n      val bookDeserializer: Deserializer[Book] = reflectionDeserializer4S[Book]\n      bookDeserializer.configure(schemaRegistryConfigMap.asJava, false)\n\n      logger debug \"creating the kafka consumer\"\n      val consumer = new KafkaConsumer[Bytes, Book](\n        consumerConf.clientConfig.toProperties,\n        Serdes.Bytes().deserializer(),\n        bookDeserializer\n      )\n\n      consumer.subscribe(Vector(consumerConf.topics.bookTopic.name).asJava)\n\n      while (!scheduler.isShutdown) {\n        Thread.sleep((2 second) toMillis)\n\n        logger debug s\"polling the new events\"\n        val books: Vector[Book] = consume(consumer)\n\n        if (books.nonEmpty) logger info s\"just polled ${books.size} books from kafka\"\n        books.foreach { book =>\n          bookMap += book.title -> book\n        }\n      }\n\n      Try(consumer.close()).recover {\n        case error => logger error(\"Failed to close the kafka consumer\", error)\n      }\n    }\n  }, 1, TimeUnit.SECONDS)\n\n  sys.addShutdownHook {\n    scheduler.shutdown()\n    scheduler.awaitTermination(10, TimeUnit.SECONDS)\n  }\nThis will also get ride of the while(true).", "author": "DivLoic", "createdAt": "2020-08-06T08:03:59Z", "path": "_includes/tutorials/produce-consume-lang/scala/code/src/main/scala/io/confluent/developer/Consumer.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package io.confluent.developer\n+\n+import io.confluent.developer.Configuration.ConsumerConf\n+import io.confluent.developer.schema.ScalaReflectionSerde.reflectionDeserializer4S\n+import io.confluent.developer.schema.{Book, ScalaReflectionSerde}\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG\n+import org.apache.kafka.clients.consumer.{ConsumerRecords, KafkaConsumer}\n+import org.apache.kafka.common.serialization.{Deserializer, Serdes}\n+import org.apache.kafka.common.utils.Bytes\n+import org.slf4j.{Logger, LoggerFactory}\n+import pureconfig.ConfigSource\n+import pureconfig.generic.auto._\n+import ujson.Obj\n+\n+import scala.collection.mutable\n+import scala.concurrent.duration._\n+import scala.jdk.CollectionConverters._\n+import scala.jdk.DurationConverters._\n+\n+object Consumer extends cask.MainRoutes with ScalaReflectionSerde with Configuration {\n+\n+  val bookMap: mutable.Map[String, Book] = mutable.Map[String, Book]()\n+\n+  private val consumerConf = ConfigSource.default.at(\"consumer\").loadOrThrow[ConsumerConf]\n+\n+  private val schemaRegistryConfigMap: Map[String, AnyRef] = Map[String, AnyRef](\n+    SCHEMA_REGISTRY_URL_CONFIG -> consumerConf.clientConfig.getString(SCHEMA_REGISTRY_URL_CONFIG)\n+  )\n+\n+  override def port: Int = consumerConf.port\n+  override def host: String = consumerConf.host\n+\n+  private def logger: Logger = LoggerFactory.getLogger(getClass)\n+\n+  @cask.get(\"/count\")\n+  def getCount(): Obj = {\n+    ujson.Obj(\"count\" -> bookMap.size)\n+  }\n+\n+  @cask.get(\"/books\")\n+  def getBooks(): Obj = {\n+    ujson.Obj(\"results\" -> ujson.Arr(\n+      bookMap.toArray.map { case (_: String, book: Book) =>\n+        upickle.default.writeJs(book)\n+      }: _*\n+    )\n+    )\n+  }\n+\n+  def consume(consumer: KafkaConsumer[Bytes, Book]): Vector[Book] = {\n+\n+    val books: ConsumerRecords[Bytes, Book] = consumer.poll((1 second) toJava)\n+\n+    books.asScala.toVector.map(_.value())\n+  }\n+\n+  new Thread(() => {", "originalCommit": "39b27697272a8bcc0a98518dd812c3cc81ff4885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3MTc0NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r467171745", "bodyText": "Feel free to add the change @DivLoic", "author": "rspurgeon", "createdAt": "2020-08-07T17:23:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMDYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY5ODcxMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/459#discussion_r467698713", "bodyText": "It's done!", "author": "DivLoic", "createdAt": "2020-08-10T05:51:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMDYxMw=="}], "type": "inlineReview"}, {"oid": "a2edb45f21e584aaa9f3fad43a2bac03e006c593", "url": "https://github.com/confluentinc/kafka-tutorials/commit/a2edb45f21e584aaa9f3fad43a2bac03e006c593", "message": "Replace the 'thread#run' by 'ScheduledExecutorService#schedule'", "committedDate": "2020-08-08T20:00:27Z", "type": "commit"}, {"oid": "aa5052c1b57273cfefa257e7f1a75ccffe63d025", "url": "https://github.com/confluentinc/kafka-tutorials/commit/aa5052c1b57273cfefa257e7f1a75ccffe63d025", "message": "Run a single consumer application instead of two", "committedDate": "2020-08-09T07:54:10Z", "type": "commit"}]}