{"pr_number": 187, "pr_title": "Find Distinct Events in a Stream (KSQL)", "pr_createdAt": "2020-01-17T17:45:16Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/187", "timeline": [{"oid": "58e142f7f67117474d62f272d195bb6359f3dc54", "url": "https://github.com/confluentinc/kafka-tutorials/commit/58e142f7f67117474d62f272d195bb6359f3dc54", "message": "ksql-finding-distinct-first-commit", "committedDate": "2020-01-17T17:41:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTAzMDE0Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r369030142", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To start off the implementation of this scenario, you need to create a stream that represents the clicks from the users. Since we will be handling time in this scenario, it is important that each click contains a timestamp indicating when that click was done. The field `TIMESTAMP` will be used for this purpose.\n          \n          \n            \n            To start off the implementation of this scenario, we will create a stream that represents the clicks from the users. Since we will be handling time, it is important that each click contains a timestamp indicating when that click was done. The field `TIMESTAMP` will be used for this purpose.", "author": "colinhicks", "createdAt": "2020-01-21T14:25:27Z", "path": "_includes/tutorials/finding-distinct/ksql/markup/dev/create-inputs.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+To start off the implementation of this scenario, you need to create a stream that represents the clicks from the users. Since we will be handling time in this scenario, it is important that each click contains a timestamp indicating when that click was done. The field `TIMESTAMP` will be used for this purpose.", "originalCommit": "58e142f7f67117474d62f272d195bb6359f3dc54", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTAzMDIzOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r369030239", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that you have stream with some events in it, let's start to leverage them. The first thing to do is set the following properties to ensure that you're reading from the beginning of the stream:\n          \n          \n            \n            Now that you have a stream with some events in it, let's start to leverage them. The first thing to do is set the following properties to ensure that you're reading from the beginning of the stream:", "author": "colinhicks", "createdAt": "2020-01-21T14:25:38Z", "path": "_includes/tutorials/finding-distinct/ksql/markup/dev/set-properties.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Now that you have stream with some events in it, let's start to leverage them. The first thing to do is set the following properties to ensure that you're reading from the beginning of the stream:", "originalCommit": "58e142f7f67117474d62f272d195bb6359f3dc54", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTAzMTE0Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r369031143", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Note that we called the query that find only distinct events as `DETECTED_CLICKS` and we modeled as a table, since we are performing some aggregations in the query. Then we perform a rekeying to ensure that the `IP_ADDRESS` is used as key of the record, by transforming the table back to a stream and using the `PARTITION BY` clause. To verify if everything is working as expected, run the following query:\n          \n          \n            \n            In the first statement above, we created the query that finds only distinct events, naming it `DETECTED_CLICKS`. We modeled it as a table since the query performs aggregations.\n          \n          \n            \n            \n          \n          \n            \n            Then, with the next two statements, we specified a re-keying to ensure that the `IP_ADDRESS` is used as key of the record, by transforming the table back to a stream and using the `PARTITION BY` clause.\n          \n          \n            \n            \n          \n          \n            \n            To verify everything is working as expected, run the following query:", "author": "colinhicks", "createdAt": "2020-01-21T14:27:10Z", "path": "_includes/tutorials/finding-distinct/ksql/markup/dev/transient-query.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Note that we called the query that find only distinct events as `DETECTED_CLICKS` and we modeled as a table, since we are performing some aggregations in the query. Then we perform a rekeying to ensure that the `IP_ADDRESS` is used as key of the record, by transforming the table back to a stream and using the `PARTITION BY` clause. To verify if everything is working as expected, run the following query:", "originalCommit": "58e142f7f67117474d62f272d195bb6359f3dc54", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTAzMjYxNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r369032615", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now let's experiment with these events. We need to create a query capable of displaying only distinct events, which means that duplicate IP addresses should be filtered out. Events are de-duped within a 2 minute window, and only unique clicks will be shown on that window. We are going to use the `LIMIT` keyword to limit the amount of records shown in the output, but if you want to play with this query and assess if it is displaying the correct results -- go ahead and remove the limit keyword.\n          \n          \n            \n            Let's experiment with these events. We need to create a query capable of displaying only distinct events, which means that duplicate IP addresses should be filtered out. Events are de-duped within a 2 minute window, and only unique clicks will be shown on that window. We are going to use the `LIMIT` keyword to limit the amount of records shown in the output. If you want to experiment with this query and assess if it is displaying the correct results, go ahead and remove the limit keyword.", "author": "colinhicks", "createdAt": "2020-01-21T14:29:44Z", "path": "_includes/tutorials/finding-distinct/ksql/markup/dev/transient-window.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Now let's experiment with these events. We need to create a query capable of displaying only distinct events, which means that duplicate IP addresses should be filtered out. Events are de-duped within a 2 minute window, and only unique clicks will be shown on that window. We are going to use the `LIMIT` keyword to limit the amount of records shown in the output, but if you want to play with this query and assess if it is displaying the correct results -- go ahead and remove the limit keyword.", "originalCommit": "58e142f7f67117474d62f272d195bb6359f3dc54", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f335158c3a8e69cdf66c6ca38595ba528757f5d9", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f335158c3a8e69cdf66c6ca38595ba528757f5d9", "message": "Update _includes/tutorials/finding-distinct/ksql/markup/dev/create-inputs.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-01-21T14:55:17Z", "type": "commit"}, {"oid": "b578451500b86e748a45429bd2f907c70c6bb5df", "url": "https://github.com/confluentinc/kafka-tutorials/commit/b578451500b86e748a45429bd2f907c70c6bb5df", "message": "Update _includes/tutorials/finding-distinct/ksql/markup/dev/set-properties.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-01-21T14:55:33Z", "type": "commit"}, {"oid": "77ac4907152e3ea165b3d1d83ac355bab55fa1eb", "url": "https://github.com/confluentinc/kafka-tutorials/commit/77ac4907152e3ea165b3d1d83ac355bab55fa1eb", "message": "Update _includes/tutorials/finding-distinct/ksql/markup/dev/transient-query.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-01-21T14:55:54Z", "type": "commit"}, {"oid": "d8a46d9e2a9650ec3ee65f96fc9ab71dfb258fdc", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d8a46d9e2a9650ec3ee65f96fc9ab71dfb258fdc", "message": "Update _includes/tutorials/finding-distinct/ksql/markup/dev/transient-window.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-01-21T14:56:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxOTUwOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r375019509", "bodyText": "@riferrei I think @blueedgenick's suggestion above is worth heeding for both KStreams and KSQL flavors. I suggested a slight rewording of the shared problem statement. To me the introduction seems okay as is. What do you all think?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              problem: \"you have events in a Kafka topic, and you want to filter out duplicate events based on a field in the event, producing a new stream of only unique events\"\n          \n          \n            \n              problem: \"you have events in a Kafka topic, and you want to filter out duplicate events based on a field in the event, producing a new stream of unique events per time window\"", "author": "colinhicks", "createdAt": "2020-02-05T01:42:06Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -165,7 +165,7 @@ finding-distinct:\n   problem: \"you have events in a Kafka topic, and you want to filter out duplicate events based on a field in the event, producing a new stream of only unique events\"", "originalCommit": "d8a46d9e2a9650ec3ee65f96fc9ab71dfb258fdc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMzNDY1OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/187#discussion_r375334658", "bodyText": "Yup, make sense!", "author": "riferrei", "createdAt": "2020-02-05T15:42:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxOTUwOQ=="}], "type": "inlineReview"}, {"oid": "358cc848467a432c9f205417ad0478712c826756", "url": "https://github.com/confluentinc/kafka-tutorials/commit/358cc848467a432c9f205417ad0478712c826756", "message": "Update _data/tutorials.yaml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-02-05T15:34:23Z", "type": "commit"}, {"oid": "d9042937b040bc86c6d431f368fbb69f14cb7a17", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d9042937b040bc86c6d431f368fbb69f14cb7a17", "message": "Including flatten nested data in semaphore", "committedDate": "2020-02-05T15:39:12Z", "type": "commit"}, {"oid": "e37cf16631a34b2d32d4d3baafcff4216cd451eb", "url": "https://github.com/confluentinc/kafka-tutorials/commit/e37cf16631a34b2d32d4d3baafcff4216cd451eb", "message": "Merge branch 'master' into ksql-finding-distinct", "committedDate": "2020-02-05T15:40:31Z", "type": "commit"}]}