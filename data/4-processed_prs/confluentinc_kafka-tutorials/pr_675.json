{"pr_number": 675, "pr_title": "Typo fixes for Kafka 101 tutorials", "pr_createdAt": "2020-12-22T00:55:37Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/675", "timeline": [{"oid": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "message": "typo fixes for Kafka 101 tutorials", "committedDate": "2020-12-21T23:17:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMwODA1OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547308058", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n          \n          \n            \n            - `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `acks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.", "author": "ybyzek", "createdAt": "2020-12-22T14:28:30Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -15,7 +15,7 @@ https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses th\n \n - `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n - `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgment from the followers\n-- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n+- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMwODE2Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547308163", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n          \n          \n            \n            - `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `acks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.", "author": "ybyzek", "createdAt": "2020-12-22T14:28:42Z", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -15,7 +15,7 @@ https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses th\n \n - `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n - `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgment from the followers\n-- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n+- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgment of replication from all in-sync replicas. When using `aks=all`, it's strongly recommended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMwODY1MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547308650", "bodyText": "\"You're all down now!\" is pretty hip, no? ;)", "author": "ybyzek", "createdAt": "2020-12-22T14:29:38Z", "path": "_includes/tutorials/streams-to-table/kstreams/markup/dev/clean-up.adoc", "diffHunk": "@@ -1,4 +1,4 @@\n-You're all down now!\n+You're all done now!", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMwOTE2Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547309163", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"You want to inspect/debug records written to a topic.  The keys and values are longs and doubles, respectively.  In this tutorial you'll learn how to specify key and value deserializers with the console consumer.\"\n          \n          \n            \n              introduction: \"You want to inspect/debug records written to a topic.  Each record key and value is a long and double, respectively.  In this tutorial you'll learn how to specify key and value deserializers with the console consumer.\"", "author": "ybyzek", "createdAt": "2020-12-22T14:30:36Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -293,11 +293,11 @@ console-consumer-producer-basic:\n     kafka: enabled\n \n console-consumer-primitive-keys-values:\n-  title: \"How to use the console consumer to read non-string primitive keys values\"\n+  title: \"How to use the console consumer to read non-string primitive keys and values\"\n   meta-description: \"How to use the console consumer to read non-string primitive keys and values\"\n   slug: \"/console-consumer-primitive-values\"\n   question: \"How do I specify key and value deserializers when running the Kafka console consumer?\"\n-  introduction: \"You want to inspect/debug records written to a topic.  The keys and values are long and doubles, repsectively.  In this tutorial you'll learn how to specify key and value deserializers with the console consumer\"\n+  introduction: \"You want to inspect/debug records written to a topic.  The keys and values are longs and doubles, respectively.  In this tutorial you'll learn how to specify key and value deserializers with the console consumer.\"", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMwOTU3OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547309578", "bodyText": "@brianstrauch great catch", "author": "ybyzek", "createdAt": "2020-12-22T14:31:24Z", "path": "_includes/tutorials/console-consumer-producer-avro/kafka/markup/dev/produce-topic-keys-values.adoc", "diffHunk": "@@ -1,7 +1,7 @@\n Kafka works with key-value pairs, but so far you've only sent records with values only.  Well to be fair you've sent key-value pairs, but the keys are `null`.\n Sometimes you'll need to send a valid key in addition to the value from the command line.\n \n-To enable sending full key-value pairs from the command line you add two properties to your console producer, `parse.keys` and `key.separtor`\n+To enable sending full key-value pairs from the command line you add two properties to your console producer, `parse.key` and `key.separator`", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMxMDI2NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547310265", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <1> Iterate over all of the records and store the `value`s in a `List`\n          \n          \n            \n            <1> Iterate over all of the records and store each record's `value` in a `List`", "author": "ybyzek", "createdAt": "2020-12-22T14:32:42Z", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-supporting-classes.adoc", "diffHunk": "@@ -31,13 +31,13 @@ The `FileWritingRecordsHandler` is a simple class that writes values of consumed\n       try {\n         Files.write(path, valueList, StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.APPEND);  //<3>\n       } catch (IOException e) {\n-          throw new RuntimeException(e);\n+        throw new RuntimeException(e);\n       }\n     }\n   }\n ----\n-<1> Iterate over all of the records and store the `value` in a `List`\n-<2> If the isn't empty let's do something!\n+<1> Iterate over all of the records and store the `value`s in a `List`", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMxMDM1Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r547310352", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <2> If the `List` isn't empty let's do something!\n          \n          \n            \n            <2> If the `List` isn't empty, let's do something!", "author": "ybyzek", "createdAt": "2020-12-22T14:32:53Z", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-supporting-classes.adoc", "diffHunk": "@@ -31,13 +31,13 @@ The `FileWritingRecordsHandler` is a simple class that writes values of consumed\n       try {\n         Files.write(path, valueList, StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.APPEND);  //<3>\n       } catch (IOException e) {\n-          throw new RuntimeException(e);\n+        throw new RuntimeException(e);\n       }\n     }\n   }\n ----\n-<1> Iterate over all of the records and store the `value` in a `List`\n-<2> If the isn't empty let's do something!\n+<1> Iterate over all of the records and store the `value`s in a `List`\n+<2> If the `List` isn't empty let's do something!", "originalCommit": "6b75727c7ad9ce01cfc0c6424f6be36d986edd67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "76d17b9bd297644dd5b50c53a854c0061a493163", "url": "https://github.com/confluentinc/kafka-tutorials/commit/76d17b9bd297644dd5b50c53a854c0061a493163", "message": "Update _includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-dev-file.adoc\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-12-22T21:24:57Z", "type": "commit"}, {"oid": "54edf21e06ec5cd628a86d45d89928ba67214f7c", "url": "https://github.com/confluentinc/kafka-tutorials/commit/54edf21e06ec5cd628a86d45d89928ba67214f7c", "message": "Update _includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-12-22T21:25:09Z", "type": "commit"}, {"oid": "ab27da888a967fbaa3e83a52a7f1e9621d2f5a1d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ab27da888a967fbaa3e83a52a7f1e9621d2f5a1d", "message": "Update _data/tutorials.yaml\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-12-22T21:25:52Z", "type": "commit"}, {"oid": "0c0ce1576514666e9711431b5c2f3ed9dd63c433", "url": "https://github.com/confluentinc/kafka-tutorials/commit/0c0ce1576514666e9711431b5c2f3ed9dd63c433", "message": "Update _includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-supporting-classes.adoc\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-12-22T21:26:07Z", "type": "commit"}, {"oid": "bce3109a7422d9ec453b45dad40ad1093b6df986", "url": "https://github.com/confluentinc/kafka-tutorials/commit/bce3109a7422d9ec453b45dad40ad1093b6df986", "message": "Update _includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-supporting-classes.adoc\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-12-22T21:26:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQzNDA3NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/675#discussion_r551434075", "bodyText": "Removing these two lines causes the test-harness to fail as the actual output produces 7 records instead of 5.", "author": "bbejeck", "createdAt": "2021-01-04T16:45:21Z", "path": "_includes/tutorials/console-consumer-producer-avro/kafka/code/tutorial-steps/dev/expected-output-step-two.txt", "diffHunk": "@@ -3,5 +3,3 @@\n {\"number\":2343436,\"date\":1596491687,\"shipping_address\":\"89 Addison St, Palo Alto, 94302 CA, USA\",\"subtotal\":10.0,\"shipping_cost\":0.0,\"tax\":1.0,\"grand_total\":11.0}\n {\"number\":2343437,\"date\":1596490492,\"shipping_address\":\"456 Charles St, Beverly Hills, 90209 CA, USA\",\"subtotal\":450.0,\"shipping_cost\":10.0,\"tax\":28.91,\"grand_total\":488.91}\n {\"number\":2343438,\"date\":1596490692,\"shipping_address\":\"456 Preston St, Brooklyn, 11212 NY, USA\",\"subtotal\":34.0,\"shipping_cost\":2.0,\"tax\":3.0,\"grand_total\":39.0}\n-{\"number\":2343439,\"date\":1596501510,\"shipping_address\":\"1600 Pennsylvania Avenue NW, Washington, DC 20500, USA\",\"subtotal\":1000.0,\"shipping_cost\":20.0,\"tax\":0.0,\"grand_total\":1020.0}\n-{\"number\":2343440,\"date\":1596501510,\"shipping_address\":\"55 Music Concourse Dr, San Francisco, CA 94118, USA\",\"subtotal\":345.0,\"shipping_cost\":10.0,\"tax\":10.0,\"grand_total\":365.0}", "originalCommit": "bce3109a7422d9ec453b45dad40ad1093b6df986", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "37920d428fdb30c518065ea9046b7c782d144b65", "url": "https://github.com/confluentinc/kafka-tutorials/commit/37920d428fdb30c518065ea9046b7c782d144b65", "message": "Add back two records to the expected results file", "committedDate": "2021-01-04T19:54:45Z", "type": "commit"}]}