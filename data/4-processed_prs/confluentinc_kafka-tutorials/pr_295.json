{"pr_number": 295, "pr_title": "DEVX-1628: KStreams DSL choose output topic at runtime", "pr_createdAt": "2020-03-20T21:45:30Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/295", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTUzMTI5OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r399531298", "bodyText": "@bbejeck this is redundant w/ the lines above, causing duplicates in the rendered instructions", "author": "rspurgeon", "createdAt": "2020-03-27T20:50:58Z", "path": "_data/harnesses/dynamic-output-topic/kstreams.yml", "diffHunk": "@@ -0,0 +1,177 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-dev-file.adoc\n+            \n+    - title: Create a schema for the model obect\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-avro-dir.sh\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-avro-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/avro/order.avsc\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-avro-schema.adoc\n+\n+        - action: make_file\n+          file: src/main/avro/completed-order.avsc\n+          render:\n+            file: tutorials/dynamic-output-topic/kstreams/markup/dev/make-avro-schema.adoc", "originalCommit": "abe83b3dd07e682a5a6ce8b8a728d74df201fc14", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMjM1NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400432355", "bodyText": "ack - good catch!", "author": "bbejeck", "createdAt": "2020-03-30T19:16:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTUzMTI5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTUzNDI3MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r399534270", "bodyText": "Sweet!  What a cool feature to learn about", "author": "rspurgeon", "createdAt": "2020-03-27T20:58:01Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/src/main/java/io/confluent/developer/DynamicOutputTopic.java", "diffHunk": "@@ -0,0 +1,178 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.CompletedOrder;\n+import io.confluent.developer.avro.Order;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.security.SecureRandom;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.concurrent.CountDownLatch;\n+import org.apache.avro.specific.SpecificRecord;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.ValueMapper;\n+import org.apache.kafka.streams.processor.TopicNameExtractor;\n+\n+public class DynamicOutputTopic {\n+\n+    static final double FAKE_PRICE = 0.467423D;\n+\n+\tpublic Properties buildStreamsProperties(Properties envProps) {\n+        Properties props = new Properties();\n+\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+        return props;\n+    }\n+\n+    public Topology buildTopology(Properties envProps) {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String orderInputTopic = envProps.getProperty(\"input.topic.name\");\n+        final String orderOutputTopic = envProps.getProperty(\"output.topic.name\");\n+        final String specialOrderOutput = envProps.getProperty(\"special.order.topic.name\");\n+\n+        final Serde<Long> longSerde = getPrimitiveAvroSerde(envProps, true);\n+        final Serde<Order> orderSerde = getSpecificAvroSerde(envProps);\n+        final Serde<CompletedOrder> completedOrderSerde = getSpecificAvroSerde(envProps);\n+\n+        final ValueMapper<Order, CompletedOrder> orderProcessingSimulator = v -> {\n+           double amount = v.getQuantity() * FAKE_PRICE;\n+           return CompletedOrder.newBuilder().setAmount(amount).setId(v.getId() + \"-\" + v.getSku()).setName(v.getName()).build();\n+        };\n+\n+        final TopicNameExtractor<Long, CompletedOrder> orderTopicNameExtractor = (key, completedOrder, recordContext) -> {", "originalCommit": "abe83b3dd07e682a5a6ce8b8a728d74df201fc14", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTUzNTQ5OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r399535499", "bodyText": "When I pasted all the lines, I got the following exception, which I did not get if I copy and pasted and sent one line at a time:\n\n[I] \u279c docker exec -i schema-registry /usr/bin/kafka-avro-console-producer --topic input --broker-list broker:9092\\\n  --property \"parse.key=true\"\\\n  --property 'key.schema={\"type\":\"long\"}'\\\n  --property \"key.separator=:\"\\\n  --property value.schema=\"$(< src/main/avro/order.avsc)\"\n5:{\"id\":5,\"name\":\"tp\",\"quantity\":10000, \"sku\":\"QUA00000123\"}\n6:{\"id\":6,\"name\":\"coffee\",\"quantity\":1000, \"sku\":\"COF0003456\"}\n7:{\"id\":7,\"name\":\"hand-sanitizer\",\"quantity\":6000, \"sku\":\"QUA000022334\"}\n8:{\"id\":8,\"name\":\"beer\",\"quantity\":4000, \"sku\":\"BER88899222\"}\n\nkafka.common.KafkaException: No key found in line\n        at io.confluent.kafka.formatter.AvroMessageReader.readMessage(AvroMessageReader.java:191)\n        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55)\n        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)\n\n\nIt appears that the messages actually got sent, but the last line may have had a carriage return which caused the producer to read the last line as an empty string and try and send it.", "author": "rspurgeon", "createdAt": "2020-03-27T21:00:48Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/run-producer.adoc", "diffHunk": "@@ -0,0 +1,22 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+In a new terminal, run:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/dynamic-output-topic/kstreams/code/tutorial-steps/dev/console-producer.sh %}</code></pre>\n++++++\n+\n+When the console producer starts, it will log some messages and hang, waiting for your input. Each line represents input data for the Kafka Streams application.\n+To send all of the events below, paste the following into the prompt and press enter:", "originalCommit": "abe83b3dd07e682a5a6ce8b8a728d74df201fc14", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ0NjkxOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400446918", "bodyText": "@rspurgeon good catch, that was the case, and I've removed the return at the end of the file. I re-ran it locally, and I was able to copy->paste the entire block and didn't get any errors.", "author": "bbejeck", "createdAt": "2020-03-30T19:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTUzNTQ5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ1NTEzOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400455138", "bodyText": "piggy-backing a change for all generated tutorials.  Original suggestion from @rspurgeon.", "author": "bbejeck", "createdAt": "2020-03-30T19:55:08Z", "path": "templates/kstreams/static/dev/code/docker-compose-up.sh", "diffHunk": "@@ -1 +1 @@\n-docker-compose up\n+docker-compose up -d", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzMzc3Ng==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400933776", "bodyText": "In case we add other flavors of this tutorial in the future, let's remove streams from the slug/permalink\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              slug: \"/streams-dynamic-output-topic\"\n          \n          \n            \n              slug: \"/dynamic-output-topic\"", "author": "colinhicks", "createdAt": "2020-03-31T13:56:19Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+dynamic-output-topic:\n+  title: \"How to dynamically choose the output topic\"\n+  meta-description: \"How to dynamically choose the output topic\"\n+  slug: \"/streams-dynamic-output-topic\"", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzNTg4Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400935882", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"Consider a situation where depending on data in your records, you need to choose different topics dynamically.  In this tutorial, you'll learn how to dynamically choose the output topic in Kafka Streams\"\n          \n          \n            \n              introduction: \"Consider a situation where, depending on data in your records, you need to direct output to different topic.  In this tutorial, you'll learn how to instruct Kafka Streams to choose the output topic at runtime.\"", "author": "colinhicks", "createdAt": "2020-03-31T13:58:59Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+dynamic-output-topic:\n+  title: \"How to dynamically choose the output topic\"\n+  meta-description: \"How to dynamically choose the output topic\"\n+  slug: \"/streams-dynamic-output-topic\"\n+  problem: \"you need to determine which topic to send records to dynamically\"\n+  introduction: \"Consider a situation where depending on data in your records, you need to choose different topics dynamically.  In this tutorial, you'll learn how to dynamically choose the output topic in Kafka Streams\"", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzODIxMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400938212", "bodyText": "Let's bump the images to 5.4.1 here and below.", "author": "colinhicks", "createdAt": "2020-03-31T14:02:09Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk4MDMyNg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400980326", "bodyText": "ack", "author": "bbejeck", "createdAt": "2020-03-31T14:55:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzODIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDkzOTU3MA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400939570", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Then create this Avro schmea file at `src/main/avro/completed-order.avsc` to create `CompletedOrder` objects:\n          \n          \n            \n            Then create this Avro schema file at `src/main/avro/completed-order.avsc` to create `CompletedOrder` objects:", "author": "colinhicks", "createdAt": "2020-03-31T14:03:55Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-avro-schema-completed-order.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Then create this Avro schmea file at `src/main/avro/completed-order.avsc` to create `CompletedOrder` objects:", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NDY4OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400944688", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n          \n          \n            \n            The focus of this tutorial is using attributes in the output records to determine the correct output topic.  For sending fully-processed records, typically you would use the `KStream.to()` method, which takes the name of the output topic. You can think of this as setting the output topic statically.", "author": "colinhicks", "createdAt": "2020-03-31T14:10:39Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NTI0OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400945248", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.\n          \n          \n            \n            For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method, `extract`. This means you can use a lambda in most cases, instead of a concrete class.", "author": "colinhicks", "createdAt": "2020-03-31T14:11:25Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n+\n+For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NTk4OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400945988", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `TopicNameExtractor.extract()` method accepts three parameters the key, value and `RecordContext` of the current record and returns a `String`, the output topic to use. You can use the parameters as you need to determine which output topic to use.\n          \n          \n            \n            The `TopicNameExtractor.extract()` method accepts three parameters: the key, value, and `RecordContext` of the current record. It returns a `String` \u2013 the output topic to use.", "author": "colinhicks", "createdAt": "2020-03-31T14:12:24Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n+\n+For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.\n+\n+The `TopicNameExtractor.extract()` method accepts three parameters the key, value and `RecordContext` of the current record and returns a `String`, the output topic to use. You can use the parameters as you need to determine which output topic to use.", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0Njc4Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400946782", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now take a detailed look the `TopicNameExtractor` you'll use in this tutoral (found on line 67 in `DynamicOutputTopic.java`)\n          \n          \n            \n            Now take a detailed look at the `TopicNameExtractor` you'll use in this tutorial (found on line 67 in `DynamicOutputTopic.java`)", "author": "colinhicks", "createdAt": "2020-03-31T14:13:24Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n+\n+For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.\n+\n+The `TopicNameExtractor.extract()` method accepts three parameters the key, value and `RecordContext` of the current record and returns a `String`, the output topic to use. You can use the parameters as you need to determine which output topic to use.\n+\n+Now take a detailed look the `TopicNameExtractor` you'll use in this tutoral (found on line 67 in `DynamicOutputTopic.java`)", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0NzI4Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400947287", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In the code above, the `TopicNameExtractor` takes the `CompletedOrder.id` field and based on the extracted substring, returns the name of the appropriate topic to use.  You should also note that the topics need to be created ahead of time as with any of the topics used by Kafka Streams.\n          \n          \n            \n            In the code above, the `TopicNameExtractor` takes the `CompletedOrder.id` field. Based on the extracted substring, it returns the name of the topic to use.  You should also note that the topics need to be created ahead of time as with any of the topics used by Kafka Streams.", "author": "colinhicks", "createdAt": "2020-03-31T14:14:02Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n+\n+For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.\n+\n+The `TopicNameExtractor.extract()` method accepts three parameters the key, value and `RecordContext` of the current record and returns a `String`, the output topic to use. You can use the parameters as you need to determine which output topic to use.\n+\n+Now take a detailed look the `TopicNameExtractor` you'll use in this tutoral (found on line 67 in `DynamicOutputTopic.java`)\n+\n+++++\n+<pre class=\"snippet\"><code class=\"java\">\n+final TopicNameExtractor&lt;Long, CompletedOrder&gt; orderTopicNameExtractor = (key, completedOrder, recordContext) -> {\n+      final String compositeId = completedOrder.getId();\n+      final String skuPart = compositeId.substring(compositeId.indexOf('-') + 1, 5);\n+      final String outTopic;\n+      if (skuPart.equals(\"QUA\")) {\n+           outTopic = specialOrderOutput;\n+      } else {\n+           outTopic = orderOutputTopic;\n+      }\n+      return outTopic;\n+};\n+</code></pre>\n+++++\n+\n+In the code above, the `TopicNameExtractor` takes the `CompletedOrder.id` field and based on the extracted substring, returns the name of the appropriate topic to use.  You should also note that the topics need to be created ahead of time as with any of the topics used by Kafka Streams.", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0ODQxOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400948418", "bodyText": "I think this paragraph can be left out.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            While this preseented example of a `TopicNameExtractor` implementation, it's enough to demonstrate the point of dynamically choosing the output topic.  You can modify the example to fit your purposes.", "author": "colinhicks", "createdAt": "2020-03-31T14:15:32Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,41 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+\n+The focus of this tutorial is using attributes in the output records to determine or choose the correct output topic to send to.  For sending fully processed records typically you would use the `KStream.to()` method which takes the name of the output topic where to processed records are sent. But you set this topic name when building the topology, which is fine in many cases, but what if you want to route your processed events to different output topics at runtime?\n+\n+For dynamic output topic choice, Kafka Streams has an overloaded version of the `KStream.to()` method that takes a `TopicNameExtractor` interface instead of a singular topic name.  The `TopicNameExtractor` interface contains only one method `extract`, which means you can use a lambda in most cases instead of a concrete class.\n+\n+The `TopicNameExtractor.extract()` method accepts three parameters the key, value and `RecordContext` of the current record and returns a `String`, the output topic to use. You can use the parameters as you need to determine which output topic to use.\n+\n+Now take a detailed look the `TopicNameExtractor` you'll use in this tutoral (found on line 67 in `DynamicOutputTopic.java`)\n+\n+++++\n+<pre class=\"snippet\"><code class=\"java\">\n+final TopicNameExtractor&lt;Long, CompletedOrder&gt; orderTopicNameExtractor = (key, completedOrder, recordContext) -> {\n+      final String compositeId = completedOrder.getId();\n+      final String skuPart = compositeId.substring(compositeId.indexOf('-') + 1, 5);\n+      final String outTopic;\n+      if (skuPart.equals(\"QUA\")) {\n+           outTopic = specialOrderOutput;\n+      } else {\n+           outTopic = orderOutputTopic;\n+      }\n+      return outTopic;\n+};\n+</code></pre>\n+++++\n+\n+In the code above, the `TopicNameExtractor` takes the `CompletedOrder.id` field and based on the extracted substring, returns the name of the appropriate topic to use.  You should also note that the topics need to be created ahead of time as with any of the topics used by Kafka Streams.\n+\n+While this preseented example of a `TopicNameExtractor` implementation, it's enough to demonstrate the point of dynamically choosing the output topic.  You can modify the example to fit your purposes.", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk0OTc3NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400949774", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that you have produced some orders, you should set up a consumer to view the results. But in this case, you need to start two consumers as the Kafka Streams\n          \n          \n            \n            Now that you have produced some orders, you should set up a consumer to view the results. In this case, you need to start two consumers as the Kafka Streams\n          \n      \n    \n    \n  \n\nLooks like you can also join this line with the line below. Not sure if asciidoc renders an unwanted line break here or not, as-is.", "author": "colinhicks", "createdAt": "2020-03-31T14:17:15Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/run-consumer.adoc", "diffHunk": "@@ -0,0 +1,20 @@\n+////\n+  This is a sample content file for how to include a console consumer to the tutorial, probably a good idea so the end user can watch the results\n+  of the tutorial.  Change the text as needed.\n+\n+////\n+\n+Now that you have produced some orders, you should set up a consumer to view the results. But in this case, you need to start two consumers as the Kafka Streams", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk1MDQ1NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400950455", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Then the special order console consumer should yeild this output:\n          \n          \n            \n            The special order console consumer should yield this output:", "author": "colinhicks", "createdAt": "2020-03-31T14:18:06Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/run-special-order-consumer.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Then close the current console consumer or open a second terminal window and start another console consumer to view the special `CompletedOrder` objects.  Remember the Kafka Streams application determines at runtime where to send each order based on the information contained in the `CompletedOrder` object.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/dynamic-output-topic/kstreams/code/tutorial-steps/dev/console-consumer-special.sh %}</code></pre>\n++++++\n+\n+Then the special order console consumer should yeild this output:", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDk1MTE4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r400951184", "bodyText": "In case we add other flavors of this tutorial in the future, let's remove streams from the slug/permalink\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            permalink: /streams-dynamic-output-topic/kstreams\n          \n          \n            \n            permalink: /dynamic-output-topic/kstreams", "author": "colinhicks", "createdAt": "2020-03-31T14:19:00Z", "path": "tutorials/dynamic-output-topic/kstreams.html", "diffHunk": "@@ -0,0 +1,6 @@\n+---\n+layout: tutorial\n+permalink: /streams-dynamic-output-topic/kstreams", "originalCommit": "7dbcef08277574d2963bb137b291615ef3719bb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "33046525bbc18f3712aabd785c78c9d536b8a762", "url": "https://github.com/confluentinc/kafka-tutorials/commit/33046525bbc18f3712aabd785c78c9d536b8a762", "message": "remove log file update CP versions in docker file", "committedDate": "2020-03-31T15:05:40Z", "type": "forcePushed"}, {"oid": "7e2a0d43cf21b8a807883550cfd0050cbdd860d8", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7e2a0d43cf21b8a807883550cfd0050cbdd860d8", "message": "update link on index page", "committedDate": "2020-04-08T22:49:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI4ODA3NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406288074", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <pre class=\"snippet\"><code class=\"dockerfile\">{% include_raw tutorials/filtering/kstreams/code/docker-compose.yml %}</code></pre>\n          \n          \n            \n            <pre class=\"snippet\"><code class=\"dockerfile\">{% include_raw tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml %}</code></pre>", "author": "gAmUssA", "createdAt": "2020-04-09T15:27:49Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/markup/dev/make-docker-compose.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Next, create the following `docker-compose.yml` file to obtain Confluent Platform:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"dockerfile\">{% include_raw tutorials/filtering/kstreams/code/docker-compose.yml %}</code></pre>", "originalCommit": "7e2a0d43cf21b8a807883550cfd0050cbdd860d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI5NjcyNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406296724", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n          \n          \n            \n                id \"com.google.cloud.tools.jib\" version \"2.1.0\"\n          \n      \n    \n    \n  \n\nupdated jib (shows warnings in gradle v>6.0 build)", "author": "gAmUssA", "createdAt": "2020-04-09T15:40:35Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,64 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:5.2.0\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwNDUwMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406304503", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            example.topic.name=<<FILL ME IN>>\n          \n          \n            \n            example.topic.partitions=<<FILL ME IN>>\n          \n          \n            \n            example.topic.replication.factor=<<FILL ME IN>>\n          \n          \n            \n            input.topic.name=<<FILL ME IN>>\n          \n          \n            \n             input.topic.partitions=<<FILL ME IN>>\n          \n          \n            \n             input.topic.replication.factor=<<FILL ME IN>>\n          \n          \n            \n            \n          \n          \n            \n              output.topic.name=<<FILL ME IN>>\n          \n          \n            \n             output.topic.partitions=<<FILL ME IN>>\n          \n          \n            \n             output.topic.replication.factor=<<FILL ME IN>>\n          \n          \n            \n            \n          \n          \n            \n              special.order.topic.name=<<FILL ME IN>>\n          \n          \n            \n             special.order.topic.partitions=<<FILL ME IN>>\n          \n          \n            \n             special.order.topic.replication.factor=<<FILL ME IN>>", "author": "gAmUssA", "createdAt": "2020-04-09T15:52:17Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/configuration/prod.properties", "diffHunk": "@@ -0,0 +1,7 @@\n+application.id=dynamic-output-topic\n+bootstrap.servers=<<FILL ME IN>>\n+schema.registry.url=<<FILL ME IN>>\n+\n+example.topic.name=<<FILL ME IN>>\n+example.topic.partitions=<<FILL ME IN>>\n+example.topic.replication.factor=<<FILL ME IN>>", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwNzY5Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406307697", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run -v $PWD/configuration/prod.properties:/config.properties io.confluent.developer/dynamic-output-topic-join:0.0.1 config.properties\n          \n          \n            \n            docker run -v $PWD/configuration/prod.properties:/config.properties --network cp_network io.confluent.developer/dynamic-output-topic-join:0.0.1 config.properties", "author": "gAmUssA", "createdAt": "2020-04-09T15:57:10Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/tutorial-steps/prod/launch-container.sh", "diffHunk": "@@ -0,0 +1 @@\n+docker run -v $PWD/configuration/prod.properties:/config.properties io.confluent.developer/dynamic-output-topic-join:0.0.1 config.properties", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwNzk0Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406307947", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            version: '2'\n          \n          \n            \n            version: '3.5'", "author": "gAmUssA", "createdAt": "2020-04-09T15:57:35Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwODYxMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406308611", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                environment:\n          \n          \n            \n                  ZOOKEEPER_CLIENT_PORT: 2181\n          \n          \n            \n                  ZOOKEEPER_TICK_TIME: 2000\n          \n          \n            \n                environment:\n          \n          \n            \n                  ZOOKEEPER_CLIENT_PORT: 2181\n          \n          \n            \n                  ZOOKEEPER_TICK_TIME: 2000\n          \n          \n            \n                networks:\n          \n          \n            \n                  - cp", "author": "gAmUssA", "createdAt": "2020-04-09T15:58:29Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwOTA2OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406309068", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                environment:\n          \n          \n            \n                  KAFKA_BROKER_ID: 1\n          \n          \n            \n                  KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n          \n          \n            \n                  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n          \n          \n            \n                  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n          \n          \n            \n                  KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n          \n          \n            \n                  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n          \n          \n            \n                  KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n          \n          \n            \n                  KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9092\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n          \n          \n            \n                  CONFLUENT_METRICS_ENABLE: 'true'\n          \n          \n            \n                  CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'\n          \n          \n            \n                environment:\n          \n          \n            \n                  KAFKA_BROKER_ID: 1\n          \n          \n            \n                  KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n          \n          \n            \n                  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n          \n          \n            \n                  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n          \n          \n            \n                  KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n          \n          \n            \n                  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n          \n          \n            \n                  KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n          \n          \n            \n                  KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9092\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181\n          \n          \n            \n                  CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n          \n          \n            \n                  CONFLUENT_METRICS_ENABLE: 'true'\n          \n          \n            \n                  CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'\n          \n          \n            \n                networks:\n          \n          \n            \n                  - cp", "author": "gAmUssA", "createdAt": "2020-04-09T15:59:04Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n+      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9092\n+      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181\n+      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n+      CONFLUENT_METRICS_ENABLE: 'true'\n+      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwOTQzNg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406309436", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                environment:\n          \n          \n            \n                  SCHEMA_REGISTRY_HOST_NAME: schema-registry\n          \n          \n            \n                  SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n          \n          \n            \n                  SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: WARN\n          \n          \n            \n                environment:\n          \n          \n            \n                  SCHEMA_REGISTRY_HOST_NAME: schema-registry\n          \n          \n            \n                  SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n          \n          \n            \n                  SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: WARN\n          \n          \n            \n                networks:\n          \n          \n            \n                  - cp\n          \n          \n            \n            \n          \n          \n            \n            networks:\n          \n          \n            \n              cp:\n          \n          \n            \n                name: cp_network", "author": "gAmUssA", "createdAt": "2020-04-09T15:59:41Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.1\n+    hostname: zookeeper\n+    container_name: zookeeper\n+    ports:\n+      - \"2181:2181\"\n+    environment:\n+      ZOOKEEPER_CLIENT_PORT: 2181\n+      ZOOKEEPER_TICK_TIME: 2000\n+\n+  broker:\n+    image: confluentinc/cp-enterprise-kafka:5.4.1\n+    hostname: broker\n+    container_name: broker\n+    depends_on:\n+      - zookeeper\n+    ports:\n+      - \"29092:29092\"\n+    environment:\n+      KAFKA_BROKER_ID: 1\n+      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n+      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n+      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092\n+      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n+      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n+      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n+      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR\n+      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9092\n+      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181\n+      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n+      CONFLUENT_METRICS_ENABLE: 'true'\n+      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'\n+\n+  schema-registry:\n+    image: confluentinc/cp-schema-registry:5.4.1\n+    hostname: schema-registry\n+    container_name: schema-registry\n+    depends_on:\n+      - zookeeper\n+      - broker\n+    ports:\n+      - \"8081:8081\"\n+    environment:\n+      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n+      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'\n+      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: WARN", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMwOTkwNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/295#discussion_r406309905", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            application.id=dynamic-output-topic\n          \n          \n            \n            bootstrap.servers=<<FILL ME IN>>\n          \n          \n            \n            schema.registry.url=<<FILL ME IN>>\n          \n          \n            \n            application.id=dynamic-output-topic\n          \n          \n            \n            bootstrap.servers=broker:9092\n          \n          \n            \n            schema.registry.url=http://schema-registry:8081", "author": "gAmUssA", "createdAt": "2020-04-09T16:00:24Z", "path": "_includes/tutorials/dynamic-output-topic/kstreams/code/configuration/prod.properties", "diffHunk": "@@ -0,0 +1,7 @@\n+application.id=dynamic-output-topic\n+bootstrap.servers=<<FILL ME IN>>\n+schema.registry.url=<<FILL ME IN>>", "originalCommit": "e397933dc9bf398e5932c62a55dbf9cd337bd417", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "71d2fe2b0e7bbe3e5a74d067459e36bac9e0e48b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/71d2fe2b0e7bbe3e5a74d067459e36bac9e0e48b", "message": "Initial commit for dynamic output topics", "committedDate": "2020-04-10T15:51:18Z", "type": "commit"}, {"oid": "586169e1aaea5f70fa0c6bd5181b34dc10b006fa", "url": "https://github.com/confluentinc/kafka-tutorials/commit/586169e1aaea5f70fa0c6bd5181b34dc10b006fa", "message": "Final commit for dynamic output tutorial", "committedDate": "2020-04-10T15:51:18Z", "type": "commit"}, {"oid": "61b718ca96f501282a9fb5b8d2ebd7abdb649943", "url": "https://github.com/confluentinc/kafka-tutorials/commit/61b718ca96f501282a9fb5b8d2ebd7abdb649943", "message": "updates per comments", "committedDate": "2020-04-10T15:51:18Z", "type": "commit"}, {"oid": "653568ea77694cf83451f4cf02346ecbf2283d1f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/653568ea77694cf83451f4cf02346ecbf2283d1f", "message": "Apply suggestions from code review\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-04-10T15:51:18Z", "type": "commit"}, {"oid": "f4600303e5d6955f258ee018756206eca44b9d05", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f4600303e5d6955f258ee018756206eca44b9d05", "message": "remove log file update CP versions in docker file", "committedDate": "2020-04-10T15:51:18Z", "type": "commit"}, {"oid": "5a5d8712730af8436227e026472e1decfc3d5005", "url": "https://github.com/confluentinc/kafka-tutorials/commit/5a5d8712730af8436227e026472e1decfc3d5005", "message": "update link on index page", "committedDate": "2020-04-10T15:53:16Z", "type": "commit"}, {"oid": "004ba8555eb773c4b7129497e9e1ef31bdb2166d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/004ba8555eb773c4b7129497e9e1ef31bdb2166d", "message": "added command to cleanup.sh to kill any gradle procs", "committedDate": "2020-04-10T16:00:48Z", "type": "commit"}, {"oid": "004ba8555eb773c4b7129497e9e1ef31bdb2166d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/004ba8555eb773c4b7129497e9e1ef31bdb2166d", "message": "added command to cleanup.sh to kill any gradle procs", "committedDate": "2020-04-10T16:00:48Z", "type": "forcePushed"}]}