{"pr_number": 531, "pr_title": "DEVX-1979: Using callbacks in  producer reqeusts", "pr_createdAt": "2020-09-02T16:36:25Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/531", "timeline": [{"oid": "94521243ef32239d84d4a7d953f415f630be345b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/94521243ef32239d84d4a7d953f415f630be345b", "message": "initial commit for new tutorial", "committedDate": "2020-08-31T20:52:26Z", "type": "commit"}, {"oid": "0340eefa63ed340e6aca06dc97a09fbaa5592ace", "url": "https://github.com/confluentinc/kafka-tutorials/commit/0340eefa63ed340e6aca06dc97a09fbaa5592ace", "message": "first renames", "committedDate": "2020-08-31T21:24:45Z", "type": "commit"}, {"oid": "2ea564bf080c17421d65738ba5cee69c215c9cee", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2ea564bf080c17421d65738ba5cee69c215c9cee", "message": "complete the renaming", "committedDate": "2020-08-31T21:49:30Z", "type": "commit"}, {"oid": "23a3f8d39c6756a77d0ba10cf9f7e03428a25349", "url": "https://github.com/confluentinc/kafka-tutorials/commit/23a3f8d39c6756a77d0ba10cf9f7e03428a25349", "message": "updated to work with callbacks", "committedDate": "2020-09-01T21:26:57Z", "type": "commit"}, {"oid": "27f94a81ead6e8b751e965479739a8dd61f0f217", "url": "https://github.com/confluentinc/kafka-tutorials/commit/27f94a81ead6e8b751e965479739a8dd61f0f217", "message": "Final changes for new tutorial", "committedDate": "2020-09-02T16:32:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjUyNw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r482212527", "bodyText": "@ybyzek  this is only significant diff from PR #491 which you reviewed. Can you take a look at this?", "author": "bbejeck", "createdAt": "2020-09-02T16:43:00Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,86 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerCallbackApplication constructor\n+----\n+\n+public class KafkaProducerCallbackApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerCallbackApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerCallbackApplication.main()` method.\n+Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+(In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).\n+\n+\n+Next let's take a look at the `KafkaProducerCallbackApplication.produce` method\n+[source, java]\n+.KafkaProducerCallbackApplication.produce\n+----\n+public void produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    producer.send(producerRecord, (recordMetadata, exception) -> {  <3>\n+              if (exception == null) {                          <4>\n+                  System.out.println(\"Record written to offset \" +\n+                          recordMetadata.offset() + \" timestamp \" +\n+                          recordMetadata.timestamp());\n+              } else {\n+                  System.err.println(\"An error occurred\"); <5>\n+                  exception.printStackTrace(System.err);\n+              }\n+        });\n+  }\n+\n+----\n+\n+<1> Process the String for sending message\n+<2> Create the `ProducerRecord`\n+<3> Send the record to the broker specifying a `Callback` instance as a lambda function\n+<4> If there's no exceptions print the offset and timestamp of the acknowleged record\n+<5> Error handling portion-in this case printing the stacktrace to `System.err`\n+\n+The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that this overload of the https://javadoc.io/static/org.apache.kafka/kafka-clients/2.6.0/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-org.apache.kafka.clients.producer.Callback--[`KafkaProducer.send`] method accepts a second parameter, an instance of the https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/producer/Callback.html[Callback] interface.\n+\n+The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread so any actions here should execute quickly.  Any time consuming tasks could cause a delay in sending new records.\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.\n+\n+The code in the callback prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.\n+\n+\n+Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerCallbackApplication.java`.", "originalCommit": "27f94a81ead6e8b751e965479739a8dd61f0f217", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzc4Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r482223787", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread so any actions here should execute quickly.  Any time consuming tasks could cause a delay in sending new records.\n          \n          \n            \n            The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread and any time consuming tasks could cause a delay in sending new records, so any code here should be designed to execute quickly.", "author": "ybyzek", "createdAt": "2020-09-02T17:00:36Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,86 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerCallbackApplication constructor\n+----\n+\n+public class KafkaProducerCallbackApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerCallbackApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerCallbackApplication.main()` method.\n+Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+(In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).\n+\n+\n+Next let's take a look at the `KafkaProducerCallbackApplication.produce` method\n+[source, java]\n+.KafkaProducerCallbackApplication.produce\n+----\n+public void produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    producer.send(producerRecord, (recordMetadata, exception) -> {  <3>\n+              if (exception == null) {                          <4>\n+                  System.out.println(\"Record written to offset \" +\n+                          recordMetadata.offset() + \" timestamp \" +\n+                          recordMetadata.timestamp());\n+              } else {\n+                  System.err.println(\"An error occurred\"); <5>\n+                  exception.printStackTrace(System.err);\n+              }\n+        });\n+  }\n+\n+----\n+\n+<1> Process the String for sending message\n+<2> Create the `ProducerRecord`\n+<3> Send the record to the broker specifying a `Callback` instance as a lambda function\n+<4> If there's no exceptions print the offset and timestamp of the acknowleged record\n+<5> Error handling portion-in this case printing the stacktrace to `System.err`\n+\n+The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that this overload of the https://javadoc.io/static/org.apache.kafka/kafka-clients/2.6.0/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-org.apache.kafka.clients.producer.Callback--[`KafkaProducer.send`] method accepts a second parameter, an instance of the https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/producer/Callback.html[Callback] interface.\n+\n+The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread so any actions here should execute quickly.  Any time consuming tasks could cause a delay in sending new records.", "originalCommit": "27f94a81ead6e8b751e965479739a8dd61f0f217", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNTIyMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r482225222", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The code in the callback prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.\n          \n          \n            \n            In this example, the code in the callback just prints information from each record's `RecordMetadata` object, specifically the `timestamp` and `offset`.", "author": "ybyzek", "createdAt": "2020-09-02T17:02:56Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,86 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerCallbackApplication constructor\n+----\n+\n+public class KafkaProducerCallbackApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerCallbackApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerCallbackApplication.main()` method.\n+Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+(In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).\n+\n+\n+Next let's take a look at the `KafkaProducerCallbackApplication.produce` method\n+[source, java]\n+.KafkaProducerCallbackApplication.produce\n+----\n+public void produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    producer.send(producerRecord, (recordMetadata, exception) -> {  <3>\n+              if (exception == null) {                          <4>\n+                  System.out.println(\"Record written to offset \" +\n+                          recordMetadata.offset() + \" timestamp \" +\n+                          recordMetadata.timestamp());\n+              } else {\n+                  System.err.println(\"An error occurred\"); <5>\n+                  exception.printStackTrace(System.err);\n+              }\n+        });\n+  }\n+\n+----\n+\n+<1> Process the String for sending message\n+<2> Create the `ProducerRecord`\n+<3> Send the record to the broker specifying a `Callback` instance as a lambda function\n+<4> If there's no exceptions print the offset and timestamp of the acknowleged record\n+<5> Error handling portion-in this case printing the stacktrace to `System.err`\n+\n+The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that this overload of the https://javadoc.io/static/org.apache.kafka/kafka-clients/2.6.0/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-org.apache.kafka.clients.producer.Callback--[`KafkaProducer.send`] method accepts a second parameter, an instance of the https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/producer/Callback.html[Callback] interface.\n+\n+The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread so any actions here should execute quickly.  Any time consuming tasks could cause a delay in sending new records.\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.\n+\n+The code in the callback prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.", "originalCommit": "27f94a81ead6e8b751e965479739a8dd61f0f217", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNjI1NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r482226254", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n          \n          \n            \n            The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends it as a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "author": "ybyzek", "createdAt": "2020-09-02T17:04:41Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,86 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerCallbackApplication constructor\n+----\n+\n+public class KafkaProducerCallbackApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerCallbackApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerCallbackApplication.main()` method.\n+Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+(In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).\n+\n+\n+Next let's take a look at the `KafkaProducerCallbackApplication.produce` method\n+[source, java]\n+.KafkaProducerCallbackApplication.produce\n+----\n+public void produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    producer.send(producerRecord, (recordMetadata, exception) -> {  <3>\n+              if (exception == null) {                          <4>\n+                  System.out.println(\"Record written to offset \" +\n+                          recordMetadata.offset() + \" timestamp \" +\n+                          recordMetadata.timestamp());\n+              } else {\n+                  System.err.println(\"An error occurred\"); <5>\n+                  exception.printStackTrace(System.err);\n+              }\n+        });\n+  }\n+\n+----\n+\n+<1> Process the String for sending message\n+<2> Create the `ProducerRecord`\n+<3> Send the record to the broker specifying a `Callback` instance as a lambda function\n+<4> If there's no exceptions print the offset and timestamp of the acknowleged record\n+<5> Error handling portion-in this case printing the stacktrace to `System.err`\n+\n+The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "originalCommit": "27f94a81ead6e8b751e965479739a8dd61f0f217", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNzY2NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r482227664", "bodyText": "Is the send method itself technically asynchronous?  Or the callback is asynchronous?  Not sure if this is a distinction worth making, but FYC maybe tweak as:\nThe `KafkaProducer.send` method returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request by asynchronously sending back to the producer a `RecordMetadata` object\u2014information about the committed message.\n\n(or something like this)", "author": "ybyzek", "createdAt": "2020-09-02T17:06:43Z", "path": "_includes/tutorials/kafka-producer-application-callback/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,86 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerCallbackApplication constructor\n+----\n+\n+public class KafkaProducerCallbackApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerCallbackApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerCallbackApplication.main()` method.\n+Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+(In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).\n+\n+\n+Next let's take a look at the `KafkaProducerCallbackApplication.produce` method\n+[source, java]\n+.KafkaProducerCallbackApplication.produce\n+----\n+public void produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    producer.send(producerRecord, (recordMetadata, exception) -> {  <3>\n+              if (exception == null) {                          <4>\n+                  System.out.println(\"Record written to offset \" +\n+                          recordMetadata.offset() + \" timestamp \" +\n+                          recordMetadata.timestamp());\n+              } else {\n+                  System.err.println(\"An error occurred\"); <5>\n+                  exception.printStackTrace(System.err);\n+              }\n+        });\n+  }\n+\n+----\n+\n+<1> Process the String for sending message\n+<2> Create the `ProducerRecord`\n+<3> Send the record to the broker specifying a `Callback` instance as a lambda function\n+<4> If there's no exceptions print the offset and timestamp of the acknowleged record\n+<5> Error handling portion-in this case printing the stacktrace to `System.err`\n+\n+The `KafkaProducerCallbackApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that this overload of the https://javadoc.io/static/org.apache.kafka/kafka-clients/2.6.0/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-org.apache.kafka.clients.producer.Callback--[`KafkaProducer.send`] method accepts a second parameter, an instance of the https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/producer/Callback.html[Callback] interface.\n+\n+The `Callback` provides a way of handling any actions you want to take on request completion *_asynchronously_*.  Note that the `Callback` code executes on the producer's I/O thread so any actions here should execute quickly.  Any time consuming tasks could cause a delay in sending new records.\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.", "originalCommit": "27f94a81ead6e8b751e965479739a8dd61f0f217", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE2OTAzMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/531#discussion_r487169031", "bodyText": "The send method itself is asynchronous. The callback is executed on the same thread that executes the request. What you have could work as well, but I think I'm going to leave this as is.", "author": "bbejeck", "createdAt": "2020-09-11T16:52:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNzY2NA=="}], "type": "inlineReview"}, {"oid": "eacfefc3a4509262445112f5465d1e9359e844e2", "url": "https://github.com/confluentinc/kafka-tutorials/commit/eacfefc3a4509262445112f5465d1e9359e844e2", "message": "Apply suggestions from code review\r\n\r\nAdd changes suggested from @ybyzek\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-09-11T16:46:57Z", "type": "commit"}]}