{"pr_number": 330, "pr_title": "DEVX-1520: Add tutorial for KStreams Cogrouping", "pr_createdAt": "2020-04-23T19:58:34Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/330", "timeline": [{"oid": "d903428455d6b205c77b5d4df034b420e000b58b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d903428455d6b205c77b5d4df034b420e000b58b", "message": "removed the intermediate customers_with_area_code stream\nupdated customers_by_area_code to newer syntax; allows PARTITION BY on the UDF results", "committedDate": "2020-04-13T22:48:20Z", "type": "commit"}, {"oid": "fac68657a15c17f92deb2b1df3b8c76bf571e543", "url": "https://github.com/confluentinc/kafka-tutorials/commit/fac68657a15c17f92deb2b1df3b8c76bf571e543", "message": "typo fix", "committedDate": "2020-04-13T22:48:20Z", "type": "commit"}, {"oid": "f36ae7031340077172efc2b67c9ce81a417e0e40", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f36ae7031340077172efc2b67c9ce81a417e0e40", "message": "DEVX-1520: Add tutorial for KStreasm Cogrouping", "committedDate": "2020-04-23T19:55:45Z", "type": "commit"}, {"oid": "634a83a4dd1bf7a3bc0b0df272bbafe4e2831bfb", "url": "https://github.com/confluentinc/kafka-tutorials/commit/634a83a4dd1bf7a3bc0b0df272bbafe4e2831bfb", "message": "add actual-output.json to gitignore, update make-avro-dir.adoc text", "committedDate": "2020-04-23T20:11:12Z", "type": "commit"}, {"oid": "68910d2a968548a399575c0ff924ef8495ed7566", "url": "https://github.com/confluentinc/kafka-tutorials/commit/68910d2a968548a399575c0ff924ef8495ed7566", "message": "Upgrade to CP 5.5 fix template file that had harded coded path", "committedDate": "2020-04-28T19:42:56Z", "type": "commit"}, {"oid": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4f0fd6f1732dab41bca4e09aca88798fb1717b43", "message": "add KIP link", "committedDate": "2020-04-28T22:32:20Z", "type": "commit"}, {"oid": "c399f257519d9a060015f8844f6833552bfaaa4d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c399f257519d9a060015f8844f6833552bfaaa4d", "message": "add original KIP proposal link", "committedDate": "2020-04-28T22:38:45Z", "type": "commit"}, {"oid": "c9b860b1424c4761f799247dfe28a49fb751e784", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c9b860b1424c4761f799247dfe28a49fb751e784", "message": "Update _includes/tutorials/fk-joins/kstreams/markup/dev/make-topology.adoc\n\nCo-Authored-By: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-04-29T15:12:57Z", "type": "commit"}, {"oid": "ec267277850e14dcac604c5b875e906bc4dd5059", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ec267277850e14dcac604c5b875e906bc4dd5059", "message": "Merge pull request #335 from confluentinc/MINOR_add_kip_link_to_fk_joins\n\nMINOR: Add original KIP proposal link", "committedDate": "2020-04-29T15:14:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTY5Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417651692", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n          \n          \n            \n                    props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));", "author": "gAmUssA", "createdAt": "2020-04-29T22:33:47Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import org.apache.avro.specific.SpecificRecord;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Produced;\n+\n+public class CogroupingStreams {\n+\n+\n+\tpublic Properties buildStreamsProperties(Properties envProps) {\n+        Properties props = new Properties();\n+\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTg2OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417651869", "bodyText": "AbstractKafkaAvroSerDeConfig is deprecated in 5.5", "author": "gAmUssA", "createdAt": "2020-04-29T22:34:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MjQ4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417652484", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;", "author": "gAmUssA", "createdAt": "2020-04-29T22:35:51Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MjYyOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417652628", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;", "author": "gAmUssA", "createdAt": "2020-04-29T22:36:12Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5959dd935463de66401abc0521284b167eaaa15e", "url": "https://github.com/confluentinc/kafka-tutorials/commit/5959dd935463de66401abc0521284b167eaaa15e", "message": "Merge pull request #327 from russau/rekeying-syntax-update\n\nRekeying syntax update", "committedDate": "2020-05-01T16:47:41Z", "type": "commit"}, {"oid": "6aab2b763a3643865b3173c00df01f180e08773a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6aab2b763a3643865b3173c00df01f180e08773a", "message": "Updated KSQL tutorials - 0.8.x (#337)\n\nTested KSQL joins tutorials against 0.8.1 and found some issues, which are fixed below:\r\n\r\n- PRINT command no longer needs the topic name quoting: it's now case sensitive. (fixed across all examples)\r\n- TIMESTAMPTOSTRING should be passed a timezone for consistent results\r\n- Added explicit ROWKEY columns to all CT/CS statements, as this is to be encouraged.\r\n- Fixed key format output of PRINT that was outputting MIXED, which is not a valid output.\r\n\r\nfixes #336\r\n\r\n\r\nCo-authored-by: Andy Coates <big-andy-coates@users.noreply.github.com>", "committedDate": "2020-05-01T16:54:48Z", "type": "commit"}, {"oid": "7d38529bec5820ad809c1556185a2f2f0b3784ed", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7d38529bec5820ad809c1556185a2f2f0b3784ed", "message": "Adapt expected output for cli column-width setting", "committedDate": "2020-05-01T17:41:19Z", "type": "commit"}, {"oid": "d8b974bb270951ee3801cc9fb40ac7459287a094", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d8b974bb270951ee3801cc9fb40ac7459287a094", "message": "DEVX-1520: Add tutorial for KStreasm Cogrouping", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "1281b81aaf09327ccfd1dbb8b05a33d7d470ca1d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/1281b81aaf09327ccfd1dbb8b05a33d7d470ca1d", "message": "add actual-output.json to gitignore, update make-avro-dir.adoc text", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "004f054e0081c68298a8534b4580c2553659af58", "url": "https://github.com/confluentinc/kafka-tutorials/commit/004f054e0081c68298a8534b4580c2553659af58", "message": "Upgrade to CP 5.5 fix template file that had harded coded path", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "f59d7aaee20442d3256ce6a242afc9033bd613c1", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f59d7aaee20442d3256ce6a242afc9033bd613c1", "message": "add KIP link", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "e19c5de61b913bdcec44ae2cf53a424550676ae9", "url": "https://github.com/confluentinc/kafka-tutorials/commit/e19c5de61b913bdcec44ae2cf53a424550676ae9", "message": "Apply suggestions from code review\n\nCo-authored-by: Viktor Gamov <viktor@confluent.io>", "committedDate": "2020-05-01T21:23:19Z", "type": "commit"}, {"oid": "ef798ab67c2468010c37ce668d4ee97ff3ac36ae", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ef798ab67c2468010c37ce668d4ee97ff3ac36ae", "message": "fix merge conflict", "committedDate": "2020-05-01T21:27:26Z", "type": "commit"}, {"oid": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "message": "ignore output files", "committedDate": "2020-05-01T21:35:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ1NDY3OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419454679", "bodyText": "Many tutorials reference output files in this directory pattern via adoc includes. We have #217 tracking some consistency/clarification of this behavior. It seems like it would create confusion to ignore them across the repo now. For this specific tutorial, you could consider adding a .gitignore to the tutorial's respective subdirectory.", "author": "colinhicks", "createdAt": "2020-05-04T13:55:38Z", "path": ".gitignore", "diffHunk": "@@ -9,7 +9,7 @@ build/\n .jekyll-metadata\n node_modules/\n out/\n-*/outputs/*\n+**/tutorial-steps/dev/outputs/", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTU2ODc3MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419568771", "bodyText": "That's a good point, I'll add a separate .gitignore file for the tutorial.", "author": "bbejeck", "createdAt": "2020-05-04T16:35:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ1NDY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ1OTg0Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419459842", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `Aggregator` you saw in the previous step, constructs a map keeping a count of user logins per user, per application, so a map of maps.  Here you'll see the core logic of the `LoginAggregator`, and the code is straightforward, as you can see below.\n          \n          \n            \n            The `Aggregator` you saw in the previous step constructs a map of maps: the count of logins per user, per application.  Below is the core logic of the `LoginAggregator`.", "author": "colinhicks", "createdAt": "2020-05-04T14:02:42Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/markup/dev/make-aggregator.adoc", "diffHunk": "@@ -0,0 +1,27 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+The `Aggregator` you saw in the previous step, constructs a map keeping a count of user logins per user, per application, so a map of maps.  Here you'll see the core logic of the `LoginAggregator`, and the code is straightforward, as you can see below.", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ2MDExMQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419460111", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Each call to `Aggregator.apply` retrieves the user login map for the given application id (or creating one if it doesn't exist).  From there, the `Aggregator` increments the login count for the given user.\n          \n          \n            \n            Each call to `Aggregator.apply` retrieves the user login map for the given application id (or creates one if it doesn't exist).  From there, the `Aggregator` increments the login count for the given user.", "author": "colinhicks", "createdAt": "2020-05-04T14:03:08Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/markup/dev/make-aggregator.adoc", "diffHunk": "@@ -0,0 +1,27 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+The `Aggregator` you saw in the previous step, constructs a map keeping a count of user logins per user, per application, so a map of maps.  Here you'll see the core logic of the `LoginAggregator`, and the code is straightforward, as you can see below.\n+\n+Each call to `Aggregator.apply` retrieves the user login map for the given application id (or creating one if it doesn't exist).  From there, the `Aggregator` increments the login count for the given user.", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ2MDU4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419460584", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This tutorial uses 4 streams.  The three input streams have a record type of `LoginEvent` used to represent a user logging into an application.  The fourth stream is an ouput stream that writes a `LoginRollup` object out to a topic.  In the next steps you'll create the avro schemas for these objects.\n          \n          \n            \n            This tutorial uses 4 streams.  The three input streams have a record type of `LoginEvent` used to represent a user logging into an application.  The fourth stream is an output stream that writes a `LoginRollup` object out to a topic.  In the next steps you'll create the Avro schemas for these objects.", "author": "colinhicks", "createdAt": "2020-05-04T14:03:49Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/markup/dev/make-avro-dir.adoc", "diffHunk": "@@ -0,0 +1,8 @@\n+\n+This tutorial uses 4 streams.  The three input streams have a record type of `LoginEvent` used to represent a user logging into an application.  The fourth stream is an ouput stream that writes a `LoginRollup` object out to a topic.  In the next steps you'll create the avro schemas for these objects.", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ2NDAwOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419464008", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Next, you have three input streams (`appOneStream`, `appTwoStream`, and `appThreeStream`).  You need the intermediate object `KGroupedStream`  so you execute the\n          \n          \n            \n            `groupByKey()` method on each stream.  For this tutorial, we have assumed the incoming records already have keys.  Otherwise, you'll need to use a key selecting method (`selectKey()`, `map()`), or `groupBy()` to successfully group by key.\n          \n          \n            \n            Next, you have three input streams: `appOneStream`, `appTwoStream`, and `appThreeStream`.  You need the intermediate object `KGroupedStream`, so you execute the `groupByKey()` method on each stream.  For this tutorial, we have assumed the incoming records already have keys.  In cases where records lack keys, you need to use a key-selecting method (`selectKey()`, `map()`, or `groupBy()`) to successfully group by key.", "author": "colinhicks", "createdAt": "2020-05-04T14:08:13Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,39 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Before you create the Java class to run the `Cogrouping` example, let's dive into the main point of this tutorial, how we use cogrouping:\n+\n+++++\n+<pre class=\"snippet\">\n+    <code class=\"java\">\n+        final Aggregator&lt;String, LoginEvent, LoginRollup&gt; loginAggregator = new LoginAggregator();\n+\n+        final KGroupedStream&lt;String, LoginEvent&gt; appOneGrouped = appOneStream.groupByKey();\n+        final KGroupedStream&lt;String, LoginEvent&gt; appTwoGrouped = appTwoStream.groupByKey();\n+        final KGroupedStream&lt;String, LoginEvent&gt; appThreeGrouped = appThreeStream.groupByKey();\n+\n+        appOneGrouped.cogroup(loginAggregator)\n+            .cogroup(appTwoGrouped, loginAggregator)\n+            .cogroup(appThreeGrouped, loginAggregator)\n+            .aggregate(() -&gt; new LoginRollup(new HashMap&lt;>()), Materialized.with(Serdes.String(), loginRollupSerde))\n+            .toStream().to(totalResultOutputTopic, Produced.with(stringSerde, loginRollupSerde));\n+    </code>\n+</pre>\n+++++\n+\n+You're using the cogrouping functionality here to get an overall grouping of logins per application.  Kafka Streams creates this total grouping by using an `Aggregator` who knows how to extract records from each grouped stream.  Your `Aggregator` instance here knows how to correctly combine each `LoginEvent` into the larger `LoginRollup` object.  You'll learn more about `Aggregator` in the next step.\n+\n+Next, you have three input streams (`appOneStream`, `appTwoStream`, and `appThreeStream`).  You need the intermediate object `KGroupedStream`  so you execute the\n+`groupByKey()` method on each stream.  For this tutorial, we have assumed the incoming records already have keys.  Otherwise, you'll need to use a key selecting method (`selectKey()`, `map()`), or `groupBy()` to successfully group by key.", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ2NDkxOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419464918", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For more background on cogrouping functionality in stream you can https://cwiki.apache.org/confluence/display/KAFKA/KIP-150+-+Kafka-Streams+Cogroup[read the KIP proposal].\n          \n          \n            \n            For more background on cogrouping functionality in stream you can https://cwiki.apache.org/confluence/display/KAFKA/KIP-150+-+Kafka-Streams+Cogroup[read the KIP-150 proposal].", "author": "colinhicks", "createdAt": "2020-05-04T14:09:28Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,39 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Before you create the Java class to run the `Cogrouping` example, let's dive into the main point of this tutorial, how we use cogrouping:\n+\n+++++\n+<pre class=\"snippet\">\n+    <code class=\"java\">\n+        final Aggregator&lt;String, LoginEvent, LoginRollup&gt; loginAggregator = new LoginAggregator();\n+\n+        final KGroupedStream&lt;String, LoginEvent&gt; appOneGrouped = appOneStream.groupByKey();\n+        final KGroupedStream&lt;String, LoginEvent&gt; appTwoGrouped = appTwoStream.groupByKey();\n+        final KGroupedStream&lt;String, LoginEvent&gt; appThreeGrouped = appThreeStream.groupByKey();\n+\n+        appOneGrouped.cogroup(loginAggregator)\n+            .cogroup(appTwoGrouped, loginAggregator)\n+            .cogroup(appThreeGrouped, loginAggregator)\n+            .aggregate(() -&gt; new LoginRollup(new HashMap&lt;>()), Materialized.with(Serdes.String(), loginRollupSerde))\n+            .toStream().to(totalResultOutputTopic, Produced.with(stringSerde, loginRollupSerde));\n+    </code>\n+</pre>\n+++++\n+\n+You're using the cogrouping functionality here to get an overall grouping of logins per application.  Kafka Streams creates this total grouping by using an `Aggregator` who knows how to extract records from each grouped stream.  Your `Aggregator` instance here knows how to correctly combine each `LoginEvent` into the larger `LoginRollup` object.  You'll learn more about `Aggregator` in the next step.\n+\n+Next, you have three input streams (`appOneStream`, `appTwoStream`, and `appThreeStream`).  You need the intermediate object `KGroupedStream`  so you execute the\n+`groupByKey()` method on each stream.  For this tutorial, we have assumed the incoming records already have keys.  Otherwise, you'll need to use a key selecting method (`selectKey()`, `map()`), or `groupBy()` to successfully group by key.\n+\n+Now with your `KGroupedStream` objects, you start creating your larger aggregate by calling `KGroupedStream.cogroup()` on the first stream, using your `Aggregator`.  This first step returns a `CogroupedKStream` instance.  Then for each remaining `KGroupedStream`, you execute `CogroupedKSteam.cogroup()` using one of the `KGroupedStream` instances and the `Aggregator` you created previously.  You repeat this sequence of calls for all of the `KGroupedStream` objects you want to combine into an overall aggregate.\n+\n+For more background on cogrouping functionality in stream you can https://cwiki.apache.org/confluence/display/KAFKA/KIP-150+-+Kafka-Streams+Cogroup[read the KIP proposal].", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ3MDM1Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419470357", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              problem: \"you have multiple streams with an aggregate value that you want to combine into a single object\"\n          \n          \n            \n              problem: \"you have multiple streams, each with an aggregate value like `count`,  that you want to combine into a single result\"", "author": "colinhicks", "createdAt": "2020-05-04T14:17:09Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -257,3 +257,14 @@ naming-changelog-repartition-topics:\n     ksql: disabled\n     kstreams: enabled\n     kafka: disabled\n+\n+cogrouping-streams:\n+  title: \"How to combine stream aggregates together in a single larger object\"\n+  meta-description: \"How to combine stream aggregates together in a single larger object\"\n+  slug: \"/cogrouping-streams\"\n+  problem: \"you have multiple streams with an aggregate value that you want to combine into a single object\"", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ3ODkzMA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r419478930", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"You have mulitple streams with an aggregate value but you want to group them all together - cogrouping.  In this tutorial we'll cover how to use the KStreams CoGrouping functionality to merge mulitple aggregates into a single aggregate object\"\n          \n          \n            \n              introduction: \"You want to compute the count of user login events per application in your system, grouping the individual result from each source stream into one aggregated object.  In this tutorial we'll cover how to use the Kafka Streams Cogrouping functionality to accomplish this task with clear, performant code\"", "author": "colinhicks", "createdAt": "2020-05-04T14:28:40Z", "path": "_data/tutorials.yaml", "diffHunk": "@@ -257,3 +257,14 @@ naming-changelog-repartition-topics:\n     ksql: disabled\n     kstreams: enabled\n     kafka: disabled\n+\n+cogrouping-streams:\n+  title: \"How to combine stream aggregates together in a single larger object\"\n+  meta-description: \"How to combine stream aggregates together in a single larger object\"\n+  slug: \"/cogrouping-streams\"\n+  problem: \"you have multiple streams with an aggregate value that you want to combine into a single object\"\n+  introduction: \"You have mulitple streams with an aggregate value but you want to group them all together - cogrouping.  In this tutorial we'll cover how to use the KStreams CoGrouping functionality to merge mulitple aggregates into a single aggregate object\"", "originalCommit": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4ca77d4033953b157cd8903e5f2af6699994ce38", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4ca77d4033953b157cd8903e5f2af6699994ce38", "message": "Apply suggestions from code review\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-05-04T20:48:50Z", "type": "commit"}, {"oid": "c44be55e156b5a6b11ea02165424023ce769077a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c44be55e156b5a6b11ea02165424023ce769077a", "message": "updates for gitignore", "committedDate": "2020-05-04T21:06:35Z", "type": "commit"}]}