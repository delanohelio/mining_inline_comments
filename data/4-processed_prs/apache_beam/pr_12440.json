{"pr_number": 12440, "pr_title": "[BEAM-10619] Report ratio of implemented pandas tests", "pr_createdAt": "2020-07-31T22:21:04Z", "pr_url": "https://github.com/apache/beam/pull/12440", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA==", "url": "https://github.com/apache/beam/pull/12440#discussion_r464565848", "bodyText": "It'd be nice to track how many actually triggered this error (and perhaps track the reasons why).", "author": "robertwb", "createdAt": "2020-08-03T17:42:48Z", "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1", "originalCommit": "69b1b391628860a75b63d0dc9b5f0153af8eb017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NDMzMg==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466684332", "bodyText": "Yeah that would be really good. Is it possible to get at that information though?", "author": "TheNeuralBit", "createdAt": "2020-08-06T21:02:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NjA0Mg==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466686042", "bodyText": "We could record it here: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/dataframe/doctests.py#L242", "author": "robertwb", "createdAt": "2020-08-06T21:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyMTg3Nw==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466721877", "bodyText": "I found that I could override report_success to collect this", "author": "TheNeuralBit", "createdAt": "2020-08-06T22:35:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NjUyMQ==", "url": "https://github.com/apache/beam/pull/12440#discussion_r464566521", "bodyText": "Worth printing the stats regardless of failure?", "author": "robertwb", "createdAt": "2020-08-03T17:44:04Z", "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return", "originalCommit": "69b1b391628860a75b63d0dc9b5f0153af8eb017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4ODQxOA==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466688418", "bodyText": "I added this because I wasn't sure how to accurately report the stats in the case of failures. In theory one of the wont implement tests could fail, then skipped/wont implement/passed/failed wouldn't be a partitioning of tries.\nIf there's a way to get specific information about the wont implement test cases we could de-dup it though.", "author": "TheNeuralBit", "createdAt": "2020-08-06T21:10:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NjUyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzQ1OQ==", "url": "https://github.com/apache/beam/pull/12440#discussion_r464567459", "bodyText": "The way this is implemented, self.tries includes the skipped ones (which get mutated to pass). I suppose we could instead remove them from the examples list altogether.", "author": "robertwb", "createdAt": "2020-08-03T17:45:44Z", "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return\n+    total_test_cases = self.skipped + self.tries", "originalCommit": "69b1b391628860a75b63d0dc9b5f0153af8eb017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyMzA1OQ==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466723059", "bodyText": "Good catch, I'm not sure what I was thinking here.", "author": "TheNeuralBit", "createdAt": "2020-08-06T22:39:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzQ1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODk3OQ==", "url": "https://github.com/apache/beam/pull/12440#discussion_r464568979", "bodyText": "Just because they're not won't implement, doesn't mean we will (e.g .they could be skipped, or they're already implemented in which case the future tense is odd). Maybe break this down as\nXxx total test cases.\n    Xxx skipped\n    Xxx won't implement\n        Yyy reason A\n        Yyy reason B\n        ...\n    Xxx passed", "author": "robertwb", "createdAt": "2020-08-03T17:48:46Z", "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return\n+    total_test_cases = self.skipped + self.tries\n+    will_implement = total_test_cases - self.wont_implement\n+    print(\"%d total test cases.\" % total_test_cases)\n+    print(\n+        \"%d will implement, %d won't implement.\" %", "originalCommit": "69b1b391628860a75b63d0dc9b5f0153af8eb017", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "41830e56916ff583bbda6b299f62ebf8888995b2", "url": "https://github.com/apache/beam/commit/41830e56916ff583bbda6b299f62ebf8888995b2", "message": "pandas_doctest_test now logs a report about the number of skipped vs wont implement vs passing tests", "committedDate": "2020-08-06T22:38:39Z", "type": "commit"}, {"oid": "41830e56916ff583bbda6b299f62ebf8888995b2", "url": "https://github.com/apache/beam/commit/41830e56916ff583bbda6b299f62ebf8888995b2", "message": "pandas_doctest_test now logs a report about the number of skipped vs wont implement vs passing tests", "committedDate": "2020-08-06T22:38:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc2MDI5OA==", "url": "https://github.com/apache/beam/pull/12440#discussion_r466760298", "bodyText": "Maybe rename this to \"wont_implement_ok\"?", "author": "robertwb", "createdAt": "2020-08-07T00:47:53Z", "path": "sdks/python/apache_beam/dataframe/pandas_doctests_test.py", "diffHunk": "@@ -90,6 +113,16 @@ def test_series_tests(self):\n     result = doctests.testmod(\n         pd.core.series,\n         use_beam=False,\n+        report=True,\n+        wont_implement={", "originalCommit": "41830e56916ff583bbda6b299f62ebf8888995b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIzODg2Ng==", "url": "https://github.com/apache/beam/pull/12440#discussion_r467238866", "bodyText": "Done!", "author": "TheNeuralBit", "createdAt": "2020-08-07T19:50:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc2MDI5OA=="}], "type": "inlineReview"}, {"oid": "d6965c8b67467dd76ed14a23a1b9b999804cbfbe", "url": "https://github.com/apache/beam/commit/d6965c8b67467dd76ed14a23a1b9b999804cbfbe", "message": "wont_implement_ok", "committedDate": "2020-08-07T19:48:18Z", "type": "commit"}, {"oid": "e182a0728a3a7bb7bd02a583862c0d90f04035f0", "url": "https://github.com/apache/beam/commit/e182a0728a3a7bb7bd02a583862c0d90f04035f0", "message": "fix tests", "committedDate": "2020-08-07T20:37:17Z", "type": "commit"}, {"oid": "b3dd9653e269c919d2130dab53847f8cf6c356e3", "url": "https://github.com/apache/beam/commit/b3dd9653e269c919d2130dab53847f8cf6c356e3", "message": "yapf", "committedDate": "2020-08-07T21:02:30Z", "type": "commit"}, {"oid": "dbfbaeb5fe25922a07e5208925ca5ab1939bca7b", "url": "https://github.com/apache/beam/commit/dbfbaeb5fe25922a07e5208925ca5ab1939bca7b", "message": "Merge remote-tracking branch 'origin/master' into pandas-report-implemented", "committedDate": "2020-08-07T21:03:19Z", "type": "commit"}]}