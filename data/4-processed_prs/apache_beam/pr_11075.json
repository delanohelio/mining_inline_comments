{"pr_number": 11075, "pr_title": "[BEAM-9421] Website section that describes getting predictions using AI Platform Prediciton", "pr_createdAt": "2020-03-09T09:04:36Z", "pr_url": "https://github.com/apache/beam/pull/11075", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTExNDAxNA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r391114014", "bodyText": "We can create tracking Jira for this and add reference.", "author": "Ardagan", "createdAt": "2020-03-11T16:48:24Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use your cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+We are going to use [tfx_bsl](https://github.com/tensorflow/tfx-bsl) library which provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. In this section we are going to consider one of them that uses a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before we get started, we have to deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Let's show an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+Here is the code:\n+\n+{:.language-java}\n+```java\n+// Getting predictions is not available for Java.", "originalCommit": "ddfb70210752480f4f38282f888be12489c3b588", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjMwNjczNw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392306737", "bodyText": "Done.", "author": "kamilwu", "createdAt": "2020-03-13T15:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTExNDAxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0ODU0OQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392548549", "bodyText": "\"one of them can use a service endpoint\", just out of curiosity, what is the other type of inference", "author": "wenchenglu", "createdAt": "2020-03-14T02:25:22Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk2NjQ1MA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392966450", "bodyText": "Offline inference from a SavedModel instance. I decided it's not worth mentioning here, since the article is focused on AI Platform patterns.", "author": "kamilwu", "createdAt": "2020-03-16T12:00:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0ODU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392549097", "bodyText": "Does that mean users will need to separately deploy a model first? will it be a better user experience if some initial setup stage for Beam can call AI Platform prediction public API to deploy model and get the service endpoint?\nAlso, for batch inference scenario, model deployment is a one-off job, users then need to un-deploy models to avoid unnecessary charges. Should they do that separately, or is there a BEAM final stage we could plug in a API call to delete that model deployment?", "author": "wenchenglu", "createdAt": "2020-03-14T02:33:46Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAwNTM4NQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393005385", "bodyText": "Yes, the transform expects that a specified model is up and running. Users should also un-deploy redundant model manually.\nIt is theoretically possible to implement such initial setup and clean-up operations, however, I wonder whether this is a good use-case for Beam: those are long-running operations and we'd have to wait until they are finished. As a result, Dataflow workers (or any other runner) would be idle for some time.", "author": "kamilwu", "createdAt": "2020-03-16T13:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAwNzIxMw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393007213", "bodyText": "I'm curious what others think about this. @aaltay @Ardagan", "author": "kamilwu", "createdAt": "2020-03-16T13:03:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE0NjEwOQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393146109", "bodyText": "I am also curious about the best practice in similar scenarios for Beam customers. When they need to setup something, which can take about 10 minutes, do they normally do that setup manually or as part of pipeline.\nFor up-deploy, we are considering autoscaling down to 0 when batch job is done, then customers don't need to do that un-deploy manually, neither we need to figure out the Beam clean-up phase.", "author": "wenchenglu", "createdAt": "2020-03-16T16:20:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM2NTE2OQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393365169", "bodyText": "Beam does not have hooks for job start and job finish. One way to emulate those would be using an airflow wrapper to run some setup and cleanup tasks before and after a task. I do not believe we have a recommendation here but an example pattern might be useful (@rezarokni -- in case this will be a good pattern to add to the backlog.)\nFor this version, it is probably fine to assume that the life cycle of the model will be managed by the user separately. For a next phase, we can try to build a graph similar to IOs (like BQ IO) to execute a certain setup and clean task in a single worker. (Although we cannot guarantee that it will not be retried)", "author": "aaltay", "createdAt": "2020-03-16T23:23:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM2NTgzOQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393365839", "bodyText": "LGTM", "author": "wenchenglu", "createdAt": "2020-03-16T23:26:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTA5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTM4Mw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392549383", "bodyText": "is this a single inference input item? or a set of items? FYI, I think AI Platform Prediction supports both. For the latter one, a single HTTP request will embed multiple input items, which might provide better throughput once AI Platform Prediction enables batching mode in their model server.", "author": "wenchenglu", "createdAt": "2020-03-14T02:38:19Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. \n+\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input, but other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform. To send binary data, make sure that the name of an input ends in `_bytes`.\n+\n+Here is the code:\n+\n+{:.language-java}\n+```java\n+// Getting predictions is not yet available for Java. [BEAM-9501]\n+```\n+\n+{:.language-py}\n+```py\n+import json\n+\n+import apache_beam as beam\n+\n+import tensorflow as tf\n+from tfx_bsl.beam.run_inference import RunInference\n+from tfx_bsl.proto import model_spec_pb2\n+\n+def convert_json_to_tf_example(json_obj):\n+  dict_ = json.loads(json_obj)\n+  for name, text in dict_.items():\n+      value = tf.train.Feature(bytes_list=tf.train.BytesList(\n+        value=[text.encode('utf-8')]))\n+      feature = {name: value}\n+      return tf.train.Example(features=tf.train.Features(feature=feature))\n+\n+with beam.Pipeline() as p:\n+     _ = (p\n+         | beam.io.ReadFromText('gs://my-bucket/samples.json')", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk5MTU1Nw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392991557", "bodyText": "RunInference uses a Beam's built-in transform BatchElements, which batches elements (it means that it consumes a PCollection of tf.train.Example and produces a PCollection of List[tf.train.Example]). Thanks to that, multiple input items could be sent using a single HTTP request.", "author": "kamilwu", "createdAt": "2020-03-16T12:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTM4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTk2MQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392549961", "bodyText": "Is RunInference a HTTP call? is there a plan to support gRPC in the future?", "author": "wenchenglu", "createdAt": "2020-03-14T02:46:45Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. \n+\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input, but other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform. To send binary data, make sure that the name of an input ends in `_bytes`.\n+\n+Here is the code:\n+\n+{:.language-java}\n+```java\n+// Getting predictions is not yet available for Java. [BEAM-9501]\n+```\n+\n+{:.language-py}\n+```py\n+import json\n+\n+import apache_beam as beam\n+\n+import tensorflow as tf\n+from tfx_bsl.beam.run_inference import RunInference\n+from tfx_bsl.proto import model_spec_pb2\n+\n+def convert_json_to_tf_example(json_obj):\n+  dict_ = json.loads(json_obj)\n+  for name, text in dict_.items():\n+      value = tf.train.Feature(bytes_list=tf.train.BytesList(\n+        value=[text.encode('utf-8')]))\n+      feature = {name: value}\n+      return tf.train.Example(features=tf.train.Features(feature=feature))\n+\n+with beam.Pipeline() as p:\n+     _ = (p\n+         | beam.io.ReadFromText('gs://my-bucket/samples.json')\n+         | beam.Map(convert_json_to_tf_example)\n+         | RunInference(", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk3MjY2OQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r392972669", "bodyText": "Yes. The transform uses this client library: https://github.com/googleapis/google-api-python-client.\n\nis there a plan to support gRPC in the future?\n\nAs far as I know, there is no such plan at the moment", "author": "kamilwu", "createdAt": "2020-03-16T12:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU0OTk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3ODM4Ng==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393378386", "bodyText": "I think it would be good to list all possible input formats and output formats here or somewhere else for reference. And for the last sentence, do you mean that we need to change l74 to something like:\nfeature={name+'_bytes', value} for sending binary data to endpoint?", "author": "limingxi", "createdAt": "2020-03-17T00:12:32Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. \n+\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input, but other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform. To send binary data, make sure that the name of an input ends in `_bytes`.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc2NDM2Nw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393764367", "bodyText": "Basically, the input format is very easy: the transform expects a PCollection of tf.train.Example objects, which are pretty well-known in tensorflow world. But I agree that the information about sending binary data is not visible enough. I'll try to improve it.\n\ndo you mean that we need to change l74 to something like:\nfeature={name+'_bytes', value} for sending binary data to endpoint?\n\nYes. It's a sign for the transform that data should be b64-encoded. In a request we'd have something like this:\n\"instances\": [{\"input_bytes\": {\"b64\": \"xxx\"}}]", "author": "kamilwu", "createdAt": "2020-03-17T15:28:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3ODM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3MDQyMw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393770423", "bodyText": "Is AI Platform the \"cloud-hosted machine learning model\"?\nI think we can remove the last part (\"within Beam's pipeline\").", "author": "soyrice", "createdAt": "2020-03-17T15:36:28Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAzMzkxOA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395033918", "bodyText": "No, AI Platform is a service that manages cloud-hosted machine learning models.\nHow about something like this?\n\nThis section shows how to use Google Cloud AI Platform Prediction  to make predictions about new data from a cloud-hosted machine learning model.\n\nI'll also put an url link to an overview of AI Platform Prediction.", "author": "kamilwu", "createdAt": "2020-03-19T13:43:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3MDQyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NDYwOQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395094609", "bodyText": "SG :)", "author": "soyrice", "createdAt": "2020-03-19T15:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3MDQyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3MzAzOQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393773039", "bodyText": "To avoid the phrase \"Beam's PTransform\", I'd recommend something like \"is a library with a Beam PTransform called RunInterface\".", "author": "soyrice", "createdAt": "2020-03-17T15:39:48Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NDIxMg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393774212", "bodyText": "\"able to perform two types of inference\"\nI only see one type of inference discussed in this paragraph. If we're only addressing one type of inference here, then we don't need to mention that there's another type. A reader might expect there to be information on both types of inference.", "author": "soyrice", "createdAt": "2020-03-17T15:41:24Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAzNzcyOA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395037728", "bodyText": "Fair enough, this is an another evidence that this mention causes confusion. I'll remote it.", "author": "kamilwu", "createdAt": "2020-03-19T13:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NDIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NDg5OA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393774898", "bodyText": "Add a comma before \"which\" in \"which contains predictions\"", "author": "soyrice", "createdAt": "2020-03-17T15:42:13Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NTY1Ng==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393775656", "bodyText": "\"One of them can use a service endpoint.\"\nFirst state the inference you can do with the transform, then describe how to do it.", "author": "soyrice", "createdAt": "2020-03-17T15:43:04Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NzMwNw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393777307", "bodyText": "If only TensorFlow models are supported, we should say \"deploy a TensorFlow model to the cloud.\"\nAlso, does the cloud service to which you deploy the model have to be AI Platform? If so, we should say \"deploy a TensorFlow model to AI Platform.\"", "author": "soyrice", "createdAt": "2020-03-17T15:45:12Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA0OTQzMA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395049430", "bodyText": "Alright, but I'd rather keep this sentence: \"only TensorFlow models are supported by the transform\" anyway. AI Platform, unlike RunInference, doesn't only support TensorFlow models.  It's important to put an emphasis on that.", "author": "kamilwu", "createdAt": "2020-03-19T14:04:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NzMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTk2MA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395095960", "bodyText": "SG - that definitely helps remove the ambiguity", "author": "soyrice", "createdAt": "2020-03-19T15:04:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NzMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMDUxMQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395100511", "bodyText": "The new text says \"RunInference is able to perform an inference that can use a service endpoint.\" Is the service endpoint for receiving data or sending requests? It might be helpful to clarify.", "author": "soyrice", "createdAt": "2020-03-19T15:10:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NzMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExODY4Ng==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395118686", "bodyText": "In this case it's for receiving data. In other words, AI Platform Prediction exposes a service endpoint that can receive data. I'll clarify it.", "author": "kamilwu", "createdAt": "2020-03-19T15:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3NzMwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3ODEyNw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393778127", "bodyText": "\"and sends data to the service\"\n\"the service\" is \"AI Platform\", right? We should state this explicitly.", "author": "soyrice", "createdAt": "2020-03-17T15:46:16Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. \n+\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc3ODk2Nw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393778967", "bodyText": "The first sentence of this paragraph is long and hard to follow. Consider splitting in up into two (or even three) shorter sentences.", "author": "soyrice", "createdAt": "2020-03-17T15:47:24Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use a cloud-hosted machine learning model to make predictions about new data using Google Cloud AI Platform Prediction within Beam's pipeline.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library that provides `RunInference` Beam's PTransform. `RunInference` is a PTransform able to perform two types of inference. One of them can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for each element, sends a request to Google Cloud AI Platform Prediction service. The transform produces a PCollection of type `PredictLog` which contains predictions.\n+\n+Before getting started, deploy a machine learning model to the cloud. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Only Tensorflow models are supported. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. \n+\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to the service. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input, but other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform. To send binary data, make sure that the name of an input ends in `_bytes`.", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc4MzEzNQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r393783135", "bodyText": "Use the full service name here: \"Patterns for using the Google Cloud AI Platform Prediction\"\nMake sure the the service name is consistent throughout the pattern. I noticed that sometimes it's \"Google Cloud AI Platform Prediction service\", \"Google Cloud AI Platform Prediction\", etc.\nI'd recommend using the full service name (\"Google Cloud AI Platform Prediction\") the first time its mentioned. Then, refer to the service with the shorthand \"AI Platform Prediction\". This way, you can use some shorthand but there's no ambiguity about the name of service.", "author": "soyrice", "createdAt": "2020-03-17T15:52:50Z", "path": "website/src/documentation/patterns/overview.md", "diffHunk": "@@ -38,6 +38,9 @@ Pipeline patterns demonstrate common Beam use cases. Pipeline patterns are based\n **Custom window patterns** - Patterns for windowing functions\n * [Using data to dynamically set session window gaps]({{ site.baseurl }}/documentation/patterns/custom-windows/#using-data-to-dynamically-set-session-window-gaps)\n \n+**AI Platform integration patterns** - Patterns for Google AI Platform transforms", "originalCommit": "a3b1bb3c731e814be1c9314f0ef872a7bfae78c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA3MTg3Mw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395071873", "bodyText": "Looks good. One minor comment: there are more Google Cloud AI Platform transforms. This pull request is just about Prediction, more patterns could be added later. So I'd stick with Patterns for Google Cloud AI Platform transforms", "author": "kamilwu", "createdAt": "2020-03-19T14:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc4MzEzNQ=="}], "type": "inlineReview"}, {"oid": "7d174eb913dea3e1294811a1282b572ec2aecfc6", "url": "https://github.com/apache/beam/commit/7d174eb913dea3e1294811a1282b572ec2aecfc6", "message": "fix: additional fixes after review", "committedDate": "2020-03-19T14:27:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMzI2Mg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395103262", "bodyText": "\"for every bunch of elements, sends a request to AI Platform Prediction.\"\nI assume this refers to batching data to AI Platform. Is there away to configure how many elements are in the batch? I think the previous version of this paragraph stated that one request is sent per element.", "author": "soyrice", "createdAt": "2020-03-19T15:14:33Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,87 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use a service endpoint. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every bunch of elements, sends a request to AI Platform Prediction. The transform produces a PCollection of type `PredictLog`, which contains predictions.", "originalCommit": "7d174eb913dea3e1294811a1282b572ec2aecfc6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExMzgyMA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395113820", "bodyText": "Is there away to configure how many elements are in the batch?\n\nNo, in this case Beam handles the size of a batch on its own.\n\nI think the previous version of this paragraph stated that one request is sent per element.\n\nThat's right. After a discussion on batching it comes to my mind this is not truly correct.", "author": "kamilwu", "createdAt": "2020-03-19T15:28:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMzI2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEzMDA3Mw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395130073", "bodyText": "Is there another doc (or some source code) we can link to that describes how Beam handles the batch size? It's probably not worth adding the extra details to this pattern but it may be helpful to provide a link for more information.\nI also think we should use the term \"batch\" instead of \"bunch\" for additional clarity.", "author": "soyrice", "createdAt": "2020-03-19T15:50:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMzI2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE0NjE2MA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395146160", "bodyText": "I believe BatchElements (the transform responsible for batching here) doc is sufficient. Here's a link", "author": "kamilwu", "createdAt": "2020-03-19T16:12:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMzI2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIyODMxNg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r395228316", "bodyText": "Nice. We should add the link to the page", "author": "soyrice", "createdAt": "2020-03-19T18:16:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwMzI2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE2OTMwMA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r421169300", "bodyText": "s/PredictLog/PredictionLog", "author": "rose-rong-liu", "createdAt": "2020-05-07T00:29:28Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,90 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictLog`, which contains predictions. ", "originalCommit": "265063fc04ca7f6ab71b3ca864d6ad4cb09d9f6f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE2OTc0Mg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r421169742", "bodyText": "Is there extra space?", "author": "rose-rong-liu", "createdAt": "2020-05-07T00:30:47Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,90 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictLog`, which contains predictions. \n+\n+Before getting started, deploy a TensorFlow model to AI Platform Prediction. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Do note that only TensorFlow models are supported by the transform. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. To send binary data, make sure that the name of an input ends in `_bytes`. This will base64-encode data before sending a request.\n+\n+### Example\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to AI Platform Prediction. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input. However, other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform.\n+\n+Here is the code:\n+\n+{:.language-java}\n+```java\n+// Getting predictions is not yet available for Java. [BEAM-9501]\n+```\n+\n+{:.language-py}\n+```py\n+import json\n+\n+import apache_beam as beam\n+\n+import tensorflow as tf\n+from tfx_bsl.beam.run_inference import RunInference\n+from tfx_bsl.proto import model_spec_pb2\n+\n+def convert_json_to_tf_example(json_obj):\n+  dict_ = json.loads(json_obj)\n+  for name, text in dict_.items():\n+      value = tf.train.Feature(bytes_list=tf.train.BytesList(\n+        value=[text.encode('utf-8')]))\n+      feature = {name: value}\n+      return tf.train.Example(features=tf.train.Features(feature=feature))", "originalCommit": "265063fc04ca7f6ab71b3ca864d6ad4cb09d9f6f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTQ0NTY1NQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r421445655", "bodyText": "No, I think it's fine. There's only one space, after the return.", "author": "kamilwu", "createdAt": "2020-05-07T11:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE2OTc0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTE3MDE0NA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r421170144", "bodyText": "ModelEndpointSpec will be changed to AIPlatformPredictionModelSpec in next release of tfx_bsl.", "author": "rose-rong-liu", "createdAt": "2020-05-07T00:32:10Z", "path": "website/src/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,90 @@\n+---\n+layout: section\n+title: \"AI Platform integration patterns\"\n+section_menu: section-menu/documentation.html\n+permalink: /documentation/patterns/ai-platform/\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+<nav class=\"language-switcher\">\n+  <strong>Adapt for:</strong>\n+  <ul>\n+    <li data-type=\"language-java\">Java SDK</li>\n+    <li data-type=\"language-py\" class=\"active\">Python SDK</li>\n+  </ul>\n+</nav>\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictLog`, which contains predictions. \n+\n+Before getting started, deploy a TensorFlow model to AI Platform Prediction. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Do note that only TensorFlow models are supported by the transform. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. To send binary data, make sure that the name of an input ends in `_bytes`. This will base64-encode data before sending a request.\n+\n+### Example\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to AI Platform Prediction. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input. However, other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform.\n+\n+Here is the code:\n+\n+{:.language-java}\n+```java\n+// Getting predictions is not yet available for Java. [BEAM-9501]\n+```\n+\n+{:.language-py}\n+```py\n+import json\n+\n+import apache_beam as beam\n+\n+import tensorflow as tf\n+from tfx_bsl.beam.run_inference import RunInference\n+from tfx_bsl.proto import model_spec_pb2\n+\n+def convert_json_to_tf_example(json_obj):\n+  dict_ = json.loads(json_obj)\n+  for name, text in dict_.items():\n+      value = tf.train.Feature(bytes_list=tf.train.BytesList(\n+        value=[text.encode('utf-8')]))\n+      feature = {name: value}\n+      return tf.train.Example(features=tf.train.Features(feature=feature))\n+\n+with beam.Pipeline() as p:\n+     _ = (p\n+         | beam.io.ReadFromText('gs://my-bucket/samples.json')\n+         | beam.Map(convert_json_to_tf_example)\n+         | RunInference(\n+             model_spec_pb2.InferenceEndpoint(\n+                 model_endpoint_spec=model_spec_pb2.ModelEndpointSpec(", "originalCommit": "265063fc04ca7f6ab71b3ca864d6ad4cb09d9f6f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c8a2246f3541b5e979a40b7b402cad8727729777", "url": "https://github.com/apache/beam/commit/c8a2246f3541b5e979a40b7b402cad8727729777", "message": "fix: addressing comments", "committedDate": "2020-05-07T11:54:10Z", "type": "forcePushed"}, {"oid": "d2fcdf025eb95de69f110daab3d58e7d18fd0c80", "url": "https://github.com/apache/beam/commit/d2fcdf025eb95de69f110daab3d58e7d18fd0c80", "message": "[BEAM-9421] Website section that describes getting predictions using AI Platform Prediction", "committedDate": "2020-05-18T13:49:41Z", "type": "forcePushed"}, {"oid": "2889af47ac3462c9c35b891c1452e0de70d64a25", "url": "https://github.com/apache/beam/commit/2889af47ac3462c9c35b891c1452e0de70d64a25", "message": "[BEAM-9421] Website section that describes getting predictions using AI Platform Prediction", "committedDate": "2020-05-18T14:17:33Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzMDE1NA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r426830154", "bodyText": "From the implementation side, it does not limit to tensorflow model, right? As cloud serves other model formats with the same predict API. Or is this for branding purpose?", "author": "rose-rong-liu", "createdAt": "2020-05-18T18:54:52Z", "path": "website/www/site/content/en/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,79 @@\n+---\n+title: \"AI Platform integration patterns\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+{{< language-switcher java py >}}\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictionLog`, which contains predictions.\n+\n+Before getting started, deploy a TensorFlow model to AI Platform Prediction. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Do note that only TensorFlow models are supported by the transform. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).", "originalCommit": "2889af47ac3462c9c35b891c1452e0de70d64a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyNzQ1Ng==", "url": "https://github.com/apache/beam/pull/11075#discussion_r427227456", "bodyText": "IIRC yes, it does not limit to tensorflow model, as long as prediction input is valid (for instance, binary data is accepted only by tensorflow model)", "author": "kamilwu", "createdAt": "2020-05-19T11:26:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzMDE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTI1NA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r426889254", "bodyText": "tf.train.Example -> tf.train.Example or tf.train.SequenceExample", "author": "aaltay", "createdAt": "2020-05-18T20:57:34Z", "path": "website/www/site/content/en/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,79 @@\n+---\n+title: \"AI Platform integration patterns\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+{{< language-switcher java py >}}\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).", "originalCommit": "2889af47ac3462c9c35b891c1452e0de70d64a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyMDE2Nw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r427220167", "bodyText": "tf.train.SequenceExample is not supported for remote prediction: link", "author": "kamilwu", "createdAt": "2020-05-19T11:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUzOTEzMA==", "url": "https://github.com/apache/beam/pull/11075#discussion_r427539130", "bodyText": "@rose-rong-liu - any suggestions here?", "author": "aaltay", "createdAt": "2020-05-19T19:13:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMwNzA5Mg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r428307092", "bodyText": "We're actively looking for solutions for sequence example support, which we may leverage arrow on. Maybe add a note to mention sequence example will be supported shortly?", "author": "rose-rong-liu", "createdAt": "2020-05-20T21:05:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTcxOTgzNw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r431719837", "bodyText": "I think adding such a note makes sense. @aaltay Any thoughs?", "author": "kamilwu", "createdAt": "2020-05-28T09:57:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTUxOQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r426889519", "bodyText": "The size of a batch may vary. -> The size of a batch is automatically computed.", "author": "aaltay", "createdAt": "2020-05-18T20:58:04Z", "path": "website/www/site/content/en/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,79 @@\n+---\n+title: \"AI Platform integration patterns\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+{{< language-switcher java py >}}\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).", "originalCommit": "2889af47ac3462c9c35b891c1452e0de70d64a25", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTgwNg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r426889806", "bodyText": "Is this still applicable?\n/cc @rose-rong-liu", "author": "aaltay", "createdAt": "2020-05-18T20:58:48Z", "path": "website/www/site/content/en/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,79 @@\n+---\n+title: \"AI Platform integration patterns\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+{{< language-switcher java py >}}\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictionLog`, which contains predictions.\n+\n+Before getting started, deploy a TensorFlow model to AI Platform Prediction. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Do note that only TensorFlow models are supported by the transform. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. To send binary data, make sure that the name of an input ends in `_bytes`. This will base64-encode data before sending a request.", "originalCommit": "2889af47ac3462c9c35b891c1452e0de70d64a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ5NDQ4NQ==", "url": "https://github.com/apache/beam/pull/11075#discussion_r427494485", "bodyText": "Yes, this is still applicable, and required by cloud inference [1].\n[1] https://cloud.google.com/ai-platform/prediction/docs/online-predict#binary_data_in_prediction_input", "author": "rose-rong-liu", "createdAt": "2020-05-19T17:58:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4OTgwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg5MDAxMw==", "url": "https://github.com/apache/beam/pull/11075#discussion_r426890013", "bodyText": "remove the underscore. dict_ -> dict", "author": "aaltay", "createdAt": "2020-05-18T20:59:17Z", "path": "website/www/site/content/en/documentation/patterns/ai-platform.md", "diffHunk": "@@ -0,0 +1,79 @@\n+---\n+title: \"AI Platform integration patterns\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# AI Platform integration patterns\n+\n+This page describes common patterns in pipelines with Google Cloud AI Platform transforms.\n+\n+{{< language-switcher java py >}}\n+\n+## Getting predictions\n+\n+This section shows how to use [Google Cloud AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs/overview) to make predictions about new data from a cloud-hosted machine learning model.\n+ \n+[tfx_bsl](https://github.com/tensorflow/tfx-bsl) is a library with a Beam PTransform called `RunInference`. `RunInference` is able to perform an inference that can use an external service endpoint for receiving data. When using a service endpoint, the transform takes a PCollection of type `tf.train.Example` and, for every batch of elements, sends a request to AI Platform Prediction. The size of a batch may vary. For more details on how Beam finds the best batch size, refer to a docstring for [BatchElements](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html?highlight=batchelements#apache_beam.transforms.util.BatchElements).\n+ \n+ The transform produces a PCollection of type `PredictionLog`, which contains predictions.\n+\n+Before getting started, deploy a TensorFlow model to AI Platform Prediction. The cloud service manages the infrastructure needed to handle prediction requests in both efficient and scalable way. Do note that only TensorFlow models are supported by the transform. For more information, see [Exporting a SavedModel for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction).\n+\n+Once a machine learning model is deployed, prepare a list of instances to get predictions for. To send binary data, make sure that the name of an input ends in `_bytes`. This will base64-encode data before sending a request.\n+\n+### Example\n+Here is an example of a pipeline that reads input instances from the file, converts JSON objects to `tf.train.Example` objects and sends data to AI Platform Prediction. The content of a file can look like this:\n+\n+```\n+{\"input\": \"the quick brown\"}\n+{\"input\": \"la bruja le\"}\n+``` \n+\n+The example creates `tf.train.BytesList` instances, thus it expects byte-like strings as input. However, other data types, like `tf.train.FloatList` and `tf.train.Int64List`, are also supported by the transform.\n+\n+Here is the code:\n+\n+{{< highlight java >}}\n+// Getting predictions is not yet available for Java. [BEAM-9501]\n+{{< /highlight >}}\n+\n+{{< highlight py >}}\n+import json\n+\n+import apache_beam as beam\n+\n+import tensorflow as tf\n+from tfx_bsl.beam.run_inference import RunInference\n+from tfx_bsl.proto import model_spec_pb2\n+\n+def convert_json_to_tf_example(json_obj):\n+  dict_ = json.loads(json_obj)", "originalCommit": "2889af47ac3462c9c35b891c1452e0de70d64a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzIyNDE2Mg==", "url": "https://github.com/apache/beam/pull/11075#discussion_r427224162", "bodyText": "I changed to samples, dict is the name for dictionary build-in type.", "author": "kamilwu", "createdAt": "2020-05-19T11:19:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg5MDAxMw=="}], "type": "inlineReview"}, {"oid": "9bb3dd4f45f993466cb9d78dc1cb76c8ab6d1ba0", "url": "https://github.com/apache/beam/commit/9bb3dd4f45f993466cb9d78dc1cb76c8ab6d1ba0", "message": "fix: add a note about sequence example", "committedDate": "2020-05-29T09:51:46Z", "type": "forcePushed"}, {"oid": "8228baf9f2751a85ca4b545c4bba4aab45182d15", "url": "https://github.com/apache/beam/commit/8228baf9f2751a85ca4b545c4bba4aab45182d15", "message": "[BEAM-9421] Website section that describes getting predictions using AI Platform Prediction", "committedDate": "2020-06-01T09:19:09Z", "type": "commit"}, {"oid": "520dd0a0abf5063e3638a9956ed62dd4a3adff24", "url": "https://github.com/apache/beam/commit/520dd0a0abf5063e3638a9956ed62dd4a3adff24", "message": "fix: addressing comments", "committedDate": "2020-06-01T09:19:13Z", "type": "commit"}, {"oid": "583e2179649b04f56d1422d5ebfacc2688ee9a4e", "url": "https://github.com/apache/beam/commit/583e2179649b04f56d1422d5ebfacc2688ee9a4e", "message": "fix: add a note about sequence example", "committedDate": "2020-06-01T09:19:13Z", "type": "commit"}, {"oid": "8e8c3a1db3b4dbf979277aa4113dfb102af87036", "url": "https://github.com/apache/beam/commit/8e8c3a1db3b4dbf979277aa4113dfb102af87036", "message": "fix: python snippet first", "committedDate": "2020-06-01T09:25:17Z", "type": "commit"}, {"oid": "8e8c3a1db3b4dbf979277aa4113dfb102af87036", "url": "https://github.com/apache/beam/commit/8e8c3a1db3b4dbf979277aa4113dfb102af87036", "message": "fix: python snippet first", "committedDate": "2020-06-01T09:25:17Z", "type": "forcePushed"}]}