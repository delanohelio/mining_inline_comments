{"pr_number": 13405, "pr_title": "[BEAM-10703, BEAM-10475] Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "pr_createdAt": "2020-11-22T21:59:52Z", "pr_url": "https://github.com/apache/beam/pull/13405", "timeline": [{"oid": "0eeefba7ec5f6e1b6e3b86110673a70d4637e1f4", "url": "https://github.com/apache/beam/commit/0eeefba7ec5f6e1b6e3b86110673a70d4637e1f4", "message": "Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "committedDate": "2020-11-22T23:24:47Z", "type": "forcePushed"}, {"oid": "6ce8851952f05b0eed8cc686f86e8c111f0b3fda", "url": "https://github.com/apache/beam/commit/6ce8851952f05b0eed8cc686f86e8c111f0b3fda", "message": "Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "committedDate": "2020-11-23T17:06:13Z", "type": "forcePushed"}, {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b", "url": "https://github.com/apache/beam/commit/e1aafbf5c360738f5c042bba61855055ab92588b", "message": "Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "committedDate": "2020-11-23T17:20:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r534539958", "bodyText": "Are you going to take care of portable job submission as well? If fo, maybe we need a TODO here.", "author": "boyuanzz", "createdAt": "2020-12-02T23:00:41Z", "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "originalCommit": "e1aafbf5c360738f5c042bba61855055ab92588b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MTYzNA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r535771634", "bodyText": "Yes but supposedly the changes for portable job submission would be on backend side. What's to be done here?", "author": "nehsyc", "createdAt": "2020-12-04T01:37:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2MDg3OA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r536460878", "bodyText": "We might be able to clean this up when we migrate to portable job submission. @chamikaramj", "author": "boyuanzz", "createdAt": "2020-12-05T00:32:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ4MTEzNQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r536481135", "bodyText": "Is there a JIRA issue I can link here? Assuming that there are quite a few places in dataflow_runner.py that need to be cleaned up when we migrate to portable job submission.", "author": "nehsyc", "createdAt": "2020-12-05T02:04:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMjc3Nw==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537812777", "bodyText": "@chamikaramj Do we have a JIRA tracking cleanup task in dataflow_runner.py and DataflowRunner.java after we have portable job submission?", "author": "boyuanzz", "createdAt": "2020-12-07T20:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNTExMg==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537815112", "bodyText": "https://issues.apache.org/jira/browse/BEAM-11360 for Python. Java does not need cleanups but need to be updated to use portable job submission which is tracked separately.", "author": "chamikaramj", "createdAt": "2020-12-07T20:37:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgzNzM1MA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537837350", "bodyText": "Thanks. Added a TODO here.", "author": "nehsyc", "createdAt": "2020-12-07T21:14:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0MzI0MA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537843240", "bodyText": "Thanks. Please note that all Runner v2 jobs will use portable job submission soon. So this change has to be done for portable job submission in the service side. We won't be necessary cleaning this up due to old SDKs and Runner v1 jobs (so don't think we need a TODO).", "author": "chamikaramj", "createdAt": "2020-12-07T21:24:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODk4MA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537848980", "bodyText": "Basically, you need to able to configure the Dataflow v1beta3 step directly from the portable job description (runner API proto) without relying on the state set in intermediate pipeline object graph.", "author": "chamikaramj", "createdAt": "2020-12-07T21:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4MjI3Ng==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537882276", "bodyText": "Got it. Yes I have a cl pending for adding translation to the service which is waiting for this PR :) We are not considering adding this feature patch for python runner v1 jobs though; it is disabled for non-unified worker:\n\n  \n    \n      beam/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py\n    \n    \n         Line 365\n      in\n      c154179\n    \n    \n    \n    \n\n        \n          \n           if not apiclient._use_unified_worker(self.options):", "author": "nehsyc", "createdAt": "2020-12-07T22:30:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r534572350", "bodyText": "I think we also want to check use_runner_v2 as well since it's only supported in runner v2", "author": "boyuanzz", "createdAt": "2020-12-03T00:21:04Z", "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "originalCommit": "e1aafbf5c360738f5c042bba61855055ab92588b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjE4NA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r535772184", "bodyText": "Good catch! Actually we should disable the feature when using beam_fn_api without use_unified_worker (JRH). Updated the logic and tests according in the latest commit.", "author": "nehsyc", "createdAt": "2020-12-04T01:38:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjU4NQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r535772585", "bodyText": "A second thought, would it just work for JRH as well?", "author": "nehsyc", "createdAt": "2020-12-04T01:40:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2MTM4OQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r536461389", "bodyText": "Yeah it should just work on JRH. My question is that do we want to do the expansion even though we know that we are not going to shard it?", "author": "boyuanzz", "createdAt": "2020-12-05T00:34:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ4MTg3Ng==", "url": "https://github.com/apache/beam/pull/13405#discussion_r536481876", "bodyText": "Sorry I didn't get the question. If the feature is not enabled, the properties will not be appended to the step therefore no expansion. Also the SDK implementation applies a default sharding which != no sharding.\n\n  \n    \n      beam/sdks/python/apache_beam/transforms/util.py\n    \n    \n         Line 822\n      in\n      30f9a60\n    \n    \n    \n    \n\n        \n          \n           # Use [uuid, thread id] as the shard id.", "author": "nehsyc", "createdAt": "2020-12-05T02:08:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5Mzk0MA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537893940", "bodyText": "I think your latest changes have addressed my comment. Basically, I don't think we want to the transform override when only with beam_fn_api", "author": "boyuanzz", "createdAt": "2020-12-07T22:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537878118", "bodyText": "We can get ride of GroupIntoBatchesParams by creating GroupIntoBatchesPayload  directly here and accessing the payload directly below.", "author": "boyuanzz", "createdAt": "2020-12-07T22:22:50Z", "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())", "originalCommit": "c1541796435cbd2ee775b6c89ec628f8564a3bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzEzNg==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027136", "bodyText": "The main reason I extracted the params to a class was to reuse the code as much as possible since the params of GroupIntoBatches and WithShardedKey are exactly the same. The class also does parameter validation other than conversion from/to runner api, and perhaps it would make it easier to add more params in the future. But let me know if you have opinions.", "author": "nehsyc", "createdAt": "2020-12-08T04:32:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNjM1OA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538916358", "bodyText": "I see. Thanks for the explanation.", "author": "boyuanzz", "createdAt": "2020-12-09T00:41:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4NTUxNw==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537885517", "bodyText": "It seems like max_buffering_duration_millis is None and max_buffering_duration_millis  == 0 means the same. right? You may want to add a comment about  what will happen if max_buffering_duration_millis is not specified or max_buffering_duration_millis is set to 0. Someone might think max_buffering_duration_millis = 0 or max_buffering_duration_millis is None will make the transform output elements as soon as possible.", "author": "boyuanzz", "createdAt": "2020-12-07T22:35:54Z", "path": "model/pipeline/src/main/proto/beam_runner_api.proto", "diffHunk": "@@ -706,6 +714,16 @@ message PubSubWritePayload {\n   string id_attribute = 3;\n }\n \n+// Payload for GroupIntoBatches composite transform.\n+message GroupIntoBatchesPayload {\n+\n+  // (Required) Max size of a batch.\n+  int64 batch_size = 1;\n+\n+  // (Optional) Max duration a batch is allowed to be cached in states.\n+  int64 max_buffering_duration_millis = 2;", "originalCommit": "c1541796435cbd2ee775b6c89ec628f8564a3bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzAxOQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027019", "bodyText": "Were you suggesting adding a comment here in the runner proto or in GroupIntoBatches transform? I added some comments around here when changing the API: \n  \n    \n      beam/sdks/python/apache_beam/transforms/util.py\n    \n    \n         Line 767\n      in\n      c154179\n    \n    \n    \n    \n\n        \n          \n                   an int or float. Setting this parameter to zero effectively means no", "author": "nehsyc", "createdAt": "2020-12-08T04:32:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4NTUxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4OTYxNQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537889615", "bodyText": "Could you please add some explanations on what ALLOWS_SHARDABLE_STATE  and PRESERVES_KEYS mean?", "author": "boyuanzz", "createdAt": "2020-12-07T22:43:08Z", "path": "sdks/python/apache_beam/runners/dataflow/internal/names.py", "diffHunk": "@@ -69,6 +69,7 @@ class PropertyNames(object):\n \n   Property strings as they are expected in the CloudWorkflow protos.\n   \"\"\"\n+  ALLOWS_SHARDABLE_STATE = 'allows_shardable_state'", "originalCommit": "c1541796435cbd2ee775b6c89ec628f8564a3bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzAzNw==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027037", "bodyText": "Done.", "author": "nehsyc", "createdAt": "2020-12-08T04:32:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4OTYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjkyMw==", "url": "https://github.com/apache/beam/pull/13405#discussion_r537892923", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                # The replacement is only valid for portable Streaming Engine jobs.\n          \n          \n            \n                # The replacement is only valid for portable Streaming Engine jobs with runner_v2.", "author": "boyuanzz", "createdAt": "2020-12-07T22:49:44Z", "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,46 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.", "originalCommit": "c1541796435cbd2ee775b6c89ec628f8564a3bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzA2OQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027069", "bodyText": "Done.", "author": "nehsyc", "createdAt": "2020-12-08T04:32:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjkyMw=="}], "type": "inlineReview"}, {"oid": "c764e7bca1ab6d30df56ee7162bfa84f317eb064", "url": "https://github.com/apache/beam/commit/c764e7bca1ab6d30df56ee7162bfa84f317eb064", "message": "Updated comments", "committedDate": "2020-12-08T04:36:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNzkxMA==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538917910", "bodyText": "Please add some pydoc to describe this class. Other than this, this PR is ready to go.", "author": "boyuanzz", "createdAt": "2020-12-09T00:44:53Z", "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())\n+\n+    @staticmethod\n+    @PTransform.register_urn(\n+        common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+        beam_runner_api_pb2.GroupIntoBatchesPayload)\n+    def from_runner_api_parameter(unused_ptransform, proto, unused_context):\n+      return GroupIntoBatches.WithShardedKey(\n+          *GroupIntoBatchesParams.parse_payload(proto))\n+\n+\n+class GroupIntoBatchesParams:", "originalCommit": "c764e7bca1ab6d30df56ee7162bfa84f317eb064", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkyNTA0NQ==", "url": "https://github.com/apache/beam/pull/13405#discussion_r538925045", "bodyText": "Done. I also changed the class name to _GroupIntoBatchesParams; it's not intended to be \"public\" and referenced outside of this file.", "author": "nehsyc", "createdAt": "2020-12-09T01:02:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNzkxMA=="}], "type": "inlineReview"}, {"oid": "05c8471b27e03e5611a2a13137c4a785f2d17fc9", "url": "https://github.com/apache/beam/commit/05c8471b27e03e5611a2a13137c4a785f2d17fc9", "message": "Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "committedDate": "2020-12-09T00:59:46Z", "type": "commit"}, {"oid": "05c8471b27e03e5611a2a13137c4a785f2d17fc9", "url": "https://github.com/apache/beam/commit/05c8471b27e03e5611a2a13137c4a785f2d17fc9", "message": "Add GroupIntoBatches to runner API; add Dataflow override in Python SDK", "committedDate": "2020-12-09T00:59:46Z", "type": "forcePushed"}]}