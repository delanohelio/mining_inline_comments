{"pr_number": 10979, "pr_title": "[BEAM-8841] Support writing data to BigQuery via Avro in Python SDK", "pr_createdAt": "2020-02-26T17:31:52Z", "pr_url": "https://github.com/apache/beam/pull/10979", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MDM0OQ==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384650349", "bodyText": "open doesn't provide io.IOBase methods like writable() in Python 2.", "author": "chunyang", "createdAt": "2020-02-26T17:32:40Z", "path": "sdks/python/apache_beam/io/localfilesystem.py", "diffHunk": "@@ -139,7 +140,7 @@ def _path_open(\n     \"\"\"Helper functions to open a file in the provided mode.\n     \"\"\"\n     compression_type = FileSystem._get_compression_type(path, compression_type)\n-    raw_file = open(path, mode)\n+    raw_file = io.open(path, mode)", "originalCommit": "f4d1706016a26e0f411ce3722c9df382a4f9cf06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MTA5Ng==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384651096", "bodyText": "This was added because the bigquery_test module imports _ELEMENTS from the bigquery_file_loads_test module, which uses parameterized. Should we refactor _ELEMENTS into its own module?", "author": "chunyang", "createdAt": "2020-02-26T17:34:03Z", "path": "sdks/python/scripts/run_integration_test.sh", "diffHunk": "@@ -198,6 +198,7 @@ if [[ -z $PIPELINE_OPTS ]]; then\n   # See: https://github.com/hamcrest/PyHamcrest/issues/131.\n   echo \"pyhamcrest!=1.10.0,<2.0.0\" > postcommit_requirements.txt\n   echo \"mock<3.0.0\" >> postcommit_requirements.txt\n+  echo \"parameterized>=0.7.1,<0.8.0\" >> postcommit_requirements.txt", "originalCommit": "f4d1706016a26e0f411ce3722c9df382a4f9cf06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MjgwOA==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384652808", "bodyText": "The same code in the Java SDK doesn't seem to support logical types, so behavior is a bit different here.", "author": "chunyang", "createdAt": "2020-02-26T17:37:14Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_avro_tools.py", "diffHunk": "@@ -0,0 +1,135 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Tools used tool work with Avro files in the context of BigQuery.\n+\n+Classes, constants and functions in this file are experimental and have no\n+backwards compatibility guarantees.\n+\n+NOTHING IN THIS FILE HAS BACKWARDS COMPATIBILITY GUARANTEES.\n+\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+BIG_QUERY_TO_AVRO_TYPES = {\n+  \"RECORD\": \"record\",\n+  \"STRING\": \"string\",\n+  \"BOOLEAN\": \"boolean\",\n+  \"BYTES\": \"bytes\",\n+  \"FLOAT\": \"double\",\n+  \"INTEGER\": \"long\",\n+  \"TIME\": {\n+    \"type\": \"long\",\n+    \"logicalType\": \"time-micros\",", "originalCommit": "f4d1706016a26e0f411ce3722c9df382a4f9cf06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MzA2MQ==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384653061", "bodyText": "Moved these to avoid a cyclic import.", "author": "chunyang", "createdAt": "2020-02-26T17:37:44Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON\n \n     self.additional_bq_parameters = additional_bq_parameters or {}\n     self.table_side_inputs = table_side_inputs or ()\n     self.schema_side_inputs = schema_side_inputs or ()\n \n-  @staticmethod\n-  def get_table_schema_from_string(schema):\n-    \"\"\"Transform the string table schema into a\n-    :class:`~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema` instance.\n-\n-    Args:\n-      schema (str): The sting schema to be used if the BigQuery table to write\n-        has to be created.\n-\n-    Returns:\n-      ~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema:\n-      The schema to be used if the BigQuery table to write has to be created\n-      but in the :class:`~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema` format.\n-    \"\"\"\n-    table_schema = bigquery.TableSchema()\n-    schema_list = [s.strip() for s in schema.split(',')]\n-    for field_and_type in schema_list:\n-      field_name, field_type = field_and_type.split(':')\n-      field_schema = bigquery.TableFieldSchema()\n-      field_schema.name = field_name\n-      field_schema.type = field_type\n-      field_schema.mode = 'NULLABLE'\n-      table_schema.fields.append(field_schema)\n-    return table_schema\n-\n-  @staticmethod\n-  def table_schema_to_dict(table_schema):\n-    \"\"\"Create a dictionary representation of table schema for serialization\n-    \"\"\"\n-    def get_table_field(field):\n-      \"\"\"Create a dictionary representation of a table field\n-      \"\"\"\n-      result = {}\n-      result['name'] = field.name\n-      result['type'] = field.type\n-      result['mode'] = getattr(field, 'mode', 'NULLABLE')\n-      if hasattr(field, 'description') and field.description is not None:\n-        result['description'] = field.description\n-      if hasattr(field, 'fields') and field.fields:\n-        result['fields'] = [get_table_field(f) for f in field.fields]\n-      return result\n-\n-    if not isinstance(table_schema, bigquery.TableSchema):\n-      raise ValueError(\"Table schema must be of the type bigquery.TableSchema\")\n-    schema = {'fields': []}\n-    for field in table_schema.fields:\n-      schema['fields'].append(get_table_field(field))\n-    return schema\n-\n-  @staticmethod\n-  def get_dict_table_schema(schema):\n-    \"\"\"Transform the table schema into a dictionary instance.\n-\n-    Args:\n-      schema (~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema):\n-        The schema to be used if the BigQuery table to write has to be created.\n-        This can either be a dict or string or in the TableSchema format.\n-\n-    Returns:\n-      Dict[str, Any]: The schema to be used if the BigQuery table to write has\n-      to be created but in the dictionary format.\n-    \"\"\"\n-    if (isinstance(schema, (dict, vp.ValueProvider)) or callable(schema) or\n-        schema is None):\n-      return schema\n-    elif isinstance(schema, (str, unicode)):\n-      table_schema = WriteToBigQuery.get_table_schema_from_string(schema)\n-      return WriteToBigQuery.table_schema_to_dict(table_schema)\n-    elif isinstance(schema, bigquery.TableSchema):\n-      return WriteToBigQuery.table_schema_to_dict(schema)\n-    else:\n-      raise TypeError('Unexpected schema argument: %s.' % schema)\n+  # Dict/schema methods were moved to bigquery_tools, but keep references\n+  # here for backward compatibility.\n+  get_table_schema_from_string = \\\n+      staticmethod(bigquery_tools.get_table_schema_from_string)\n+  table_schema_to_dict = staticmethod(bigquery_tools.table_schema_to_dict)\n+  get_dict_table_schema = staticmethod(bigquery_tools.get_dict_table_schema)", "originalCommit": "f4d1706016a26e0f411ce3722c9df382a4f9cf06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384769744", "bodyText": "I'm happy to make AVRO the default format if possible. I guess the issue is that users need to provide the schema, right? Otherwise we cannot write the avro files.\nWe could make AVRO the default, and add a check that the schema was provided (i.e. is neither None nor autodetect) - and error out if that's the case? What do you think?", "author": "pabloem", "createdAt": "2020-02-26T21:15:22Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "originalCommit": "efa1137f29da8ab0d4607307ab33f8793531aacc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ0MjA0Ng==", "url": "https://github.com/apache/beam/pull/10979#discussion_r385442046", "bodyText": "AFAICT using Avro has no disadvantages compared to JSON for loading data into BigQuery, but would requiring a schema constitute a breaking API change for semantic versioning purposes?\nPersonally I'm for using Avro as default. I guess when users update Beam, they'll specify a temp_file_format explicitly to get the old behavior.", "author": "chunyang", "createdAt": "2020-02-28T00:08:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg0MjY5MA==", "url": "https://github.com/apache/beam/pull/10979#discussion_r385842690", "bodyText": "Since this is technically still experimental, and masked behind a flag, I think it makes sense to make avro the main way of doing it (and simply add a check that the schema was passed). cc: @chamikaramj", "author": "pabloem", "createdAt": "2020-02-28T18:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg1NDY0NA==", "url": "https://github.com/apache/beam/pull/10979#discussion_r385854644", "bodyText": "Oh I didn't realize it was experimental. I'll make the change then!", "author": "chunyang", "createdAt": "2020-02-28T18:32:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg1NjUwNQ==", "url": "https://github.com/apache/beam/pull/10979#discussion_r385856505", "bodyText": "+1 for making Avro the default for the new sink.", "author": "chamikaramj", "createdAt": "2020-02-28T18:36:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc5OTI2NA==", "url": "https://github.com/apache/beam/pull/10979#discussion_r384799264", "bodyText": "Can you add code to delete the dataset after the test runs?", "author": "pabloem", "createdAt": "2020-02-26T22:12:58Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_test.py", "diffHunk": "@@ -1025,6 +1027,91 @@ def test_file_loads(self):\n         WriteToBigQuery.Method.FILE_LOADS, triggering_frequency=20)\n \n \n+class BigQueryFileLoadsIntegrationTests(unittest.TestCase):\n+  BIG_QUERY_DATASET_ID = 'python_bq_file_loads_'\n+\n+  def setUp(self):\n+    self.test_pipeline = TestPipeline(is_integration_test=True)\n+    self.runner_name = type(self.test_pipeline.runner).__name__\n+    self.project = self.test_pipeline.get_option('project')\n+\n+    self.dataset_id = '%s%s%s' % (\n+        self.BIG_QUERY_DATASET_ID,\n+        str(int(time.time())),\n+        random.randint(0, 10000))\n+    self.bigquery_client = bigquery_tools.BigQueryWrapper()\n+    self.bigquery_client.get_or_create_dataset(self.project, self.dataset_id)\n+    self.output_table = '%s.output_table' % (self.dataset_id)\n+    self.table_ref = bigquery_tools.parse_table_reference(self.output_table)\n+    _LOGGER.info(\n+        'Created dataset %s in project %s', self.dataset_id, self.project)", "originalCommit": "efa1137f29da8ab0d4607307ab33f8793531aacc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0cc577134b9e98b32020efab2ca07ccd1665f7cb", "url": "https://github.com/apache/beam/commit/0cc577134b9e98b32020efab2ca07ccd1665f7cb", "message": "Use Avro format for file loads to BigQuery\n\nThis commit adds the ability to `WriteToBigQuery` via Avro file loads\nand sets the Avro format as the default format for file loads\ninto BigQuery.\n\nChanges include:\n* Add a `temp_file_format` option to `WriteToBigQuery` and\n  `BigQueryBatchFileLoads` to select which file format to use for\n  loading data.\n* Set the default `temp_file_format` to Avro.\n* Move implementation of `get_table_schema_from_string`,\n  `table_schema_to_dict`, and `get_dict_table_schema` to\n  the `bigquery_tools` module.\n* Add `bigquery_avro_tools` module with utilities for converting\n  BigQuery `TableSchema` to Avro `RecordSchema` (this is a port of\n  what's available in the Java SDK, with modified behavior for\n  logical types).\n* Modify `WriteRecordsToFile` and `WriteGroupedRecordsToFile` to accept\n  `schema` and `file format`, since in order to be read by BigQuery,\n  the Avro files must have schema headers.\n* Parameterize relevant tests to check both JSON and Avro code paths.\n* Add integration test using Avro-based file loads.\n* Introduce `JsonRowWriter` and `AvroRowWriter` classes which implement\n  the `io.IOBase` interface and are used by `WriteRecordsToFile`\n  and `WriteGroupedRecordsToFile`.", "committedDate": "2020-03-05T18:33:30Z", "type": "commit"}, {"oid": "0cc577134b9e98b32020efab2ca07ccd1665f7cb", "url": "https://github.com/apache/beam/commit/0cc577134b9e98b32020efab2ca07ccd1665f7cb", "message": "Use Avro format for file loads to BigQuery\n\nThis commit adds the ability to `WriteToBigQuery` via Avro file loads\nand sets the Avro format as the default format for file loads\ninto BigQuery.\n\nChanges include:\n* Add a `temp_file_format` option to `WriteToBigQuery` and\n  `BigQueryBatchFileLoads` to select which file format to use for\n  loading data.\n* Set the default `temp_file_format` to Avro.\n* Move implementation of `get_table_schema_from_string`,\n  `table_schema_to_dict`, and `get_dict_table_schema` to\n  the `bigquery_tools` module.\n* Add `bigquery_avro_tools` module with utilities for converting\n  BigQuery `TableSchema` to Avro `RecordSchema` (this is a port of\n  what's available in the Java SDK, with modified behavior for\n  logical types).\n* Modify `WriteRecordsToFile` and `WriteGroupedRecordsToFile` to accept\n  `schema` and `file format`, since in order to be read by BigQuery,\n  the Avro files must have schema headers.\n* Parameterize relevant tests to check both JSON and Avro code paths.\n* Add integration test using Avro-based file loads.\n* Introduce `JsonRowWriter` and `AvroRowWriter` classes which implement\n  the `io.IOBase` interface and are used by `WriteRecordsToFile`\n  and `WriteGroupedRecordsToFile`.", "committedDate": "2020-03-05T18:33:30Z", "type": "forcePushed"}]}