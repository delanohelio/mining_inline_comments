{"pr_number": 12838, "pr_title": "[BEAM-10892] Add Proto support to Kafka Table Provider", "pr_createdAt": "2020-09-14T11:53:17Z", "pr_url": "https://github.com/apache/beam/pull/12838", "timeline": [{"oid": "64c420a9c5e75cc6e8d8416787f453b2e4be8afc", "url": "https://github.com/apache/beam/commit/64c420a9c5e75cc6e8d8416787f453b2e4be8afc", "message": "[BEAM-10892] Add Proto support to Kafka table provider", "committedDate": "2020-09-14T13:36:20Z", "type": "forcePushed"}, {"oid": "1ef3668c2b8285f1f5595d8de8267e6ff8b39af1", "url": "https://github.com/apache/beam/commit/1ef3668c2b8285f1f5595d8de8267e6ff8b39af1", "message": "Remove booleans, use List of values", "committedDate": "2020-09-17T09:30:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MDgyNg==", "url": "https://github.com/apache/beam/pull/12838#discussion_r493890826", "bodyText": "This isn't actually encoding to protobuf, RowCoder has it's own serialization format, specified here: \n  \n    \n      beam/model/pipeline/src/main/proto/beam_runner_api.proto\n    \n    \n        Lines 891 to 903\n      in\n      3f71138\n    \n    \n    \n    \n\n        \n          \n           // A row is encoded as the concatenation of: \n        \n\n        \n          \n           //   - The number of attributes in the schema, encoded with \n        \n\n        \n          \n           //     beam:coder:varint:v1. This makes it possible to detect certain \n        \n\n        \n          \n           //     allowed schema changes (appending or removing columns) in \n        \n\n        \n          \n           //     long-running streaming pipelines. \n        \n\n        \n          \n           //   - A byte array representing a packed bitset indicating null fields (a \n        \n\n        \n          \n           //     1 indicating a null) encoded with beam:coder:bytes:v1. The unused \n        \n\n        \n          \n           //     bits in the last byte must be set to 0. If there are no nulls an \n        \n\n        \n          \n           //     empty byte array is encoded. \n        \n\n        \n          \n           //     The two-byte bitset (not including the lenghth-prefix) for the row \n        \n\n        \n          \n           //     [NULL, 0, 0, 0, NULL, 0, 0, NULL, 0, NULL] would be \n        \n\n        \n          \n           //     [0b10010001, 0b00000010] \n        \n\n        \n          \n           //   - An encoding for each non-null field, concatenated together. \n        \n    \n  \n\n\nInstead you'll want to look at using the protobuf extensions that @alexvanboxel added.\nThat package has utilities for converting between protobuf types and beam Rows, in ProtoDynamicMessageSchema.forSchema. There's also ProtoCoder and DynamicProtoCoder which can serialize to bytes.\nI think this may be more involved than adding Avro support, I'm not sure that we can encode to proto with just a Beam schema, we likely need a protobuf message descriptor as well, so there will need to be some way for the user to provide that.", "author": "TheNeuralBit", "createdAt": "2020-09-23T20:57:46Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.List;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+\n+public class BeamKafkaProtoTable extends BeamKafkaTable {\n+\n+  public BeamKafkaProtoTable(Schema beamSchema, String bootstrapServers, List<String> topics) {\n+    super(beamSchema, bootstrapServers, topics);\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    return new ProtoRecorderDecoder(schema);\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> getPTransformForOutput() {\n+    return new ProtoRecorderEncoder();\n+  }\n+\n+  /** A PTransform to convert {@code KV<byte[], byte[]>} to {@link Row}. */\n+  public static class ProtoRecorderDecoder\n+      extends PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> {\n+    private final Schema schema;\n+\n+    public ProtoRecorderDecoder(Schema schema) {\n+      this.schema = schema;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+      return input\n+          .apply(\n+              \"decodeProtoRecord\",\n+              ParDo.of(\n+                  new DoFn<KV<byte[], byte[]>, Row>() {\n+                    @ProcessElement\n+                    public void processElement(ProcessContext c) {\n+                      c.output(parseProtoPayloadToRow(c.element().getValue(), schema));\n+                    }\n+                  }))\n+          .setRowSchema(schema);\n+    }\n+\n+    static Row parseProtoPayloadToRow(byte[] payload, Schema payloadSchema) {\n+      try {\n+        InputStream inputStream = new ByteArrayInputStream(payload);\n+        RowCoder rowCoder = RowCoder.of(payloadSchema);\n+        return rowCoder.decode(inputStream);\n+      } catch (IOException e) {\n+        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n+      }\n+    }\n+  }\n+\n+  /** A PTransform to convert {@link Row} to {@code KV<byte[], byte[]>}. */\n+  public static class ProtoRecorderEncoder\n+      extends PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> {\n+\n+    @Override\n+    public PCollection<KV<byte[], byte[]>> expand(PCollection<Row> input) {\n+      return input.apply(\n+          \"encodeProtoRecord\",\n+          ParDo.of(\n+              new DoFn<Row, KV<byte[], byte[]>>() {\n+                @ProcessElement\n+                public void processElement(ProcessContext c) {\n+                  Row in = c.element();\n+                  c.output(KV.of(new byte[] {}, encodeRowToProtoBytes(in)));\n+                }\n+              }));\n+    }\n+\n+    static byte[] encodeRowToProtoBytes(Row row) {\n+      RowCoder rowCoder = RowCoder.of(row.getSchema());\n+      ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+      try {\n+        rowCoder.encode(row, outputStream);", "originalCommit": "1ef3668c2b8285f1f5595d8de8267e6ff8b39af1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQwOTkyNQ==", "url": "https://github.com/apache/beam/pull/12838#discussion_r494409925", "bodyText": "Right, I've mistaken Row with RowProto. Thanks for the hints! I didn't know there is a universal coder for protos, I'll take a look.", "author": "piotr-szuberski", "createdAt": "2020-09-24T15:26:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MDgyNg=="}], "type": "inlineReview"}, {"oid": "3099c1d258455ceec966814e221409ad15acab50", "url": "https://github.com/apache/beam/commit/3099c1d258455ceec966814e221409ad15acab50", "message": "[BEAM-10892] Add Proto support to Kafka table provider", "committedDate": "2020-09-29T10:27:40Z", "type": "forcePushed"}, {"oid": "7b1ba6950c344d8cf942943859b790b1e6b85a76", "url": "https://github.com/apache/beam/commit/7b1ba6950c344d8cf942943859b790b1e6b85a76", "message": "fix codecheck", "committedDate": "2020-09-29T12:43:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4Njk4Mw==", "url": "https://github.com/apache/beam/pull/12838#discussion_r499086983", "bodyText": "We should verify early that beamSchema and the schema inferred from protoClass by ProtoSchemaRegistry are equivalent. In theory we could also allow the table definition to omit the schema in the case, since it can be determined fully from the protoClass.", "author": "TheNeuralBit", "createdAt": "2020-10-02T23:24:09Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import com.google.protobuf.Message;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.List;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoCoder;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+\n+public class BeamKafkaProtoTable<ProtoT extends Message> extends BeamKafkaTable {\n+  private final transient Class<ProtoT> protoClass;\n+\n+  public BeamKafkaProtoTable(\n+      Schema beamSchema, String bootstrapServers, List<String> topics, Class<ProtoT> protoClass) {\n+    super(beamSchema, bootstrapServers, topics);\n+    this.protoClass = protoClass;", "originalCommit": "7b1ba6950c344d8cf942943859b790b1e6b85a76", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjU1NTY0NA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r506555644", "bodyText": "Great idea. I removed beamSchema and infer the schema from the protoClass. It seems to work.", "author": "piotr-szuberski", "createdAt": "2020-10-16T15:39:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4Njk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU3MDk4MA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514570980", "bodyText": "Nice!\nI think we should still handle the situation where a schema is specified though, and raise an error if the schema inferred from the proto class is not equivalent to the one the user specified.", "author": "TheNeuralBit", "createdAt": "2020-10-29T21:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4Njk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkzMzA4NA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514933084", "bodyText": "Done. I also changed assignableTo to equivalent in toProtoFn. I've just learned that proto3 removed nulls.", "author": "piotr-szuberski", "createdAt": "2020-10-30T08:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4Njk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4ODA0Mg==", "url": "https://github.com/apache/beam/pull/12838#discussion_r499088042", "bodyText": "There's the possibility for some subtle errors here because SQL's concept of the schema could have the fields in a different order than the order in the protobuf descriptor, lets make sure to test that case, and consider adding a step to \"convert\" between the protobuf-generated schema and the SQL schema when encoding and decoding.", "author": "TheNeuralBit", "createdAt": "2020-10-02T23:29:43Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import com.google.protobuf.Message;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.List;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoCoder;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+\n+public class BeamKafkaProtoTable<ProtoT extends Message> extends BeamKafkaTable {\n+  private final transient Class<ProtoT> protoClass;\n+\n+  public BeamKafkaProtoTable(\n+      Schema beamSchema, String bootstrapServers, List<String> topics, Class<ProtoT> protoClass) {\n+    super(beamSchema, bootstrapServers, topics);\n+    this.protoClass = protoClass;\n+  }\n+\n+  @Override\n+  protected BeamKafkaTable getTable() {\n+    return this;\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    return new ProtoRecorderDecoder<>(schema, protoClass);\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> getPTransformForOutput() {\n+    return new ProtoRecorderEncoder<>(protoClass);\n+  }\n+\n+  /** A PTransform to convert {@code KV<byte[], byte[]>} to {@link Row}. */\n+  private static class ProtoRecorderDecoder<ProtoT extends Message>\n+      extends PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> {\n+    private final Schema schema;\n+    private final ProtoCoder<ProtoT> protoCoder;\n+    private final SerializableFunction<ProtoT, Row> toRowFunction;\n+\n+    ProtoRecorderDecoder(Schema schema, Class<ProtoT> clazz) {\n+      this.schema = schema;\n+      this.protoCoder = ProtoCoder.of(clazz);\n+      this.toRowFunction = new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+      return input\n+          .apply(\n+              \"decodeProtoRecord\",\n+              ParDo.of(\n+                  new DoFn<KV<byte[], byte[]>, Row>() {\n+                    @ProcessElement\n+                    public void processElement(ProcessContext c) {\n+                      Row decodedRow = decodeBytesToRow(c.element().getValue());\n+                      c.output(decodedRow);\n+                    }\n+                  }))\n+          .setRowSchema(schema);\n+    }\n+\n+    private Row decodeBytesToRow(byte[] bytes) {\n+      try {\n+        InputStream inputStream = new ByteArrayInputStream(bytes);\n+        ProtoT message = protoCoder.decode(inputStream);\n+        return toRowFunction.apply(message);\n+      } catch (IOException e) {\n+        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n+      }\n+    }\n+  }\n+\n+  /** A PTransform to convert {@link Row} to {@code KV<byte[], byte[]>}. */\n+  private static class ProtoRecorderEncoder<ProtoT extends Message>\n+      extends PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> {\n+    private final SerializableFunction<Row, ProtoT> toProtoFunction;\n+    private final ProtoCoder<ProtoT> protoCoder;\n+    private final Class<ProtoT> clazz;\n+\n+    public ProtoRecorderEncoder(Class<ProtoT> clazz) {\n+      this.protoCoder = ProtoCoder.of(clazz);\n+      this.toProtoFunction = new ProtoMessageSchema().fromRowFunction(TypeDescriptor.of(clazz));\n+      this.clazz = clazz;\n+    }\n+\n+    @Override\n+    public PCollection<KV<byte[], byte[]>> expand(PCollection<Row> input) {\n+      return input.apply(\n+          \"encodeProtoRecord\",\n+          ParDo.of(\n+              new DoFn<Row, KV<byte[], byte[]>>() {\n+                @ProcessElement\n+                public void processElement(ProcessContext c) {\n+                  Row row = c.element();\n+                  c.output(KV.of(new byte[] {}, encodeRowToProtoBytes(row)));\n+                }\n+              }));\n+    }\n+\n+    byte[] encodeRowToProtoBytes(Row row) {\n+      ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+      try {\n+        Message message = toProtoFunction.apply(row);", "originalCommit": "7b1ba6950c344d8cf942943859b790b1e6b85a76", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjU1Nzc4MA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r506557780", "bodyText": "Done.", "author": "piotr-szuberski", "createdAt": "2020-10-16T15:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA4ODA0Mg=="}], "type": "inlineReview"}, {"oid": "c32be70fef91e7c148a3d9da69858f3b2519a52b", "url": "https://github.com/apache/beam/commit/c32be70fef91e7c148a3d9da69858f3b2519a52b", "message": "Add schema equicity check", "committedDate": "2020-10-16T15:37:18Z", "type": "forcePushed"}, {"oid": "d4369433f21f0e51f1d8946fb50f2a01d8cd3657", "url": "https://github.com/apache/beam/commit/d4369433f21f0e51f1d8946fb50f2a01d8cd3657", "message": "Add comments to suppress warnings", "committedDate": "2020-10-27T09:09:05Z", "type": "forcePushed"}, {"oid": "6376eb8fd68b4bed0892bfcab693087ef3012da5", "url": "https://github.com/apache/beam/commit/6376eb8fd68b4bed0892bfcab693087ef3012da5", "message": "Add comments to suppress warnings", "committedDate": "2020-10-29T09:52:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU1NTE4NQ==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514555185", "bodyText": "Please generate the proto directly rather than using getRowToProtoBytesFn", "author": "TheNeuralBit", "createdAt": "2020-10-29T20:44:41Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderProtoIT.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;\n+import org.apache.beam.sdk.transforms.SimpleFunction;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+public class KafkaTableProviderProtoIT extends KafkaTableProviderIT {\n+  private final SimpleFunction<Row, byte[]> toBytesFn =\n+      ProtoMessageSchema.getRowToProtoBytesFn(KafkaMessages.ItMessage.class);\n+\n+  @Override\n+  protected ProducerRecord<String, byte[]> generateProducerRecord(int i) {\n+    return new ProducerRecord<>(\n+        kafkaOptions.getKafkaTopic(), \"k\" + i, toBytesFn.apply(generateRow(i)));", "originalCommit": "6376eb8fd68b4bed0892bfcab693087ef3012da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU1Njg3MQ==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514556871", "bodyText": "It looks like we do a similar thing in KafkaTableProviderAvroIT, that should also create the Avro direcly", "author": "TheNeuralBit", "createdAt": "2020-10-29T20:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU1NTE4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU2MjE5Mw==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514562193", "bodyText": "I think you can just serialize the message rather than relying on ProtoCoder, you should just need something like message.writeTo(out)", "author": "TheNeuralBit", "createdAt": "2020-10-29T20:57:49Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaTableProtoTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoCoder;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.Test;\n+\n+public class BeamKafkaTableProtoTest extends BeamKafkaTableTest {\n+  private static final ProtoCoder<KafkaMessages.TestMessage> PROTO_CODER =\n+      ProtoCoder.of(KafkaMessages.TestMessage.class);\n+\n+  private static final Schema TEST_SCHEMA =\n+      Schema.builder()\n+          .addNullableField(\"f_long\", Schema.FieldType.INT64)\n+          .addNullableField(\"f_int\", Schema.FieldType.INT32)\n+          .addNullableField(\"f_double\", Schema.FieldType.DOUBLE)\n+          .addNullableField(\"f_string\", Schema.FieldType.STRING)\n+          .addNullableField(\"f_float_array\", Schema.FieldType.array(Schema.FieldType.FLOAT))\n+          .build();\n+\n+  private static final Schema SHUFFLED_SCHEMA =\n+      Schema.builder()\n+          .addNullableField(\"f_string\", Schema.FieldType.STRING)\n+          .addNullableField(\"f_int\", Schema.FieldType.INT32)\n+          .addNullableField(\"f_float_array\", Schema.FieldType.array(Schema.FieldType.FLOAT))\n+          .addNullableField(\"f_double\", Schema.FieldType.DOUBLE)\n+          .addNullableField(\"f_long\", Schema.FieldType.INT64)\n+          .build();\n+\n+  @Test\n+  public void testWithShuffledSchema() {\n+    BeamKafkaTable kafkaTable = getBeamKafkaTable();\n+    PCollection<Row> result =\n+        pipeline\n+            .apply(Create.of(shuffledRow(1), shuffledRow(2)))\n+            .apply(kafkaTable.getPTransformForOutput())\n+            .apply(kafkaTable.getPTransformForInput());\n+    PAssert.that(result).containsInAnyOrder(generateRow(1), generateRow(2));\n+    pipeline.run();\n+  }\n+\n+  @Override\n+  protected BeamKafkaTable getBeamKafkaTable() {\n+    return new BeamKafkaProtoTable(\"\", ImmutableList.of(), KafkaMessages.TestMessage.class);\n+  }\n+\n+  @Override\n+  protected Row generateRow(int i) {\n+    List<Object> values =\n+        ImmutableList.of((long) i, i, (double) i, \"proto_value\" + i, ImmutableList.of((float) i));\n+    return Row.withSchema(TEST_SCHEMA).addValues(values).build();\n+  }\n+\n+  @Override\n+  protected byte[] generateEncodedPayload(int i) {\n+    KafkaMessages.TestMessage message =\n+        KafkaMessages.TestMessage.newBuilder()\n+            .setFLong(i)\n+            .setFInt(i)\n+            .setFDouble(i)\n+            .setFString(\"proto_value\" + i)\n+            .addFFloatArray((float) i)\n+            .build();\n+\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    try {\n+      PROTO_CODER.encode(message, out);", "originalCommit": "6376eb8fd68b4bed0892bfcab693087ef3012da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkzNTk2OA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r514935968", "bodyText": "I needed to use writeDelimitedTo(out). Done.", "author": "piotr-szuberski", "createdAt": "2020-10-30T08:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU2MjE5Mw=="}], "type": "inlineReview"}, {"oid": "af3b902ca0536bad209da5b13f64783c0c00dc46", "url": "https://github.com/apache/beam/commit/af3b902ca0536bad209da5b13f64783c0c00dc46", "message": "[BEAM-10892] Add Proto support to Kafka table provider", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "289dcc27207aaab64ee256704a57be1af5cf6395", "url": "https://github.com/apache/beam/commit/289dcc27207aaab64ee256704a57be1af5cf6395", "message": "Move parsing bytes to row etc to extensions/protobuf", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "086da70f7215f888f0995410876d94fe18c2710e", "url": "https://github.com/apache/beam/commit/086da70f7215f888f0995410876d94fe18c2710e", "message": "Remove unused field in BeamKafkaTableAvroTest", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "6a87582be01472825be6f4bec32d35273f80c741", "url": "https://github.com/apache/beam/commit/6a87582be01472825be6f4bec32d35273f80c741", "message": "Fix tests", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "4efd73098430ae2d92cf740136210b351764396c", "url": "https://github.com/apache/beam/commit/4efd73098430ae2d92cf740136210b351764396c", "message": "run spotless", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "e03fd95d7e036c9b68fbe066ecf2a49aedf4c2ac", "url": "https://github.com/apache/beam/commit/e03fd95d7e036c9b68fbe066ecf2a49aedf4c2ac", "message": "Add schema equicity check", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "da1472f7e9e3ece834cb1947a2a001deca092315", "url": "https://github.com/apache/beam/commit/da1472f7e9e3ece834cb1947a2a001deca092315", "message": "Add unit tests for shuffled rows conversion", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "0b841c72f3cfef70963befd4a81d97893ac0e001", "url": "https://github.com/apache/beam/commit/0b841c72f3cfef70963befd4a81d97893ac0e001", "message": "Remove google.protobuf.Message from extensions/sql", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "52c367cfef6b3f480a220cc346bb6ea799a00233", "url": "https://github.com/apache/beam/commit/52c367cfef6b3f480a220cc346bb6ea799a00233", "message": "remove redundant transient", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "189a246611469b5ab06de61b4c9b08dfab86ea57", "url": "https://github.com/apache/beam/commit/189a246611469b5ab06de61b4c9b08dfab86ea57", "message": "Add comments to suppress warnings", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "971cb1c7dbc698871ce6036f81c9298a157d4e9d", "url": "https://github.com/apache/beam/commit/971cb1c7dbc698871ce6036f81c9298a157d4e9d", "message": "Remove coder from tests", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "b63b11484e960527a6d48f6a19ea8dc9cd2976a9", "url": "https://github.com/apache/beam/commit/b63b11484e960527a6d48f6a19ea8dc9cd2976a9", "message": "assignableTo -> equivalent", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "c38b4c18ed019ee537c265b1a4ebe72e3de5af4a", "url": "https://github.com/apache/beam/commit/c38b4c18ed019ee537c265b1a4ebe72e3de5af4a", "message": "Add message schema verification", "committedDate": "2020-10-30T08:26:04Z", "type": "commit"}, {"oid": "2630f99018781acb2ee9481fe97015a2353d7750", "url": "https://github.com/apache/beam/commit/2630f99018781acb2ee9481fe97015a2353d7750", "message": "Add test that verifies not equal schemas", "committedDate": "2020-10-30T08:49:33Z", "type": "forcePushed"}, {"oid": "4f914104ea6539522345cb08d0b514cbd3e40b50", "url": "https://github.com/apache/beam/commit/4f914104ea6539522345cb08d0b514cbd3e40b50", "message": "Fix failing test", "committedDate": "2020-10-30T10:46:25Z", "type": "forcePushed"}, {"oid": "79ec7e4fcdf34f643200bea8eba0ba30f23b60d1", "url": "https://github.com/apache/beam/commit/79ec7e4fcdf34f643200bea8eba0ba30f23b60d1", "message": "Replace nullable fields with non-nullable fields in IT", "committedDate": "2020-10-30T11:32:30Z", "type": "forcePushed"}, {"oid": "47611702985e927137f2e357b6ee1ac64f40fcfc", "url": "https://github.com/apache/beam/commit/47611702985e927137f2e357b6ee1ac64f40fcfc", "message": "Increase sleep time to reduce KafkaTableProviderIT flakiness", "committedDate": "2020-10-30T18:28:37Z", "type": "forcePushed"}, {"oid": "d8511e9fdec25fc5b05e7cabe11bf7b8e2b7a351", "url": "https://github.com/apache/beam/commit/d8511e9fdec25fc5b05e7cabe11bf7b8e2b7a351", "message": "Update changes and website", "committedDate": "2020-10-30T20:02:55Z", "type": "commit"}, {"oid": "a2c195a371ceeac542db4425ee78fa40ce26a216", "url": "https://github.com/apache/beam/commit/a2c195a371ceeac542db4425ee78fa40ce26a216", "message": "Add test that verifies not equal schemas", "committedDate": "2020-10-30T20:03:21Z", "type": "commit"}, {"oid": "e71eeac5b4439f8eca9dae69c434248801b737d6", "url": "https://github.com/apache/beam/commit/e71eeac5b4439f8eca9dae69c434248801b737d6", "message": "Add mockTable overloads and proto for KafkaTableProviderTest", "committedDate": "2020-10-30T20:03:35Z", "type": "commit"}, {"oid": "bff9b1aa54cab6ad2a9fdf4618f3cc1572fa7ac7", "url": "https://github.com/apache/beam/commit/bff9b1aa54cab6ad2a9fdf4618f3cc1572fa7ac7", "message": "Make table fields NOT NULL in KafkaTableProviderIT", "committedDate": "2020-10-30T20:04:10Z", "type": "commit"}, {"oid": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "url": "https://github.com/apache/beam/commit/5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "message": "Run KafkaTableProviderIT in a separate task with 2 threads", "committedDate": "2020-10-30T20:05:49Z", "type": "commit"}, {"oid": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "url": "https://github.com/apache/beam/commit/5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "message": "Run KafkaTableProviderIT in a separate task with 2 threads", "committedDate": "2020-10-30T20:05:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNDkwNg==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515414906", "bodyText": "Hm we shouldn't actually need writeDelimitedTo.. it adds a varint prefix with the message size. That shouldn't be necessary for a Kafka message, since consumers can just assume the entire payload is the serialized message and read to the end.", "author": "TheNeuralBit", "createdAt": "2020-10-30T22:52:23Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaTableProtoTest.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.Test;\n+\n+public class BeamKafkaTableProtoTest extends BeamKafkaTableTest {\n+\n+  private static final Schema TEST_SCHEMA =\n+      Schema.builder()\n+          .addInt64Field(\"f_long\")\n+          .addInt32Field(\"f_int\")\n+          .addDoubleField(\"f_double\")\n+          .addStringField(\"f_string\")\n+          .addArrayField(\"f_float_array\", Schema.FieldType.FLOAT)\n+          .build();\n+\n+  private static final Schema SHUFFLED_SCHEMA =\n+      Schema.builder()\n+          .addStringField(\"f_string\")\n+          .addInt32Field(\"f_int\")\n+          .addArrayField(\"f_float_array\", Schema.FieldType.FLOAT)\n+          .addDoubleField(\"f_double\")\n+          .addInt64Field(\"f_long\")\n+          .build();\n+\n+  @Test\n+  public void testWithShuffledSchema() {\n+    BeamKafkaTable kafkaTable =\n+        new BeamKafkaProtoTable(\n+            SHUFFLED_SCHEMA, \"\", ImmutableList.of(), KafkaMessages.TestMessage.class);\n+\n+    PCollection<Row> result =\n+        pipeline\n+            .apply(Create.of(shuffledRow(1), shuffledRow(2)))\n+            .apply(kafkaTable.getPTransformForOutput())\n+            .apply(kafkaTable.getPTransformForInput());\n+    PAssert.that(result).containsInAnyOrder(generateRow(1), generateRow(2));\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testSchemasDoNotMatch() {\n+    Schema schema = Schema.builder().addStringField(\"non_existing_field\").build();\n+\n+    IllegalArgumentException e =\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () ->\n+                new BeamKafkaProtoTable(\n+                    schema, \"\", ImmutableList.of(), KafkaMessages.TestMessage.class));\n+\n+    assertThat(\n+        e.getMessage(),\n+        containsString(\"does not match schema inferred from protobuf class. Protobuf class: \"));\n+  }\n+\n+  @Override\n+  protected BeamKafkaTable getBeamKafkaTable() {\n+    return new BeamKafkaProtoTable(\n+        TEST_SCHEMA, \"\", ImmutableList.of(), KafkaMessages.TestMessage.class);\n+  }\n+\n+  @Override\n+  protected Row generateRow(int i) {\n+    List<Object> values =\n+        ImmutableList.of((long) i, i, (double) i, \"proto_value\" + i, ImmutableList.of((float) i));\n+    return Row.withSchema(TEST_SCHEMA).addValues(values).build();\n+  }\n+\n+  @Override\n+  protected byte[] generateEncodedPayload(int i) throws IOException {\n+    KafkaMessages.TestMessage message =\n+        KafkaMessages.TestMessage.newBuilder()\n+            .setFLong(i)\n+            .setFInt(i)\n+            .setFDouble(i)\n+            .setFString(\"proto_value\" + i)\n+            .addFFloatArray((float) i)\n+            .build();\n+\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    message.writeDelimitedTo(out);", "originalCommit": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg3MzI2NA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515873264", "bodyText": "I thought that proto coder uses what is preferable, thanks for explanation! Done.", "author": "piotr-szuberski", "createdAt": "2020-11-02T10:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNDkwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNjA4Nw==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515416087", "bodyText": "I think this is why you had to use writeDelimitedTo, ProtoCoder uses the \"nested\" context by default which requires the length prefix:\n\n  \n    \n      beam/sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoCoder.java\n    \n    \n        Lines 179 to 181\n      in\n      168d442\n    \n    \n    \n    \n\n        \n          \n           public void encode(T value, OutputStream outStream) throws IOException { \n        \n\n        \n          \n             encode(value, outStream, Context.NESTED); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nRowToProtoBytesFn and ProtoBytesToRowFn should just use the protobuf methods parseFrom and writeToinstead of relying on ProtoCoder", "author": "TheNeuralBit", "createdAt": "2020-10-30T22:57:32Z", "path": "sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java", "diffHunk": "@@ -115,6 +122,76 @@ public SchemaUserTypeCreator schemaTypeCreator(Class<?> targetClass, Schema sche\n     return creator;\n   }\n \n+  public static <T extends Message> SimpleFunction<byte[], Row> getProtoBytesToRowFn(\n+      Class<T> clazz) {\n+    return new ProtoBytesToRowFn<>(clazz);\n+  }\n+\n+  public static class ProtoBytesToRowFn<T extends Message> extends SimpleFunction<byte[], Row> {\n+    private final ProtoCoder<T> protoCoder;\n+    private final SerializableFunction<T, Row> toRowFunction;\n+\n+    public ProtoBytesToRowFn(Class<T> clazz) {\n+      this.protoCoder = ProtoCoder.of(clazz);\n+      this.toRowFunction = new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n+    }\n+\n+    @Override\n+    public Row apply(byte[] bytes) {\n+      try {\n+        InputStream inputStream = new ByteArrayInputStream(bytes);\n+        T message = protoCoder.decode(inputStream);\n+        return toRowFunction.apply(message);\n+      } catch (IOException e) {\n+        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n+      }\n+    }\n+  }\n+\n+  public static <T extends Message> SimpleFunction<Row, byte[]> getRowToProtoBytesFn(\n+      Class<T> clazz) {\n+    return new RowToProtoBytesFn<>(clazz);\n+  }\n+\n+  public static class RowToProtoBytesFn<T extends Message> extends SimpleFunction<Row, byte[]> {\n+    private final ProtoCoder<T> protoCoder;\n+    private final SerializableFunction<Row, T> toMessageFunction;\n+    private final Class<T> clazz;\n+    private final Schema protoSchema;\n+\n+    public RowToProtoBytesFn(Class<T> clazz) {\n+      ProtoMessageSchema messageSchema = new ProtoMessageSchema();\n+      TypeDescriptor<T> typeDescriptor = TypeDescriptor.of(clazz);\n+      this.clazz = clazz;\n+      this.protoCoder = ProtoCoder.of(typeDescriptor);\n+      this.toMessageFunction = messageSchema.fromRowFunction(typeDescriptor);\n+      this.protoSchema = messageSchema.schemaFor(typeDescriptor);\n+    }\n+\n+    @Override\n+    public byte[] apply(Row row) {\n+      if (!protoSchema.equivalent(row.getSchema())) {\n+        row = switchFieldsOrder(row);\n+      }\n+      ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+      try {\n+        Message message = toMessageFunction.apply(row);\n+        protoCoder.encode(clazz.cast(message), outputStream);", "originalCommit": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg3NjE3MA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515876170", "bodyText": "Done.", "author": "piotr-szuberski", "createdAt": "2020-11-02T10:28:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNjA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxOTY5Nw==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515419697", "bodyText": "I don't think these are necessary any more are they?", "author": "TheNeuralBit", "createdAt": "2020-10-30T23:14:00Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.util.List;\n+import org.apache.beam.sdk.extensions.protobuf.ProtoMessageSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.SimpleFunction;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+\n+public class BeamKafkaProtoTable extends BeamKafkaTable {\n+  private final Class<?> protoClass;\n+\n+  public BeamKafkaProtoTable(\n+      Schema messageSchema, String bootstrapServers, List<String> topics, Class<?> protoClass) {\n+    super(inferAndVerifySchema(protoClass, messageSchema), bootstrapServers, topics);\n+    this.protoClass = protoClass;\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    return new ProtoRecorderDecoder(schema, protoClass);\n+  }\n+\n+  @Override\n+  public PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> getPTransformForOutput() {\n+    return new ProtoRecorderEncoder(protoClass);\n+  }\n+\n+  private static Schema inferAndVerifySchema(Class<?> protoClass, Schema messageSchema) {\n+    Schema inferredSchema = new ProtoMessageSchema().schemaFor(TypeDescriptor.of(protoClass));\n+    if (!messageSchema.equivalent(inferredSchema)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Given message schema '%s' does not match schema inferred from protobuf class. Protobuf class: '%s' Inferred schema: '%s'\",\n+              messageSchema, protoClass.getCanonicalName(), inferredSchema));\n+    }\n+    return inferredSchema;\n+  }\n+\n+  /** A PTransform to convert {@code KV<byte[], byte[]>} to {@link Row}. */\n+  private static class ProtoRecorderDecoder\n+      extends PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> {\n+    private final Schema schema;\n+    private final Class<?> clazz;\n+\n+    ProtoRecorderDecoder(Schema schema, Class<?> clazz) {\n+      this.schema = schema;\n+      this.clazz = clazz;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+      // We are not allowed to use non-vendored protobuf Message here to extend the wildcard\n+      @SuppressWarnings({\"unchecked\", \"rawtypes\"})", "originalCommit": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg5MjcyNw==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515892727", "bodyText": "Unfortunately they are. I haven't found a solution to get rid of that. I moved the class check to ProtoMessageSchema - more below.", "author": "piotr-szuberski", "createdAt": "2020-11-02T10:57:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxOTY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMzIxMA==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515423210", "bodyText": "If possible could you make these private and just have the get*Fn methods as the public API?", "author": "TheNeuralBit", "createdAt": "2020-10-30T23:31:22Z", "path": "sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java", "diffHunk": "@@ -115,6 +122,76 @@ public SchemaUserTypeCreator schemaTypeCreator(Class<?> targetClass, Schema sche\n     return creator;\n   }\n \n+  public static <T extends Message> SimpleFunction<byte[], Row> getProtoBytesToRowFn(\n+      Class<T> clazz) {\n+    return new ProtoBytesToRowFn<>(clazz);\n+  }\n+\n+  public static class ProtoBytesToRowFn<T extends Message> extends SimpleFunction<byte[], Row> {\n+    private final ProtoCoder<T> protoCoder;\n+    private final SerializableFunction<T, Row> toRowFunction;\n+\n+    public ProtoBytesToRowFn(Class<T> clazz) {\n+      this.protoCoder = ProtoCoder.of(clazz);\n+      this.toRowFunction = new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n+    }\n+\n+    @Override\n+    public Row apply(byte[] bytes) {\n+      try {\n+        InputStream inputStream = new ByteArrayInputStream(bytes);\n+        T message = protoCoder.decode(inputStream);\n+        return toRowFunction.apply(message);\n+      } catch (IOException e) {\n+        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n+      }\n+    }\n+  }\n+\n+  public static <T extends Message> SimpleFunction<Row, byte[]> getRowToProtoBytesFn(\n+      Class<T> clazz) {\n+    return new RowToProtoBytesFn<>(clazz);\n+  }\n+\n+  public static class RowToProtoBytesFn<T extends Message> extends SimpleFunction<Row, byte[]> {", "originalCommit": "5d01a50c3984c44480628b5d79ccf5d0b7cff65d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTg4ODI2MQ==", "url": "https://github.com/apache/beam/pull/12838#discussion_r515888261", "bodyText": "The get*Fn wants to receive Class which cannot be provided from BeamKafkaProtoTable as it's not allowed to import from com.google.protobuf. I'll move the suppresion to ProtoMessageSchema and add a validation if the provided class extends Message.\nIf you know a cleaner solution then I'll gladly change it.", "author": "piotr-szuberski", "createdAt": "2020-11-02T10:49:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMzIxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4OTU3NQ==", "url": "https://github.com/apache/beam/pull/12838#discussion_r516389575", "bodyText": "Thanks! I think this approach is best for now. In the future I think we should try to isolate all the format specific logic in a module for just that format, and just have general purpose logic in SQL. This is closer to that vision imo.", "author": "TheNeuralBit", "createdAt": "2020-11-03T01:54:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMzIxMA=="}], "type": "inlineReview"}, {"oid": "bac7681d1d2370f0a2636578e735cb9ed4ce548e", "url": "https://github.com/apache/beam/commit/bac7681d1d2370f0a2636578e735cb9ed4ce548e", "message": "Change writeDelimitedTo to writeTo", "committedDate": "2020-11-02T11:15:02Z", "type": "commit"}, {"oid": "866bb8c24a1bc413a3109271583eeed3bb39b652", "url": "https://github.com/apache/beam/commit/866bb8c24a1bc413a3109271583eeed3bb39b652", "message": "Make the function classes private and add class validation", "committedDate": "2020-11-02T11:15:41Z", "type": "commit"}, {"oid": "c03f859c1b66fb0f6064c1671174b07b425a3287", "url": "https://github.com/apache/beam/commit/c03f859c1b66fb0f6064c1671174b07b425a3287", "message": "improve error message", "committedDate": "2020-11-02T11:18:00Z", "type": "commit"}, {"oid": "c03f859c1b66fb0f6064c1671174b07b425a3287", "url": "https://github.com/apache/beam/commit/c03f859c1b66fb0f6064c1671174b07b425a3287", "message": "improve error message", "committedDate": "2020-11-02T11:18:00Z", "type": "forcePushed"}, {"oid": "722fa2a4a095eecfca6081079b957a3ddc1dba59", "url": "https://github.com/apache/beam/commit/722fa2a4a095eecfca6081079b957a3ddc1dba59", "message": "Fix spotbugs", "committedDate": "2020-11-02T11:30:43Z", "type": "commit"}]}