{"pr_number": 11055, "pr_title": "[BEAM-9436] Improve GBK in spark structured streaming runner", "pr_createdAt": "2020-03-05T15:36:40Z", "pr_url": "https://github.com/apache/beam/pull/11055", "timeline": [{"oid": "17eb9304e32da04c4f9209a8a8700236cb39c78c", "url": "https://github.com/apache/beam/commit/17eb9304e32da04c4f9209a8a8700236cb39c78c", "message": "[BEAM-9436] Add GBK load tests spark structured streaming", "committedDate": "2020-03-06T09:51:50Z", "type": "forcePushed"}, {"oid": "ceca95b9ef1d3139d64feeb24e8937eabd2ba3f9", "url": "https://github.com/apache/beam/commit/ceca95b9ef1d3139d64feeb24e8937eabd2ba3f9", "message": "[BEAM-9436] Add GBK load tests spark structured streaming", "committedDate": "2020-03-09T09:09:03Z", "type": "forcePushed"}, {"oid": "e79959c8a1851c059ee24b8350edaf971bbfcebb", "url": "https://github.com/apache/beam/commit/e79959c8a1851c059ee24b8350edaf971bbfcebb", "message": "Add spark structured streaming to smoke load tests", "committedDate": "2020-03-11T09:50:29Z", "type": "forcePushed"}, {"oid": "395c8a5a2f6977914f263344d42d9bf77136c8ba", "url": "https://github.com/apache/beam/commit/395c8a5a2f6977914f263344d42d9bf77136c8ba", "message": "[BEAM-9436] avoid one flatmap step and a KV creation per element by doing the (mandatory for ReducefnRunner) materialization when grouping by windows.", "committedDate": "2020-03-16T07:50:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg==", "url": "https://github.com/apache/beam/pull/11055#discussion_r396387682", "bodyText": "I think we still can have potential OOM here, right?", "author": "aromanenko-dev", "createdAt": "2020-03-23T11:38:17Z", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/functions/GroupAlsoByWindowViaOutputBufferFn.java", "diffHunk": "@@ -65,9 +65,15 @@ public GroupAlsoByWindowViaOutputBufferFn(\n \n   @Override\n   public Iterator<WindowedValue<KV<K, Iterable<InputT>>>> call(\n-      KV<K, Iterable<WindowedValue<InputT>>> kv) throws Exception {\n-    K key = kv.getKey();\n-    Iterable<WindowedValue<InputT>> values = kv.getValue();\n+      K key, Iterator<WindowedValue<KV<K, InputT>>> iterator) throws Exception {\n+\n+    // we have to meterialize the Iterator because ReduceFnRunner.processElements expects\n+    // ArrayList<WindowedValue<InputT>> and not Iterator<WindowedValue<KV<K, InputT>>>\n+    ArrayList<WindowedValue<InputT>> values = new ArrayList<>();\n+    while (iterator.hasNext()) {\n+      WindowedValue<KV<K, InputT>> wv = iterator.next();\n+      values.add(wv.withValue(wv.getValue().getValue()));", "originalCommit": "395c8a5a2f6977914f263344d42d9bf77136c8ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ5MTQ2Nw==", "url": "https://github.com/apache/beam/pull/11055#discussion_r396491467", "bodyText": "Yes this comment in the doc does seem to confirm this ...users must take care to avoid materializing the whole iterator for a group (for example, by calling toList) unless they are sure that this is possible given the memory constraints of their cluster. this looks like doing exactly the same than I would expect the toList call to do.\n\nSee this comment #11055 (comment)", "author": "iemejia", "createdAt": "2020-03-23T14:27:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwNzMxMg==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397207312", "bodyText": "in previous impl materialization to list was already there\nI know about the javadoc but as stated in my comment in the code, it is unavoidable due to the reduceFnRunner needing a list as input. See also my other review comment about details.\nAnyway I measured during the load test (see results above) in a tiny JVM and I got no OOM, I only got spill to disc of GB of data", "author": "echauchot", "createdAt": "2020-03-24T14:43:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI1NTYyMw==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397255623", "bodyText": "Well, I think it depends on values distribution over keys. OOM can happened here on hot keys with many or/and large values. You can try to avoid it using Iterators.transform() and creating Iterable from this (should be checked if it will work).", "author": "aromanenko-dev", "createdAt": "2020-03-24T15:44:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzcxNjU3NA==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397716574", "bodyText": "Yes sure, I agree about hot keys. But if spark spills to disk it is because it is out of memory. And this PR does not claim to fix the OOM issue that also existed in the previous version of GBK but it claims to remove a step in the translation and avoid memory consumption. I could profile the load test to give you numbers on memory because it is not integrated in load tests\nAlso see my comment #11055 (comment) regarding materialization.\nAnyway there is hotkey configuration in GBKLoadTest. I'll configure it and post the results", "author": "echauchot", "createdAt": "2020-03-25T09:34:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODUwMTgyMw==", "url": "https://github.com/apache/beam/pull/11055#discussion_r398501823", "bodyText": "I just ran this test: -Xms8g -Xmx8g -Prunner=\":runners:spark\" -PloadTest.mainClass=\"org.apache.beam.sdk.loadtests.GroupByKeyLoadTest\" -PloadTest.args=\"--fanout=1 --iterations=1 --streaming=false --runner=SparkStructuredStreamingRunner --sourceOptions={\\\"numRecords\\\":200000,\\\"keySizeBytes\\\":100000,\\\"valueSizeBytes\\\":10000, \\\"hotKeyFraction\\\":1.0, \\\"numHotKeys\\\":1}\" So one sole key that receives all the values in a 8GO JVM to simulate an OOM.\nAnd I observe a spill to disk and no out of memory as I said:\nbefore run /dev/mapper/ubuntu--vg-root   368G     23G  327G   7% /\nafter run  /dev/mapper/ubuntu--vg-root   368G     27G  323G   8% /\nSo +4G on disk in /tmp\nruntime_sec                   680.092", "author": "echauchot", "createdAt": "2020-03-26T11:31:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4NzY4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4ODA2OQ==", "url": "https://github.com/apache/beam/pull/11055#discussion_r396388069", "bodyText": "typo: \"meterialize\"", "author": "aromanenko-dev", "createdAt": "2020-03-23T11:39:01Z", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/functions/GroupAlsoByWindowViaOutputBufferFn.java", "diffHunk": "@@ -65,9 +65,15 @@ public GroupAlsoByWindowViaOutputBufferFn(\n \n   @Override\n   public Iterator<WindowedValue<KV<K, Iterable<InputT>>>> call(\n-      KV<K, Iterable<WindowedValue<InputT>>> kv) throws Exception {\n-    K key = kv.getKey();\n-    Iterable<WindowedValue<InputT>> values = kv.getValue();\n+      K key, Iterator<WindowedValue<KV<K, InputT>>> iterator) throws Exception {\n+\n+    // we have to meterialize the Iterator because ReduceFnRunner.processElements expects", "originalCommit": "395c8a5a2f6977914f263344d42d9bf77136c8ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIxMDU0Ng==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397210546", "bodyText": "+1", "author": "echauchot", "createdAt": "2020-03-24T14:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM4ODA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4ODkzMQ==", "url": "https://github.com/apache/beam/pull/11055#discussion_r396488931", "bodyText": "It expects Iterable not ArrayList. It looks like doing the refactoring in ReduceFnRunner#processElements to support Iterator instead could be a nice fix and avoid this extra materialization for the Spark runner (maybe worth to dig deeper there in a different issue/JIRA).", "author": "iemejia", "createdAt": "2020-03-23T14:24:22Z", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/functions/GroupAlsoByWindowViaOutputBufferFn.java", "diffHunk": "@@ -65,9 +65,15 @@ public GroupAlsoByWindowViaOutputBufferFn(\n \n   @Override\n   public Iterator<WindowedValue<KV<K, Iterable<InputT>>>> call(\n-      KV<K, Iterable<WindowedValue<InputT>>> kv) throws Exception {\n-    K key = kv.getKey();\n-    Iterable<WindowedValue<InputT>> values = kv.getValue();\n+      K key, Iterator<WindowedValue<KV<K, InputT>>> iterator) throws Exception {\n+\n+    // we have to meterialize the Iterator because ReduceFnRunner.processElements expects\n+    // ArrayList<WindowedValue<InputT>> and not Iterator<WindowedValue<KV<K, InputT>>>", "originalCommit": "395c8a5a2f6977914f263344d42d9bf77136c8ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI0Njg0Ng==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397246846", "bodyText": "I have already taken a look. Each ReduceFnRunner instance works in parallel in the parallel groups of the flatmapGroups and works on a materialization of the elements of the group. See among other things ReduceFnRunner line 326: reduceFnRunner collects all the windows to merge them. Changing the signature of the processElement method will not change the need for materialization. The important is to avoid materializing the whole collection in one place (which we do not do). We materialize only the part of the collection for each group.\nAnyway my comment in the code about materialization is misleading I agree and I'll change it", "author": "echauchot", "createdAt": "2020-03-24T15:33:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4ODkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ5NDIwNw==", "url": "https://github.com/apache/beam/pull/11055#discussion_r396494207", "bodyText": "I am not familiar with this function but the documentation explicitly says ...as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key, it is best to use the reduce function or an org.apache.spark.sql.expressions#Aggregator. It is probably a good idea that we test/ensure somehow that GbK + flatMapGroups do not end up producing a double shuffle otherwise the improvement would become a regression.", "author": "iemejia", "createdAt": "2020-03-23T14:31:39Z", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/GroupByKeyTranslatorBatch.java", "diffHunk": "@@ -53,50 +49,23 @@ public void translateTransform(\n \n     @SuppressWarnings(\"unchecked\")\n     final PCollection<KV<K, V>> inputPCollection = (PCollection<KV<K, V>>) context.getInput();\n-\n     Dataset<WindowedValue<KV<K, V>>> input = context.getDataset(inputPCollection);\n-\n     WindowingStrategy<?, ?> windowingStrategy = inputPCollection.getWindowingStrategy();\n     KvCoder<K, V> kvCoder = (KvCoder<K, V>) inputPCollection.getCoder();\n+    Coder<V> valueCoder = kvCoder.getValueCoder();\n \n     // group by key only\n     Coder<K> keyCoder = kvCoder.getKeyCoder();\n     KeyValueGroupedDataset<K, WindowedValue<KV<K, V>>> groupByKeyOnly =\n         input.groupByKey(KVHelpers.extractKey(), EncoderHelpers.fromBeamCoder(keyCoder));\n \n-    // Materialize groupByKeyOnly values, potential OOM because of creation of new iterable\n-    Coder<V> valueCoder = kvCoder.getValueCoder();\n-    WindowedValue.WindowedValueCoder<V> wvCoder =\n-        WindowedValue.FullWindowedValueCoder.of(\n-            valueCoder, inputPCollection.getWindowingStrategy().getWindowFn().windowCoder());\n-    IterableCoder<WindowedValue<V>> iterableCoder = IterableCoder.of(wvCoder);\n-    Dataset<KV<K, Iterable<WindowedValue<V>>>> materialized =\n-        groupByKeyOnly.mapGroups(\n-            (MapGroupsFunction<K, WindowedValue<KV<K, V>>, KV<K, Iterable<WindowedValue<V>>>>)\n-                (key, iterator) -> {\n-                  List<WindowedValue<V>> values = new ArrayList<>();\n-                  while (iterator.hasNext()) {\n-                    WindowedValue<KV<K, V>> next = iterator.next();\n-                    values.add(\n-                        WindowedValue.of(\n-                            next.getValue().getValue(),\n-                            next.getTimestamp(),\n-                            next.getWindows(),\n-                            next.getPane()));\n-                  }\n-                  KV<K, Iterable<WindowedValue<V>>> kv =\n-                      KV.of(key, Iterables.unmodifiableIterable(values));\n-                  return kv;\n-                },\n-            EncoderHelpers.fromBeamCoder(KvCoder.of(keyCoder, iterableCoder)));\n-\n     // group also by windows\n     WindowedValue.FullWindowedValueCoder<KV<K, Iterable<V>>> outputCoder =\n         WindowedValue.FullWindowedValueCoder.of(\n             KvCoder.of(keyCoder, IterableCoder.of(valueCoder)),\n             windowingStrategy.getWindowFn().windowCoder());\n     Dataset<WindowedValue<KV<K, Iterable<V>>>> output =\n-        materialized.flatMap(\n+        groupByKeyOnly.flatMapGroups(", "originalCommit": "395c8a5a2f6977914f263344d42d9bf77136c8ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzMDgyOQ==", "url": "https://github.com/apache/beam/pull/11055#discussion_r397230829", "bodyText": "GBK will always trigger a shuffle. dataset.gbk is a logical operation (see catalyst). One needs to take at the physical plans. In the previous version the physical exec plan showed that the shuffle occurred before the mapgroups. Now it occurres before the flatmapGroups. The 30% gain (see numbers) of this change resides in the fact that we previously had a mapgroups + flatmap and now we have only one flatmapgroups + we no more have to instanciate KV pairs. Take a look at the phusical plans if you need to be convinced", "author": "echauchot", "createdAt": "2020-03-24T15:13:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ5NDIwNw=="}], "type": "inlineReview"}, {"oid": "136c29b3a375fb900af7adac2518662e6e2a9e52", "url": "https://github.com/apache/beam/commit/136c29b3a375fb900af7adac2518662e6e2a9e52", "message": "[BEAM-9436] avoid one flatmap step and a KV creation per element by doing the (mandatory for ReducefnRunner) materialization when grouping by windows.", "committedDate": "2020-03-26T11:05:04Z", "type": "forcePushed"}, {"oid": "b7a02b58afc08a9f99d6007b5a1be5dcdaddde78", "url": "https://github.com/apache/beam/commit/b7a02b58afc08a9f99d6007b5a1be5dcdaddde78", "message": "Update comment on OOM in list materialization", "committedDate": "2020-03-27T14:58:20Z", "type": "forcePushed"}, {"oid": "0859001c820900de400f11a9150c8a2a1b3f6f48", "url": "https://github.com/apache/beam/commit/0859001c820900de400f11a9150c8a2a1b3f6f48", "message": "[BEAM-5980] Change load-tests build to include spark-sql for spark structured streaming runner", "committedDate": "2020-03-30T08:10:19Z", "type": "commit"}, {"oid": "7356a471e1be486aec819ac00216b2f80dd718f2", "url": "https://github.com/apache/beam/commit/7356a471e1be486aec819ac00216b2f80dd718f2", "message": "[BEAM-9436] avoid one flatmap step and a KV creation per element by doing the (mandatory for ReducefnRunner) materialization when grouping by windows.", "committedDate": "2020-03-30T08:10:19Z", "type": "commit"}, {"oid": "7356a471e1be486aec819ac00216b2f80dd718f2", "url": "https://github.com/apache/beam/commit/7356a471e1be486aec819ac00216b2f80dd718f2", "message": "[BEAM-9436] avoid one flatmap step and a KV creation per element by doing the (mandatory for ReducefnRunner) materialization when grouping by windows.", "committedDate": "2020-03-30T08:10:19Z", "type": "forcePushed"}]}