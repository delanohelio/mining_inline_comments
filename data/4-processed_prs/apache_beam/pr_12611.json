{"pr_number": 12611, "pr_title": "[BEAM-10139][BEAM-10140] Add cross-language support for Java SpannerIO with python wrapper", "pr_createdAt": "2020-08-18T14:00:40Z", "pr_url": "https://github.com/apache/beam/pull/12611", "timeline": [{"oid": "e1d90016a73cb983f18a0616dca7ab989d890fa3", "url": "https://github.com/apache/beam/commit/e1d90016a73cb983f18a0616dca7ab989d890fa3", "message": "[BEAM-10131][BEAM-10140] Add spanner IT tests to Python Portable Postcommit", "committedDate": "2020-08-20T13:03:45Z", "type": "forcePushed"}, {"oid": "2839b8e0f1bc27c0914db9bce3d0180e397495a7", "url": "https://github.com/apache/beam/commit/2839b8e0f1bc27c0914db9bce3d0180e397495a7", "message": "[BEAM-10139][BEAM-10140] Add spanner IT tests to Python Portable Postcommit", "committedDate": "2020-08-20T13:12:18Z", "type": "forcePushed"}, {"oid": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "url": "https://github.com/apache/beam/commit/fed767a2904c0c8ac4f1ae18d5461367beb985ab", "message": "[BEAM-10139][BEAM-10140] Add spanner IT tests to Python Portable Postcommit", "committedDate": "2020-08-31T07:20:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNDMwMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r477614300", "bodyText": "What is the issue here? Nullable fields should be supported in cross-language", "author": "TheNeuralBit", "createdAt": "2020-08-26T22:01:42Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,545 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+\n+final class StructUtils {\n+  public static Row translateStructToRow(Struct struct, Schema schema) {\n+    checkForSchemasEquality(schema.getFields(), struct.getType().getStructFields(), false);\n+\n+    List<Schema.Field> fields = schema.getFields();\n+    Row.FieldValueBuilder valueBuilder = null;\n+    // TODO: Remove this null-checking once nullable fields are supported in cross-language", "originalCommit": "2839b8e0f1bc27c0914db9bce3d0180e397495a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA2MzQ4NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482063484", "bodyText": "NullableCoder is not a standard coder as was mentioned here: https://issues.apache.org/jira/browse/BEAM-10529?jql=project%20%3D%20BEAM%20AND%20text%20~%20%22nullable%20python%22\nSo I suppose the only way to support null values is not to set them.\nI noticed that when I tried to read a null field from Spanner table. But I may be wrong", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNDMwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwMTM4Nw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483301387", "bodyText": "Hm so it should be supported. RowCoder encodes nulls for top-level fields separately so there's no need for NullableCoder. NullableCoder is only used when you have a nullable type in a container type, e.g. ARRAY<NULLABLE INT>. This wasn't supported in Python until recently - #12426 should have fixed it though.", "author": "TheNeuralBit", "createdAt": "2020-09-03T23:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNDMwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODQyMjEzNA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r488422134", "bodyText": "I'm not sure where my message has gone, but I wrote that nulls come up with no problems, I've just used ImmutableMap which does not allow null values. Replacing it with java.util.HashMap solved the issue.", "author": "piotr-szuberski", "createdAt": "2020-09-15T06:41:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNDMwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDM5NjA0Mw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480396043", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      .setRowSchema(schema)\n          \n          \n            \n                      .setCoder(RowCoder.of(schema));\n          \n          \n            \n                      .setRowSchema(schema);\n          \n      \n    \n    \n  \n\nsetCoder(RowCoder.of(schema)) is what setRowSchema does", "author": "TheNeuralBit", "createdAt": "2020-08-31T20:59:48Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java", "diffHunk": "@@ -678,6 +703,42 @@ public Read withPartitionOptions(PartitionOptions partitionOptions) {\n               .withTransaction(getTransaction());\n       return input.apply(Create.of(getReadOperation())).apply(\"Execute query\", readAll);\n     }\n+\n+    SerializableFunction<Struct, Row> getFormatFn() {\n+      return (SerializableFunction<Struct, Row>)\n+          input ->\n+              Row.withSchema(Schema.builder().addInt64Field(\"Key\").build())\n+                  .withFieldValue(\"Key\", 3L)\n+                  .build();\n+    }\n+  }\n+\n+  public static class ReadRows extends PTransform<PBegin, PCollection<Row>> {\n+    Read read;\n+    Schema schema;\n+\n+    public ReadRows(Read read, Schema schema) {\n+      super(\"Read rows\");\n+      this.read = read;\n+      this.schema = schema;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      return input\n+          .apply(read)\n+          .apply(\n+              MapElements.into(TypeDescriptor.of(Row.class))\n+                  .via(\n+                      new SerializableFunction<Struct, Row>() {\n+                        @Override\n+                        public Row apply(Struct struct) {\n+                          return StructUtils.translateStructToRow(struct, schema);\n+                        }\n+                      }))\n+          .setRowSchema(schema)\n+          .setCoder(RowCoder.of(schema));", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA3MDMyNQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482070325", "bodyText": "For some reason it was not obvious to me. Done.", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:31:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDM5NjA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480429441", "bodyText": "It would be really great if SpannerIO.ReadRows could determine the schema at pipeline construction time so the user doesn't have to specify it. In SpannerIO.Read#expand we require the user to specify either a query or a list of columns: \n  \n    \n      beam/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java\n    \n    \n        Lines 656 to 671\n      in\n      2872e37\n    \n    \n    \n    \n\n        \n          \n           if (getReadOperation().getQuery() != null) { \n        \n\n        \n          \n             // TODO: validate query? \n        \n\n        \n          \n           } else if (getReadOperation().getTable() != null) { \n        \n\n        \n          \n             // Assume read \n        \n\n        \n          \n             checkNotNull( \n        \n\n        \n          \n                 getReadOperation().getColumns(), \n        \n\n        \n          \n                 \"For a read operation SpannerIO.read() requires a list of \" \n        \n\n        \n          \n                     + \"columns to set with withColumns method\"); \n        \n\n        \n          \n             checkArgument( \n        \n\n        \n          \n                 !getReadOperation().getColumns().isEmpty(), \n        \n\n        \n          \n                 \"For a read operation SpannerIO.read() requires a\" \n        \n\n        \n          \n                     + \" list of columns to set with withColumns method\"); \n        \n\n        \n          \n           } else { \n        \n\n        \n          \n             throw new IllegalArgumentException( \n        \n\n        \n          \n                 \"SpannerIO.read() requires configuring query or read operation.\"); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nIn both case we're very close to a schema. We just need to analyze the query and/or get the output types for the projected columns. I looked into it a little bit, but I'm not quite sure the best way to use the spanner client to look up the schema. The only thing I could figure out was to start a read and look at the type of ResultSet#getCurrentRowAsStruct which seems less than ideal.\nCC @nielm who's done some work with SpannerIO recently - do you have any suggestions for a way to determine the types of the Structs that SpannerIO.Read will produce?", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:16:32Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerIO.java", "diffHunk": "@@ -678,6 +703,42 @@ public Read withPartitionOptions(PartitionOptions partitionOptions) {\n               .withTransaction(getTransaction());\n       return input.apply(Create.of(getReadOperation())).apply(\"Execute query\", readAll);\n     }\n+\n+    SerializableFunction<Struct, Row> getFormatFn() {\n+      return (SerializableFunction<Struct, Row>)\n+          input ->\n+              Row.withSchema(Schema.builder().addInt64Field(\"Key\").build())\n+                  .withFieldValue(\"Key\", 3L)\n+                  .build();\n+    }\n+  }\n+\n+  public static class ReadRows extends PTransform<PBegin, PCollection<Row>> {\n+    Read read;\n+    Schema schema;\n+\n+    public ReadRows(Read read, Schema schema) {\n+      super(\"Read rows\");\n+      this.read = read;\n+      this.schema = schema;", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTkxMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480429910", "bodyText": "We could also punt on this question and file a jira with a TODO here. I recognize this is a little out of scope for BEAM-10139, BEAM-10140.", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA3Mzg5OQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482073899", "bodyText": "I'd really like to do it in this PR, but the only thing that comes to mind is to do what you said - perform the read request with client and then read the schema. The obvious disadvantage is that the Spanner query will be executed twice. I researched that limit of 1 row added to the end of query will not improve the performance so this is not the thing to do for huge result sets", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNTA1OQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483315059", "bodyText": "I can reach out to the Spanner team to see if there's a good way to do this, I'll let you know if I learn anything. For now we can just plan on a jira and a TODO", "author": "TheNeuralBit", "createdAt": "2020-09-04T00:04:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTA2NjczMw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r501066733", "bodyText": "I don't see any good solution here...\nWhen reading an entire table, it could be possible to read the table's schema first, and determine what types the columns are, but this does not work for a query as the query output columns may not correspond to table columns.\nAdding LIMIT 1 would only work for simple queries, anything with joins, GROUP BY, ORDER BY will require the majority of the query to be executed before a single row is returned.\nSo the only solution I can see is for the caller to specify the row Schema as you do here..", "author": "nielm", "createdAt": "2020-10-07T14:38:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM3OTI0NQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r501379245", "bodyText": "It seems like it should be possible to analyze the query and determine the output schema, SqlTransform and JdbcIO both do this.\nI got a similar response from my internal queries though, it doesn't look like there's a good way to do this with the Spanner client", "author": "TheNeuralBit", "createdAt": "2020-10-08T00:11:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NjE0Ng==", "url": "https://github.com/apache/beam/pull/12611#discussion_r501676146", "bodyText": "Thank you @nielm ! I thought about the LIMIT approach but then I found the same arguments not to do that.\nIt appears there exist a jdbc client for Spanner: https://cloud.google.com/spanner/docs/jdbc-drivers . I'll try to figure out if I can use it.\nThere is ResultSetMetadata in Spanner's REST API which extends json object. https://cloud.google.com/spanner/docs/reference/rest/v1/ResultSetMetadata but at the end of the day it requires at least partially to fetch the data.\nBut I would leave it for another PR as it supposedly require to move SchemaUtils from io/jdbc to some more general place (extensions/sql?). As I can see Struct type is mapped to String/Varchar as is mentioned in the FAQ, so it may not be the best option\nThe Cloud Spanner STRUCT data type is mapped to a SQL VARCHAR data type, accessible through \nthis driver as String types. All other types have appropriate mappings.", "author": "piotr-szuberski", "createdAt": "2020-10-08T12:21:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQ0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMDE2Nw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480430167", "bodyText": "looks like this was just there for testing?", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:18:49Z", "path": "sdks/java/io/google-cloud-platform/expansion-service/build.gradle", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+apply plugin: 'org.apache.beam.module'\n+apply plugin: 'application'\n+mainClassName = \"org.apache.beam.sdk.expansion.service.ExpansionService\"\n+\n+applyJavaNature(\n+        enableChecker: true,\n+        automaticModuleName: 'org.apache.beam.sdk.io.gcp.expansion.service',\n+        exportJavadoc: false,\n+        validateShadowJar: false,\n+        shadowClosure: {},\n+)\n+\n+task runService(type: Exec) {\n+    dependsOn shadowJar\n+    executable 'sh'\n+    args '-c', 'java -jar /Users/piotr/beam/sdks/java/io/google-cloud-platform/expansion-service/build/libs/beam-sdks-java-io-google-cloud-platform-expansion-service-2.24.0-SNAPSHOT.jar 8097'\n+}", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA3NDExMw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482074113", "bodyText": "My fault, I'll remove this. Sorry for that.", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:36:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMDE2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMTcxNA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480431714", "bodyText": "This is lossy isn't it? I think we should just refuse to convert DECIMAL since Spanner doesn't have a corresponding type: https://cloud.google.com/spanner/docs/data-types#allowable_types", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:23:00Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,545 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+\n+final class StructUtils {\n+  public static Row translateStructToRow(Struct struct, Schema schema) {\n+    checkForSchemasEquality(schema.getFields(), struct.getType().getStructFields(), false);\n+\n+    List<Schema.Field> fields = schema.getFields();\n+    Row.FieldValueBuilder valueBuilder = null;\n+    // TODO: Remove this null-checking once nullable fields are supported in cross-language\n+    int count = 0;\n+    while (valueBuilder == null && count < fields.size()) {\n+      valueBuilder = getFirstStructValue(struct, fields.get(count), schema);\n+      ++count;\n+    }\n+    for (int i = count; i < fields.size(); ++i) {\n+      valueBuilder = getStructValue(valueBuilder, struct, fields.get(i));\n+    }\n+    return valueBuilder != null ? valueBuilder.build() : Row.withSchema(schema).build();\n+  }\n+\n+  public static Struct translateRowToStruct(Row row) {\n+    Struct.Builder structBuilder = Struct.newBuilder();\n+    List<Schema.Field> fields = row.getSchema().getFields();\n+    fields.forEach(\n+        field -> {\n+          String column = field.getName();\n+          switch (field.getType().getTypeName()) {\n+            case ROW:\n+              structBuilder\n+                  .set(column)\n+                  .to(\n+                      beamTypeToSpannerType(field.getType()),\n+                      translateRowToStruct(row.getRow(column)));\n+              break;\n+            case ARRAY:\n+              addArrayToStruct(structBuilder, row, field);\n+              break;\n+            case ITERABLE:\n+              addIterableToStruct(structBuilder, row, field);\n+              break;\n+            case FLOAT:\n+              structBuilder.set(column).to(row.getFloat(column).doubleValue());\n+              break;\n+            case DOUBLE:\n+              structBuilder.set(column).to(row.getDouble(column));\n+              break;\n+            case DECIMAL:\n+              structBuilder.set(column).to(row.getDecimal(column).doubleValue());", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA3NTQ4OQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482075489", "bodyText": "I agree, at first I didn't include decimals but it definitely is lossy.", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:38:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMTcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTA2NzUzNg==", "url": "https://github.com/apache/beam/pull/12611#discussion_r501067536", "bodyText": "Spanner now supports NUMERIC which may be a better conversion?", "author": "nielm", "createdAt": "2020-10-07T14:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMTcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NzA4Ng==", "url": "https://github.com/apache/beam/pull/12611#discussion_r501677086", "bodyText": "Great, quite a new thing in Spanner as I can see! Thanks! Done.", "author": "piotr-szuberski", "createdAt": "2020-10-08T12:22:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQzMTcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0MDg3NQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480440875", "bodyText": "woo this is a hefty class for type conversions! It does seem like there's a lot of duplicated logic, what's preventing us from combining more of it?", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:37:14Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,545 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+\n+final class StructUtils {", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA3OTg2MA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482079860", "bodyText": "There are non-related different classes that require the same things to be done on them.\nOne of them are Key and Mutation, other one Row.Builder and Row.FieldValueBuilder. In python there is duck typing and it's easy. But in Java I don't know how to reduce the repeated code. Maybe I should do more setValue(Object obj) and depend on castings instead of returning the proper type all the time. I'll try it.", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:44:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0MDg3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0NjY0OA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480446648", "bodyText": "Another reason to get the schema eagerly at pipeline construction time, this is an expensive operation to be doing for every Struct that we read.", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:45:50Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,545 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+\n+final class StructUtils {\n+  public static Row translateStructToRow(Struct struct, Schema schema) {\n+    checkForSchemasEquality(schema.getFields(), struct.getType().getStructFields(), false);", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MDMyNA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482080324", "bodyText": "Maybe we could just skip this check and let it crash when the types don't match?", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0NjY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0ODY1OQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r480448659", "bodyText": "It looks like there's already a native SpannerIO in the Python SDK in apache_beam/io/gcp/experimental/spannerio.py. Are we planning on removing that one? Should the API for this one be compliant with that one?", "author": "TheNeuralBit", "createdAt": "2020-08-31T22:49:07Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,504 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.25.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+import uuid\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+\n+__all__ = [\n+    'WriteToSpanner',\n+    'ReadFromSpanner',\n+    'MutationCreator',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+WriteToSpannerSchema = typing.NamedTuple(\n+    'WriteToSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('project_id', Optional[unicode]),\n+        ('batch_size_bytes', Optional[int]),\n+        ('max_num_mutations', Optional[int]),\n+        ('max_num_rows', Optional[int]),\n+        ('grouping_factor', Optional[int]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('commit_deadline', Optional[int]),\n+        ('max_cumulative_backoff', Optional[int]),\n+    ],\n+)\n+\n+\n+class WriteToSpanner(ExternalTransform):", "originalCommit": "fed767a2904c0c8ac4f1ae18d5461367beb985ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MTU5Nw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r482081597", "bodyText": "I can try to make the API compliant with the native one. I think it'd be valuable for Beam to compare the performance of both IOs and then decide which one to leave.", "author": "piotr-szuberski", "createdAt": "2020-09-02T13:46:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0ODY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTAzMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483321030", "bodyText": "Yeah that makes sense. There's definitely still value in adding this even if we end up preferring the native Python one, since we can use it from the Go SDK in the future.", "author": "TheNeuralBit", "createdAt": "2020-09-04T00:26:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0ODY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM2NDMzNw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r492364337", "bodyText": "Probably it makes sense to converge into one implementation. I'd prefer the Java implementation (hence cross-language) since it's being around for longer and used by many users. We have to make sure that the cross-language version works for all runners before native version can be removed. For example, cross-language version will not work for current production Dataflow (Runner v1) and we have to confirm that it works adequate for Dataflow Runner v2.", "author": "chamikaramj", "createdAt": "2020-09-21T21:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ0ODY1OQ=="}], "type": "inlineReview"}, {"oid": "55ee406ad3cd25838165b211683926f1ba80ad7c", "url": "https://github.com/apache/beam/commit/55ee406ad3cd25838165b211683926f1ba80ad7c", "message": "Remove redundant setCoder", "committedDate": "2020-09-02T13:50:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjcwNg==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483316706", "bodyText": "FYI now with #12481 it's possible to use schema inference for these configuration objects, so you should be able to use @AutoValue and AutoValueSchema which could save a lot of boiler plate There's an example here:\n\n  \n    \n      beam/sdks/java/expansion-service/src/test/java/org/apache/beam/sdk/expansion/service/ExpansionServiceTest.java\n    \n    \n        Lines 305 to 315\n      in\n      89a2d17\n    \n    \n    \n    \n\n        \n          \n           @DefaultSchema(AutoValueSchema.class) \n        \n\n        \n          \n           @AutoValue \n        \n\n        \n          \n           abstract static class TestConfigSchema { \n        \n\n        \n          \n             abstract @Nullable Long getConfigKey1(); \n        \n\n        \n          \n            \n        \n\n        \n          \n             abstract @Nullable Iterable<byte[]> getConfigKey2(); \n        \n\n        \n          \n            \n        \n\n        \n          \n             abstract @Nullable Map<String, Long> getConfigKey3(); \n        \n\n        \n          \n            \n        \n\n        \n          \n             abstract @Nullable Map<String, List<Long>> getConfigKey4(); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\n(it's fine to leave it as-is, just letting you know)", "author": "TheNeuralBit", "createdAt": "2020-09-04T00:10:03Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerTransformRegistrar.java", "diffHunk": "@@ -0,0 +1,287 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.service.AutoService;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.TimestampBound;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.model.pipeline.v1.SchemaApi;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.expansion.ExternalTransformRegistrar;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaTranslation;\n+import org.apache.beam.sdk.transforms.ExternalTransformBuilder;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.InvalidProtocolBufferException;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+\n+/**\n+ * Exposes {@link SpannerIO.WriteRows} and {@link SpannerIO.ReadRows} as an external transform for\n+ * cross-language usage.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoService(ExternalTransformRegistrar.class)\n+public class SpannerTransformRegistrar implements ExternalTransformRegistrar {\n+  public static final String WRITE_URN = \"beam:external:java:spanner:write:v1\";\n+  public static final String READ_URN = \"beam:external:java:spanner:read:v1\";\n+\n+  @Override\n+  public Map<String, ExternalTransformBuilder<?, ?, ?>> knownBuilderInstances() {\n+    return ImmutableMap.of(WRITE_URN, new WriteBuilder(), READ_URN, new ReadBuilder());\n+  }\n+\n+  public abstract static class CrossLanguageConfiguration {\n+    String instanceId;\n+    String databaseId;\n+    String projectId;\n+    @Nullable String host;\n+    @Nullable String emulatorHost;\n+\n+    public void setInstanceId(String instanceId) {\n+      this.instanceId = instanceId;\n+    }\n+\n+    public void setDatabaseId(String databaseId) {\n+      this.databaseId = databaseId;\n+    }\n+\n+    public void setProjectId(String projectId) {\n+      this.projectId = projectId;\n+    }\n+\n+    public void setHost(@Nullable String host) {\n+      this.host = host;\n+    }\n+\n+    public void setEmulatorHost(@Nullable String emulatorHost) {\n+      this.emulatorHost = emulatorHost;\n+    }\n+  }\n+\n+  @Experimental(Kind.PORTABILITY)\n+  public static class ReadBuilder\n+      implements ExternalTransformBuilder<ReadBuilder.Configuration, PBegin, PCollection<Row>> {\n+\n+    public static class Configuration extends CrossLanguageConfiguration {\n+      // TODO: BEAM-10851 Come up with something to determine schema without this explicit parameter\n+      private Schema schema;\n+      private @Nullable String sql;\n+      private @Nullable String table;\n+      private @Nullable Boolean batching;\n+      private @Nullable String timestampBoundMode;\n+      private @Nullable String readTimestamp;\n+      private @Nullable String timeUnit;\n+      private @Nullable Long exactStaleness;\n+\n+      public void setSql(@Nullable String sql) {\n+        this.sql = sql;\n+      }\n+\n+      public void setTable(@Nullable String table) {\n+        this.table = table;\n+      }\n+\n+      public void setBatching(@Nullable Boolean batching) {\n+        this.batching = batching;\n+      }\n+\n+      public void setTimestampBoundMode(@Nullable String timestampBoundMode) {\n+        this.timestampBoundMode = timestampBoundMode;\n+      }\n+\n+      public void setSchema(byte[] schema) throws InvalidProtocolBufferException {\n+        this.schema = SchemaTranslation.schemaFromProto(SchemaApi.Schema.parseFrom(schema));\n+      }\n+\n+      public void setReadTimestamp(@Nullable String readTimestamp) {\n+        this.readTimestamp = readTimestamp;\n+      }\n+\n+      public void setTimeUnit(@Nullable String timeUnit) {\n+        this.timeUnit = timeUnit;\n+      }\n+\n+      public void setExactStaleness(@Nullable Long exactStaleness) {\n+        this.exactStaleness = exactStaleness;\n+      }\n+\n+      private TimestampBound getTimestampBound() {\n+        if (timestampBoundMode == null) {\n+          return null;\n+        }\n+\n+        TimestampBound.Mode mode = TimestampBound.Mode.valueOf(timestampBoundMode);\n+        if (mode == TimestampBound.Mode.MAX_STALENESS\n+            || mode == TimestampBound.Mode.EXACT_STALENESS) {\n+          checkArgument(\n+              exactStaleness != null,\n+              \"Staleness value cannot be null when MAX_STALENESS or EXACT_STALENESS mode is selected\");\n+          checkArgument(\n+              timeUnit != null,\n+              \"Time unit cannot be null when MAX_STALENESS or EXACT_STALENESS mode is selected\");\n+        }\n+        if (mode == TimestampBound.Mode.READ_TIMESTAMP\n+            || mode == TimestampBound.Mode.MIN_READ_TIMESTAMP) {\n+          checkArgument(\n+              readTimestamp != null,\n+              \"Timestamp cannot be null when READ_TIMESTAMP or MIN_READ_TIMESTAMP mode is selected\");\n+        }\n+        switch (mode) {\n+          case STRONG:\n+            return TimestampBound.strong();\n+          case MAX_STALENESS:\n+            return TimestampBound.ofMaxStaleness(exactStaleness, TimeUnit.valueOf(timeUnit));\n+          case EXACT_STALENESS:\n+            return TimestampBound.ofExactStaleness(exactStaleness, TimeUnit.valueOf(timeUnit));\n+          case READ_TIMESTAMP:\n+            return TimestampBound.ofReadTimestamp(Timestamp.parseTimestamp(readTimestamp));\n+          case MIN_READ_TIMESTAMP:\n+            return TimestampBound.ofMinReadTimestamp(Timestamp.parseTimestamp(readTimestamp));\n+          default:\n+            throw new RuntimeException(\"Unknown timestamp bound mode: \" + mode);\n+        }\n+      }\n+\n+      public ReadOperation getReadOperation() {\n+        checkArgument(\n+            sql == null || table == null,\n+            \"Query and table params are mutually exclusive. Set just one of them.\");\n+        if (sql != null) {\n+          return ReadOperation.create().withQuery(sql);\n+        }\n+        return ReadOperation.create().withTable(table).withColumns(schema.getFieldNames());\n+      }\n+    }\n+\n+    @Override\n+    public PTransform<PBegin, PCollection<Row>> buildExternal(Configuration configuration) {\n+      SpannerIO.Read readTransform =\n+          SpannerIO.read()\n+              .withProjectId(configuration.projectId)\n+              .withDatabaseId(configuration.databaseId)\n+              .withInstanceId(configuration.instanceId)\n+              .withReadOperation(configuration.getReadOperation());\n+\n+      if (configuration.host != null) {\n+        readTransform = readTransform.withHost(configuration.host);\n+      }\n+      if (configuration.emulatorHost != null) {\n+        readTransform = readTransform.withEmulatorHost(configuration.emulatorHost);\n+      }\n+      if (configuration.getTimestampBound() != null) {\n+        readTransform = readTransform.withTimestampBound(configuration.getTimestampBound());\n+      }\n+      if (configuration.batching != null) {\n+        readTransform = readTransform.withBatching(configuration.batching);\n+      }\n+\n+      return new SpannerIO.ReadRows(readTransform, configuration.schema);\n+    }\n+  }\n+\n+  @Experimental(Kind.PORTABILITY)\n+  public static class WriteBuilder\n+      implements ExternalTransformBuilder<WriteBuilder.Configuration, PCollection<Row>, PDone> {\n+\n+    public static class Configuration extends CrossLanguageConfiguration {", "originalCommit": "5feb3d9332e2da6caeeae15822e725b8ef5122be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU3NDgwMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483574800", "bodyText": "Many setters have some logic inside them and AutoValue does not allow inheritance so there would be some code duplication in getter methods. As this is just a cosmetic thing and I have other things to do I'll leave it as it is for now.", "author": "piotr-szuberski", "createdAt": "2020-09-04T12:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjcwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxNDQ5Mg==", "url": "https://github.com/apache/beam/pull/12611#discussion_r488314492", "bodyText": "Sounds good", "author": "TheNeuralBit", "createdAt": "2020-09-15T00:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjcwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTY3NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483325674", "bodyText": "Overall I think it makes a lot of sense to use Rows for the Mutations, with a nested Row for the data, but this API is pretty tricky. Could you look into adding a separate PTransform (or multiple PTransforms) for converting the Rows to mutations? I think an API like this should be possible:\npc = ... #some PCollection with a schema\n\npc | RowToMutation.insert('table')\n     | WriteToSpanner(...)\n\nOR \n\npc | RowToMutation.insertOrUpdate('table')\n     | WriteToSpanner(...)\n\nOR\n\npc | RowToMutation.delete('table')\n     | WriteToSpanner(...)\nThe PTransform would be able to look at the element_type of the input PCollection and create a mutation type that wraps it  in the expand method. There's not a lot of examples of logic like this in the Python SDK (yet) the only one I know of is here: \n  \n    \n      beam/sdks/python/apache_beam/dataframe/schemas.py\n    \n    \n        Lines 50 to 55\n      in\n      cfa448d\n    \n    \n    \n    \n\n        \n          \n           def expand(self, pcoll): \n        \n\n        \n          \n             columns = [ \n        \n\n        \n          \n                 name for name, _ in named_fields_from_element_type(pcoll.element_type) \n        \n\n        \n          \n             ] \n        \n\n        \n          \n             return pcoll | self._batch_elements_transform | beam.Map( \n        \n\n        \n          \n                 lambda batch: pd.DataFrame.from_records(batch, columns=columns)) \n        \n    \n  \n\n\nThat way the user wouldn't need to pass the type they're planning on using to MutationCreator. What do you think of that?", "author": "TheNeuralBit", "createdAt": "2020-09-04T00:45:37Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,503 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.25.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+import uuid\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+\n+__all__ = [\n+    'WriteToSpanner',\n+    'ReadFromSpanner',\n+    'MutationCreator',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+WriteToSpannerSchema = typing.NamedTuple(\n+    'WriteToSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('project_id', Optional[unicode]),\n+        ('max_batch_size_bytes', Optional[int]),\n+        ('max_number_mutations', Optional[int]),\n+        ('max_number_rows', Optional[int]),\n+        ('grouping_factor', Optional[int]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('commit_deadline', Optional[int]),\n+        ('max_cumulative_backoff', Optional[int]),\n+    ],\n+)\n+\n+\n+class WriteToSpanner(ExternalTransform):\n+  \"\"\"\n+  A PTransform which writes mutations to the specified instance's database\n+  via Spanner.\n+\n+  This transform receives Mutations defined as NamedTuple which are created\n+  via utility class MutationCreator. Mutation needs to know what row type does\n+  it wrap. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+    coders.registry.register_coder(ExampleRow, coders.RowCoder)\n+\n+    mutation_creator = MutationCreator('table', ExampleRow, 'ExampleMutation')", "originalCommit": "5feb3d9332e2da6caeeae15822e725b8ef5122be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzUyNTkzMQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r483525931", "bodyText": "That way we loose possibility of mixing different kinds of mutations. I don't imagine any sane usage of mixed insert/delete as the order is not guaranteed so I aggree that removing this assumption is justified.\nSince we will always map rows to mutations before then it would be good to enclose mapping rows to mutations inside WriteToSpanner. How about such an API?:\npc.with_output_types(CustomRow) | WriteToSpanner(...).insert(table)\npc.with_output_types(CustomRow) | WriteToSpanner(...).delete(table)\npc.with_output_types(List[CustomRow]) | WriteToSpanner(...).delete(table)\n\nIt's not consistent with ReadFromSpanner(...) but I think it's better than forcing the user to call RowToMutation each time.\nTo be more consistent I could do something like ReadFromSpanner(...).from_table(table) and ReadFromSpanner(...).from_sql(sql_query)", "author": "piotr-szuberski", "createdAt": "2020-09-04T10:16:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTY3NA=="}], "type": "inlineReview"}, {"oid": "c24e8cba7bd09a20f4eae37dc84e4ac764105296", "url": "https://github.com/apache/beam/commit/c24e8cba7bd09a20f4eae37dc84e4ac764105296", "message": "Fix docstrings", "committedDate": "2020-09-08T09:43:47Z", "type": "forcePushed"}, {"oid": "1c43284593c2f7350b67b3ddb1878cb521ced284", "url": "https://github.com/apache/beam/commit/1c43284593c2f7350b67b3ddb1878cb521ced284", "message": "Add support for numeric type", "committedDate": "2020-10-08T12:19:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU5MzEwNA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r502593104", "bodyText": "\"GCP Java IOs\". Currently just Spanner but this will pick up any IOs that are made external.", "author": "TheNeuralBit", "createdAt": "2020-10-09T18:05:37Z", "path": "sdks/java/io/google-cloud-platform/expansion-service/build.gradle", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+apply plugin: 'org.apache.beam.module'\n+apply plugin: 'application'\n+mainClassName = \"org.apache.beam.sdk.expansion.service.ExpansionService\"\n+\n+applyJavaNature(\n+        enableChecker: true,\n+        automaticModuleName: 'org.apache.beam.sdk.io.gcp.expansion.service',\n+        exportJavadoc: false,\n+        validateShadowJar: false,\n+        shadowClosure: {},\n+)\n+\n+description = \"Apache Beam :: SDKs :: Java :: IO :: Google Cloud Platform :: Expansion Service\"\n+ext.summary = \"Expansion service serving Spanner Java IO\"", "originalCommit": "1c43284593c2f7350b67b3ddb1878cb521ced284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1ODA5OA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505658098", "bodyText": "Done.", "author": "piotr-szuberski", "createdAt": "2020-10-15T15:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU5MzEwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxMTU4Mg==", "url": "https://github.com/apache/beam/pull/12611#discussion_r502611582", "bodyText": "Please add some documentation about the mapping from Mutation to Beam schemas, I think this would be the appropriate place. You might duplicate the doc somewhere in the Python code as well, or just reference this.", "author": "TheNeuralBit", "createdAt": "2020-10-09T18:43:56Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/MutationUtils.java", "diffHunk": "@@ -34,4 +49,237 @@ public static boolean isPointDelete(Mutation m) {\n         && Iterables.isEmpty(m.getKeySet().getRanges())\n         && Iterables.size(m.getKeySet().getKeys()) == 1;\n   }\n+\n+  /**\n+   * Utility function to convert row to mutation.\n+   *\n+   * @return function that can convert row to mutation\n+   */\n+  public static SerializableFunction<Row, Mutation> beamRowToMutationFn() {", "originalCommit": "1c43284593c2f7350b67b3ddb1878cb521ced284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5MzA1NQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505693055", "bodyText": "Done. I hope it's not too brief.", "author": "piotr-szuberski", "createdAt": "2020-10-15T16:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxMTU4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxNDQwMw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r502614403", "bodyText": "Now that we've dropped support for Python 2 you could use the class syntax here if you want:\nclass WriteToSpannerSchema(typing.NamedTuple):\n  instance_id: str\n  database_id: str\n  ...", "author": "TheNeuralBit", "createdAt": "2020-10-09T18:49:43Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,483 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.25.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+import uuid\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import Map\n+from apache_beam import PTransform\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+\n+__all__ = [\n+    'WriteToSpanner',\n+    'ReadFromSpanner',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+WriteToSpannerSchema = typing.NamedTuple(\n+    'WriteToSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('project_id', Optional[unicode]),\n+        ('max_batch_size_bytes', Optional[int]),\n+        ('max_number_mutations', Optional[int]),\n+        ('max_number_rows', Optional[int]),\n+        ('grouping_factor', Optional[int]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('commit_deadline', Optional[int]),\n+        ('max_cumulative_backoff', Optional[int]),\n+    ],\n+)", "originalCommit": "1c43284593c2f7350b67b3ddb1878cb521ced284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1NzI2NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505657264", "bodyText": "Great!", "author": "piotr-szuberski", "createdAt": "2020-10-15T15:56:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxNDQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxOTM4Ng==", "url": "https://github.com/apache/beam/pull/12611#discussion_r502619386", "bodyText": "Could you explain the issue here?\nAlso the logic between is_list and is_delete is pretty confusing. Could this be simplified by only allowing lists for delete operations?", "author": "TheNeuralBit", "createdAt": "2020-10-09T19:00:25Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,483 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.25.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+import uuid\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import Map\n+from apache_beam import PTransform\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+\n+__all__ = [\n+    'WriteToSpanner',\n+    'ReadFromSpanner',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+WriteToSpannerSchema = typing.NamedTuple(\n+    'WriteToSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('project_id', Optional[unicode]),\n+        ('max_batch_size_bytes', Optional[int]),\n+        ('max_number_mutations', Optional[int]),\n+        ('max_number_rows', Optional[int]),\n+        ('grouping_factor', Optional[int]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('commit_deadline', Optional[int]),\n+        ('max_cumulative_backoff', Optional[int]),\n+    ],\n+)\n+\n+\n+class WriteToSpanner:\n+  \"\"\"\n+  A PTransform which writes mutations to the specified instance's database\n+  via Spanner.\n+\n+  This transform receives rows defined as NamedTuple or as List[NamedTuple]\n+  in case of delete operation. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: ExampleRow(n, str(n))\n+              .with_output_types(ExampleRow)\n+          | 'Write to Spanner' >> WriteToSpanner(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id').insert('your_table'))\n+\n+  In addition you can pass List[ExampleRow] to delete transform::\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: [ExampleRow(n, str(n),\n+              ExampleRow(n * 2, str(n * 2)])\n+              .with_output_types(List[ExampleRow])\n+          | 'Write to Spanner' >> WriteToSpanner(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id').delete('your_table'))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a write operation to Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param max_batch_size_bytes: Specifies the batch size limit (max number of\n+        bytes mutated per batch). Default value is 1048576 bytes = 1MB.\n+    :param max_number_mutations: Specifies the cell mutation limit (maximum\n+        number of mutated cells per batch). Default value is 5000.\n+    :param max_number_rows: Specifies the row mutation limit (maximum number of\n+        mutated rows per batch). Default value is 500.\n+    :param grouping_factor: Specifies the multiple of max mutation (in terms\n+        of both bytes per batch and cells per batch) that is used to select a\n+        set of mutations to sort by key for batching. This sort uses local\n+        memory on the workers, so using large values can cause out of memory\n+        errors. Default value is 1000.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param commit_deadline: Specifies the deadline for the Commit API call.\n+        Default is 15 secs. DEADLINE_EXCEEDED errors will prompt a backoff/retry\n+        until the value of commit_deadline is reached. DEADLINE_EXCEEDED errors\n+        are ar reported with logging and counters. Pass seconds as value.\n+    :param max_cumulative_backoff: Specifies the maximum cumulative backoff\n+        time when retrying after DEADLINE_EXCEEDED errors. Default is 900s\n+        (15min). If the mutations still have not been written after this time,\n+        they are treated as a failure, and handled according to the setting of\n+        failure_mode. Pass seconds as value.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.config = NamedTupleBasedPayloadBuilder(\n+        WriteToSpannerSchema(\n+            project_id=project_id,\n+            instance_id=instance_id,\n+            database_id=database_id,\n+            max_batch_size_bytes=max_batch_size_bytes,\n+            max_number_mutations=max_number_mutations,\n+            max_number_rows=max_number_rows,\n+            grouping_factor=grouping_factor,\n+            host=host,\n+            emulator_host=emulator_host,\n+            commit_deadline=commit_deadline,\n+            max_cumulative_backoff=max_cumulative_backoff,\n+        ),\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def insert(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.INSERT, table)\n+\n+  def delete(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.DELETE, table)\n+\n+  def update(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.UPDATE, table)\n+\n+  def replace(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.REPLACE, table)\n+\n+  def insert_or_update(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.INSERT_OR_UPDATE, table)\n+\n+\n+ReadFromSpannerSchema = NamedTuple(\n+    'ReadFromSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('schema', bytes),\n+        ('sql', Optional[unicode]),\n+        ('table', Optional[unicode]),\n+        ('project_id', Optional[unicode]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('batching', Optional[bool]),\n+        ('timestamp_bound_mode', Optional[unicode]),\n+        ('read_timestamp', Optional[unicode]),\n+        ('exact_staleness', Optional[int]),\n+        ('time_unit', Optional[unicode]),\n+    ],\n+)\n+\n+\n+class ReadFromSpanner(ExternalTransform):\n+  \"\"\"\n+  A PTransform which reads from the specified Spanner instance's database.\n+\n+  This transform required type of the row it has to return to provide the\n+  schema. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      result = (\n+          p\n+          | ReadFromSpanner(\n+              instance_id='your_instance_id',\n+              database_id='your_database_id',\n+              project_id='your_project_id',\n+              row_type=ExampleRow,\n+              query='SELECT * FROM some_table',\n+          ).with_output_types(ExampleRow))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  URN = 'beam:external:java:spanner:read:v1'\n+\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      row_type=None,\n+      sql=None,\n+      table=None,\n+      host=None,\n+      emulator_host=None,\n+      batching=None,\n+      timestamp_bound_mode=None,\n+      read_timestamp=None,\n+      exact_staleness=None,\n+      time_unit=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a read operation from Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param row_type: Row type that fits the given query or table. Passed as\n+        NamedTuple, e.g. NamedTuple('name', [('row_name', unicode)])\n+    :param sql: An sql query to execute. It's results must fit the\n+        provided row_type. Don't use when table is set.\n+    :param table: A spanner table. When provided all columns from row_type\n+        will be selected to query. Don't use when query is set.\n+    :param batching: By default Batch API is used to read data from Cloud\n+        Spanner. It is useful to disable batching when the underlying query\n+        is not root-partitionable.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param timestamp_bound_mode: Defines how Cloud Spanner will choose a\n+        timestamp for a read-only transaction or a single read/query.\n+        Possible values:\n+        STRONG: A timestamp bound that will perform reads and queries at a\n+        timestamp where all previously committed transactions are visible.\n+        READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at the given timestamp.\n+        MIN_READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at a timestamp chosen to be at least given timestamp value.\n+        EXACT_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at an exact staleness. The timestamp is chosen soon after the\n+        read is started.\n+        MAX_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at a timestamp chosen to be at most time_unit stale.\n+    :param read_timestamp: Timestamp in string. Use only when\n+        timestamp_bound_mode is set to READ_TIMESTAMP or MIN_READ_TIMESTAMP.\n+    :param exact_staleness: Staleness value as int. Use only when\n+        timestamp_bound_mode is set to EXACT_STALENESS or MAX_STALENESS.\n+        time_unit has to be set along with this param.\n+    :param time_unit: Time unit for staleness_value. Possible values:\n+        NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, HOURS, DAYS.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    assert row_type\n+    assert sql or table and not (sql and table)\n+    TimeUnit.verify_param(time_unit)\n+    TimestampBoundMode.verify_param(timestamp_bound_mode)\n+    staleness_value = int(exact_staleness) if exact_staleness else None\n+\n+    if staleness_value or time_unit:\n+      assert staleness_value and time_unit and \\\n+             timestamp_bound_mode == TimestampBoundMode.MAX_STALENESS or \\\n+             timestamp_bound_mode == TimestampBoundMode.EXACT_STALENESS\n+\n+    if read_timestamp:\n+      assert timestamp_bound_mode == TimestampBoundMode.MIN_READ_TIMESTAMP\\\n+             or timestamp_bound_mode == TimestampBoundMode.READ_TIMESTAMP\n+\n+    coders.registry.register_coder(row_type, coders.RowCoder)\n+\n+    super(ReadFromSpanner, self).__init__(\n+        self.URN,\n+        NamedTupleBasedPayloadBuilder(\n+            ReadFromSpannerSchema(\n+                instance_id=instance_id,\n+                database_id=database_id,\n+                sql=sql,\n+                table=table,\n+                schema=named_tuple_to_schema(row_type).SerializeToString(),\n+                project_id=project_id,\n+                host=host,\n+                emulator_host=emulator_host,\n+                batching=batching,\n+                timestamp_bound_mode=timestamp_bound_mode,\n+                read_timestamp=read_timestamp,\n+                exact_staleness=exact_staleness,\n+                time_unit=time_unit,\n+            ),\n+        ),\n+        expansion_service or default_io_expansion_service(),\n+    )\n+\n+\n+class Operation:\n+  INSERT = 'INSERT'\n+  DELETE = 'DELETE'\n+  UPDATE = 'UPDATE'\n+  REPLACE = 'REPLACE'\n+  INSERT_OR_UPDATE = 'INSERT_OR_UPDATE'\n+\n+\n+class TimeUnit:\n+  NANOSECONDS = 'NANOSECONDS'\n+  MICROSECONDS = 'MICROSECONDS'\n+  MILLISECONDS = 'MILLISECONDS'\n+  SECONDS = 'SECONDS'\n+  HOURS = 'HOURS'\n+  DAYS = 'DAYS'\n+\n+  @staticmethod\n+  def verify_param(param):\n+    if param and not hasattr(TimeUnit, param):\n+      raise RuntimeError(\n+          'Invalid param for TimestampBoundMode: {}'.format(param))\n+\n+\n+class TimestampBoundMode:\n+  MAX_STALENESS = 'MAX_STALENESS'\n+  EXACT_STALENESS = 'EXACT_STALENESS'\n+  READ_TIMESTAMP = 'READ_TIMESTAMP'\n+  MIN_READ_TIMESTAMP = 'MIN_READ_TIMESTAMP'\n+  STRONG = 'STRONG'\n+\n+  @staticmethod\n+  def verify_param(param):\n+    if param and not hasattr(TimestampBoundMode, param):\n+      raise RuntimeError(\n+          'Invalid param for TimestampBoundMode: {}'.format(param))\n+\n+\n+class WriteToSpannerTransform(PTransform):\n+  URN = 'beam:external:java:spanner:write:v1'\n+\n+  def __init__(self, config, expansion_service, operation, table):\n+    super(WriteToSpannerTransform, self).__init__()\n+    self.config = config\n+    self.expansion_service = expansion_service\n+    self.operation = operation\n+    self.table = table\n+\n+  def expand(self, row_pcoll):\n+    return (\n+        row_pcoll\n+        | RowToMutation(self.operation, self.table)\n+        | ExternalTransform(self.URN, self.config, self.expansion_service))\n+\n+\n+class RowToMutation(PTransform):\n+  def __init__(self, operation, table):\n+    super(RowToMutation, self).__init__()\n+    self.operation = operation\n+    self.table = table\n+\n+  def expand(self, pcoll):\n+    is_delete = self.operation == Operation.DELETE\n+    mutation_name = 'Mutation_%s_%s' % (\n+        self.operation, str(uuid.uuid4()).replace('-', ''))\n+\n+    # There is an error when pcoll.element_type is List[row_type] so pass\n+    # a list of inner element types to NamedTuple explicitly.\n+    is_list = hasattr(pcoll.element_type, 'inner_type')\n+    row_type = pcoll.element_type.inner_type if is_list else pcoll.element_type", "originalCommit": "1c43284593c2f7350b67b3ddb1878cb521ced284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTYxMTA4MA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505611080", "bodyText": "The issue was that the type returned from pcoll.element_type List[SpannerTestKey] was not compatible with typing.List[SpannerTestKey] and caused a silly message:\nException: NamedTuple('Name', [(f0, t0), (f1, t1), ...]); each t must be a type Got List[SpannerTestKey].", "author": "piotr-szuberski", "createdAt": "2020-10-15T14:56:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxOTM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1NzAyNg==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505657026", "bodyText": "I've enforced lists only for delete operation.", "author": "piotr-szuberski", "createdAt": "2020-10-15T15:56:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYxOTM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYzNTIzMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r502635230", "bodyText": "We should make sure this works when schemas are specified via beam.Row as well, right now I think this will only work with the NamedTuple style.\nYou could use element_type = named_tuple_from_schema(schema_from_element_type(pcoll.element_type)) to make sure element_type is a NamedTuple that you can use here (it might be worth adding a convenience function for that patttern).\n\n  \n    \n      beam/sdks/python/apache_beam/typehints/schemas.py\n    \n    \n         Line 273\n      in\n      a66454b\n    \n    \n    \n    \n\n        \n          \n           def named_tuple_from_schema(schema): \n        \n    \n  \n\n\n\n  \n    \n      beam/sdks/python/apache_beam/typehints/schemas.py\n    \n    \n         Line 282\n      in\n      a66454b\n    \n    \n    \n    \n\n        \n          \n           def schema_from_element_type(element_type):  # (type) -> schema_pb2.Schema", "author": "TheNeuralBit", "createdAt": "2020-10-09T19:36:41Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,483 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.25.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+import uuid\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import Map\n+from apache_beam import PTransform\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+\n+__all__ = [\n+    'WriteToSpanner',\n+    'ReadFromSpanner',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+WriteToSpannerSchema = typing.NamedTuple(\n+    'WriteToSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('project_id', Optional[unicode]),\n+        ('max_batch_size_bytes', Optional[int]),\n+        ('max_number_mutations', Optional[int]),\n+        ('max_number_rows', Optional[int]),\n+        ('grouping_factor', Optional[int]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('commit_deadline', Optional[int]),\n+        ('max_cumulative_backoff', Optional[int]),\n+    ],\n+)\n+\n+\n+class WriteToSpanner:\n+  \"\"\"\n+  A PTransform which writes mutations to the specified instance's database\n+  via Spanner.\n+\n+  This transform receives rows defined as NamedTuple or as List[NamedTuple]\n+  in case of delete operation. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: ExampleRow(n, str(n))\n+              .with_output_types(ExampleRow)\n+          | 'Write to Spanner' >> WriteToSpanner(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id').insert('your_table'))\n+\n+  In addition you can pass List[ExampleRow] to delete transform::\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: [ExampleRow(n, str(n),\n+              ExampleRow(n * 2, str(n * 2)])\n+              .with_output_types(List[ExampleRow])\n+          | 'Write to Spanner' >> WriteToSpanner(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id').delete('your_table'))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a write operation to Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param max_batch_size_bytes: Specifies the batch size limit (max number of\n+        bytes mutated per batch). Default value is 1048576 bytes = 1MB.\n+    :param max_number_mutations: Specifies the cell mutation limit (maximum\n+        number of mutated cells per batch). Default value is 5000.\n+    :param max_number_rows: Specifies the row mutation limit (maximum number of\n+        mutated rows per batch). Default value is 500.\n+    :param grouping_factor: Specifies the multiple of max mutation (in terms\n+        of both bytes per batch and cells per batch) that is used to select a\n+        set of mutations to sort by key for batching. This sort uses local\n+        memory on the workers, so using large values can cause out of memory\n+        errors. Default value is 1000.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param commit_deadline: Specifies the deadline for the Commit API call.\n+        Default is 15 secs. DEADLINE_EXCEEDED errors will prompt a backoff/retry\n+        until the value of commit_deadline is reached. DEADLINE_EXCEEDED errors\n+        are ar reported with logging and counters. Pass seconds as value.\n+    :param max_cumulative_backoff: Specifies the maximum cumulative backoff\n+        time when retrying after DEADLINE_EXCEEDED errors. Default is 900s\n+        (15min). If the mutations still have not been written after this time,\n+        they are treated as a failure, and handled according to the setting of\n+        failure_mode. Pass seconds as value.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.config = NamedTupleBasedPayloadBuilder(\n+        WriteToSpannerSchema(\n+            project_id=project_id,\n+            instance_id=instance_id,\n+            database_id=database_id,\n+            max_batch_size_bytes=max_batch_size_bytes,\n+            max_number_mutations=max_number_mutations,\n+            max_number_rows=max_number_rows,\n+            grouping_factor=grouping_factor,\n+            host=host,\n+            emulator_host=emulator_host,\n+            commit_deadline=commit_deadline,\n+            max_cumulative_backoff=max_cumulative_backoff,\n+        ),\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def insert(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.INSERT, table)\n+\n+  def delete(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.DELETE, table)\n+\n+  def update(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.UPDATE, table)\n+\n+  def replace(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.REPLACE, table)\n+\n+  def insert_or_update(self, table):\n+    return WriteToSpannerTransform(\n+        self.config, self.expansion_service, Operation.INSERT_OR_UPDATE, table)\n+\n+\n+ReadFromSpannerSchema = NamedTuple(\n+    'ReadFromSpannerSchema',\n+    [\n+        ('instance_id', unicode),\n+        ('database_id', unicode),\n+        ('schema', bytes),\n+        ('sql', Optional[unicode]),\n+        ('table', Optional[unicode]),\n+        ('project_id', Optional[unicode]),\n+        ('host', Optional[unicode]),\n+        ('emulator_host', Optional[unicode]),\n+        ('batching', Optional[bool]),\n+        ('timestamp_bound_mode', Optional[unicode]),\n+        ('read_timestamp', Optional[unicode]),\n+        ('exact_staleness', Optional[int]),\n+        ('time_unit', Optional[unicode]),\n+    ],\n+)\n+\n+\n+class ReadFromSpanner(ExternalTransform):\n+  \"\"\"\n+  A PTransform which reads from the specified Spanner instance's database.\n+\n+  This transform required type of the row it has to return to provide the\n+  schema. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      result = (\n+          p\n+          | ReadFromSpanner(\n+              instance_id='your_instance_id',\n+              database_id='your_database_id',\n+              project_id='your_project_id',\n+              row_type=ExampleRow,\n+              query='SELECT * FROM some_table',\n+          ).with_output_types(ExampleRow))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  URN = 'beam:external:java:spanner:read:v1'\n+\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      row_type=None,\n+      sql=None,\n+      table=None,\n+      host=None,\n+      emulator_host=None,\n+      batching=None,\n+      timestamp_bound_mode=None,\n+      read_timestamp=None,\n+      exact_staleness=None,\n+      time_unit=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a read operation from Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param row_type: Row type that fits the given query or table. Passed as\n+        NamedTuple, e.g. NamedTuple('name', [('row_name', unicode)])\n+    :param sql: An sql query to execute. It's results must fit the\n+        provided row_type. Don't use when table is set.\n+    :param table: A spanner table. When provided all columns from row_type\n+        will be selected to query. Don't use when query is set.\n+    :param batching: By default Batch API is used to read data from Cloud\n+        Spanner. It is useful to disable batching when the underlying query\n+        is not root-partitionable.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param timestamp_bound_mode: Defines how Cloud Spanner will choose a\n+        timestamp for a read-only transaction or a single read/query.\n+        Possible values:\n+        STRONG: A timestamp bound that will perform reads and queries at a\n+        timestamp where all previously committed transactions are visible.\n+        READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at the given timestamp.\n+        MIN_READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at a timestamp chosen to be at least given timestamp value.\n+        EXACT_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at an exact staleness. The timestamp is chosen soon after the\n+        read is started.\n+        MAX_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at a timestamp chosen to be at most time_unit stale.\n+    :param read_timestamp: Timestamp in string. Use only when\n+        timestamp_bound_mode is set to READ_TIMESTAMP or MIN_READ_TIMESTAMP.\n+    :param exact_staleness: Staleness value as int. Use only when\n+        timestamp_bound_mode is set to EXACT_STALENESS or MAX_STALENESS.\n+        time_unit has to be set along with this param.\n+    :param time_unit: Time unit for staleness_value. Possible values:\n+        NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, HOURS, DAYS.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    assert row_type\n+    assert sql or table and not (sql and table)\n+    TimeUnit.verify_param(time_unit)\n+    TimestampBoundMode.verify_param(timestamp_bound_mode)\n+    staleness_value = int(exact_staleness) if exact_staleness else None\n+\n+    if staleness_value or time_unit:\n+      assert staleness_value and time_unit and \\\n+             timestamp_bound_mode == TimestampBoundMode.MAX_STALENESS or \\\n+             timestamp_bound_mode == TimestampBoundMode.EXACT_STALENESS\n+\n+    if read_timestamp:\n+      assert timestamp_bound_mode == TimestampBoundMode.MIN_READ_TIMESTAMP\\\n+             or timestamp_bound_mode == TimestampBoundMode.READ_TIMESTAMP\n+\n+    coders.registry.register_coder(row_type, coders.RowCoder)\n+\n+    super(ReadFromSpanner, self).__init__(\n+        self.URN,\n+        NamedTupleBasedPayloadBuilder(\n+            ReadFromSpannerSchema(\n+                instance_id=instance_id,\n+                database_id=database_id,\n+                sql=sql,\n+                table=table,\n+                schema=named_tuple_to_schema(row_type).SerializeToString(),\n+                project_id=project_id,\n+                host=host,\n+                emulator_host=emulator_host,\n+                batching=batching,\n+                timestamp_bound_mode=timestamp_bound_mode,\n+                read_timestamp=read_timestamp,\n+                exact_staleness=exact_staleness,\n+                time_unit=time_unit,\n+            ),\n+        ),\n+        expansion_service or default_io_expansion_service(),\n+    )\n+\n+\n+class Operation:\n+  INSERT = 'INSERT'\n+  DELETE = 'DELETE'\n+  UPDATE = 'UPDATE'\n+  REPLACE = 'REPLACE'\n+  INSERT_OR_UPDATE = 'INSERT_OR_UPDATE'\n+\n+\n+class TimeUnit:\n+  NANOSECONDS = 'NANOSECONDS'\n+  MICROSECONDS = 'MICROSECONDS'\n+  MILLISECONDS = 'MILLISECONDS'\n+  SECONDS = 'SECONDS'\n+  HOURS = 'HOURS'\n+  DAYS = 'DAYS'\n+\n+  @staticmethod\n+  def verify_param(param):\n+    if param and not hasattr(TimeUnit, param):\n+      raise RuntimeError(\n+          'Invalid param for TimestampBoundMode: {}'.format(param))\n+\n+\n+class TimestampBoundMode:\n+  MAX_STALENESS = 'MAX_STALENESS'\n+  EXACT_STALENESS = 'EXACT_STALENESS'\n+  READ_TIMESTAMP = 'READ_TIMESTAMP'\n+  MIN_READ_TIMESTAMP = 'MIN_READ_TIMESTAMP'\n+  STRONG = 'STRONG'\n+\n+  @staticmethod\n+  def verify_param(param):\n+    if param and not hasattr(TimestampBoundMode, param):\n+      raise RuntimeError(\n+          'Invalid param for TimestampBoundMode: {}'.format(param))\n+\n+\n+class WriteToSpannerTransform(PTransform):\n+  URN = 'beam:external:java:spanner:write:v1'\n+\n+  def __init__(self, config, expansion_service, operation, table):\n+    super(WriteToSpannerTransform, self).__init__()\n+    self.config = config\n+    self.expansion_service = expansion_service\n+    self.operation = operation\n+    self.table = table\n+\n+  def expand(self, row_pcoll):\n+    return (\n+        row_pcoll\n+        | RowToMutation(self.operation, self.table)\n+        | ExternalTransform(self.URN, self.config, self.expansion_service))\n+\n+\n+class RowToMutation(PTransform):\n+  def __init__(self, operation, table):\n+    super(RowToMutation, self).__init__()\n+    self.operation = operation\n+    self.table = table\n+\n+  def expand(self, pcoll):\n+    is_delete = self.operation == Operation.DELETE\n+    mutation_name = 'Mutation_%s_%s' % (\n+        self.operation, str(uuid.uuid4()).replace('-', ''))\n+\n+    # There is an error when pcoll.element_type is List[row_type] so pass\n+    # a list of inner element types to NamedTuple explicitly.\n+    is_list = hasattr(pcoll.element_type, 'inner_type')\n+    row_type = pcoll.element_type.inner_type if is_list else pcoll.element_type\n+    mutation_type = NamedTuple(\n+        mutation_name,\n+        [\n+            ('operation', unicode),\n+            ('table', unicode),\n+            ('keyset', List[row_type]) if is_delete else ('row', row_type),", "originalCommit": "1c43284593c2f7350b67b3ddb1878cb521ced284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5MzYzNQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r505693635", "bodyText": "Done. I'm not sure if you didn't mean to add that convenience to schemas.py. I'm leaving it in spanner.py for now", "author": "piotr-szuberski", "createdAt": "2020-10-15T16:51:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjYzNTIzMA=="}], "type": "inlineReview"}, {"oid": "159def6e6fbf46bf7c7894d1d7008fa4a032e1f7", "url": "https://github.com/apache/beam/commit/159def6e6fbf46bf7c7894d1d7008fa4a032e1f7", "message": "Change test method names", "committedDate": "2020-10-15T16:48:57Z", "type": "forcePushed"}, {"oid": "a120f310a671a484d55b1867d48e840860f1ee12", "url": "https://github.com/apache/beam/commit/a120f310a671a484d55b1867d48e840860f1ee12", "message": "Add missing imports", "committedDate": "2020-10-16T08:23:07Z", "type": "forcePushed"}, {"oid": "8f64294c4505134d1077548047fc486d3c9f12d5", "url": "https://github.com/apache/beam/commit/8f64294c4505134d1077548047fc486d3c9f12d5", "message": "Move tests to apache_beam.io.gcp.tests", "committedDate": "2020-10-26T18:19:45Z", "type": "forcePushed"}, {"oid": "e07de53266518ff4751bf9baf91a85aa8a1bedbd", "url": "https://github.com/apache/beam/commit/e07de53266518ff4751bf9baf91a85aa8a1bedbd", "message": "Move tests to apache_beam.io.gcp.tests", "committedDate": "2020-10-26T18:25:45Z", "type": "forcePushed"}, {"oid": "a5af89435211f3122912f6ee3a85a3d08a8777a7", "url": "https://github.com/apache/beam/commit/a5af89435211f3122912f6ee3a85a3d08a8777a7", "message": "Remove lambdas", "committedDate": "2020-10-26T20:23:57Z", "type": "forcePushed"}, {"oid": "b76a62a12d622ebc523146d587feb786e7d4e29e", "url": "https://github.com/apache/beam/commit/b76a62a12d622ebc523146d587feb786e7d4e29e", "message": "Remove lambdas", "committedDate": "2020-10-26T21:39:21Z", "type": "forcePushed"}, {"oid": "537aa7faf89ed3131bf792384fa0a631f4d931a7", "url": "https://github.com/apache/beam/commit/537aa7faf89ed3131bf792384fa0a631f4d931a7", "message": "change to environment_config = None instead od choosing docker", "committedDate": "2020-10-27T11:25:23Z", "type": "forcePushed"}, {"oid": "46e0f1a632c9ff3416d0a64d1fc166bcc6ffc411", "url": "https://github.com/apache/beam/commit/46e0f1a632c9ff3416d0a64d1fc166bcc6ffc411", "message": "Move spanner tests to apache_beam.io.gcp.tests", "committedDate": "2020-10-27T14:31:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMDE3NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r520210174", "bodyText": "What I had in mind was that there would be a separate URN for each possible write operation like beam:external:java:spanner:delete, beam:external:java:spanner:insert_or_update, ...\nRather than accepting mutations encoded as rows to go over the xlang boundary, each of these transforms would have an input of just  PCollection<Row> representing the actual data (or PCollection<List<Row>> representing the keyset in the Delete case). Then python doesn't even need to have a concept of mutations.\nThere's definitely value in a generic beam:external:java:spanner:write transform which accepts arbitrary mutations, but I think we should leave that for future work.", "author": "TheNeuralBit", "createdAt": "2020-11-10T00:36:11Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/SpannerTransformRegistrar.java", "diffHunk": "@@ -0,0 +1,287 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.service.AutoService;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.TimestampBound;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.model.pipeline.v1.SchemaApi;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.expansion.ExternalTransformRegistrar;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaTranslation;\n+import org.apache.beam.sdk.transforms.ExternalTransformBuilder;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.InvalidProtocolBufferException;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+\n+/**\n+ * Exposes {@link SpannerIO.WriteRows} and {@link SpannerIO.ReadRows} as an external transform for\n+ * cross-language usage.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoService(ExternalTransformRegistrar.class)\n+public class SpannerTransformRegistrar implements ExternalTransformRegistrar {\n+  public static final String WRITE_URN = \"beam:external:java:spanner:write:v1\";\n+  public static final String READ_URN = \"beam:external:java:spanner:read:v1\";\n+\n+  @Override\n+  public Map<String, ExternalTransformBuilder<?, ?, ?>> knownBuilderInstances() {\n+    return ImmutableMap.of(WRITE_URN, new WriteBuilder(), READ_URN, new ReadBuilder());", "originalCommit": "356788f69b56cd7ead4c95c94d589dcea71b7e20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA1MjkxNQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r522052915", "bodyText": "That makes sense. Done.", "author": "piotr-szuberski", "createdAt": "2020-11-12T11:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMDE3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMjI4Ng==", "url": "https://github.com/apache/beam/pull/12611#discussion_r520212286", "bodyText": "As pointed out in the last comment I think it would be preferable if Python didn't even need to have a concept of Mutations (for now). Instead it just sends the Rows (or Keysets) over to Java, which can wrap them in mutations for use in SpannerIO", "author": "TheNeuralBit", "createdAt": "2020-11-10T00:42:41Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,662 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.26.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import uuid\n+from enum import Enum\n+from enum import auto\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import Map\n+from apache_beam import PTransform\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_from_schema\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+from apache_beam.typehints.schemas import schema_from_element_type\n+\n+__all__ = [\n+    'ReadFromSpanner',\n+    'SpannerDelete',\n+    'SpannerInsert',\n+    'SpannerInsertOrUpdate',\n+    'SpannerReplace',\n+    'SpannerUpdate',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+_READ_URN = 'beam:external:java:spanner:read:v1'\n+_WRITE_URN = 'beam:external:java:spanner:write:v1'\n+\n+\n+class TimeUnit(Enum):\n+  NANOSECONDS = auto()\n+  MICROSECONDS = auto()\n+  MILLISECONDS = auto()\n+  SECONDS = auto()\n+  HOURS = auto()\n+  DAYS = auto()\n+\n+\n+class TimestampBoundMode(Enum):\n+  MAX_STALENESS = auto()\n+  EXACT_STALENESS = auto()\n+  READ_TIMESTAMP = auto()\n+  MIN_READ_TIMESTAMP = auto()\n+  STRONG = auto()\n+\n+\n+class ReadFromSpannerSchema(NamedTuple):\n+  instance_id: unicode\n+  database_id: unicode\n+  schema: bytes\n+  sql: Optional[unicode]\n+  table: Optional[unicode]\n+  project_id: Optional[unicode]\n+  host: Optional[unicode]\n+  emulator_host: Optional[unicode]\n+  batching: Optional[bool]\n+  timestamp_bound_mode: Optional[unicode]\n+  read_timestamp: Optional[unicode]\n+  exact_staleness: Optional[int]\n+  time_unit: Optional[unicode]\n+\n+\n+class ReadFromSpanner(ExternalTransform):\n+  \"\"\"\n+  A PTransform which reads from the specified Spanner instance's database.\n+\n+  This transform required type of the row it has to return to provide the\n+  schema. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      result = (\n+          p\n+          | ReadFromSpanner(\n+              instance_id='your_instance_id',\n+              database_id='your_database_id',\n+              project_id='your_project_id',\n+              row_type=ExampleRow,\n+              query='SELECT * FROM some_table',\n+              timestamp_bound_mode=TimestampBoundMode.MAX_STALENESS,\n+              exact_staleness=3,\n+              time_unit=TimeUnit.HOURS,\n+          ).with_output_types(ExampleRow))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      row_type=None,\n+      sql=None,\n+      table=None,\n+      host=None,\n+      emulator_host=None,\n+      batching=None,\n+      timestamp_bound_mode=None,\n+      read_timestamp=None,\n+      exact_staleness=None,\n+      time_unit=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a read operation from Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param row_type: Row type that fits the given query or table. Passed as\n+        NamedTuple, e.g. NamedTuple('name', [('row_name', unicode)])\n+    :param sql: An sql query to execute. It's results must fit the\n+        provided row_type. Don't use when table is set.\n+    :param table: A spanner table. When provided all columns from row_type\n+        will be selected to query. Don't use when query is set.\n+    :param batching: By default Batch API is used to read data from Cloud\n+        Spanner. It is useful to disable batching when the underlying query\n+        is not root-partitionable.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param timestamp_bound_mode: Defines how Cloud Spanner will choose a\n+        timestamp for a read-only transaction or a single read/query.\n+        Passed as TimestampBoundMode enum. Possible values:\n+        STRONG: A timestamp bound that will perform reads and queries at a\n+        timestamp where all previously committed transactions are visible.\n+        READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at the given timestamp.\n+        MIN_READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at a timestamp chosen to be at least given timestamp value.\n+        EXACT_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at an exact staleness. The timestamp is chosen soon after the\n+        read is started.\n+        MAX_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at a timestamp chosen to be at most time_unit stale.\n+    :param read_timestamp: Timestamp in string. Use only when\n+        timestamp_bound_mode is set to READ_TIMESTAMP or MIN_READ_TIMESTAMP.\n+    :param exact_staleness: Staleness value as int. Use only when\n+        timestamp_bound_mode is set to EXACT_STALENESS or MAX_STALENESS.\n+        time_unit has to be set along with this param.\n+    :param time_unit: Time unit for staleness_value passed as TimeUnit enum.\n+        Possible values: NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS,\n+        HOURS, DAYS.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    assert row_type\n+    assert sql or table and not (sql and table)\n+    staleness_value = int(exact_staleness) if exact_staleness else None\n+\n+    if staleness_value or time_unit:\n+      assert staleness_value and time_unit and \\\n+             timestamp_bound_mode is TimestampBoundMode.MAX_STALENESS or \\\n+             timestamp_bound_mode is TimestampBoundMode.EXACT_STALENESS\n+\n+    if read_timestamp:\n+      assert timestamp_bound_mode is TimestampBoundMode.MIN_READ_TIMESTAMP\\\n+             or timestamp_bound_mode is TimestampBoundMode.READ_TIMESTAMP\n+\n+    coders.registry.register_coder(row_type, coders.RowCoder)\n+\n+    super(ReadFromSpanner, self).__init__(\n+        _READ_URN,\n+        NamedTupleBasedPayloadBuilder(\n+            ReadFromSpannerSchema(\n+                instance_id=instance_id,\n+                database_id=database_id,\n+                sql=sql,\n+                table=table,\n+                schema=named_tuple_to_schema(row_type).SerializeToString(),\n+                project_id=project_id,\n+                host=host,\n+                emulator_host=emulator_host,\n+                batching=batching,\n+                timestamp_bound_mode=_get_enum_name(timestamp_bound_mode),\n+                read_timestamp=read_timestamp,\n+                exact_staleness=exact_staleness,\n+                time_unit=_get_enum_name(time_unit),\n+            ),\n+        ),\n+        expansion_service or default_io_expansion_service(),\n+    )\n+\n+\n+class WriteToSpannerSchema(NamedTuple):\n+  project_id: unicode\n+  instance_id: unicode\n+  database_id: unicode\n+  max_batch_size_bytes: Optional[int]\n+  max_number_mutations: Optional[int]\n+  max_number_rows: Optional[int]\n+  grouping_factor: Optional[int]\n+  host: Optional[unicode]\n+  emulator_host: Optional[unicode]\n+  commit_deadline: Optional[int]\n+  max_cumulative_backoff: Optional[int]\n+\n+\n+_CLASS_DOC = \\\n+  \"\"\"\n+  A PTransform which writes {0} mutations to the specified Spanner table.\n+\n+  This transform receives rows defined as NamedTuple. Example::\n+\n+    {1} = typing.NamedTuple('{1}',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: {1}(n, str(n))\n+              .with_output_types({2})\n+          | 'Write to Spanner' >> Spanner{3}(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id',\n+              table='your_table'))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+\n+_INIT_DOC = \\\n+  \"\"\"\n+  Initializes {} operation to a Spanner table.\n+\n+  :param project_id: Specifies the Cloud Spanner project.\n+  :param instance_id: Specifies the Cloud Spanner instance.\n+  :param database_id: Specifies the Cloud Spanner database.\n+  :param table: Specifies the Cloud Spanner table.\n+  :param max_batch_size_bytes: Specifies the batch size limit (max number of\n+      bytes mutated per batch). Default value is 1048576 bytes = 1MB.\n+  :param max_number_mutations: Specifies the cell mutation limit (maximum\n+      number of mutated cells per batch). Default value is 5000.\n+  :param max_number_rows: Specifies the row mutation limit (maximum number of\n+      mutated rows per batch). Default value is 500.\n+  :param grouping_factor: Specifies the multiple of max mutation (in terms\n+      of both bytes per batch and cells per batch) that is used to select a\n+      set of mutations to sort by key for batching. This sort uses local\n+      memory on the workers, so using large values can cause out of memory\n+      errors. Default value is 1000.\n+  :param host: Specifies the Cloud Spanner host.\n+  :param emulator_host: Specifies Spanner emulator host.\n+  :param commit_deadline: Specifies the deadline for the Commit API call.\n+      Default is 15 secs. DEADLINE_EXCEEDED errors will prompt a backoff/retry\n+      until the value of commit_deadline is reached. DEADLINE_EXCEEDED errors\n+      are ar reported with logging and counters. Pass seconds as value.\n+  :param max_cumulative_backoff: Specifies the maximum cumulative backoff\n+      time when retrying after DEADLINE_EXCEEDED errors. Default is 900s\n+      (15min). If the mutations still have not been written after this time,\n+      they are treated as a failure, and handled according to the setting of\n+      failure_mode. Pass seconds as value.\n+  :param expansion_service: The address (host:port) of the ExpansionService.\n+  \"\"\"\n+\n+\n+def _add_doc(value, *args):\n+  def _doc(obj):\n+    obj.__doc__ = value.format(*args)\n+    return obj\n+\n+  return _doc\n+\n+\n+@_add_doc(_CLASS_DOC, 'delete', 'ExampleKey', 'List[ExampleKey]', 'Delete')\n+class SpannerDelete(PTransform):\n+  @_add_doc(_INIT_DOC, 'a delete')\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      table,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    super().__init__()\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.table = table\n+    self.params = WriteToSpannerSchema(\n+        project_id=project_id,\n+        instance_id=instance_id,\n+        database_id=database_id,\n+        max_batch_size_bytes=max_batch_size_bytes,\n+        max_number_mutations=max_number_mutations,\n+        max_number_rows=max_number_rows,\n+        grouping_factor=grouping_factor,\n+        host=host,\n+        emulator_host=emulator_host,\n+        commit_deadline=commit_deadline,\n+        max_cumulative_backoff=max_cumulative_backoff,\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def expand(self, pbegin):\n+    return _apply_write_transform(\n+        pbegin,\n+        _RowToMutation(_Operation.DELETE, self.table),\n+        self.params,\n+        self.expansion_service)\n+\n+\n+@_add_doc(_CLASS_DOC, 'insert', 'ExampleRow', 'ExampleRow', 'Insert')\n+class SpannerInsert(PTransform):\n+  @_add_doc(_INIT_DOC, 'an insert')\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      table,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    super().__init__()\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.table = table\n+    self.params = WriteToSpannerSchema(\n+        project_id=project_id,\n+        instance_id=instance_id,\n+        database_id=database_id,\n+        max_batch_size_bytes=max_batch_size_bytes,\n+        max_number_mutations=max_number_mutations,\n+        max_number_rows=max_number_rows,\n+        grouping_factor=grouping_factor,\n+        host=host,\n+        emulator_host=emulator_host,\n+        commit_deadline=commit_deadline,\n+        max_cumulative_backoff=max_cumulative_backoff,\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def expand(self, pbegin):\n+    return _apply_write_transform(\n+        pbegin,\n+        _RowToMutation(_Operation.INSERT, self.table),\n+        self.params,\n+        self.expansion_service)\n+\n+\n+@_add_doc(_CLASS_DOC, 'replace', 'ExampleRow', 'ExampleRow', 'Replace')\n+class SpannerReplace(PTransform):\n+  @_add_doc(_INIT_DOC, 'a replace')\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      table,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    super().__init__()\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.table = table\n+    self.params = WriteToSpannerSchema(\n+        project_id=project_id,\n+        instance_id=instance_id,\n+        database_id=database_id,\n+        max_batch_size_bytes=max_batch_size_bytes,\n+        max_number_mutations=max_number_mutations,\n+        max_number_rows=max_number_rows,\n+        grouping_factor=grouping_factor,\n+        host=host,\n+        emulator_host=emulator_host,\n+        commit_deadline=commit_deadline,\n+        max_cumulative_backoff=max_cumulative_backoff,\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def expand(self, pbegin):\n+    return _apply_write_transform(\n+        pbegin,\n+        _RowToMutation(_Operation.REPLACE, self.table),\n+        self.params,\n+        self.expansion_service)\n+\n+\n+@_add_doc(\n+    _CLASS_DOC,\n+    'insert-or-update',\n+    'ExampleRow',\n+    'ExampleRow',\n+    'InsertOrUpdate')\n+class SpannerInsertOrUpdate(PTransform):\n+  @_add_doc(_INIT_DOC, 'an insert-or-update')\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      table,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    super().__init__()\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.table = table\n+    self.params = WriteToSpannerSchema(\n+        project_id=project_id,\n+        instance_id=instance_id,\n+        database_id=database_id,\n+        max_batch_size_bytes=max_batch_size_bytes,\n+        max_number_mutations=max_number_mutations,\n+        max_number_rows=max_number_rows,\n+        grouping_factor=grouping_factor,\n+        host=host,\n+        emulator_host=emulator_host,\n+        commit_deadline=commit_deadline,\n+        max_cumulative_backoff=max_cumulative_backoff,\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def expand(self, pbegin):\n+    return _apply_write_transform(\n+        pbegin,\n+        _RowToMutation(_Operation.INSERT_OR_UPDATE, self.table),\n+        self.params,\n+        self.expansion_service)\n+\n+\n+@_add_doc(_CLASS_DOC, 'update', 'ExampleRow', 'ExampleRow', 'Update')\n+class SpannerUpdate(PTransform):\n+  @_add_doc(_INIT_DOC, 'an update')\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      table,\n+      max_batch_size_bytes=None,\n+      max_number_mutations=None,\n+      max_number_rows=None,\n+      grouping_factor=None,\n+      host=None,\n+      emulator_host=None,\n+      commit_deadline=None,\n+      max_cumulative_backoff=None,\n+      expansion_service=None,\n+  ):\n+    super().__init__()\n+    max_cumulative_backoff = int(\n+        max_cumulative_backoff) if max_cumulative_backoff else None\n+    commit_deadline = int(commit_deadline) if commit_deadline else None\n+    self.table = table\n+    self.params = WriteToSpannerSchema(\n+        project_id=project_id,\n+        instance_id=instance_id,\n+        database_id=database_id,\n+        max_batch_size_bytes=max_batch_size_bytes,\n+        max_number_mutations=max_number_mutations,\n+        max_number_rows=max_number_rows,\n+        grouping_factor=grouping_factor,\n+        host=host,\n+        emulator_host=emulator_host,\n+        commit_deadline=commit_deadline,\n+        max_cumulative_backoff=max_cumulative_backoff,\n+    )\n+    self.expansion_service = expansion_service or default_io_expansion_service()\n+\n+  def expand(self, pbegin):\n+    return _apply_write_transform(\n+        pbegin,\n+        _RowToMutation(_Operation.UPDATE, self.table),\n+        self.params,\n+        self.expansion_service)\n+\n+\n+def _apply_write_transform(pbegin, to_mutation, params, expansion_service):\n+  return (\n+      pbegin\n+      | to_mutation\n+      | ExternalTransform(\n+          _WRITE_URN, NamedTupleBasedPayloadBuilder(params), expansion_service))\n+\n+\n+class _RowToMutation(PTransform):", "originalCommit": "356788f69b56cd7ead4c95c94d589dcea71b7e20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA1NDAyOQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r522054029", "bodyText": "Done. I've replaced keysets with keys - it's horrible to distinguish List from Row in runtime. With rows/keys only it's way cleaner.", "author": "piotr-szuberski", "createdAt": "2020-11-12T12:00:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMjI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMjY0Ng==", "url": "https://github.com/apache/beam/pull/12611#discussion_r520212646", "bodyText": "nice job keeping this concise \ud83d\udc4d\nMy only nit is that it would be a bit more readable if you used keyword args and had named parameters in the docstring template.", "author": "TheNeuralBit", "createdAt": "2020-11-10T00:43:52Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,662 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.26.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import uuid\n+from enum import Enum\n+from enum import auto\n+from typing import List\n+from typing import NamedTuple\n+from typing import Optional\n+\n+from past.builtins import unicode\n+\n+from apache_beam import Map\n+from apache_beam import PTransform\n+from apache_beam import coders\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+from apache_beam.typehints.schemas import named_tuple_from_schema\n+from apache_beam.typehints.schemas import named_tuple_to_schema\n+from apache_beam.typehints.schemas import schema_from_element_type\n+\n+__all__ = [\n+    'ReadFromSpanner',\n+    'SpannerDelete',\n+    'SpannerInsert',\n+    'SpannerInsertOrUpdate',\n+    'SpannerReplace',\n+    'SpannerUpdate',\n+    'TimestampBoundMode',\n+    'TimeUnit',\n+]\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService(\n+      'sdks:java:io:google-cloud-platform:expansion-service:shadowJar')\n+\n+\n+_READ_URN = 'beam:external:java:spanner:read:v1'\n+_WRITE_URN = 'beam:external:java:spanner:write:v1'\n+\n+\n+class TimeUnit(Enum):\n+  NANOSECONDS = auto()\n+  MICROSECONDS = auto()\n+  MILLISECONDS = auto()\n+  SECONDS = auto()\n+  HOURS = auto()\n+  DAYS = auto()\n+\n+\n+class TimestampBoundMode(Enum):\n+  MAX_STALENESS = auto()\n+  EXACT_STALENESS = auto()\n+  READ_TIMESTAMP = auto()\n+  MIN_READ_TIMESTAMP = auto()\n+  STRONG = auto()\n+\n+\n+class ReadFromSpannerSchema(NamedTuple):\n+  instance_id: unicode\n+  database_id: unicode\n+  schema: bytes\n+  sql: Optional[unicode]\n+  table: Optional[unicode]\n+  project_id: Optional[unicode]\n+  host: Optional[unicode]\n+  emulator_host: Optional[unicode]\n+  batching: Optional[bool]\n+  timestamp_bound_mode: Optional[unicode]\n+  read_timestamp: Optional[unicode]\n+  exact_staleness: Optional[int]\n+  time_unit: Optional[unicode]\n+\n+\n+class ReadFromSpanner(ExternalTransform):\n+  \"\"\"\n+  A PTransform which reads from the specified Spanner instance's database.\n+\n+  This transform required type of the row it has to return to provide the\n+  schema. Example::\n+\n+    ExampleRow = typing.NamedTuple('ExampleRow',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      result = (\n+          p\n+          | ReadFromSpanner(\n+              instance_id='your_instance_id',\n+              database_id='your_database_id',\n+              project_id='your_project_id',\n+              row_type=ExampleRow,\n+              query='SELECT * FROM some_table',\n+              timestamp_bound_mode=TimestampBoundMode.MAX_STALENESS,\n+              exact_staleness=3,\n+              time_unit=TimeUnit.HOURS,\n+          ).with_output_types(ExampleRow))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      project_id,\n+      instance_id,\n+      database_id,\n+      row_type=None,\n+      sql=None,\n+      table=None,\n+      host=None,\n+      emulator_host=None,\n+      batching=None,\n+      timestamp_bound_mode=None,\n+      read_timestamp=None,\n+      exact_staleness=None,\n+      time_unit=None,\n+      expansion_service=None,\n+  ):\n+    \"\"\"\n+    Initializes a read operation from Spanner.\n+\n+    :param project_id: Specifies the Cloud Spanner project.\n+    :param instance_id: Specifies the Cloud Spanner instance.\n+    :param database_id: Specifies the Cloud Spanner database.\n+    :param row_type: Row type that fits the given query or table. Passed as\n+        NamedTuple, e.g. NamedTuple('name', [('row_name', unicode)])\n+    :param sql: An sql query to execute. It's results must fit the\n+        provided row_type. Don't use when table is set.\n+    :param table: A spanner table. When provided all columns from row_type\n+        will be selected to query. Don't use when query is set.\n+    :param batching: By default Batch API is used to read data from Cloud\n+        Spanner. It is useful to disable batching when the underlying query\n+        is not root-partitionable.\n+    :param host: Specifies the Cloud Spanner host.\n+    :param emulator_host: Specifies Spanner emulator host.\n+    :param timestamp_bound_mode: Defines how Cloud Spanner will choose a\n+        timestamp for a read-only transaction or a single read/query.\n+        Passed as TimestampBoundMode enum. Possible values:\n+        STRONG: A timestamp bound that will perform reads and queries at a\n+        timestamp where all previously committed transactions are visible.\n+        READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at the given timestamp.\n+        MIN_READ_TIMESTAMP: Returns a timestamp bound that will perform reads\n+        and queries at a timestamp chosen to be at least given timestamp value.\n+        EXACT_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at an exact staleness. The timestamp is chosen soon after the\n+        read is started.\n+        MAX_STALENESS: Returns a timestamp bound that will perform reads and\n+        queries at a timestamp chosen to be at most time_unit stale.\n+    :param read_timestamp: Timestamp in string. Use only when\n+        timestamp_bound_mode is set to READ_TIMESTAMP or MIN_READ_TIMESTAMP.\n+    :param exact_staleness: Staleness value as int. Use only when\n+        timestamp_bound_mode is set to EXACT_STALENESS or MAX_STALENESS.\n+        time_unit has to be set along with this param.\n+    :param time_unit: Time unit for staleness_value passed as TimeUnit enum.\n+        Possible values: NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS,\n+        HOURS, DAYS.\n+    :param expansion_service: The address (host:port) of the ExpansionService.\n+    \"\"\"\n+    assert row_type\n+    assert sql or table and not (sql and table)\n+    staleness_value = int(exact_staleness) if exact_staleness else None\n+\n+    if staleness_value or time_unit:\n+      assert staleness_value and time_unit and \\\n+             timestamp_bound_mode is TimestampBoundMode.MAX_STALENESS or \\\n+             timestamp_bound_mode is TimestampBoundMode.EXACT_STALENESS\n+\n+    if read_timestamp:\n+      assert timestamp_bound_mode is TimestampBoundMode.MIN_READ_TIMESTAMP\\\n+             or timestamp_bound_mode is TimestampBoundMode.READ_TIMESTAMP\n+\n+    coders.registry.register_coder(row_type, coders.RowCoder)\n+\n+    super(ReadFromSpanner, self).__init__(\n+        _READ_URN,\n+        NamedTupleBasedPayloadBuilder(\n+            ReadFromSpannerSchema(\n+                instance_id=instance_id,\n+                database_id=database_id,\n+                sql=sql,\n+                table=table,\n+                schema=named_tuple_to_schema(row_type).SerializeToString(),\n+                project_id=project_id,\n+                host=host,\n+                emulator_host=emulator_host,\n+                batching=batching,\n+                timestamp_bound_mode=_get_enum_name(timestamp_bound_mode),\n+                read_timestamp=read_timestamp,\n+                exact_staleness=exact_staleness,\n+                time_unit=_get_enum_name(time_unit),\n+            ),\n+        ),\n+        expansion_service or default_io_expansion_service(),\n+    )\n+\n+\n+class WriteToSpannerSchema(NamedTuple):\n+  project_id: unicode\n+  instance_id: unicode\n+  database_id: unicode\n+  max_batch_size_bytes: Optional[int]\n+  max_number_mutations: Optional[int]\n+  max_number_rows: Optional[int]\n+  grouping_factor: Optional[int]\n+  host: Optional[unicode]\n+  emulator_host: Optional[unicode]\n+  commit_deadline: Optional[int]\n+  max_cumulative_backoff: Optional[int]\n+\n+\n+_CLASS_DOC = \\\n+  \"\"\"\n+  A PTransform which writes {0} mutations to the specified Spanner table.\n+\n+  This transform receives rows defined as NamedTuple. Example::\n+\n+    {1} = typing.NamedTuple('{1}',\n+                                   [('id', int), ('name', unicode)])\n+\n+    with Pipeline() as p:\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(num_rows))\n+          | 'To row' >> beam.Map(lambda n: {1}(n, str(n))\n+              .with_output_types({2})\n+          | 'Write to Spanner' >> Spanner{3}(\n+              instance_id='your_instance',\n+              database_id='existing_database',\n+              project_id='your_project_id',\n+              table='your_table'))\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+\n+_INIT_DOC = \\\n+  \"\"\"\n+  Initializes {} operation to a Spanner table.\n+\n+  :param project_id: Specifies the Cloud Spanner project.\n+  :param instance_id: Specifies the Cloud Spanner instance.\n+  :param database_id: Specifies the Cloud Spanner database.\n+  :param table: Specifies the Cloud Spanner table.\n+  :param max_batch_size_bytes: Specifies the batch size limit (max number of\n+      bytes mutated per batch). Default value is 1048576 bytes = 1MB.\n+  :param max_number_mutations: Specifies the cell mutation limit (maximum\n+      number of mutated cells per batch). Default value is 5000.\n+  :param max_number_rows: Specifies the row mutation limit (maximum number of\n+      mutated rows per batch). Default value is 500.\n+  :param grouping_factor: Specifies the multiple of max mutation (in terms\n+      of both bytes per batch and cells per batch) that is used to select a\n+      set of mutations to sort by key for batching. This sort uses local\n+      memory on the workers, so using large values can cause out of memory\n+      errors. Default value is 1000.\n+  :param host: Specifies the Cloud Spanner host.\n+  :param emulator_host: Specifies Spanner emulator host.\n+  :param commit_deadline: Specifies the deadline for the Commit API call.\n+      Default is 15 secs. DEADLINE_EXCEEDED errors will prompt a backoff/retry\n+      until the value of commit_deadline is reached. DEADLINE_EXCEEDED errors\n+      are ar reported with logging and counters. Pass seconds as value.\n+  :param max_cumulative_backoff: Specifies the maximum cumulative backoff\n+      time when retrying after DEADLINE_EXCEEDED errors. Default is 900s\n+      (15min). If the mutations still have not been written after this time,\n+      they are treated as a failure, and handled according to the setting of\n+      failure_mode. Pass seconds as value.\n+  :param expansion_service: The address (host:port) of the ExpansionService.\n+  \"\"\"\n+\n+\n+def _add_doc(value, *args):\n+  def _doc(obj):\n+    obj.__doc__ = value.format(*args)\n+    return obj\n+\n+  return _doc\n+\n+\n+@_add_doc(_CLASS_DOC, 'delete', 'ExampleKey', 'List[ExampleKey]', 'Delete')", "originalCommit": "356788f69b56cd7ead4c95c94d589dcea71b7e20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjA1MzAxOA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r522053018", "bodyText": "Done", "author": "piotr-szuberski", "createdAt": "2020-11-12T11:59:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMjY0Ng=="}], "type": "inlineReview"}, {"oid": "1602c262fd2d8ba1f008c9bda32d99d5b0dd21b3", "url": "https://github.com/apache/beam/commit/1602c262fd2d8ba1f008c9bda32d99d5b0dd21b3", "message": "[BEAM-10139][BEAM-10140] Add Support for cross-language transforms with python wrapper to Java SpannerIO", "committedDate": "2020-11-12T08:54:25Z", "type": "commit"}, {"oid": "1602c262fd2d8ba1f008c9bda32d99d5b0dd21b3", "url": "https://github.com/apache/beam/commit/1602c262fd2d8ba1f008c9bda32d99d5b0dd21b3", "message": "[BEAM-10139][BEAM-10140] Add Support for cross-language transforms with python wrapper to Java SpannerIO", "committedDate": "2020-11-12T08:54:25Z", "type": "forcePushed"}, {"oid": "17248f3fa0871e2b193dd8d56ce89f0eda3b4fb6", "url": "https://github.com/apache/beam/commit/17248f3fa0871e2b193dd8d56ce89f0eda3b4fb6", "message": "Change docstrings to use named parameters", "committedDate": "2020-11-12T09:40:13Z", "type": "commit"}, {"oid": "9efa8c7106b2dfd3838bf9c105956e1ed8e93551", "url": "https://github.com/apache/beam/commit/9efa8c7106b2dfd3838bf9c105956e1ed8e93551", "message": "Update Read docstring", "committedDate": "2020-11-12T14:33:15Z", "type": "forcePushed"}, {"oid": "72286222229f5984b7244722f15b7dac3ae7ed22", "url": "https://github.com/apache/beam/commit/72286222229f5984b7244722f15b7dac3ae7ed22", "message": "Remove mutation row concept from write operations", "committedDate": "2020-11-12T14:49:42Z", "type": "forcePushed"}, {"oid": "3a153aedd7220ba962e74d081758b2eda2ee825f", "url": "https://github.com/apache/beam/commit/3a153aedd7220ba962e74d081758b2eda2ee825f", "message": "Remove mutation row concept from write operations", "committedDate": "2020-11-12T18:20:58Z", "type": "commit"}, {"oid": "3a153aedd7220ba962e74d081758b2eda2ee825f", "url": "https://github.com/apache/beam/commit/3a153aedd7220ba962e74d081758b2eda2ee825f", "message": "Remove mutation row concept from write operations", "committedDate": "2020-11-12T18:20:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2ODUyMA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r522468520", "bodyText": "This information is getting duplicated across a lot of docstrings. It looks like #13317 will actually add similar information to the programming guide. I think we should re-write all these docstrings to refer to that once its complete.", "author": "TheNeuralBit", "createdAt": "2020-11-12T22:22:08Z", "path": "sdks/python/apache_beam/io/gcp/spanner.py", "diffHunk": "@@ -0,0 +1,635 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"PTransforms for supporting Spanner in Python pipelines.\n+\n+  These transforms are currently supported by Beam portable\n+  Flink and Spark runners.\n+\n+  **Setup**\n+\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+\n+  There are several ways to setup cross-language Spanner transforms.\n+\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+\n+  See below for details regarding each of these options.\n+\n+  *Option 1: Use the default expansion service*\n+\n+  This is the recommended and easiest setup option for using Python Spanner\n+  transforms. This option is only available for Beam 2.26.0 and later.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Install Java runtime in the computer from where the pipeline is constructed\n+    and make sure that 'java' command is available.\n+\n+  In this option, Python SDK will either download (for released Beam version) or\n+  build (when running from a Beam Git clone) a expansion service jar and use\n+  that to expand transforms. Currently Spanner transforms use the\n+  'beam-sdks-java-io-google-cloud-platform-expansion-service' jar for this\n+  purpose.\n+\n+  *Option 2: specify a custom expansion service*\n+\n+  In this option, you startup your own expansion service and provide that as\n+  a parameter when using the transforms provided in this module.\n+\n+  This option requires following pre-requisites before running the Beam\n+  pipeline.\n+\n+  * Startup your own expansion service.\n+  * Update your pipeline to provide the expansion service address when\n+    initiating Spanner transforms provided in this module.\n+\n+  Flink Users can use the built-in Expansion Service of the Flink Runner's\n+  Job Server. If you start Flink's Job Server, the expansion service will be\n+  started on port 8097. For a different address, please set the\n+  expansion_service parameter.\n+\n+  **More information**\n+\n+  For more information regarding cross-language transforms see:\n+  - https://beam.apache.org/roadmap/portability/\n+\n+  For more information specific to Flink runner see:\n+  - https://beam.apache.org/documentation/runners/flink/", "originalCommit": "3a153aedd7220ba962e74d081758b2eda2ee825f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAxNzc0NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r523017744", "bodyText": "I agree - it refers to all the existing xlang transforms, so it'll be done in another PR?", "author": "piotr-szuberski", "createdAt": "2020-11-13T15:19:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2ODUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ3MTc2MQ==", "url": "https://github.com/apache/beam/pull/12611#discussion_r524471761", "bodyText": "Yeah it can be done in another PR. Filed BEAM-11269 to track this.", "author": "TheNeuralBit", "createdAt": "2020-11-16T18:08:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2ODUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ3MDkyOA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r522470928", "bodyText": "Could you try to address any lingering nullness errors here and in the other files that have it suppressed? If there are any intractable issues we could consider a smaller @SuppressWarnings blocks around a few functions, but in general we should make sure that new classes pass the null checker.", "author": "TheNeuralBit", "createdAt": "2020-11-12T22:25:32Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+\n+@SuppressWarnings({\n+  \"nullness\" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)\n+})", "originalCommit": "3a153aedd7220ba962e74d081758b2eda2ee825f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAyNDc4OA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r523024788", "bodyText": "Done. Oh, it was quite painful as all of the row getters return a @nullable value. Especially that checkNotNull doesn't work with the checker and there is even no possibility to check for null in a function (only if (var == null) { throw new NullPointerException(\"Null var\"); } seem to work.\nIt doesn't even work in chained functions as in this example:\n@Nullable Object var = new Object();\nif (var != null) {\n  someObject.doSth().doChained(var); // checker doesn't understand that var is checked for nullness)\n}\n\nSo it's quite unfriendly. In general I'm really excited about dealing with NPE problem, but for now it adds much more complexity and reduces the contributor friendliness. But I guess that it's worth it, especially when the checker gets smarter and will work with the Guava checks and chained functions (if it's even possible?)", "author": "piotr-szuberski", "createdAt": "2020-11-13T15:30:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ3MDkyOA=="}], "type": "inlineReview"}, {"oid": "d9a8342e1e702b84dfc2edafb697dff3af58a968", "url": "https://github.com/apache/beam/commit/d9a8342e1e702b84dfc2edafb697dff3af58a968", "message": "exactStaleness -> staleness", "committedDate": "2020-11-13T14:10:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA5MjA3Mw==", "url": "https://github.com/apache/beam/pull/12611#discussion_r523092073", "bodyText": "Let's make these null checks use org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull, that will be a bit more concise, and it avoids throwing an NPE", "author": "TheNeuralBit", "createdAt": "2020-11-13T17:08:28Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/spanner/StructUtils.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.spanner;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+import com.google.cloud.ByteArray;\n+import com.google.cloud.Timestamp;\n+import com.google.cloud.spanner.Struct;\n+import com.google.cloud.spanner.Type;\n+import java.math.BigDecimal;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.values.Row;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.DateTime;\n+import org.joda.time.Instant;\n+import org.joda.time.ReadableDateTime;\n+\n+final class StructUtils {\n+  public static Row structToBeamRow(Struct struct, Schema schema) {\n+    Map<String, Object> structValues =\n+        schema.getFields().stream()\n+            .collect(\n+                HashMap::new,\n+                (map, field) -> {\n+                  @Nullable Object structValue = getStructValue(struct, field);\n+                  if (structValue == null) {\n+                    throw new NullPointerException(\"Null struct value at field \" + field.getName());\n+                  }\n+                  map.put(field.getName(), structValue);\n+                },\n+                Map::putAll);\n+    return Row.withSchema(schema).withFieldValues(structValues).build();\n+  }\n+\n+  public static Struct beamRowToStruct(Row row) {\n+    Struct.Builder structBuilder = Struct.newBuilder();\n+    List<Schema.Field> fields = row.getSchema().getFields();\n+    fields.forEach(\n+        field -> {\n+          String column = field.getName();\n+          switch (field.getType().getTypeName()) {\n+            case ROW:\n+              @Nullable Row subRow = row.getRow(column);\n+              if (subRow == null) {\n+                throw new NullPointerException(String.format(\"Null subRow at '%s' column\", column));\n+              }", "originalCommit": "7eb457634c3f52fec3435ab0f185d47cc0644cb3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEzMjk5OA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r523132998", "bodyText": "Guava's checkNotNull doesn't work - checker doesn't consider fields checked this way as not nulls. It also isn't expected to throw anything so I get missing return statements in the switch statements. So for now I'd leave it throwing NPE.", "author": "piotr-szuberski", "createdAt": "2020-11-13T18:07:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA5MjA3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ2ODY2NA==", "url": "https://github.com/apache/beam/pull/12611#discussion_r524468664", "bodyText": "Ack, ok", "author": "TheNeuralBit", "createdAt": "2020-11-16T18:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA5MjA3Mw=="}], "type": "inlineReview"}, {"oid": "2e1ad017fd87689888e59f13b6cb55e03b8a4fae", "url": "https://github.com/apache/beam/commit/2e1ad017fd87689888e59f13b6cb55e03b8a4fae", "message": "Add nulls instead of throwing NPE", "committedDate": "2020-11-13T18:11:57Z", "type": "forcePushed"}, {"oid": "d05e0e49b115efccfc935a9ff481a19e6a7f0b7f", "url": "https://github.com/apache/beam/commit/d05e0e49b115efccfc935a9ff481a19e6a7f0b7f", "message": "Add nulls instead of throwing NPE", "committedDate": "2020-11-13T20:46:35Z", "type": "forcePushed"}, {"oid": "aa73ae144a0c7ecb754a35082c86bfff841d4ad7", "url": "https://github.com/apache/beam/commit/aa73ae144a0c7ecb754a35082c86bfff841d4ad7", "message": "Add null values tests", "committedDate": "2020-11-14T04:52:10Z", "type": "forcePushed"}, {"oid": "315ef36141c947890ba9ec40dba39ab20107edad", "url": "https://github.com/apache/beam/commit/315ef36141c947890ba9ec40dba39ab20107edad", "message": "Deal with nullness", "committedDate": "2020-11-14T06:27:38Z", "type": "commit"}, {"oid": "6370a871eda29f61728e7a3ce49d884325b8a632", "url": "https://github.com/apache/beam/commit/6370a871eda29f61728e7a3ce49d884325b8a632", "message": "Remove deprecated javadoc", "committedDate": "2020-11-14T06:27:54Z", "type": "commit"}, {"oid": "6370a871eda29f61728e7a3ce49d884325b8a632", "url": "https://github.com/apache/beam/commit/6370a871eda29f61728e7a3ce49d884325b8a632", "message": "Remove deprecated javadoc", "committedDate": "2020-11-14T06:27:54Z", "type": "forcePushed"}]}