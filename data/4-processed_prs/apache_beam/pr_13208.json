{"pr_number": 13208, "pr_title": "[BEAM-10703, BEAM-10475] Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "pr_createdAt": "2020-10-27T22:11:07Z", "pr_url": "https://github.com/apache/beam/pull/13208", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2Nzk2NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513067965", "bodyText": "Non-portable is not ready either. Should we say that it's not supported in general and enable it when the backend is ready?", "author": "nehsyc", "createdAt": "2020-10-27T22:22:03Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1281,8 +1286,12 @@ void addPCollectionRequiringIndexedFormat(PCollection<?> pcol) {\n   }\n \n   void maybeRecordPCollectionWithAutoSharding(PCollection<?> pcol) {\n-    if (hasExperiment(options, \"enable_streaming_auto_sharding\")\n-        && !hasExperiment(options, \"beam_fn_api\")) {\n+    if (hasExperiment(options, \"beam_fn_api\")) {", "originalCommit": "a50f4b2d7b0d82994c4847ad633bc688a9c92c78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzgyOA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513093828", "bodyText": "Actually would it make more sense to put this warning in the transform default implementation?", "author": "nehsyc", "createdAt": "2020-10-27T23:35:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2Nzk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2OTI0Nw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513669247", "bodyText": "I think it makes the most sense here.", "author": "robertwb", "createdAt": "2020-10-28T18:24:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2Nzk2NQ=="}], "type": "inlineReview"}, {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "url": "https://github.com/apache/beam/commit/bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-10-27T22:28:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3MjU5NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513072595", "bodyText": "Currently WithShardedKey is only applied with withShardedKey option. We could alternatively always apply WithShardedKey and strip the shard id in the no-sharded-output case. Not sure if it's safe to do that since it introduces coder changes for all current uses.", "author": "nehsyc", "createdAt": "2020-10-27T22:33:21Z", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey", "originalCommit": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3NDA5Mg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513674092", "bodyText": "Correct, we can't do this by default due to the coder change.", "author": "robertwb", "createdAt": "2020-10-28T18:32:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3MjU5NQ=="}], "type": "inlineReview"}, {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "url": "https://github.com/apache/beam/commit/da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-10-27T22:39:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513093562", "bodyText": "So far we don't have plans to do anything fancy for batch but added this override to avoid breaking the transform in batch if withShardedKey is specified.", "author": "nehsyc", "createdAt": "2020-10-27T23:34:45Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,9 +96,58 @@ public void process(ProcessContext c) {\n     }\n   }\n \n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>", "originalCommit": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2OTUzMA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513669530", "bodyText": "Do we need an override or is the default implementation good enough?", "author": "robertwb", "createdAt": "2020-10-28T18:25:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc4NDI1NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513784255", "bodyText": "Two things that made me add the override for withShardedKeys\n\nWe insert an explicit GBK for batch stateful dofns (although I feel this should be done in the backend).\n\n\n  \n    \n      beam/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/BatchStatefulParDoOverrides.java\n    \n    \n         Line 228\n      in\n      5620419\n    \n    \n    \n    \n\n        \n          \n           return input.apply(new GbkBeforeStatefulParDo<>()).apply(statefulParDo); \n        \n    \n  \n\n\n\nCurrently the batch GIB doesn't use keyed states but instead partitions the iterables after GBK:\n\n\n  \n    \n      beam/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java\n    \n    \n         Line 84\n      in\n      5620419\n    \n    \n    \n    \n\n        \n          \n           Iterators.partition(c.element().getValue().iterator(), (int) batchSize);", "author": "nehsyc", "createdAt": "2020-10-28T21:52:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE3ODEyNg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516178126", "bodyText": "Yeah, GBK before batch stateful DoFns should definitely be done on the runner.\nShould we be using the GBK + Iterators.partition on all runners, not just Dataflow?", "author": "robertwb", "createdAt": "2020-11-02T18:35:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzOTU1MQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516839551", "bodyText": "Did you mean that we should use that as a default implementation? I am assuming that GBK + Iterators.partition only works for batch and the default implementation should work for both streaming and batch.", "author": "nehsyc", "createdAt": "2020-11-03T17:31:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxMDY2NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r518910665", "bodyText": "I was assuming we could branch on streaming vs. batch.", "author": "robertwb", "createdAt": "2020-11-06T17:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NTYzNg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r518985636", "bodyText": "hmm how would the transform know whether the data is bounded or unbounded? I thought in general everything should work without differentiating batch and streaming.", "author": "nehsyc", "createdAt": "2020-11-06T20:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NTIyMA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527145220", "bodyText": "Well, we currently give different overrides in the two cases for Dataflow. Everything works, but there are different performance characteristics that encourage different implementations. (One could argue that this is runner-specific logic.) Fine not to change now.", "author": "robertwb", "createdAt": "2020-11-19T19:31:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513671483", "bodyText": "This doesn't look like it'll scale if more options are used. Why not just apply original?", "author": "robertwb", "createdAt": "2020-10-28T18:28:15Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -103,43 +156,76 @@ public void process(ProcessContext c) {\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the input of {@code GroupIntoBatchesDoFn} in order to append relevant step properties during\n+   * the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner, GroupIntoBatches<K, V>.WithShardedKey original) {\n       this.runner = runner;\n       this.original = original;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = ShardKeys(input);\n+\n+      runner.maybeRecordPCollectionWithAutoSharding(intermediate_input);\n+\n+      if (original.getMaxBufferingDuration() != null) {", "originalCommit": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc3NDI1NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513774255", "bodyText": "We need to recognize the GroupIntoBatchesDoFn which is a private member of the transform. Here we record the input pcoll of GroupIntoBatchesDoFn, which would be the output of the key-sharding DoFn, so in the translation we can append autosharding properties for those steps whose input was recorded. This doesn't scale indeed. Any suggestions?", "author": "nehsyc", "createdAt": "2020-10-28T21:32:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE3OTIxMQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516179211", "bodyText": "Sounds like privacy is getting in the way of writing clean code here. What about making it package private, and adding a (package) helper to identify it?", "author": "robertwb", "createdAt": "2020-11-02T18:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzOTY0NA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516839644", "bodyText": "Tried to make the changes but seems like making the GroupIntoBatchesDoFn visible to runners is not enough, since now we need to differentiate the DoFn in GroupIntoBatches transform and in GroupIntoBatches.withShardedKey transform. DataflowPipelineTranslator seems to only recognize primitive transforms, e.g., ParDoSingle and ParDo.MultiOutput.\n\n  \n    \n      beam/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslator.java\n    \n    \n         Line 997\n      in\n      f67cb9a\n    \n    \n    \n    \n\n        \n          \n           ParDoSingle.class, \n        \n    \n  \n\n\nPossible workarounds:\n\nMatch the DoFn name (withShardedKey) or PCollection type name (ShardedKey).\nRecord the output pcollection of GIB transform and use that to recognize GIB DoFn. Not 100% sure if that will work but I can try.\nOr...?", "author": "nehsyc", "createdAt": "2020-11-03T17:31:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4NzEwNQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r517787105", "bodyText": "The latest commit I pushed took the second approach above. In the current version we record the output PCollection of GroupIntoBatches.WithShardedKey when replacing the original transform and add necessary properties to the DoFn that produces the recorded PCollection. That way we don't need to replicate the implementation but instead simply apply the original transform as is.", "author": "nehsyc", "createdAt": "2020-11-05T04:34:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MjY3NA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513672674", "bodyText": "Methods shouldn't be capitalized.", "author": "robertwb", "createdAt": "2020-10-28T18:30:10Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -103,43 +156,76 @@ public void process(ProcessContext c) {\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the input of {@code GroupIntoBatchesDoFn} in order to append relevant step properties during\n+   * the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner, GroupIntoBatches<K, V>.WithShardedKey original) {\n       this.runner = runner;\n       this.original = original;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = ShardKeys(input);\n+\n+      runner.maybeRecordPCollectionWithAutoSharding(intermediate_input);\n+\n+      if (original.getMaxBufferingDuration() != null) {\n+        return intermediate_input.apply(\n+            GroupIntoBatches.<ShardedKey<K>, V>ofSize(original.getBatchSize())\n+                .withMaxBufferingDuration(original.getMaxBufferingDuration()));\n+      } else {\n+        return intermediate_input.apply(GroupIntoBatches.ofSize(original.getBatchSize()));\n+      }\n     }\n   }\n+\n+  private static <K, V> PCollection<KV<ShardedKey<K>, V>> ShardKeys(PCollection<KV<K, V>> input) {", "originalCommit": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMjM5Nw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r514602397", "bodyText": "Corrected.", "author": "nehsyc", "createdAt": "2020-10-29T22:24:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MjY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3NzM4NA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513677384", "bodyText": "Alternatively one could apply the original GroupIntoBatches that this was derived from here.", "author": "robertwb", "createdAt": "2020-10-28T18:38:11Z", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey\n+      extends PTransform<\n+          PCollection<KV<K, InputT>>, PCollection<KV<ShardedKey<K>, Iterable<InputT>>>> {\n+\n+    /** Returns the size of the batch. */\n+    public long getBatchSize() {\n+      return batchSize;\n+    }\n+\n+    /** Returns the size of the batch. */\n+    @Nullable\n+    public Duration getMaxBufferingDuration() {\n+      return maxBufferingDuration;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<InputT>>> expand(\n+        PCollection<KV<K, InputT>> input) {\n+      Duration allowedLateness = input.getWindowingStrategy().getAllowedLateness();\n+\n+      checkArgument(\n+          input.getCoder() instanceof KvCoder,\n+          \"coder specified in the input PCollection is not a KvCoder\");\n+      KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) input.getCoder();\n+      Coder<K> keyCoder = (Coder<K>) inputCoder.getCoderArguments().get(0);\n+      Coder<InputT> valueCoder = (Coder<InputT>) inputCoder.getCoderArguments().get(1);\n+\n+      return input\n+          .apply(\n+              MapElements.via(\n+                  new SimpleFunction<KV<K, InputT>, KV<ShardedKey<K>, InputT>>() {\n+                    @Override\n+                    public KV<ShardedKey<K>, InputT> apply(KV<K, InputT> input) {\n+                      // By default every input key has only one shard.\n+                      return KV.of(\n+                          ShardedKey.of(input.getKey(), DEFAULT_SHARD_ID), input.getValue());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.of(ShardedKey.Coder.of(keyCoder), valueCoder))\n+          .apply(", "originalCommit": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDkyMg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r513680922", "bodyText": "A single subshard by default will make this virtually unusable for runners that don't implement the optimization (including batch Dataflow). Instead use something like the thread id here, or at the very lease initialize DEFAULT_SHARD_ID to be different for each worker and add a small nonce. We could alternatively take a hint as to the number of subshards that would be nice (but that has its own downsides).", "author": "robertwb", "createdAt": "2020-10-28T18:44:15Z", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey\n+      extends PTransform<\n+          PCollection<KV<K, InputT>>, PCollection<KV<ShardedKey<K>, Iterable<InputT>>>> {\n+\n+    /** Returns the size of the batch. */\n+    public long getBatchSize() {\n+      return batchSize;\n+    }\n+\n+    /** Returns the size of the batch. */\n+    @Nullable\n+    public Duration getMaxBufferingDuration() {\n+      return maxBufferingDuration;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<InputT>>> expand(\n+        PCollection<KV<K, InputT>> input) {\n+      Duration allowedLateness = input.getWindowingStrategy().getAllowedLateness();\n+\n+      checkArgument(\n+          input.getCoder() instanceof KvCoder,\n+          \"coder specified in the input PCollection is not a KvCoder\");\n+      KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) input.getCoder();\n+      Coder<K> keyCoder = (Coder<K>) inputCoder.getCoderArguments().get(0);\n+      Coder<InputT> valueCoder = (Coder<InputT>) inputCoder.getCoderArguments().get(1);\n+\n+      return input\n+          .apply(\n+              MapElements.via(\n+                  new SimpleFunction<KV<K, InputT>, KV<ShardedKey<K>, InputT>>() {\n+                    @Override\n+                    public KV<ShardedKey<K>, InputT> apply(KV<K, InputT> input) {\n+                      // By default every input key has only one shard.\n+                      return KV.of(\n+                          ShardedKey.of(input.getKey(), DEFAULT_SHARD_ID), input.getValue());", "originalCommit": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMjM3Mg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r514602372", "bodyText": "Good point! I changed the default implementation to use thread id as shard id.", "author": "nehsyc", "createdAt": "2020-10-29T22:24:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDkyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4MDAyMg==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516180022", "bodyText": "Thanks. These are likely to be re-used for all workers. Can you add in a statically initialized (for each worker) random long as well?", "author": "robertwb", "createdAt": "2020-11-02T18:39:25Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -210,20 +211,23 @@ public StreamingGroupIntoBatchesWithShardedKey(\n     }\n   }\n \n-  private static <K, V> PCollection<KV<ShardedKey<K>, V>> ShardKeys(PCollection<KV<K, V>> input) {\n+  private static <K, V> PCollection<KV<ShardedKey<K>, V>> shardKeys(PCollection<KV<K, V>> input) {\n     KvCoder<K, V> inputCoder = (KvCoder<K, V>) input.getCoder();\n     org.apache.beam.sdk.coders.Coder<K> keyCoder =\n         (org.apache.beam.sdk.coders.Coder<K>) inputCoder.getCoderArguments().get(0);\n     org.apache.beam.sdk.coders.Coder<V> valueCoder =\n         (org.apache.beam.sdk.coders.Coder<V>) inputCoder.getCoderArguments().get(1);\n     return input\n         .apply(\n-            \"ShardKeys\",\n+            \"Shard Keys\",\n             MapElements.via(\n                 new SimpleFunction<KV<K, V>, KV<ShardedKey<K>, V>>() {\n                   @Override\n                   public KV<ShardedKey<K>, V> apply(KV<K, V> input) {\n-                    return KV.of(ShardedKey.of(input.getKey(), new byte[0]), input.getValue());\n+                    long tid = Thread.currentThread().getId();", "originalCommit": "be34365c6ada47c5ca74dac20a357f1cff5d0b9b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzOTY4MA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r516839680", "bodyText": "Hmm I am a bit confused. The statically initialized random is at GroupIntoBatches class level. Is that correct? Would it be initialized only once the transform is created in the user pipeline and shared across workers? In the unit test, I found that the tid was different almost every time SimpleFunction.apply was called while the static tid was the same all the time.", "author": "nehsyc", "createdAt": "2020-11-03T17:31:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4MDAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxMTQ2NQ==", "url": "https://github.com/apache/beam/pull/13208#discussion_r518911465", "bodyText": "I meant you could have a static-level long created by java.util.UUID or similar, plus the thread id queried here.", "author": "robertwb", "createdAt": "2020-11-06T17:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4MDAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NTY3Nw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r518985677", "bodyText": "Ah I see. Addressed this in the latest commit.", "author": "nehsyc", "createdAt": "2020-11-06T20:20:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4MDAyMg=="}], "type": "inlineReview"}, {"oid": "8372dd85a2dcd1dd3dbab3d4233a0db3560ed71e", "url": "https://github.com/apache/beam/commit/8372dd85a2dcd1dd3dbab3d4233a0db3560ed71e", "message": "record sharded output to avoid duplicating code in the override", "committedDate": "2020-11-06T17:06:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODExMw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527148113", "bodyText": "Maybe take the sum of getMostSignificantBits() and getLeastSignificantBits()?", "author": "robertwb", "createdAt": "2020-11-19T19:36:32Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,54 +98,141 @@ public void process(ProcessContext c) {\n     }\n   }\n \n-  static class StreamingGroupIntoBatchesOverrideFactory<K, V>\n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n       implements PTransformOverrideFactory<\n-          PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>> {\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n+\n+    @Override\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n+        getReplacementTransform(\n+            AppliedPTransform<\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n+                transform) {\n+      return PTransformReplacement.of(\n+          PTransformReplacements.getSingletonMainInput(transform),\n+          new BatchGroupIntoBatchesWithShardedKey<>(transform.getTransform().getBatchSize()));\n+    }\n+\n+    @Override\n+    public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n+      return ReplacementOutputs.singleton(outputs, newOutput);\n+    }\n+  }\n+\n+  /**\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for bounded Dataflow\n+   * pipelines.\n+   */\n+  static class BatchGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n+\n+    private final long batchSize;\n+\n+    private BatchGroupIntoBatchesWithShardedKey(long batchSize) {\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = shardKeys(input);\n+      return intermediate_input.apply(new BatchGroupIntoBatches<>(batchSize));\n+    }\n+  }\n+\n+  static class StreamingGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n+      implements PTransformOverrideFactory<\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n \n     private final DataflowRunner runner;\n \n-    StreamingGroupIntoBatchesOverrideFactory(DataflowRunner runner) {\n+    StreamingGroupIntoBatchesWithShardedKeyOverrideFactory(DataflowRunner runner) {\n       this.runner = runner;\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(\n+              runner,\n+              transform.getTransform(),\n+              PTransformReplacements.getSingletonMainOutput(transform)));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the output in order to append required step properties during the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original_transform;\n+    private final PCollection<KV<ShardedKey<K>, Iterable<V>>> original_output;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner,\n+        GroupIntoBatches<K, V>.WithShardedKey original,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> output) {\n       this.runner = runner;\n-      this.original = original;\n+      this.original_transform = original;\n+      this.original_output = output;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      // Record the output PCollection of the original transform since the new output will be\n+      // replaced by the original one when the replacement transform is wired to other nodes in the\n+      // graph, although the old and the new outputs are effectively the same.\n+      runner.maybeRecordPCollectionWithAutoSharding(original_output);\n+      return input.apply(original_transform);\n     }\n   }\n+\n+  private static final long uuid = UUID.randomUUID().getMostSignificantBits();", "originalCommit": "5eeb59ceceea58febdc72b2a75cd15fb681b7188", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzQxMDg3NA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527410874", "bodyText": "I changed the shard id to be the concatenation of the most and the least significant bits.", "author": "nehsyc", "createdAt": "2020-11-20T05:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODQ1Mw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527148453", "bodyText": "Call this workerUuid?", "author": "robertwb", "createdAt": "2020-11-19T19:37:10Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,54 +98,141 @@ public void process(ProcessContext c) {\n     }\n   }\n \n-  static class StreamingGroupIntoBatchesOverrideFactory<K, V>\n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n       implements PTransformOverrideFactory<\n-          PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>> {\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n+\n+    @Override\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n+        getReplacementTransform(\n+            AppliedPTransform<\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n+                transform) {\n+      return PTransformReplacement.of(\n+          PTransformReplacements.getSingletonMainInput(transform),\n+          new BatchGroupIntoBatchesWithShardedKey<>(transform.getTransform().getBatchSize()));\n+    }\n+\n+    @Override\n+    public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n+      return ReplacementOutputs.singleton(outputs, newOutput);\n+    }\n+  }\n+\n+  /**\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for bounded Dataflow\n+   * pipelines.\n+   */\n+  static class BatchGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n+\n+    private final long batchSize;\n+\n+    private BatchGroupIntoBatchesWithShardedKey(long batchSize) {\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = shardKeys(input);\n+      return intermediate_input.apply(new BatchGroupIntoBatches<>(batchSize));\n+    }\n+  }\n+\n+  static class StreamingGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n+      implements PTransformOverrideFactory<\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n \n     private final DataflowRunner runner;\n \n-    StreamingGroupIntoBatchesOverrideFactory(DataflowRunner runner) {\n+    StreamingGroupIntoBatchesWithShardedKeyOverrideFactory(DataflowRunner runner) {\n       this.runner = runner;\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(\n+              runner,\n+              transform.getTransform(),\n+              PTransformReplacements.getSingletonMainOutput(transform)));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the output in order to append required step properties during the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original_transform;\n+    private final PCollection<KV<ShardedKey<K>, Iterable<V>>> original_output;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner,\n+        GroupIntoBatches<K, V>.WithShardedKey original,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> output) {\n       this.runner = runner;\n-      this.original = original;\n+      this.original_transform = original;\n+      this.original_output = output;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      // Record the output PCollection of the original transform since the new output will be\n+      // replaced by the original one when the replacement transform is wired to other nodes in the\n+      // graph, although the old and the new outputs are effectively the same.\n+      runner.maybeRecordPCollectionWithAutoSharding(original_output);\n+      return input.apply(original_transform);\n     }\n   }\n+\n+  private static final long uuid = UUID.randomUUID().getMostSignificantBits();", "originalCommit": "5eeb59ceceea58febdc72b2a75cd15fb681b7188", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzQxMDU5MA==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527410590", "bodyText": "Done.", "author": "nehsyc", "createdAt": "2020-11-20T05:28:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0OTA2Mw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527149063", "bodyText": "Could we go with a larger batch size (say 5 or 10) and also verify that most batches are of the expected size?", "author": "robertwb", "createdAt": "2020-11-19T19:38:21Z", "path": "sdks/java/core/src/test/java/org/apache/beam/sdk/transforms/GroupIntoBatchesTest.java", "diffHunk": "@@ -125,6 +126,45 @@ public Void apply(Iterable<KV<String, Iterable<String>>> input) {\n     pipeline.run();\n   }\n \n+  @Test\n+  @Category({NeedsRunner.class, UsesTimersInParDo.class, UsesStatefulParDo.class})\n+  public void testWithShardedKeyInGlobalWindow() {\n+    // Since with default sharding, the number of subshards of of a key is nondeterministic, create", "originalCommit": "5eeb59ceceea58febdc72b2a75cd15fb681b7188", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzQxMDUyMw==", "url": "https://github.com/apache/beam/pull/13208#discussion_r527410523", "bodyText": "Done. Added an additional check in the test.", "author": "nehsyc", "createdAt": "2020-11-20T05:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0OTA2Mw=="}], "type": "inlineReview"}, {"oid": "fea27034372375f7db0f0f3b09a62ac5ac0979cd", "url": "https://github.com/apache/beam/commit/fea27034372375f7db0f0f3b09a62ac5ac0979cd", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-20T05:28:26Z", "type": "forcePushed"}, {"oid": "797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "url": "https://github.com/apache/beam/commit/797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-20T18:20:40Z", "type": "forcePushed"}, {"oid": "4cc0f688397771adb985b62a6319796ba8c350a5", "url": "https://github.com/apache/beam/commit/4cc0f688397771adb985b62a6319796ba8c350a5", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-23T18:38:32Z", "type": "forcePushed"}, {"oid": "1ceeaed84cfcf7b77becabaefe5d69632e294e70", "url": "https://github.com/apache/beam/commit/1ceeaed84cfcf7b77becabaefe5d69632e294e70", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-23T20:23:16Z", "type": "forcePushed"}, {"oid": "b51d64e0eee662a1cc75f1a558ef99c2e812813e", "url": "https://github.com/apache/beam/commit/b51d64e0eee662a1cc75f1a558ef99c2e812813e", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-24T00:01:16Z", "type": "commit"}, {"oid": "b51d64e0eee662a1cc75f1a558ef99c2e812813e", "url": "https://github.com/apache/beam/commit/b51d64e0eee662a1cc75f1a558ef99c2e812813e", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "committedDate": "2020-11-24T00:01:16Z", "type": "forcePushed"}]}