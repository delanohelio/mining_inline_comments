{"pr_number": 12385, "pr_title": "[BEAM-10527] Migrate Flink and Spark tests to pytest.", "pr_createdAt": "2020-07-28T01:52:47Z", "pr_url": "https://github.com/apache/beam/pull/12385", "timeline": [{"oid": "8ae28f6999146e839afecda82936e2b89a455409", "url": "https://github.com/apache/beam/commit/8ae28f6999146e839afecda82936e2b89a455409", "message": "[BEAM-10527] Migrate Flink and Spark tests to pytest.", "committedDate": "2020-07-28T01:55:23Z", "type": "forcePushed"}, {"oid": "a51fed8584d3fcdf3d3bccbd124fe66e42333797", "url": "https://github.com/apache/beam/commit/a51fed8584d3fcdf3d3bccbd124fe66e42333797", "message": "[BEAM-10527] Migrate Flink and Spark tests to pytest.", "committedDate": "2020-07-28T02:00:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMTYwMA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463131600", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    request.config.option.test_pipeline_options\n          \n          \n            \n                    if request.config.option.test_pipeline_options else '')\n          \n          \n            \n                    request.config.option.test_pipeline_options or '')", "author": "mxm", "createdAt": "2020-07-30T16:45:08Z", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')", "originalCommit": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTg4OA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463345888", "bodyText": "done", "author": "ibzib", "createdAt": "2020-07-31T00:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMTYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463132344", "bodyText": "Could this be problematic if one of the option values contains a space, e.g. as part of a json string for the environment config?", "author": "mxm", "createdAt": "2020-07-30T16:46:23Z", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--flink_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "originalCommit": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NjQwMg==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463346402", "bodyText": "Good catch. I used shlex.split(...) instead.", "author": "ibzib", "createdAt": "2020-07-31T00:43:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0ODk3NQ==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463348975", "bodyText": "I also had to make many changes to preserve the correct string quoting. Tox in particular does something weird with quotes in its commands, so I had to double-escape the strings before passing to Tox and unescape them after.", "author": "ibzib", "createdAt": "2020-07-31T00:51:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMzgyMg==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463133822", "bodyText": "Same here, what about whitespace in the values?", "author": "mxm", "createdAt": "2020-07-30T16:48:42Z", "path": "sdks/python/apache_beam/runners/portability/spark_runner_test.py", "diffHunk": "@@ -21,141 +21,158 @@\n \n import argparse\n import logging\n-import sys\n import unittest\n from shutil import rmtree\n from tempfile import mkdtemp\n \n-from apache_beam.options.pipeline_options import DebugOptions\n+import pytest\n+\n from apache_beam.options.pipeline_options import PortableOptions\n from apache_beam.runners.portability import job_server\n from apache_beam.runners.portability import portable_runner\n from apache_beam.runners.portability import portable_runner_test\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.spark_runner_test \\\n-  #     --spark_job_server_jar=/path/to/job_server.jar \\\n-  #     [SparkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--spark_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--environment_cache_millis',\n-      help='Environment cache TTL in milliseconds.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  spark_job_server_jar = (\n-      known_args.spark_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:spark:job-server:shadowJar'))\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  environment_cache_millis = known_args.environment_cache_millis\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='sparktest')\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dbeam.spark.test.reuseSparkContext=true',\n-            '-jar',\n-            spark_job_server_jar,\n-            '--spark-master-url',\n-            'local',\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    def create_options(self):\n-      options = super(SparkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      portable_options = options.view_as(PortableOptions)\n-      portable_options.environment_type = environment_type.upper()\n-      if environment_config:\n-        portable_options.environment_config = environment_config\n-      if environment_cache_millis:\n-        portable_options.environment_cache_millis = environment_cache_millis\n-\n-      return options\n-\n-    def test_metrics(self):\n-      # Skip until Spark runner supports metrics.\n-      raise unittest.SkipTest(\"BEAM-7219\")\n-\n-    def test_sdf(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_watermark_tracking(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_sdf_initiated_checkpointing(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_synthetic_source(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_external_transforms(self):\n-      # Skip until Spark runner supports external transforms.\n-      raise unittest.SkipTest(\"BEAM-7232\")\n-\n-    def test_callbacks_with_exception(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_register_finalizations(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_flattened_side_input(self):\n-      # Blocked on support for transcoding\n-      # https://jira.apache.org/jira/browse/BEAM-7236\n-      super(SparkRunnerTest,\n-            self).test_flattened_side_input(with_transcoding=False)\n-\n-    # Inherits all other tests from PortableRunnerTest.\n+# Run as\n+#\n+# pytest spark_runner_test.py \\\n+#     [--test_pipeline_options \"--spark_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [SparkRunnerTest.test_method, ...]\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+\n+class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  expansion_port = None\n+  spark_job_server_jar = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--spark_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "originalCommit": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTAxNg==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463349016", "bodyText": "done", "author": "ibzib", "createdAt": "2020-07-31T00:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMzgyMg=="}], "type": "inlineReview"}, {"oid": "5cf972004379a210996b0a2394f92a441bc3cba7", "url": "https://github.com/apache/beam/commit/5cf972004379a210996b0a2394f92a441bc3cba7", "message": "Fix PROCESS configuration string handling.", "committedDate": "2020-07-31T00:39:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3NTIxNQ==", "url": "https://github.com/apache/beam/pull/12385#discussion_r463875215", "bodyText": "Can you instead check if test-pipeline-options is set? I believe that's how all the other ITs are skipped (see TestPipeline code).", "author": "udim", "createdAt": "2020-07-31T22:40:39Z", "path": "sdks/python/scripts/run_pytest.sh", "diffHunk": "@@ -29,10 +29,16 @@ posargs=$2\n \n # Run with pytest-xdist and without.\n pytest -o junit_suite_name=${envname} \\\n-  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 --pyargs ${posargs}\n+  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 \\\n+  --ignore=apache_beam/runners/portability/flink_runner_test.py \\", "originalCommit": "25713d8cf88113bf84dad82f038bdc0122d27db0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM1ODg2MQ==", "url": "https://github.com/apache/beam/pull/12385#discussion_r498358861", "bodyText": "Done", "author": "ibzib", "createdAt": "2020-10-01T16:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3NTIxNQ=="}], "type": "inlineReview"}, {"oid": "266c9c604005127a127ce3b59d5820b2b62495e4", "url": "https://github.com/apache/beam/commit/266c9c604005127a127ce3b59d5820b2b62495e4", "message": "fixup! use shlex in spark runner test", "committedDate": "2020-08-10T22:16:51Z", "type": "forcePushed"}, {"oid": "6c233c23f2b82bf29b339c04e354dc90722a8109", "url": "https://github.com/apache/beam/commit/6c233c23f2b82bf29b339c04e354dc90722a8109", "message": "fixup! use shlex in spark runner test", "committedDate": "2020-09-30T17:30:16Z", "type": "forcePushed"}, {"oid": "2a0ad5db8d9c41cbda090d126dcdc1d82e445a05", "url": "https://github.com/apache/beam/commit/2a0ad5db8d9c41cbda090d126dcdc1d82e445a05", "message": "Publish pytest Junit results.", "committedDate": "2020-10-01T01:45:39Z", "type": "forcePushed"}, {"oid": "134f17756a18b50fa911fcc4a23285415e3d882f", "url": "https://github.com/apache/beam/commit/134f17756a18b50fa911fcc4a23285415e3d882f", "message": "[BEAM-10527] Migrate Flink and Spark tests to pytest.", "committedDate": "2020-10-01T01:49:22Z", "type": "commit"}, {"oid": "7219836bd567531287e880e3aa5b5d3a24e21122", "url": "https://github.com/apache/beam/commit/7219836bd567531287e880e3aa5b5d3a24e21122", "message": "Publish pytest Junit results.", "committedDate": "2020-10-01T01:49:30Z", "type": "commit"}, {"oid": "7219836bd567531287e880e3aa5b5d3a24e21122", "url": "https://github.com/apache/beam/commit/7219836bd567531287e880e3aa5b5d3a24e21122", "message": "Publish pytest Junit results.", "committedDate": "2020-10-01T01:49:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MDQ3MA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r499940470", "bodyText": "--test-pipeline-options", "author": "boyuanzz", "createdAt": "2020-10-06T00:20:17Z", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +54,386 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\", "originalCommit": "7219836bd567531287e880e3aa5b5d3a24e21122", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MTIxNA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r499941214", "bodyText": "Oh yeah, --test_pipeline_options is now required (even though it should be possible to leave it empty). Boyuan, would you mind filing a PR to fix this?", "author": "ibzib", "createdAt": "2020-10-06T00:23:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MDQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0NTE4MA==", "url": "https://github.com/apache/beam/pull/12385#discussion_r499945180", "bodyText": "And the test filter here doesn't work for me properly. The working version for me is\npytest flink_runner_test.py::TestClass:test_case --test-pipeline-options \"--flink_job_server_jar=XXX --environment_type=XXX \"", "author": "boyuanzz", "createdAt": "2020-10-06T00:39:31Z", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +54,386 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]", "originalCommit": "7219836bd567531287e880e3aa5b5d3a24e21122", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}