{"pr_number": 11661, "pr_title": "[BEAM-7774] Remove perfkit benchmarking tool from python performance \u2026", "pr_createdAt": "2020-05-11T13:43:33Z", "pr_url": "https://github.com/apache/beam/pull/11661", "timeline": [{"oid": "a9d345c1304e5f7eec93a3859ad4f62094eceeb6", "url": "https://github.com/apache/beam/commit/a9d345c1304e5f7eec93a3859ad4f62094eceeb6", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-11T15:52:29Z", "type": "forcePushed"}, {"oid": "9c49645656afcb9efdb3927ff2e09769dc9f525c", "url": "https://github.com/apache/beam/commit/9c49645656afcb9efdb3927ff2e09769dc9f525c", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-11T16:04:02Z", "type": "forcePushed"}, {"oid": "6eeab88fa2ba3403c88c175d3a1570748d62d994", "url": "https://github.com/apache/beam/commit/6eeab88fa2ba3403c88c175d3a1570748d62d994", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-11T16:07:13Z", "type": "forcePushed"}, {"oid": "b1a3c51effd3250308bc4c96ec31140933e19268", "url": "https://github.com/apache/beam/commit/b1a3c51effd3250308bc4c96ec31140933e19268", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-11T16:10:13Z", "type": "forcePushed"}, {"oid": "9ec9f6620fb0bd38e309931dc335494f9d7b7178", "url": "https://github.com/apache/beam/commit/9ec9f6620fb0bd38e309931dc335494f9d7b7178", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-19T13:05:00Z", "type": "forcePushed"}, {"oid": "414239cd30aa62ff3db1bfe29c0cac6fdaeeb8ba", "url": "https://github.com/apache/beam/commit/414239cd30aa62ff3db1bfe29c0cac6fdaeeb8ba", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-19T13:59:36Z", "type": "forcePushed"}, {"oid": "90f5745b47e84bff0bf8c87123046d44c85d0deb", "url": "https://github.com/apache/beam/commit/90f5745b47e84bff0bf8c87123046d44c85d0deb", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-19T14:11:22Z", "type": "forcePushed"}, {"oid": "ae329623f561e2ac95972f336e751068abae0674", "url": "https://github.com/apache/beam/commit/ae329623f561e2ac95972f336e751068abae0674", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-19T14:47:18Z", "type": "forcePushed"}, {"oid": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "url": "https://github.com/apache/beam/commit/ca6200110135814a90bfcdf9f24e53d01f83ea0e", "message": "[BEAM-7774] Remove perfkit benchmarking tool from python performance tests", "committedDate": "2020-05-19T15:06:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMDY1Nw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427430657", "bodyText": "Can we import this from common.gradle?", "author": "tvalentyn", "createdAt": "2020-05-19T16:19:50Z", "path": "sdks/python/test-suites/dataflow/py2/build.gradle", "diffHunk": "@@ -205,3 +205,20 @@ task chicagoTaxiExample {\n     }\n   }\n }\n+\n+task runPerformanceTest {", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4NzIxOQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r429087219", "bodyText": "Sure, there can even be more code moved to common.gradle", "author": "piotr-szuberski", "createdAt": "2020-05-22T07:29:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMDY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxNzk1MA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427517950", "bodyText": "I don't think there is significant value to run across all Python version. We can keep Py 27 and Py37 for now, and then switch to one of \"high-priority\"[1] versions once we introduce that concept. cc: @lazylynx\n[1] https://lists.apache.org/thread.html/re621331e10896ac65f487c1a83cc4a91152e2fd6d7e363c115b1857f%40%3Cdev.beam.apache.org%3E", "author": "tvalentyn", "createdAt": "2020-05-19T18:37:35Z", "path": ".test-infra/jenkins/job_PerformanceTests_Python.groovy", "diffHunk": "@@ -58,117 +26,59 @@ def dataflowPipelineArgs = [\n     temp_location   : 'gs://temp-storage-for-end-to-end-tests/temp-it',\n ]\n \n-\n-// Configurations of each Jenkins job.\n-def testConfigurations = [\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py27',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py27 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python27 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py27_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py2',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py35',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py35 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python35 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py35_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py35',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py36',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py36 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python36 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py36_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py36',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py37',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py37 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python37 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py37_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py37',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-]\n-\n+testConfigurations = []\n+pythonVersions = ['27', '35', '36', '37']", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4ODAwNA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r429088004", "bodyText": "I tried to keep the effect of the job as close to original as possible. I agree that 2 versions of python sound sufficient.", "author": "piotr-szuberski", "createdAt": "2020-05-22T07:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxNzk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxODQzNA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427518434", "bodyText": "Where do we configure the dashboards for these tests? Do we need to configure python version bit in the dashboard configuration as well?", "author": "tvalentyn", "createdAt": "2020-05-19T18:38:23Z", "path": ".test-infra/jenkins/job_PerformanceTests_Python.groovy", "diffHunk": "@@ -58,117 +26,59 @@ def dataflowPipelineArgs = [\n     temp_location   : 'gs://temp-storage-for-end-to-end-tests/temp-it',\n ]\n \n-\n-// Configurations of each Jenkins job.\n-def testConfigurations = [\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py27',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py27 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python27 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py27_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py2',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py35',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py35 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python35 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py35_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py35',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py36',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py36 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python36 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py36_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py36',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py37',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py37 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python37 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py37_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py37',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-]\n-\n+testConfigurations = []\n+pythonVersions = ['27', '35', '36', '37']\n+\n+for (pythonVersion in pythonVersions) {", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4ODc2NQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r429088765", "bodyText": "I'm not sure if I understand meaning of \"dashboards\" correctly here, but we are running tasks via proper python modules that have pythonVersion variable already set up, so there is no need to set -PpythonVersion manually.\nEdit: As far as I know there is no dashboard for those tests yet, we just publish the metrics results in BQ. I could add reporting to InfluxDB and draw grafana dashboards for this test, WDYT?\nEdit2: I made changes to publish the results on the grafana dashboards", "author": "piotr-szuberski", "createdAt": "2020-05-22T07:33:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxODQzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwNjQxMA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r430606410", "bodyText": "Thanks @piotr-szuberski , I think you understood the meaning correctly, left a few minor comments.", "author": "tvalentyn", "createdAt": "2020-05-26T18:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxODQzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUxOTE5MA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427519190", "bodyText": "fyi, @lazylynx - I think this stanza would be useful to configure low priority jobs.", "author": "tvalentyn", "createdAt": "2020-05-19T18:39:41Z", "path": ".test-infra/jenkins/job_PerformanceTests_Python.groovy", "diffHunk": "@@ -58,117 +26,59 @@ def dataflowPipelineArgs = [\n     temp_location   : 'gs://temp-storage-for-end-to-end-tests/temp-it',\n ]\n \n-\n-// Configurations of each Jenkins job.\n-def testConfigurations = [\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py27',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py27 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python27 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py27_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py2',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py35',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py35 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python35 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py35_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py35',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py36',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py36 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python36 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py36_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py36',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-    new PerformanceTestConfigurations(\n-        jobName           : 'beam_PerformanceTests_WordCountIT_Py37',\n-        jobDescription    : 'Python SDK Performance Test - Run WordCountIT in Py37 with 1Gb files',\n-        jobTriggerPhrase  : 'Run Python37 WordCountIT Performance Test',\n-        resultTable       : 'beam_performance.wordcount_py37_pkb_results',\n-        test              : 'apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it',\n-        itModule          : ':sdks:python:test-suites:dataflow:py37',\n-        extraPipelineArgs : dataflowPipelineArgs + [\n-            input: 'gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*', // 1Gb\n-            output: 'gs://temp-storage-for-end-to-end-tests/py-it-cloud/output',\n-            expect_checksum: 'ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710',\n-            num_workers: '10',\n-            autoscaling_algorithm: 'NONE',  // Disable autoscale the worker pool.\n-        ],\n-    ),\n-]\n-\n+testConfigurations = []\n+pythonVersions = ['27', '35', '36', '37']\n+\n+for (pythonVersion in pythonVersions) {\n+    def taskVersion = pythonVersion == '27' ? '2' : pythonVersion\n+    testConfigurations.add([\n+            jobName           : \"beam_PerformanceTests_WordCountIT_Py${pythonVersion}\",\n+            jobDescription    : \"Python SDK Performance Test - Run WordCountIT in Py${pythonVersion} with 1Gb files\",\n+            jobTriggerPhrase  : \"Run Python${pythonVersion} WordCountIT Performance Test\",\n+            test              : \"apache_beam.examples.wordcount_it_test:WordCountIT.test_wordcount_it\",\n+            gradleTaskName    : \":sdks:python:test-suites:dataflow:py${taskVersion}:runPerformanceTest\",\n+            pipelineOptions   : dataflowPipelineArgs + [\n+                    runner               : 'TestDataflowRunner',\n+                    publish_to_big_query : true,\n+                    metrics_dataset      : 'beam_performance',\n+                    metrics_table        : \"wordcount_py${pythonVersion}_pkb_results\",\n+                    input                : \"gs://apache-beam-samples/input_small_files/ascii_sort_1MB_input.0000*\", // 1Gb\n+                    output               : \"gs://temp-storage-for-end-to-end-tests/py-it-cloud/output\",\n+                    expect_checksum      : \"ea0ca2e5ee4ea5f218790f28d0b9fe7d09d8d710\",\n+                    num_workers          : '10',\n+                    autoscaling_algorithm: \"NONE\",  // Disable autoscale the worker pool.\n+            ]\n+    ])\n+}\n \n for (testConfig in testConfigurations) {\n   createPythonPerformanceTestJob(testConfig)\n }\n \n-\n-private void createPythonPerformanceTestJob(PerformanceTestConfigurations testConfig) {\n-  // This job runs the Beam Python performance tests on PerfKit Benchmarker.\n+private void createPythonPerformanceTestJob(Map testConfig) {\n+  // This job runs the Beam Python performance tests\n   job(testConfig.jobName) {\n     // Set default Beam job properties.\n     commonJobProperties.setTopLevelMainJobProperties(delegate)\n \n     // Run job in postcommit, don't trigger every push.\n-    commonJobProperties.setAutoJob(\n-        delegate,\n-        testConfig.buildSchedule)\n+    commonJobProperties.setAutoJob(delegate, 'H */6 * * *')", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTExOA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427541118", "bodyText": "Please make sure these tests export the xml logs that can be inspected in Jenkins in case of test failure:\nRelevant bits are:\n\n\n  \n    \n      beam/.test-infra/jenkins/job_PostCommit_Python37.groovy\n    \n    \n         Line 32\n      in\n      03d99df\n    \n    \n    \n    \n\n        \n          \n           publishers { \n        \n    \n  \n\n\n\n  \n    \n      beam/sdks/python/scripts/run_integration_test.sh\n    \n    \n         Line 276\n      in\n      03d99df\n    \n    \n    \n    \n\n        \n          \n           --with-xunitmp --xunitmp-file=$XUNIT_FILE \\ \n        \n    \n  \n\n\n\ncc: @udim who may have additional feedback on this. Udi, would it make sense to use pytest here instead of nose?", "author": "tvalentyn", "createdAt": "2020-05-19T19:17:09Z", "path": "sdks/python/test-suites/dataflow/common.gradle", "diffHunk": "@@ -109,4 +109,21 @@ task validatesRunnerStreamingTests {\n       args '-c', \". ${envdir}/bin/activate && ${runScriptsDir}/run_integration_test.sh $cmdArgs\"\n     }\n   }\n-}\n\\ No newline at end of file\n+}\n+\n+task runPerformanceTest {\n+    dependsOn 'installGcpTest'\n+    dependsOn ':sdks:python:sdist'\n+\n+    def test = project.findProperty('test')\n+    def testOpts = project.findProperty('test-pipeline-options')\n+    testOpts += \" --sdk_location=${files(configurations.distTarBall.files).singleFile}\"\n+\n+  doLast {\n+    exec {\n+      workingDir \"${project.rootDir}/sdks/python\"\n+      executable 'sh'\n+      args '-c', \". ${envdir}/bin/activate && ${envdir}/bin/python setup.py nosetests --tests=${test}  --test-pipeline-options=\\\"${testOpts}\\\" --ignore-files \\'.*py3\\\\d?\\\\.py\\$\\'\"", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODgzODQ4NA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r428838484", "bodyText": "I don't believe we have --test-pipeline-options support yet in pytest, so nose is the solution for now.", "author": "udim", "createdAt": "2020-05-21T18:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA5NDY0OA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r429094648", "bodyText": "Done", "author": "piotr-szuberski", "createdAt": "2020-05-22T07:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzkxMw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r427543913", "bodyText": "Do we need to pass --ignore-files given that we control which tests to run?", "author": "tvalentyn", "createdAt": "2020-05-19T19:22:08Z", "path": "sdks/python/test-suites/dataflow/common.gradle", "diffHunk": "@@ -109,4 +109,21 @@ task validatesRunnerStreamingTests {\n       args '-c', \". ${envdir}/bin/activate && ${runScriptsDir}/run_integration_test.sh $cmdArgs\"\n     }\n   }\n-}\n\\ No newline at end of file\n+}\n+\n+task runPerformanceTest {\n+    dependsOn 'installGcpTest'\n+    dependsOn ':sdks:python:sdist'\n+\n+    def test = project.findProperty('test')\n+    def testOpts = project.findProperty('test-pipeline-options')\n+    testOpts += \" --sdk_location=${files(configurations.distTarBall.files).singleFile}\"\n+\n+  doLast {\n+    exec {\n+      workingDir \"${project.rootDir}/sdks/python\"\n+      executable 'sh'\n+      args '-c', \". ${envdir}/bin/activate && ${envdir}/bin/python setup.py nosetests --tests=${test}  --test-pipeline-options=\\\"${testOpts}\\\" --ignore-files \\'.*py3\\\\d?\\\\.py\\$\\'\"", "originalCommit": "ca6200110135814a90bfcdf9f24e53d01f83ea0e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA5NTcxNA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r429095714", "bodyText": "I wanted to be on the safe side because in most places (not only in the bash scripts) it is added. But when I think about it we absolutely don't need to ignore anything, we run just one test. I'll remove it.", "author": "piotr-szuberski", "createdAt": "2020-05-22T07:49:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjY2MQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r430592661", "bodyText": "Sounds like this code can be reused across other performance tests, is there a module shared across perf tests that we can move it to? cc: @kamilwu", "author": "tvalentyn", "createdAt": "2020-05-26T17:42:56Z", "path": "sdks/python/apache_beam/examples/wordcount_it_test.py", "diffHunk": "@@ -104,18 +107,33 @@ def _run_wordcount_it(self, run_wordcount, **opts):\n     run_time = end_time - start_time\n \n     if publish_to_bq:\n-      bq_publisher = BigQueryMetricsPublisher(\n-          project_name=test_pipeline.get_option('project'),\n-          table=test_pipeline.get_option('metrics_table'),\n-          dataset=test_pipeline.get_option('metrics_dataset'),\n-      )\n-      result = Metric(\n-          submit_timestamp=time.time(),\n-          metric_id=uuid.uuid4().hex,\n-          value=run_time,\n-          label='Python performance test',\n-      )\n-      bq_publisher.publish([result.as_dict()])\n+      self._publish_metrics(test_pipeline, run_time)\n+\n+  def _publish_metrics(self, pipeline, metric_value):", "originalCommit": "1516e9728ba502fdd9304fc1657b639a855c25a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5Mjg1OQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r430592859", "bodyText": "Looks like we have  apache_beam.testing.load_tests.load_test_metrics_utils. Should we move it there?", "author": "tvalentyn", "createdAt": "2020-05-26T17:43:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkzNTQ3NA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r430935474", "bodyText": "I can add something like \"publish_single_value_(bq/influx/console)\" for each publisher", "author": "piotr-szuberski", "createdAt": "2020-05-27T08:12:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTA2MTY2MQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431061661", "bodyText": "MetricsReader from load_test_metrics_utils module has a publish_metrics method. This method takes a pipeline result as an input, extracts metrics from it and calls all publishers. I think we could add a similar method, e.g. publish_value, which would take a list of kv pairs. @piotr-szuberski Does it make sense?", "author": "kamilwu", "createdAt": "2020-05-27T11:58:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTE0MjUxNw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431142517", "bodyText": "I think that's a very good idea to add a method to the MetricsReader.", "author": "piotr-szuberski", "createdAt": "2020-05-27T13:45:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r430597553", "bodyText": "@kamilwu , could you please review the dashboard config?\nSome questions:\nIs there a way to visualize the dashboard on an in-progress PR?\nIs it possible to reduce duplication in the configs? We can do it in a separate change. For example: most settings for Python 2.7, and 3.7 are similar.  Have we considered  auto-generating  the final config from a smaller set of settings?", "author": "tvalentyn", "createdAt": "2020-05-26T17:51:14Z", "path": ".test-infra/metrics/grafana/dashboards/perftests_metrics/Python_Performance_Tests.json", "diffHunk": "@@ -0,0 +1,297 @@\n+{", "originalCommit": "1516e9728ba502fdd9304fc1657b639a855c25a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTA1Mjk5OA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431052998", "bodyText": "@tvalentyn\n\nIs there a way to visualize the dashboard on an in-progress PR?\n\nCurrently, the best way is to: pull branch, deploy metrics stack locally (docker-compose build; docker-compose up), open web browser and go to localhost:3000.\n\nIs it possible to reduce duplication in the configs?\n\nThere's a concept called Scripted Dashboards, but we didn't implement it. So, if we want to create two charts next to each other, we have to provide two configs for them (even if they are very similar).\nApart from that, we can also consider another question: is having those .json files version-controlled profitable? Diffs are often very large, so the standard review process does not apply here.", "author": "kamilwu", "createdAt": "2020-05-27T11:41:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTA1NTIzNA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431055234", "bodyText": "@piotr-szuberski\nCould you rename your dashboard and charts? Python Performance Tests and Python27/37 Performance Tests | 1GB doesn't say much. How about using Python WordCount IT Benchmarks from the old Perfkit? [1]\n[1] https://apache-beam-testing.appspot.com/explore?dashboard=5691127080419328", "author": "kamilwu", "createdAt": "2020-05-27T11:45:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTE0Mzg4NA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431143884", "bodyText": "Python WordCount IT Benchmarks definitely sounds better.\nBTW, there was a typo WorldCount instead of WordCount :D", "author": "piotr-szuberski", "createdAt": "2020-05-27T13:47:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMxNDA0NQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431314045", "bodyText": "Good catch.\nRe:\n\nApart from that, we can also consider another question: is having those .json files version-controlled profitable? Diffs are often very large, so the standard review process does not apply here.\n\nI agree that those files are hard to review - I was thinking that perhaps we can autogenerate the final configs from a much smaller config that just captures the minimal necessary information (name of the benchmark to display,  table/row name in the database that has the datapoints). The idea is that final configs are not edited manually. It would make sense for the visualization tool to provide a capability to generate the dashboard based on a common template + required minimal config. Is that what Scripted Dashboards is? The webpage page says this feature is deprecated.", "author": "tvalentyn", "createdAt": "2020-05-27T17:23:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg2NjQ0Mw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431866443", "bodyText": "The idea is that final configs are not edited manually.\n\nLet me explain: the configs are not edited manually. At least they shouldn't be. The current solution involves creating a local instance of Grafana, exporting a new dashboard to JSON file, and pasting that file into the git repo. I found an interesting discussion on Grafana's repo: grafana/grafana#13823. It looks like this workflow is quite popular among users.\n\nIs that what Scripted Dashboards is?\n\nProbably. But I guess we'd rather avoid using deprecated features.", "author": "kamilwu", "createdAt": "2020-05-28T14:12:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwMTI4OQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432101289", "bodyText": "exporting a new dashboard to JSON file\n\nThanks, does this mean: creating a new dashboard manually via UI?", "author": "tvalentyn", "createdAt": "2020-05-28T20:22:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM2MTA3NQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432361075", "bodyText": "Yes, you're right.", "author": "kamilwu", "createdAt": "2020-05-29T09:19:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY1NDA2OA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432654068", "bodyText": "Thanks - is this workflow documented somewhere on Beam's website? It could be a few pointers to Grafana's documentation,  maybe screenshots.", "author": "tvalentyn", "createdAt": "2020-05-29T18:10:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzE1Mjc3Mw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r433152773", "bodyText": "There's an instruction on wiki: https://cwiki.apache.org/confluence/display/BEAM/Community+Metrics#CommunityMetrics-UpdatingDashboards.", "author": "kamilwu", "createdAt": "2020-06-01T10:16:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMxOTE2MQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r431319161", "bodyText": "I think the metric name should be 'Runtime', or something like that, not the test suite name.\nAlso, it may be more common to pass (Key,Value) pairs instead of (Value, Key)", "author": "tvalentyn", "createdAt": "2020-05-27T17:31:18Z", "path": "sdks/python/apache_beam/examples/wordcount_it_test.py", "diffHunk": "@@ -84,11 +87,45 @@ def _run_wordcount_it(self, run_wordcount, **opts):\n     # Register clean up before pipeline execution\n     self.addCleanup(delete_files, [test_output + '*'])\n \n+    publish_to_bq = bool(\n+        test_pipeline.get_option('publish_to_big_query') or False)\n+\n+    # Start measure time for performance test\n+    start_time = time.time()\n+\n     # Get pipeline options from command argument: --test-pipeline-options,\n     # and start pipeline job by calling pipeline main function.\n     run_wordcount(\n         test_pipeline.get_full_options_as_args(**extra_opts),\n-        save_main_session=False)\n+        save_main_session=False,\n+    )\n+\n+    end_time = time.time()\n+    run_time = end_time - start_time\n+\n+    if publish_to_bq:\n+      self._publish_metrics(test_pipeline, run_time)\n+\n+  def _publish_metrics(self, pipeline, metric_value):\n+    influx_options = InfluxDBMetricsPublisherOptions(\n+        pipeline.get_option('influx_measurement'),\n+        pipeline.get_option('influx_db_name'),\n+        pipeline.get_option('influx_hostname'),\n+        os.getenv('INFLUXDB_USER'),\n+        os.getenv('INFLUXDB_USER_PASSWORD'),\n+    )\n+    metric_reader = MetricsReader(\n+        project_name=pipeline.get_option('project'),\n+        bq_table=pipeline.get_option('metrics_table'),\n+        bq_dataset=pipeline.get_option('metrics_dataset'),\n+        publish_to_bq=True,\n+        influxdb_options=influx_options,\n+    )\n+\n+    metric_reader.publish_values((\n+        metric_value,", "originalCommit": "f84ff0d04a3abbf0568f5fab0ecdbd6eb2133c55", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA2MTgwOQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432061809", "bodyText": "Good point, I changed it to wordcount_it_runtime and the order of key, value.", "author": "piotr-szuberski", "createdAt": "2020-05-28T19:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMxOTE2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNjA5Mw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432106093", "bodyText": "Do we need \"wordcount_it\" in the metric name? It depends on how these metrics will be stored, if they are already associated in the database with a test suite that launches the pipeline  Python WordCount IT Benchmarks, then this information is captured and perhaps we don't need to repeat it in two different places. Leaving this up to you and @kamilwu.", "author": "tvalentyn", "createdAt": "2020-05-28T20:31:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMxOTE2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNzI1MQ==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432107251", "bodyText": "Seeing this query now - yes, I wound just keep the metric 'runtime', since we already know it is wordcount_py27_results, and it would be simpler that pipeline does not need to know the name of the suite. In the future we might add different metrics like 'cost' or total cputime consumed by other workers as opposed to runtime.", "author": "tvalentyn", "createdAt": "2020-05-28T20:33:47Z", "path": ".test-infra/metrics/grafana/dashboards/perftests_metrics/Python_Performance_Tests.json", "diffHunk": "@@ -77,7 +77,7 @@\n           ],\n           \"orderByTime\": \"ASC\",\n           \"policy\": \"default\",\n-          \"query\": \"SELECT mean(\\\"value\\\") FROM \\\"wordcount_py27_results\\\" WHERE metric = 'Python performance test' AND $timeFilter GROUP BY time($__interval),  \\\"metric\\\"\",\n+          \"query\": \"SELECT mean(\\\"value\\\") FROM \\\"wordcount_py27_results\\\" WHERE metric = 'wordcount_it_runtime' AND $timeFilter GROUP BY time($__interval),  \\\"metric\\\"\",", "originalCommit": "3ee636005a74c112286b7c66663ed5cb82ceeddd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM2NDgyNw==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432364827", "bodyText": "I agree, runtime will be enough.", "author": "kamilwu", "createdAt": "2020-05-29T09:26:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNzI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQwOTQ4MA==", "url": "https://github.com/apache/beam/pull/11661#discussion_r432409480", "bodyText": "I changed it to 'runtime'", "author": "piotr-szuberski", "createdAt": "2020-05-29T10:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNzI1MQ=="}], "type": "inlineReview"}, {"oid": "a1a1c659197dcbee3cc2d2995904e8c4c6ae1927", "url": "https://github.com/apache/beam/commit/a1a1c659197dcbee3cc2d2995904e8c4c6ae1927", "message": "[BEAM-7774] Add python wordcount performance test grafana dashboard", "committedDate": "2020-05-29T10:57:33Z", "type": "forcePushed"}, {"oid": "90e3dcd68d460e2464e4a1b972b181460d8cbf2c", "url": "https://github.com/apache/beam/commit/90e3dcd68d460e2464e4a1b972b181460d8cbf2c", "message": "[BEAM-7774] Move duplicated code from py2 and py37 suites to common.gradle", "committedDate": "2020-05-29T15:16:10Z", "type": "commit"}, {"oid": "927aad2d21473ceaf53214413c1bee2d45da9256", "url": "https://github.com/apache/beam/commit/927aad2d21473ceaf53214413c1bee2d45da9256", "message": "[BEAM-7774] Add python wordcount performance test grafana dashboard", "committedDate": "2020-05-29T15:19:23Z", "type": "forcePushed"}, {"oid": "2097cd074e2b1ca9f52dbaaa965b9aaad29c0d21", "url": "https://github.com/apache/beam/commit/2097cd074e2b1ca9f52dbaaa965b9aaad29c0d21", "message": "[BEAM-7774] Update jenkins jobs readme - remove python 35 and 36 wordcount it performance tests", "committedDate": "2020-06-01T10:27:39Z", "type": "forcePushed"}, {"oid": "d5e277aa60af4092dcf90515b2c9d56fdf05e7cc", "url": "https://github.com/apache/beam/commit/d5e277aa60af4092dcf90515b2c9d56fdf05e7cc", "message": "[BEAM-7774] Update jenkins jobs readme - remove python 35 and 36 wordcount it performance tests", "committedDate": "2020-06-01T13:45:42Z", "type": "forcePushed"}, {"oid": "684261e4175db136723c9bed5cee2f293469e465", "url": "https://github.com/apache/beam/commit/684261e4175db136723c9bed5cee2f293469e465", "message": "[BEAM-7774] Update jenkins jobs readme - remove python 35 and 36 wordcount it performance tests", "committedDate": "2020-06-01T15:25:02Z", "type": "forcePushed"}, {"oid": "21d8edd484cadf888d75b02ebc8efd98b4ca641b", "url": "https://github.com/apache/beam/commit/21d8edd484cadf888d75b02ebc8efd98b4ca641b", "message": "[BEAM-7774] Add use influx credentials to the jenkins job", "committedDate": "2020-06-02T08:22:07Z", "type": "forcePushed"}, {"oid": "aedef64a6f6ba87e6d7221c33758b0826e95a40c", "url": "https://github.com/apache/beam/commit/aedef64a6f6ba87e6d7221c33758b0826e95a40c", "message": "[BEAM-7774] Update jenkins jobs readme - remove python 35 and 36 wordcount it performance tests", "committedDate": "2020-06-02T09:02:41Z", "type": "forcePushed"}, {"oid": "92694b9f7f392f00c2f91a18579c0af27f734ede", "url": "https://github.com/apache/beam/commit/92694b9f7f392f00c2f91a18579c0af27f734ede", "message": "[BEAM-7774] Update jenkins jobs readme - remove python 35 and 36 wordcount it performance tests", "committedDate": "2020-06-02T12:26:51Z", "type": "forcePushed"}, {"oid": "cf93a2841ef5821f4654312f7ecb7b841af1bf7a", "url": "https://github.com/apache/beam/commit/cf93a2841ef5821f4654312f7ecb7b841af1bf7a", "message": "[BEAM-7774] Remove Perfkit Benchmarking tool from python wordcount performance tests jobs", "committedDate": "2020-06-02T14:33:44Z", "type": "commit"}, {"oid": "6ef8e2a27d8b780119e6ef2fdf8cc4d90bb765e2", "url": "https://github.com/apache/beam/commit/6ef8e2a27d8b780119e6ef2fdf8cc4d90bb765e2", "message": "[BEAM-7774] Add python wordcount performance test grafana dashboard", "committedDate": "2020-06-02T14:33:59Z", "type": "forcePushed"}, {"oid": "56e853e72d82eac9567a8ceebdcad8eec12ad107", "url": "https://github.com/apache/beam/commit/56e853e72d82eac9567a8ceebdcad8eec12ad107", "message": "[BEAM-7774] Add python wordcount performance test grafana dashboard", "committedDate": "2020-06-02T15:46:51Z", "type": "commit"}, {"oid": "56e853e72d82eac9567a8ceebdcad8eec12ad107", "url": "https://github.com/apache/beam/commit/56e853e72d82eac9567a8ceebdcad8eec12ad107", "message": "[BEAM-7774] Add python wordcount performance test grafana dashboard", "committedDate": "2020-06-02T15:46:51Z", "type": "forcePushed"}]}