{"pr_number": 13170, "pr_title": "[BEAM-9650] Adding support for ReadAll from BigQuery transform", "pr_createdAt": "2020-10-22T20:19:58Z", "pr_url": "https://github.com/apache/beam/pull/13170", "timeline": [{"oid": "814e241c76f258c61f0795c4a11e02580b1f10a6", "url": "https://github.com/apache/beam/commit/814e241c76f258c61f0795c4a11e02580b1f10a6", "message": "fixup", "committedDate": "2020-11-03T23:58:53Z", "type": "forcePushed"}, {"oid": "0842eaa8d5f68946814f51df29971dda790fb7ad", "url": "https://github.com/apache/beam/commit/0842eaa8d5f68946814f51df29971dda790fb7ad", "message": "fix formatting", "committedDate": "2020-11-05T18:23:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0MzEzNg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r518943136", "bodyText": "I see a few different names here,\nReadAllFromBigQuery\nReadFromBigQueryRequest\nBigQueryReadRequest\nI'm a bit confused by the differences and interaction between these classes.\nIf ReadFromBigQueryRequest is something users interact with it should not be in an internal file (e.g. bigquery_read_internal.py). Is there a need to expose that at all? Instead could it just be:\nside_input = (\n  p\n  | 'PeriodicImpulse' >> PeriodicImpulse(...)\n  | beam.io.ReadAllFromBigQuery(table=...))\n\nThough this would make the initial example of several requests being included in a single ReadAll not possible. Is this something that needs to be special cased, as opposed to say, using a flatten?", "author": "tysonjh", "createdAt": "2020-11-06T18:55:21Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -79,6 +79,41 @@\n `ReadFromBigQuery`, you can use the flag `use_json_exports` to export\n data as JSON, and receive base64-encoded bytes.\n \n+ReadAllFromBigQuery\n+-------------------\n+Beam 2.27.0 introduces a new transform called `ReadAllFromBigQuery` which\n+allows you to define table and query reads from BigQuery at pipeline\n+runtime.:::\n+\n+  read_requests = p | beam.Create([\n+      ReadFromBigQueryRequest(query='SELECT * FROM mydataset.mytable'),\n+      ReadFromBigQueryRequest(table='myproject.mydataset.mytable')])\n+  results = read_requests | ReadAllFromBigQuery()\n+\n+A good application for this transform is in streaming pipelines to\n+refresh a side input coming from BigQuery. This would work like so:::\n+\n+  side_input = (\n+      p\n+      | 'PeriodicImpulse' >> PeriodicImpulse(\n+          first_timestamp, last_timestamp, interval, True)\n+      | 'MapToReadRequest' >> beam.Map(\n+          lambda x: BigQueryReadRequest(table='dataset.table'))", "originalCommit": "da42abab1ea6f987ba5e121b4c249455f968ee7f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwNTg2OA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r522405868", "bodyText": "Regarding names - yes, that's a little confusing. The only names should be:\n\nReadFromBigQueryRequest - this is an input element for ReadAllFromBigQuery, and it represents a query or a table to be read (with a few other parameters).\nReadAllFromBigQuery - This is the transform that issues BQ reads.\n\nAll other names are misnaming in the configuration\n\nRegarding your example - that's interesting. I recognize that what you show would be the most common use case (same query/table always, rather than varying) - with the only exception that some queries could be slightly updated over time (e.g. read only partitions of the last few days).\notoh, this would create two ways of using the transform, and complicate the constructor (all of the parameters in ReadFromBQRequest would need to be available in the constructor).\nUsers could build this functionality themselves though. My feeling is that it's better to build a transform that is more composable, and provide an example for users trying to build the functionality you propose. WDYT?", "author": "pabloem", "createdAt": "2020-11-12T20:28:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0MzEzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI5MjQ4Mg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r523292482", "bodyText": "My feeling is that it's better to build a transform that is more composable, and provide an example for users trying to build the functionality you propose. WDYT?\n\n+1.", "author": "tysonjh", "createdAt": "2020-11-13T23:56:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0MzEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NTIzNA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r520185234", "bodyText": "This should be resolved or attributed to a Jira issue.", "author": "tysonjh", "createdAt": "2020-11-09T23:25:04Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1919,3 +1890,77 @@ def file_path_to_remove(unused_elm):\n                 *self._args,\n                 **self._kwargs))\n         | _PassThroughThenCleanup(files_to_remove_pcoll))\n+\n+\n+@experimental()\n+class ReadAllFromBigQuery(PTransform):\n+  \"\"\"Read data from BigQuery.\n+\n+    PTransform:ReadAllFromBigQueryRequest->Rows\n+\n+    This PTransform uses a BigQuery export job to take a snapshot of the table\n+    on GCS, and then reads from each produced JSON file.\n+\n+    It is recommended not to use this PTransform for streaming jobs on\n+    GlobalWindow, since it will not be able to cleanup snapshots.\n+\n+  Args:\n+    gcs_location (str): The name of the Google Cloud Storage\n+      bucket where the extracted table should be written as a string. If\n+      :data:`None`, then the temp_location parameter is used.\n+    validate (bool): If :data:`True`, various checks will be done when source\n+      gets initialized (e.g., is table present?).\n+    kms_key (str): Experimental. Optional Cloud KMS key name for use when\n+      creating new temporary tables.\n+   \"\"\"\n+  COUNTER = 0\n+\n+  def __init__(\n+      self,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      validate: bool = False,\n+      kms_key: str = None,\n+      bigquery_job_labels: Dict[str, str] = None):\n+    if gcs_location:\n+      if not isinstance(gcs_location, (str, ValueProvider)):\n+        raise TypeError(\n+            '%s: gcs_location must be of type string'\n+            ' or ValueProvider; got %r instead' %\n+            (self.__class__.__name__, type(gcs_location)))\n+\n+    self.gcs_location = gcs_location\n+    self.validate = validate\n+    self.kms_key = kms_key\n+    self.bigquery_job_labels = bigquery_job_labels\n+\n+  def expand(self, pcoll):\n+    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n+    project = pcoll.pipeline.options.view_as(GoogleCloudOptions).project\n+    unique_id = str(uuid.uuid4())[0:10]\n+\n+    try:\n+      step_name = self.label\n+    except AttributeError:\n+      step_name = 'ReadAllFromBigQuery_%d' % ReadAllFromBigQuery.COUNTER\n+      ReadAllFromBigQuery.COUNTER += 1\n+\n+    sources_to_read, cleanup_locations = (\n+        pcoll\n+        | beam.ParDo(\n+        # TODO(pabloem): Make sure we have all necessary args.", "originalCommit": "da42abab1ea6f987ba5e121b4c249455f968ee7f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM5OTk2OA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r522399968", "bodyText": "removed. thanks Tyson!", "author": "pabloem", "createdAt": "2020-11-12T20:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NTIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NzE1Ng==", "url": "https://github.com/apache/beam/pull/13170#discussion_r520187156", "bodyText": "The default here is False.", "author": "tysonjh", "createdAt": "2020-11-09T23:30:04Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +121,290 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class ReadFromBigQueryRequest:\n+  \"\"\"\n+  Class that defines data to read from BQ.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      query: str = None,\n+      use_standard_sql: bool = True,\n+      table: Union[str, TableReference] = None,\n+      flatten_results: bool = False):\n+    \"\"\"\n+    Only one of query or table should be specified.\n+\n+    :param query: SQL query to fetch data.\n+    :param use_standard_sql:\n+      Specifies whether to use BigQuery's standard SQL dialect for this query.\n+      The default value is :data:`True`. If set to :data:`False`,\n+      the query will use BigQuery's legacy SQL dialect.\n+      This parameter is ignored for table inputs.\n+    :param table:\n+      The ID of the table to read. The ID must contain only letters\n+      ``a-z``, ``A-Z``, numbers ``0-9``, or underscores ``_``. Table should\n+      define project and dataset (ex.: ``'PROJECT:DATASET.TABLE'``).\n+    :param flatten_results:\n+      Flattens all nested and repeated fields in the query results.\n+      The default value is :data:`True`.", "originalCommit": "da42abab1ea6f987ba5e121b4c249455f968ee7f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM5OTk5NA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r522399994", "bodyText": "oops good catch. Thanks Tyson!", "author": "pabloem", "createdAt": "2020-11-12T20:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NzE1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE5NzY2NQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r520197665", "bodyText": "Can this be moved up to the other if element.query condition? That may allow putting the yield into the for loop above, getting rid of the intermediate split_result and avoiding the additional iteration.", "author": "tysonjh", "createdAt": "2020-11-09T23:58:31Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +121,290 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class ReadFromBigQueryRequest:\n+  \"\"\"\n+  Class that defines data to read from BQ.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      query: str = None,\n+      use_standard_sql: bool = True,\n+      table: Union[str, TableReference] = None,\n+      flatten_results: bool = False):\n+    \"\"\"\n+    Only one of query or table should be specified.\n+\n+    :param query: SQL query to fetch data.\n+    :param use_standard_sql:\n+      Specifies whether to use BigQuery's standard SQL dialect for this query.\n+      The default value is :data:`True`. If set to :data:`False`,\n+      the query will use BigQuery's legacy SQL dialect.\n+      This parameter is ignored for table inputs.\n+    :param table:\n+      The ID of the table to read. The ID must contain only letters\n+      ``a-z``, ``A-Z``, numbers ``0-9``, or underscores ``_``. Table should\n+      define project and dataset (ex.: ``'PROJECT:DATASET.TABLE'``).\n+    :param flatten_results:\n+      Flattens all nested and repeated fields in the query results.\n+      The default value is :data:`True`.\n+    \"\"\"\n+    self.flatten_results = flatten_results\n+    self.query = query\n+    self.use_standard_sql = use_standard_sql\n+    self.table = table\n+    self.validate()\n+\n+    # We use this internal object ID to generate BigQuery export directories.\n+    self.obj_id = random.randint(0, 100000)\n+\n+  def validate(self):\n+    if self.table is not None and self.query is not None:\n+      raise ValueError(\n+          'Both a BigQuery table and a query were specified.'\n+          ' Please specify only one of these.')\n+    elif self.table is None and self.query is None:\n+      raise ValueError('A BigQuery table or a query must be specified')\n+    if self.table is not None:\n+      if isinstance(self.table, str):\n+        assert self.table.find('.'), (\n+            'Expected a table reference '\n+            '(PROJECT:DATASET.TABLE or DATASET.TABLE) instead of %s'\n+            % self.table)\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):\n+  def __init__(\n+      self,\n+      options: PipelineOptions,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      use_json_exports: bool = False,\n+      bigquery_job_labels: Dict[str, str] = None,\n+      step_name: str = None,\n+      job_name: str = None,\n+      unique_id: str = None,\n+      kms_key: str = None,\n+      project: str = None):\n+    self.options = options\n+    self.use_json_exports = use_json_exports\n+    self.gcs_location = gcs_location\n+    self.bigquery_job_labels = bigquery_job_labels or {}\n+    self._step_name = step_name\n+    self._job_name = job_name or 'BQ_READ_SPLIT'\n+    self._source_uuid = unique_id\n+    self.kms_key = kms_key\n+    self.project = project\n+    self.bq_io_metadata = None\n+\n+  def process(self, element: ReadFromBigQueryRequest, *args,\n+              **kwargs) -> Iterable[BoundedSource]:\n+    bq = bigquery_tools.BigQueryWrapper()\n+\n+    if element.query is not None:\n+      self._setup_temporary_dataset(bq, element)\n+      table_reference = self._execute_query(bq, element)\n+    else:\n+      assert element.table\n+      table_reference = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+\n+    if not table_reference.projectId:\n+      table_reference.projectId = self._get_project()\n+\n+    schema, metadata_list = self._export_files(bq, element, table_reference)\n+    split_result = [\n+        self._create_source(metadata.path, schema) for metadata in metadata_list\n+    ]\n+\n+    if element.query is not None:\n+      bq.clean_up_temporary_dataset(self._get_project())", "originalCommit": "da42abab1ea6f987ba5e121b4c249455f968ee7f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwMDAxNQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r522400015", "bodyText": "that's not possible in this case. We issue a table export in export_files, and only after that's finished is that we can delete the dataset. But I've moved the yield above.", "author": "pabloem", "createdAt": "2020-11-12T20:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE5NzY2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU5OTM2Mg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r521599362", "bodyText": "May be worth noting that there is also a UUID involved. I was worried about collisions until I read on a bit further.", "author": "tysonjh", "createdAt": "2020-11-11T19:49:19Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +121,290 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class ReadFromBigQueryRequest:\n+  \"\"\"\n+  Class that defines data to read from BQ.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      query: str = None,\n+      use_standard_sql: bool = True,\n+      table: Union[str, TableReference] = None,\n+      flatten_results: bool = False):\n+    \"\"\"\n+    Only one of query or table should be specified.\n+\n+    :param query: SQL query to fetch data.\n+    :param use_standard_sql:\n+      Specifies whether to use BigQuery's standard SQL dialect for this query.\n+      The default value is :data:`True`. If set to :data:`False`,\n+      the query will use BigQuery's legacy SQL dialect.\n+      This parameter is ignored for table inputs.\n+    :param table:\n+      The ID of the table to read. The ID must contain only letters\n+      ``a-z``, ``A-Z``, numbers ``0-9``, or underscores ``_``. Table should\n+      define project and dataset (ex.: ``'PROJECT:DATASET.TABLE'``).\n+    :param flatten_results:\n+      Flattens all nested and repeated fields in the query results.\n+      The default value is :data:`True`.\n+    \"\"\"\n+    self.flatten_results = flatten_results\n+    self.query = query\n+    self.use_standard_sql = use_standard_sql\n+    self.table = table\n+    self.validate()\n+\n+    # We use this internal object ID to generate BigQuery export directories.", "originalCommit": "da42abab1ea6f987ba5e121b4c249455f968ee7f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwMTMxMg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r522401312", "bodyText": "I've added this to the Pydoc of the transform.", "author": "pabloem", "createdAt": "2020-11-12T20:23:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU5OTM2Mg=="}], "type": "inlineReview"}, {"oid": "51d42439533c98d809d0dfcf850fccfe6f407e19", "url": "https://github.com/apache/beam/commit/51d42439533c98d809d0dfcf850fccfe6f407e19", "message": "Addressing comments", "committedDate": "2020-11-12T20:23:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI5MzM1OQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r523293359", "bodyText": "I still find it strange that this is a user API but is inside bigquery_read_internal.py. Is this normal for python? I would expect that users should not be using classes in internal files.", "author": "tysonjh", "createdAt": "2020-11-14T00:00:37Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +123,309 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class ReadFromBigQueryRequest:", "originalCommit": "a20b7d6994efba7c63c976a312f30cb9adebef04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzgwODEyNA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527808124", "bodyText": "You're right. I've moved it to a public-facing file.", "author": "pabloem", "createdAt": "2020-11-20T16:28:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI5MzM1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NTEzMw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r526495133", "bodyText": "Does this need to be here? Or can it be in the groovy config for running these tests instead?", "author": "tysonjh", "createdAt": "2020-11-18T23:40:00Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py", "diffHunk": "@@ -298,6 +298,108 @@ def test_iobase_source(self):\n       assert_that(result, equal_to(self.get_expected_data(native=False)))\n \n \n+class ReadAllBQTests(BigQueryReadIntegrationTests):\n+  TABLE_DATA_1 = [{\n+      'number': 1, 'str': 'abc'\n+  }, {\n+      'number': 2, 'str': 'def'\n+  }, {\n+      'number': 3, 'str': u'\u4f60\u597d'\n+  }, {\n+      'number': 4, 'str': u'\u043f\u0440\u0438\u0432\u0435\u0442'\n+  }]\n+\n+  TABLE_DATA_2 = [{\n+      'number': 10, 'str': 'abcd'\n+  }, {\n+      'number': 20, 'str': 'defg'\n+  }, {\n+      'number': 30, 'str': u'\u4f60\u597d'\n+  }, {\n+      'number': 40, 'str': u'\u043f\u0440\u0438\u0432\u0435\u0442'\n+  }]\n+\n+  TABLE_DATA_3 = [{'number': 10, 'str': 'abcde', 'extra': 3}]\n+\n+  @classmethod\n+  def setUpClass(cls):\n+    super(ReadAllBQTests, cls).setUpClass()\n+    cls.SCHEMA_BQ = cls.create_bq_schema()\n+    cls.SCHEMA_BQ_WITH_EXTRA = cls.create_bq_schema(True)\n+\n+    cls.table_name1 = 'python_rd_table_1'\n+    cls.table_schema1 = cls.create_table(\n+        cls.table_name1, cls.TABLE_DATA_1, cls.SCHEMA_BQ)\n+    table_id1 = '{}.{}'.format(cls.dataset_id, cls.table_name1)\n+    cls.query1 = 'SELECT number, str FROM `%s`' % table_id1\n+\n+    cls.table_name2 = 'python_rd_table_2'\n+    cls.table_schema2 = cls.create_table(\n+        cls.table_name2, cls.TABLE_DATA_2, cls.SCHEMA_BQ)\n+    table_id2 = '{}.{}'.format(cls.dataset_id, cls.table_name2)\n+    cls.query2 = 'SELECT number, str FROM %s' % table_id2\n+\n+    cls.table_name3 = 'python_rd_table_3'\n+    cls.table_schema3 = cls.create_table(\n+        cls.table_name3, cls.TABLE_DATA_3, cls.SCHEMA_BQ_WITH_EXTRA)\n+    table_id3 = '{}.{}'.format(cls.dataset_id, cls.table_name3)\n+    cls.query3 = 'SELECT number, str, extra FROM `%s`' % table_id3\n+\n+  @classmethod\n+  def create_table(cls, table_name, data, table_schema):\n+    table = bigquery.Table(\n+        tableReference=bigquery.TableReference(\n+            projectId=cls.project, datasetId=cls.dataset_id,\n+            tableId=table_name),\n+        schema=table_schema)\n+    request = bigquery.BigqueryTablesInsertRequest(\n+        projectId=cls.project, datasetId=cls.dataset_id, table=table)\n+    cls.bigquery_client.client.tables.Insert(request)\n+    cls.bigquery_client.insert_rows(\n+        cls.project, cls.dataset_id, table_name, data)\n+    return table_schema\n+\n+  @classmethod\n+  def create_bq_schema(cls, with_extra=False):\n+    table_schema = bigquery.TableSchema()\n+    table_field = bigquery.TableFieldSchema()\n+    table_field.name = 'number'\n+    table_field.type = 'INTEGER'\n+    table_field.mode = 'NULLABLE'\n+    table_schema.fields.append(table_field)\n+    table_field = bigquery.TableFieldSchema()\n+    table_field.name = 'str'\n+    table_field.type = 'STRING'\n+    table_field.mode = 'NULLABLE'\n+    table_schema.fields.append(table_field)\n+    if with_extra:\n+      table_field = bigquery.TableFieldSchema()\n+      table_field.name = 'extra'\n+      table_field.type = 'INTEGER'\n+      table_field.mode = 'NULLABLE'\n+      table_schema.fields.append(table_field)\n+    return table_schema\n+\n+  @skip(['PortableRunner', 'FlinkRunner'])\n+  @attr('IT')\n+  def test_read_queries(self):\n+    args = self.args + [\"--experiments=use_runner_v2\"]", "originalCommit": "a20b7d6994efba7c63c976a312f30cb9adebef04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzgxMzI2MA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527813260", "bodyText": "let me see if I can do that....", "author": "pabloem", "createdAt": "2020-11-20T16:36:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NTEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzgxODI0Nw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527818247", "bodyText": "I think we dont have external runner_v2 tests, and this feature works only on runner_v2, so to avoid deactivating it on Dataflow, I'm adding the flag. I can add a TODO to fix when we move to runner_v2 for DF tests.", "author": "pabloem", "createdAt": "2020-11-20T16:44:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NTEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5OTI4Mg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r526499282", "bodyText": "Why ? Also is this supported for Dataflow Runnner v2 ?", "author": "chamikaramj", "createdAt": "2020-11-18T23:50:58Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -79,6 +79,41 @@\n `ReadFromBigQuery`, you can use the flag `use_json_exports` to export\n data as JSON, and receive base64-encoded bytes.\n \n+ReadAllFromBigQuery\n+-------------------\n+Beam 2.27.0 introduces a new transform called `ReadAllFromBigQuery` which\n+allows you to define table and query reads from BigQuery at pipeline\n+runtime.:::\n+\n+  read_requests = p | beam.Create([\n+      ReadFromBigQueryRequest(query='SELECT * FROM mydataset.mytable'),\n+      ReadFromBigQueryRequest(table='myproject.mydataset.mytable')])\n+  results = read_requests | ReadAllFromBigQuery()\n+\n+A good application for this transform is in streaming pipelines to\n+refresh a side input coming from BigQuery. This would work like so:::\n+\n+  side_input = (\n+      p\n+      | 'PeriodicImpulse' >> PeriodicImpulse(\n+          first_timestamp, last_timestamp, interval, True)\n+      | 'MapToReadRequest' >> beam.Map(\n+          lambda x: ReadFromBigQueryRequest(table='dataset.table'))\n+      | beam.io.ReadAllFromBigQuery())\n+  main_input = (\n+      p\n+      | 'MpImpulse' >> beam.Create(sample_main_input_elements)\n+      |\n+      'MapMpToTimestamped' >> beam.Map(lambda src: TimestampedValue(src, src))\n+      | 'WindowMpInto' >> beam.WindowInto(\n+          window.FixedWindows(main_input_windowing_interval)))\n+  result = (\n+      main_input\n+      | 'ApplyCrossJoin' >> beam.FlatMap(\n+          cross_join, rights=beam.pvalue.AsIter(side_input)))\n+\n+**Note**: This transform is supported on Portable runners only.", "originalCommit": "a20b7d6994efba7c63c976a312f30cb9adebef04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAyOTg4MA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528029880", "bodyText": "yes, runner v2 supports this as well", "author": "pabloem", "createdAt": "2020-11-20T23:54:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5OTI4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzkzMTM0MA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527931340", "bodyText": "What about other args here ? https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py#L1823\n(probably this can be a followup PR)", "author": "chamikaramj", "createdAt": "2020-11-20T19:40:19Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1931,3 +1903,132 @@ def file_path_to_remove(unused_elm):\n                 *self._args,\n                 **self._kwargs))\n         | _PassThroughThenCleanup(files_to_remove_pcoll))\n+\n+\n+class ReadFromBigQueryRequest:\n+  \"\"\"\n+  Class that defines data to read from BQ.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      query: str = None,\n+      use_standard_sql: bool = True,", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAzMDA0OQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528030049", "bodyText": "args are split between the read request and the transform. I would say all args exist in either.", "author": "pabloem", "createdAt": "2020-11-20T23:55:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzkzMTM0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzkzOTM4NQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527939385", "bodyText": "Add some pydocs here to clarify what this does ?", "author": "chamikaramj", "createdAt": "2020-11-20T19:57:23Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +127,258 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MDA4Mw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527940083", "bodyText": "Will we run into BQ quotas if we try to create a large number of datasets ?", "author": "chamikaramj", "createdAt": "2020-11-20T19:58:58Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +127,258 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):\n+  def __init__(\n+      self,\n+      options: PipelineOptions,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      use_json_exports: bool = False,\n+      bigquery_job_labels: Dict[str, str] = None,\n+      step_name: str = None,\n+      job_name: str = None,\n+      unique_id: str = None,\n+      kms_key: str = None,\n+      project: str = None,\n+      temp_dataset: Union[str, DatasetReference] = None):\n+    self.options = options\n+    self.use_json_exports = use_json_exports\n+    self.gcs_location = gcs_location\n+    self.bigquery_job_labels = bigquery_job_labels or {}\n+    self._step_name = step_name\n+    self._job_name = job_name or 'BQ_READ_SPLIT'\n+    self._source_uuid = unique_id\n+    self.kms_key = kms_key\n+    self.project = project\n+    self.temp_dataset = temp_dataset\n+    self.bq_io_metadata = None\n+\n+  def display_data(self):\n+    return {\n+        'use_json_exports': str(self.use_json_exports),\n+        'gcs_location': str(self.gcs_location),\n+        'bigquery_job_labels': json.dumps(self.bigquery_job_labels),\n+        'kms_key': str(self.kms_key),\n+        'project': str(self.project),\n+        'temp_dataset': str(self.temp_dataset)\n+    }\n+\n+  def _get_temp_dataset(self):\n+    if isinstance(self.temp_dataset, str):\n+      return DatasetReference(\n+          datasetId=self.temp_dataset, projectId=self._get_project())\n+    else:\n+      return self.temp_dataset\n+\n+  def process(self, element: 'ReadFromBigQueryRequest', *args,\n+              **kwargs) -> Iterable[BoundedSource]:\n+    bq = bigquery_tools.BigQueryWrapper(\n+        temp_dataset_id=(\n+            self._get_temp_dataset().datasetId if self._get_temp_dataset(", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAzNzEyNw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528037127", "bodyText": "hm this is a great catch Cham. I've changed this so we always use the same Dataset, and we just delete the temporary table. WDYT?", "author": "pabloem", "createdAt": "2020-11-21T00:26:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MDA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MDg2NA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527940864", "bodyText": "Is there a reason why we couldn't use existing coders ?", "author": "chamikaramj", "createdAt": "2020-11-20T20:00:35Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +127,258 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):\n+  def __init__(\n+      self,\n+      options: PipelineOptions,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      use_json_exports: bool = False,\n+      bigquery_job_labels: Dict[str, str] = None,\n+      step_name: str = None,\n+      job_name: str = None,\n+      unique_id: str = None,\n+      kms_key: str = None,\n+      project: str = None,\n+      temp_dataset: Union[str, DatasetReference] = None):\n+    self.options = options\n+    self.use_json_exports = use_json_exports\n+    self.gcs_location = gcs_location\n+    self.bigquery_job_labels = bigquery_job_labels or {}\n+    self._step_name = step_name\n+    self._job_name = job_name or 'BQ_READ_SPLIT'\n+    self._source_uuid = unique_id\n+    self.kms_key = kms_key\n+    self.project = project\n+    self.temp_dataset = temp_dataset\n+    self.bq_io_metadata = None\n+\n+  def display_data(self):\n+    return {\n+        'use_json_exports': str(self.use_json_exports),\n+        'gcs_location': str(self.gcs_location),\n+        'bigquery_job_labels': json.dumps(self.bigquery_job_labels),\n+        'kms_key': str(self.kms_key),\n+        'project': str(self.project),\n+        'temp_dataset': str(self.temp_dataset)\n+    }\n+\n+  def _get_temp_dataset(self):\n+    if isinstance(self.temp_dataset, str):\n+      return DatasetReference(\n+          datasetId=self.temp_dataset, projectId=self._get_project())\n+    else:\n+      return self.temp_dataset\n+\n+  def process(self, element: 'ReadFromBigQueryRequest', *args,\n+              **kwargs) -> Iterable[BoundedSource]:\n+    bq = bigquery_tools.BigQueryWrapper(\n+        temp_dataset_id=(\n+            self._get_temp_dataset().datasetId if self._get_temp_dataset(\n+            ) else None))\n+\n+    if element.query is not None:\n+      self._setup_temporary_dataset(bq, element)\n+      table_reference = self._execute_query(bq, element)\n+    else:\n+      assert element.table\n+      table_reference = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+\n+    if not table_reference.projectId:\n+      table_reference.projectId = self._get_project()\n+\n+    schema, metadata_list = self._export_files(bq, element, table_reference)\n+\n+    for metadata in metadata_list:\n+      yield self._create_source(metadata.path, schema)\n+\n+    if element.query is not None:\n+      bq.clean_up_temporary_dataset(self._get_project())\n+\n+  def _get_bq_metadata(self):\n+    if not self.bq_io_metadata:\n+      self.bq_io_metadata = create_bigquery_io_metadata(self._step_name)\n+    return self.bq_io_metadata\n+\n+  def _create_source(self, path, schema):\n+    if not self.use_json_exports:\n+      return _create_avro_source(path, use_fastavro=True)\n+    else:\n+      return _TextSource(\n+          path,\n+          min_bundle_size=0,\n+          compression_type=CompressionTypes.UNCOMPRESSED,\n+          strip_trailing_newlines=True,\n+          coder=_JsonToDictCoder(schema))\n+\n+  def _setup_temporary_dataset(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest'):\n+    location = bq.get_query_location(\n+        self._get_project(), element.query, not element.use_standard_sql)\n+    bq.create_temporary_dataset(self._get_project(), location)\n+\n+  def _execute_query(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest'):\n+    query_job_name = bigquery_tools.generate_bq_job_name(\n+        self._job_name,\n+        self._source_uuid,\n+        bigquery_tools.BigQueryJobTypes.QUERY,\n+        random.randint(0, 1000))\n+    job = bq._start_query_job(\n+        self._get_project(),\n+        element.query,\n+        not element.use_standard_sql,\n+        element.flatten_results,\n+        job_id=query_job_name,\n+        kms_key=self.kms_key,\n+        job_labels=self._get_bq_metadata().add_additional_bq_job_labels(\n+            self.bigquery_job_labels))\n+    job_ref = job.jobReference\n+    bq.wait_for_bq_job(job_ref, max_retries=0)\n+    return bq._get_temp_table(self._get_project())\n+\n+  def _export_files(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest',\n+      table_reference: TableReference):\n+    \"\"\"Runs a BigQuery export job.\n+\n+    Returns:\n+      bigquery.TableSchema instance, a list of FileMetadata instances\n+    \"\"\"\n+    job_labels = self._get_bq_metadata().add_additional_bq_job_labels(\n+        self.bigquery_job_labels)\n+    export_job_name = bigquery_tools.generate_bq_job_name(\n+        self._job_name,\n+        self._source_uuid,\n+        bigquery_tools.BigQueryJobTypes.EXPORT,\n+        element.obj_id)\n+    temp_location = self.options.view_as(GoogleCloudOptions).temp_location\n+    gcs_location = bigquery_export_destination_uri(\n+        self.gcs_location,\n+        temp_location,\n+        '%s%s' % (self._source_uuid, element.obj_id))\n+    if self.use_json_exports:\n+      job_ref = bq.perform_extract_job([gcs_location],\n+                                       export_job_name,\n+                                       table_reference,\n+                                       bigquery_tools.FileFormat.JSON,\n+                                       project=self._get_project(),\n+                                       job_labels=job_labels,\n+                                       include_header=False)\n+    else:\n+      job_ref = bq.perform_extract_job([gcs_location],\n+                                       export_job_name,\n+                                       table_reference,\n+                                       bigquery_tools.FileFormat.AVRO,\n+                                       project=self._get_project(),\n+                                       include_header=False,\n+                                       job_labels=job_labels,\n+                                       use_avro_logical_types=True)\n+    bq.wait_for_bq_job(job_ref)\n+    metadata_list = FileSystems.match([gcs_location])[0].metadata_list\n+\n+    if isinstance(table_reference, ValueProvider):\n+      table_ref = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+    else:\n+      table_ref = table_reference\n+    table = bq.get_table(\n+        table_ref.projectId, table_ref.datasetId, table_ref.tableId)\n+\n+    return table.schema, metadata_list\n+\n+  def _get_project(self):\n+    \"\"\"Returns the project that queries and exports will be billed to.\"\"\"\n+\n+    project = self.options.view_as(GoogleCloudOptions).project\n+    if isinstance(project, ValueProvider):\n+      project = project.get()\n+    if not project:\n+      project = self.project\n+    return project\n+\n+\n+FieldSchema = collections.namedtuple('FieldSchema', 'fields mode name type')\n+\n+\n+class _JsonToDictCoder(coders.Coder):", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAyNDc0OQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528024749", "bodyText": "This coder was just moved from bigquery.py to bq_read_internal.py", "author": "pabloem", "createdAt": "2020-11-20T23:34:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MDg2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAzOTkwNA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528039904", "bodyText": "Ah ok. Thanks.", "author": "chamikaramj", "createdAt": "2020-11-21T00:40:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MDg2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MTE3Ng==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527941176", "bodyText": "Please add unit tests for the new coder.", "author": "chamikaramj", "createdAt": "2020-11-20T20:01:15Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +127,258 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):\n+  def __init__(\n+      self,\n+      options: PipelineOptions,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      use_json_exports: bool = False,\n+      bigquery_job_labels: Dict[str, str] = None,\n+      step_name: str = None,\n+      job_name: str = None,\n+      unique_id: str = None,\n+      kms_key: str = None,\n+      project: str = None,\n+      temp_dataset: Union[str, DatasetReference] = None):\n+    self.options = options\n+    self.use_json_exports = use_json_exports\n+    self.gcs_location = gcs_location\n+    self.bigquery_job_labels = bigquery_job_labels or {}\n+    self._step_name = step_name\n+    self._job_name = job_name or 'BQ_READ_SPLIT'\n+    self._source_uuid = unique_id\n+    self.kms_key = kms_key\n+    self.project = project\n+    self.temp_dataset = temp_dataset\n+    self.bq_io_metadata = None\n+\n+  def display_data(self):\n+    return {\n+        'use_json_exports': str(self.use_json_exports),\n+        'gcs_location': str(self.gcs_location),\n+        'bigquery_job_labels': json.dumps(self.bigquery_job_labels),\n+        'kms_key': str(self.kms_key),\n+        'project': str(self.project),\n+        'temp_dataset': str(self.temp_dataset)\n+    }\n+\n+  def _get_temp_dataset(self):\n+    if isinstance(self.temp_dataset, str):\n+      return DatasetReference(\n+          datasetId=self.temp_dataset, projectId=self._get_project())\n+    else:\n+      return self.temp_dataset\n+\n+  def process(self, element: 'ReadFromBigQueryRequest', *args,\n+              **kwargs) -> Iterable[BoundedSource]:\n+    bq = bigquery_tools.BigQueryWrapper(\n+        temp_dataset_id=(\n+            self._get_temp_dataset().datasetId if self._get_temp_dataset(\n+            ) else None))\n+\n+    if element.query is not None:\n+      self._setup_temporary_dataset(bq, element)\n+      table_reference = self._execute_query(bq, element)\n+    else:\n+      assert element.table\n+      table_reference = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+\n+    if not table_reference.projectId:\n+      table_reference.projectId = self._get_project()\n+\n+    schema, metadata_list = self._export_files(bq, element, table_reference)\n+\n+    for metadata in metadata_list:\n+      yield self._create_source(metadata.path, schema)\n+\n+    if element.query is not None:\n+      bq.clean_up_temporary_dataset(self._get_project())\n+\n+  def _get_bq_metadata(self):\n+    if not self.bq_io_metadata:\n+      self.bq_io_metadata = create_bigquery_io_metadata(self._step_name)\n+    return self.bq_io_metadata\n+\n+  def _create_source(self, path, schema):\n+    if not self.use_json_exports:\n+      return _create_avro_source(path, use_fastavro=True)\n+    else:\n+      return _TextSource(\n+          path,\n+          min_bundle_size=0,\n+          compression_type=CompressionTypes.UNCOMPRESSED,\n+          strip_trailing_newlines=True,\n+          coder=_JsonToDictCoder(schema))\n+\n+  def _setup_temporary_dataset(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest'):\n+    location = bq.get_query_location(\n+        self._get_project(), element.query, not element.use_standard_sql)\n+    bq.create_temporary_dataset(self._get_project(), location)\n+\n+  def _execute_query(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest'):\n+    query_job_name = bigquery_tools.generate_bq_job_name(\n+        self._job_name,\n+        self._source_uuid,\n+        bigquery_tools.BigQueryJobTypes.QUERY,\n+        random.randint(0, 1000))\n+    job = bq._start_query_job(\n+        self._get_project(),\n+        element.query,\n+        not element.use_standard_sql,\n+        element.flatten_results,\n+        job_id=query_job_name,\n+        kms_key=self.kms_key,\n+        job_labels=self._get_bq_metadata().add_additional_bq_job_labels(\n+            self.bigquery_job_labels))\n+    job_ref = job.jobReference\n+    bq.wait_for_bq_job(job_ref, max_retries=0)\n+    return bq._get_temp_table(self._get_project())\n+\n+  def _export_files(\n+      self,\n+      bq: bigquery_tools.BigQueryWrapper,\n+      element: 'ReadFromBigQueryRequest',\n+      table_reference: TableReference):\n+    \"\"\"Runs a BigQuery export job.\n+\n+    Returns:\n+      bigquery.TableSchema instance, a list of FileMetadata instances\n+    \"\"\"\n+    job_labels = self._get_bq_metadata().add_additional_bq_job_labels(\n+        self.bigquery_job_labels)\n+    export_job_name = bigquery_tools.generate_bq_job_name(\n+        self._job_name,\n+        self._source_uuid,\n+        bigquery_tools.BigQueryJobTypes.EXPORT,\n+        element.obj_id)\n+    temp_location = self.options.view_as(GoogleCloudOptions).temp_location\n+    gcs_location = bigquery_export_destination_uri(\n+        self.gcs_location,\n+        temp_location,\n+        '%s%s' % (self._source_uuid, element.obj_id))\n+    if self.use_json_exports:\n+      job_ref = bq.perform_extract_job([gcs_location],\n+                                       export_job_name,\n+                                       table_reference,\n+                                       bigquery_tools.FileFormat.JSON,\n+                                       project=self._get_project(),\n+                                       job_labels=job_labels,\n+                                       include_header=False)\n+    else:\n+      job_ref = bq.perform_extract_job([gcs_location],\n+                                       export_job_name,\n+                                       table_reference,\n+                                       bigquery_tools.FileFormat.AVRO,\n+                                       project=self._get_project(),\n+                                       include_header=False,\n+                                       job_labels=job_labels,\n+                                       use_avro_logical_types=True)\n+    bq.wait_for_bq_job(job_ref)\n+    metadata_list = FileSystems.match([gcs_location])[0].metadata_list\n+\n+    if isinstance(table_reference, ValueProvider):\n+      table_ref = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+    else:\n+      table_ref = table_reference\n+    table = bq.get_table(\n+        table_ref.projectId, table_ref.datasetId, table_ref.tableId)\n+\n+    return table.schema, metadata_list\n+\n+  def _get_project(self):\n+    \"\"\"Returns the project that queries and exports will be billed to.\"\"\"\n+\n+    project = self.options.view_as(GoogleCloudOptions).project\n+    if isinstance(project, ValueProvider):\n+      project = project.get()\n+    if not project:\n+      project = self.project\n+    return project\n+\n+\n+FieldSchema = collections.namedtuple('FieldSchema', 'fields mode name type')\n+\n+\n+class _JsonToDictCoder(coders.Coder):\n+  \"\"\"A coder for a JSON string to a Python dict.\"\"\"\n+  def __init__(self, table_schema):", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAyNDg5OQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528024899", "bodyText": "The coder was just moved from bq.py to bq_read_internal.py", "author": "pabloem", "createdAt": "2020-11-20T23:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MTE3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MTU5OA==", "url": "https://github.com/apache/beam/pull/13170#discussion_r527941598", "bodyText": "We should also add an integration test.", "author": "chamikaramj", "createdAt": "2020-11-20T20:02:12Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_it_test.py", "diffHunk": "@@ -298,6 +298,109 @@ def test_iobase_source(self):\n       assert_that(result, equal_to(self.get_expected_data(native=False)))\n \n \n+class ReadAllBQTests(BigQueryReadIntegrationTests):\n+  TABLE_DATA_1 = [{\n+      'number': 1, 'str': 'abc'\n+  }, {\n+      'number': 2, 'str': 'def'\n+  }, {\n+      'number': 3, 'str': u'\u4f60\u597d'\n+  }, {\n+      'number': 4, 'str': u'\u043f\u0440\u0438\u0432\u0435\u0442'\n+  }]\n+\n+  TABLE_DATA_2 = [{\n+      'number': 10, 'str': 'abcd'\n+  }, {\n+      'number': 20, 'str': 'defg'\n+  }, {\n+      'number': 30, 'str': u'\u4f60\u597d'\n+  }, {\n+      'number': 40, 'str': u'\u043f\u0440\u0438\u0432\u0435\u0442'\n+  }]\n+\n+  TABLE_DATA_3 = [{'number': 10, 'str': 'abcde', 'extra': 3}]\n+\n+  @classmethod\n+  def setUpClass(cls):\n+    super(ReadAllBQTests, cls).setUpClass()\n+    cls.SCHEMA_BQ = cls.create_bq_schema()\n+    cls.SCHEMA_BQ_WITH_EXTRA = cls.create_bq_schema(True)\n+\n+    cls.table_name1 = 'python_rd_table_1'\n+    cls.table_schema1 = cls.create_table(\n+        cls.table_name1, cls.TABLE_DATA_1, cls.SCHEMA_BQ)\n+    table_id1 = '{}.{}'.format(cls.dataset_id, cls.table_name1)\n+    cls.query1 = 'SELECT number, str FROM `%s`' % table_id1\n+\n+    cls.table_name2 = 'python_rd_table_2'\n+    cls.table_schema2 = cls.create_table(\n+        cls.table_name2, cls.TABLE_DATA_2, cls.SCHEMA_BQ)\n+    table_id2 = '{}.{}'.format(cls.dataset_id, cls.table_name2)\n+    cls.query2 = 'SELECT number, str FROM %s' % table_id2\n+\n+    cls.table_name3 = 'python_rd_table_3'\n+    cls.table_schema3 = cls.create_table(\n+        cls.table_name3, cls.TABLE_DATA_3, cls.SCHEMA_BQ_WITH_EXTRA)\n+    table_id3 = '{}.{}'.format(cls.dataset_id, cls.table_name3)\n+    cls.query3 = 'SELECT number, str, extra FROM `%s`' % table_id3\n+\n+  @classmethod\n+  def create_table(cls, table_name, data, table_schema):\n+    table = bigquery.Table(\n+        tableReference=bigquery.TableReference(\n+            projectId=cls.project, datasetId=cls.dataset_id,\n+            tableId=table_name),\n+        schema=table_schema)\n+    request = bigquery.BigqueryTablesInsertRequest(\n+        projectId=cls.project, datasetId=cls.dataset_id, table=table)\n+    cls.bigquery_client.client.tables.Insert(request)\n+    cls.bigquery_client.insert_rows(\n+        cls.project, cls.dataset_id, table_name, data)\n+    return table_schema\n+\n+  @classmethod\n+  def create_bq_schema(cls, with_extra=False):\n+    table_schema = bigquery.TableSchema()\n+    table_field = bigquery.TableFieldSchema()\n+    table_field.name = 'number'\n+    table_field.type = 'INTEGER'\n+    table_field.mode = 'NULLABLE'\n+    table_schema.fields.append(table_field)\n+    table_field = bigquery.TableFieldSchema()\n+    table_field.name = 'str'\n+    table_field.type = 'STRING'\n+    table_field.mode = 'NULLABLE'\n+    table_schema.fields.append(table_field)\n+    if with_extra:\n+      table_field = bigquery.TableFieldSchema()\n+      table_field.name = 'extra'\n+      table_field.type = 'INTEGER'\n+      table_field.mode = 'NULLABLE'\n+      table_schema.fields.append(table_field)\n+    return table_schema\n+\n+  @skip(['PortableRunner', 'FlinkRunner'])\n+  @attr('IT')\n+  def test_read_queries(self):\n+    # TODO(BEAM-11311): Remove experiment when tests run on r_v2.\n+    args = self.args + [\"--experiments=use_runner_v2\"]\n+    with beam.Pipeline(argv=args) as p:\n+      result = (\n+          p\n+          | beam.Create([\n+              beam.io.ReadFromBigQueryRequest(query=self.query1),\n+              beam.io.ReadFromBigQueryRequest(\n+                  query=self.query2, use_standard_sql=False),", "originalCommit": "d5b718b9acfefffb660efe8a8b11cc8e775ad5f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODAyNTAyOQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528025029", "bodyText": "This is an integration test (attr('IT')) - do you mean something else?", "author": "pabloem", "createdAt": "2020-11-20T23:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0MDA2Mw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528040063", "bodyText": "I see. Thanks. No this is good.", "author": "chamikaramj", "createdAt": "2020-11-21T00:41:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk0MTU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA1NjE5MQ==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528056191", "bodyText": "We should also cleanup the temporary datasets that are created by the pipeline (may be at the end of every window).", "author": "chamikaramj", "createdAt": "2020-11-21T02:26:05Z", "path": "sdks/python/apache_beam/io/gcp/bigquery_read_internal.py", "diffHunk": "@@ -100,3 +127,264 @@ def process(self, unused_element, unused_signal, gcs_locations):\n     )\n \n     return main_output\n+\n+\n+class _BigQueryReadSplit(beam.transforms.DoFn):\n+  \"\"\"Starts the process of reading from BigQuery.\n+\n+  This transform will start a BigQuery export job, and output a number of\n+  file sources that are consumed downstream.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      options: PipelineOptions,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      use_json_exports: bool = False,\n+      bigquery_job_labels: Dict[str, str] = None,\n+      step_name: str = None,\n+      job_name: str = None,\n+      unique_id: str = None,\n+      kms_key: str = None,\n+      project: str = None,\n+      temp_dataset: Union[str, DatasetReference] = None):\n+    self.options = options\n+    self.use_json_exports = use_json_exports\n+    self.gcs_location = gcs_location\n+    self.bigquery_job_labels = bigquery_job_labels or {}\n+    self._step_name = step_name\n+    self._job_name = job_name or 'BQ_READ_SPLIT'\n+    self._source_uuid = unique_id\n+    self.kms_key = kms_key\n+    self.project = project\n+    self.temp_dataset = temp_dataset or 'bq_read_all_%s' % uuid.uuid4().hex\n+    self.bq_io_metadata = None\n+\n+  def display_data(self):\n+    return {\n+        'use_json_exports': str(self.use_json_exports),\n+        'gcs_location': str(self.gcs_location),\n+        'bigquery_job_labels': json.dumps(self.bigquery_job_labels),\n+        'kms_key': str(self.kms_key),\n+        'project': str(self.project),\n+        'temp_dataset': str(self.temp_dataset)\n+    }\n+\n+  def _get_temp_dataset(self):\n+    if isinstance(self.temp_dataset, str):\n+      return DatasetReference(\n+          datasetId=self.temp_dataset, projectId=self._get_project())\n+    else:\n+      return self.temp_dataset\n+\n+  def process(self, element: 'ReadFromBigQueryRequest', *args,\n+              **kwargs) -> Iterable[BoundedSource]:\n+    bq = bigquery_tools.BigQueryWrapper(\n+        temp_dataset_id=self._get_temp_dataset().datasetId)\n+\n+    if element.query is not None:\n+      self._setup_temporary_dataset(bq, element)\n+      table_reference = self._execute_query(bq, element)\n+    else:\n+      assert element.table\n+      table_reference = bigquery_tools.parse_table_reference(\n+          element.table, project=self._get_project())\n+\n+    if not table_reference.projectId:\n+      table_reference.projectId = self._get_project()\n+\n+    schema, metadata_list = self._export_files(bq, element, table_reference)\n+\n+    for metadata in metadata_list:\n+      yield self._create_source(metadata.path, schema)\n+\n+    if element.query is not None:\n+      bq._delete_table(", "originalCommit": "d470c4f34e0616e548b4f0cae10054bdb314125b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA1NjM3Mw==", "url": "https://github.com/apache/beam/pull/13170#discussion_r528056373", "bodyText": "We might need a reshuffle before this cleanup step to make sure that above read is stable in case of a failure (unless we already have a reshuffle inside one of these transforms).", "author": "chamikaramj", "createdAt": "2020-11-21T02:27:36Z", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1931,3 +1903,132 @@ def file_path_to_remove(unused_elm):\n                 *self._args,\n                 **self._kwargs))\n         | _PassThroughThenCleanup(files_to_remove_pcoll))\n+\n+\n+class ReadFromBigQueryRequest:\n+  \"\"\"\n+  Class that defines data to read from BQ.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      query: str = None,\n+      use_standard_sql: bool = True,\n+      table: Union[str, TableReference] = None,\n+      flatten_results: bool = False):\n+    \"\"\"\n+    Only one of query or table should be specified.\n+\n+    :param query: SQL query to fetch data.\n+    :param use_standard_sql:\n+      Specifies whether to use BigQuery's standard SQL dialect for this query.\n+      The default value is :data:`True`. If set to :data:`False`,\n+      the query will use BigQuery's legacy SQL dialect.\n+      This parameter is ignored for table inputs.\n+    :param table:\n+      The ID of the table to read. The ID must contain only letters\n+      ``a-z``, ``A-Z``, numbers ``0-9``, or underscores ``_``. Table should\n+      define project and dataset (ex.: ``'PROJECT:DATASET.TABLE'``).\n+    :param flatten_results:\n+      Flattens all nested and repeated fields in the query results.\n+      The default value is :data:`False`.\n+    \"\"\"\n+    self.flatten_results = flatten_results\n+    self.query = query\n+    self.use_standard_sql = use_standard_sql\n+    self.table = table\n+    self.validate()\n+\n+    # We use this internal object ID to generate BigQuery export directories.\n+    self.obj_id = random.randint(0, 100000)\n+\n+  def validate(self):\n+    if self.table is not None and self.query is not None:\n+      raise ValueError(\n+          'Both a BigQuery table and a query were specified.'\n+          ' Please specify only one of these.')\n+    elif self.table is None and self.query is None:\n+      raise ValueError('A BigQuery table or a query must be specified')\n+    if self.table is not None:\n+      if isinstance(self.table, str):\n+        assert self.table.find('.'), (\n+            'Expected a table reference '\n+            '(PROJECT:DATASET.TABLE or DATASET.TABLE) instead of %s'\n+            % self.table)\n+\n+\n+@experimental()\n+class ReadAllFromBigQuery(PTransform):\n+  \"\"\"Read data from BigQuery.\n+\n+    PTransform:ReadFromBigQueryRequest->Rows\n+\n+    This PTransform uses a BigQuery export job to take a snapshot of the table\n+    on GCS, and then reads from each produced file. Data is exported into\n+    a new subdirectory for each export using UUIDs generated in\n+    `ReadFromBigQueryRequest` objects.\n+\n+    It is recommended not to use this PTransform for streaming jobs on\n+    GlobalWindow, since it will not be able to cleanup snapshots.\n+\n+  Args:\n+    gcs_location (str): The name of the Google Cloud Storage\n+      bucket where the extracted table should be written as a string. If\n+      :data:`None`, then the temp_location parameter is used.\n+    validate (bool): If :data:`True`, various checks will be done when source\n+      gets initialized (e.g., is table present?).\n+    kms_key (str): Experimental. Optional Cloud KMS key name for use when\n+      creating new temporary tables.\n+   \"\"\"\n+  COUNTER = 0\n+\n+  def __init__(\n+      self,\n+      gcs_location: Union[str, ValueProvider] = None,\n+      validate: bool = False,\n+      kms_key: str = None,\n+      temp_dataset: Union[str, DatasetReference] = None,\n+      bigquery_job_labels: Dict[str, str] = None):\n+    if gcs_location:\n+      if not isinstance(gcs_location, (str, ValueProvider)):\n+        raise TypeError(\n+            '%s: gcs_location must be of type string'\n+            ' or ValueProvider; got %r instead' %\n+            (self.__class__.__name__, type(gcs_location)))\n+\n+    self.gcs_location = gcs_location\n+    self.validate = validate\n+    self.kms_key = kms_key\n+    self.bigquery_job_labels = bigquery_job_labels\n+    self.temp_dataset = temp_dataset\n+\n+  def expand(self, pcoll):\n+    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n+    project = pcoll.pipeline.options.view_as(GoogleCloudOptions).project\n+    unique_id = str(uuid.uuid4())[0:10]\n+\n+    try:\n+      step_name = self.label\n+    except AttributeError:\n+      step_name = 'ReadAllFromBigQuery_%d' % ReadAllFromBigQuery.COUNTER\n+      ReadAllFromBigQuery.COUNTER += 1\n+\n+    sources_to_read, cleanup_locations = (\n+        pcoll\n+        | beam.ParDo(\n+        _BigQueryReadSplit(\n+            options=pcoll.pipeline.options,\n+            gcs_location=self.gcs_location,\n+            bigquery_job_labels=self.bigquery_job_labels,\n+            job_name=job_name,\n+            step_name=step_name,\n+            unique_id=unique_id,\n+            kms_key=self.kms_key,\n+            project=project,\n+            temp_dataset=self.temp_dataset)).with_outputs(\n+        \"location_to_cleanup\", main=\"files_to_read\")\n+    )\n+\n+    return (\n+        sources_to_read\n+        | SDFBoundedSourceReader()\n+        | _PassThroughThenCleanup(beam.pvalue.AsIter(cleanup_locations)))", "originalCommit": "d470c4f34e0616e548b4f0cae10054bdb314125b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NTcxNg==", "url": "https://github.com/apache/beam/pull/13170#discussion_r532645716", "bodyText": "The fusion break comes form the side input from AsIter. The files to cleaned up are passed as a side input to a downstream transform, which only executes after the previous is 'committed'", "author": "pabloem", "createdAt": "2020-11-30T14:41:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA1NjM3Mw=="}], "type": "inlineReview"}, {"oid": "de61dd015824fe7a3ea6b88fcb87895885a4d36c", "url": "https://github.com/apache/beam/commit/de61dd015824fe7a3ea6b88fcb87895885a4d36c", "message": "Moving ReadFromBQ components to internal module", "committedDate": "2020-11-30T14:41:21Z", "type": "commit"}, {"oid": "34f12697461cf8a3fda313e63c82500c8cd6d2e3", "url": "https://github.com/apache/beam/commit/34f12697461cf8a3fda313e63c82500c8cd6d2e3", "message": "Fix lint and formatting", "committedDate": "2020-11-30T14:41:21Z", "type": "commit"}, {"oid": "3b9516ca04270b63381791dda15a10ae871d87f4", "url": "https://github.com/apache/beam/commit/3b9516ca04270b63381791dda15a10ae871d87f4", "message": "testing ReadFromBQ from ReadAllFromBQ", "committedDate": "2020-11-30T14:41:22Z", "type": "commit"}, {"oid": "22a4b10b21531cd350190ee063940cab5c253098", "url": "https://github.com/apache/beam/commit/22a4b10b21531cd350190ee063940cab5c253098", "message": "Implementing Python Bounded Source Reader DoFn", "committedDate": "2020-11-30T14:42:03Z", "type": "commit"}, {"oid": "86ee9f4997c030e2c03902d4e90f69f7710ef7a1", "url": "https://github.com/apache/beam/commit/86ee9f4997c030e2c03902d4e90f69f7710ef7a1", "message": "Adding annotations", "committedDate": "2020-11-30T14:42:05Z", "type": "commit"}, {"oid": "629d29f0c7b89fb65fca199d231e1688fa11d486", "url": "https://github.com/apache/beam/commit/629d29f0c7b89fb65fca199d231e1688fa11d486", "message": "fixing import issue", "committedDate": "2020-11-30T14:42:05Z", "type": "commit"}, {"oid": "5551fd582c68d8394e1a1320e3a23bf1f2f8fe4b", "url": "https://github.com/apache/beam/commit/5551fd582c68d8394e1a1320e3a23bf1f2f8fe4b", "message": "fix lint", "committedDate": "2020-11-30T14:42:05Z", "type": "commit"}, {"oid": "2d7828178de18ce34bdc1e1f93942b88aabffe06", "url": "https://github.com/apache/beam/commit/2d7828178de18ce34bdc1e1f93942b88aabffe06", "message": "Adding ReadAllFromBQ", "committedDate": "2020-11-30T14:42:05Z", "type": "commit"}, {"oid": "8da1a5813d122b1a56c566dab3a3f677b6a79b2f", "url": "https://github.com/apache/beam/commit/8da1a5813d122b1a56c566dab3a3f677b6a79b2f", "message": "fix issue", "committedDate": "2020-11-30T14:42:05Z", "type": "commit"}, {"oid": "beb48215b9b41d605a504c1dbd223cc8dd75a2f8", "url": "https://github.com/apache/beam/commit/beb48215b9b41d605a504c1dbd223cc8dd75a2f8", "message": "Fixup", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "b9434dc170a81183bf1d2ea9cc1a0de8e3833f2e", "url": "https://github.com/apache/beam/commit/b9434dc170a81183bf1d2ea9cc1a0de8e3833f2e", "message": "Fix precommit", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "dd540ca93eb020a734020f292d667bbc5d53f371", "url": "https://github.com/apache/beam/commit/dd540ca93eb020a734020f292d667bbc5d53f371", "message": "fixup", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "127eb693b5183802feaeaf32472f942e5e48568b", "url": "https://github.com/apache/beam/commit/127eb693b5183802feaeaf32472f942e5e48568b", "message": "fixup", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "5b7e02a5b1d88a2ca3c11fe64a957d44c6d96ea6", "url": "https://github.com/apache/beam/commit/5b7e02a5b1d88a2ca3c11fe64a957d44c6d96ea6", "message": "fixup", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "d848f9f5133e68552ecc8a2b2bbb212261c6bf61", "url": "https://github.com/apache/beam/commit/d848f9f5133e68552ecc8a2b2bbb212261c6bf61", "message": "Fix tests", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "6b75f49dce1a00dafeafa7eea3e52a1205c1809c", "url": "https://github.com/apache/beam/commit/6b75f49dce1a00dafeafa7eea3e52a1205c1809c", "message": "fix formatting", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "909e188d2723afb77038342d97a1dd6a449ff6ab", "url": "https://github.com/apache/beam/commit/909e188d2723afb77038342d97a1dd6a449ff6ab", "message": "Moving test to runner_v2", "committedDate": "2020-11-30T14:42:06Z", "type": "commit"}, {"oid": "4d11f83020010982ba18767255b711d82395d013", "url": "https://github.com/apache/beam/commit/4d11f83020010982ba18767255b711d82395d013", "message": "Add documentation and changes for ReadAllFromBigQuery", "committedDate": "2020-11-30T14:42:49Z", "type": "commit"}, {"oid": "54385edc584ffc6ae16954f1533dc79ba4cbee7c", "url": "https://github.com/apache/beam/commit/54385edc584ffc6ae16954f1533dc79ba4cbee7c", "message": "fix docs test", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "abf65f45f2c03606915bd64a687806bbf8b8e08d", "url": "https://github.com/apache/beam/commit/abf65f45f2c03606915bd64a687806bbf8b8e08d", "message": "Addressing comments", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "373449241908bdcf0255cff9b3e150e9ea819be1", "url": "https://github.com/apache/beam/commit/373449241908bdcf0255cff9b3e150e9ea819be1", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "3376143cd4b82a26d585e770d9ee5cfbe5e0d918", "url": "https://github.com/apache/beam/commit/3376143cd4b82a26d585e770d9ee5cfbe5e0d918", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "c3f3b723a225d99b881666ec14504c0c12e59c03", "url": "https://github.com/apache/beam/commit/c3f3b723a225d99b881666ec14504c0c12e59c03", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "f83ed92bd05e2357fe42a47ee08819942f7638cb", "url": "https://github.com/apache/beam/commit/f83ed92bd05e2357fe42a47ee08819942f7638cb", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "69fd651225aa6733a2583bd947e05e2738da4406", "url": "https://github.com/apache/beam/commit/69fd651225aa6733a2583bd947e05e2738da4406", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "551e0263006ac030ebbebc4dd1e08f76a73b06a0", "url": "https://github.com/apache/beam/commit/551e0263006ac030ebbebc4dd1e08f76a73b06a0", "message": "Moving RFBQRequest to public file", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "f3288706962298e7306e7d5d9cb0cd13a2ac6d41", "url": "https://github.com/apache/beam/commit/f3288706962298e7306e7d5d9cb0cd13a2ac6d41", "message": "fixup", "committedDate": "2020-11-30T14:42:51Z", "type": "commit"}, {"oid": "06c774cb5c9607ab6d593d7df2279298a420a634", "url": "https://github.com/apache/beam/commit/06c774cb5c9607ab6d593d7df2279298a420a634", "message": "Fixup", "committedDate": "2020-11-30T14:56:36Z", "type": "commit"}, {"oid": "06c774cb5c9607ab6d593d7df2279298a420a634", "url": "https://github.com/apache/beam/commit/06c774cb5c9607ab6d593d7df2279298a420a634", "message": "Fixup", "committedDate": "2020-11-30T14:56:36Z", "type": "forcePushed"}, {"oid": "026f733fa059b5f41c6f337c634125f5c8bd2d2f", "url": "https://github.com/apache/beam/commit/026f733fa059b5f41c6f337c634125f5c8bd2d2f", "message": "comment", "committedDate": "2020-11-30T15:07:22Z", "type": "commit"}]}