{"pr_number": 13619, "pr_title": "[BEAM-11527] Allow user defined Hadoop ReadSupport flags for ParquetReader", "pr_createdAt": "2020-12-28T09:12:37Z", "pr_url": "https://github.com/apache/beam/pull/13619", "timeline": [{"oid": "a7301b751791308965715df247657a8755e75b13", "url": "https://github.com/apache/beam/commit/a7301b751791308965715df247657a8755e75b13", "message": "[BEAM-11527] Add builder parameter to allow user defined Hadoop ReadSupport flags in Hadoop Configuration.", "committedDate": "2020-12-28T09:07:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTEzMA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345130", "bodyText": "Can you please remove this method and replace its uses with setConfiguration(makeHadoopConfiguration(...))", "author": "iemejia", "createdAt": "2020-12-28T13:18:40Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -311,6 +313,12 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder setHadoopConfigurationFlags(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTg4OA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345888", "bodyText": "Please remove all definitions of this method and replace its uses with setConfiguration(makeHadoopConfiguration(...)) in all classes where it appears", "author": "iemejia", "createdAt": "2020-12-28T13:21:12Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -388,6 +402,12 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder<T> setParseFn(SerializableFunction<GenericRecord, T> parseFn);\n \n+      abstract Builder<T> setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder<T> setHadoopConfigurationFlags(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NjcxOA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346718", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:24:07Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0Njc2Mg==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346762", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:24:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;\n+\n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n       SplitReadFn(\n-          GenericData model, Schema requestSchema, SerializableFunction<GenericRecord, T> parseFn) {\n+          GenericData model,\n+          Schema requestSchema,\n+          SerializableFunction<GenericRecord, T> parseFn,\n+          SerializableConfiguration hadoopBaseConfig) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzA1MQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347051", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:25:13Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -819,9 +884,15 @@ public Progress getProgress() {\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n-      ReadFn(GenericData model, SerializableFunction<GenericRecord, T> parseFn) {\n+      private final SerializableConfiguration hadoopBaseConfig;", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzUwNQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347505", "bodyText": "\ud83d\udc4d", "author": "iemejia", "createdAt": "2020-12-28T13:26:33Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -920,13 +996,7 @@ public Sink withCompressionCodec(CompressionCodecName compressionCodecName) {\n \n     /** Specifies configuration to be passed into the sink's writer. */\n     public Sink withConfiguration(Map<String, String> configuration) {\n-      Configuration hadoopConfiguration = new Configuration();\n-      for (Map.Entry<String, String> entry : configuration.entrySet()) {\n-        hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-      }\n-      return toBuilder()\n-          .setConfiguration(new SerializableConfiguration(hadoopConfiguration))\n-          .build();\n+      return toBuilder().setConfiguration(makeHadoopConfigurationUsingFlags(configuration)).build();", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349509", "bodyText": "Can we move this method into the SerializableConfiguration class and make it public static SerializableConfiguration fromMap(Map<String, string> entries) {", "author": "iemejia", "createdAt": "2020-12-28T13:33:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n+  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTgzMw==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561833", "bodyText": "Done, added a single test for the new method.\nInitially I was contemplating this alternative but had discarded to reduce touching more files.", "author": "anantdamle", "createdAt": "2020-12-29T04:15:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349655", "bodyText": "test or remove", "author": "iemejia", "createdAt": "2020-12-28T13:33:48Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -416,6 +416,9 @@ public void testWriteAndReadwithSplitUsingReflectDataSchemaWithDataModel() {\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testConfigurationReadFile() {}", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIzNQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562235", "bodyText": "Thanks, Removed.", "author": "anantdamle", "createdAt": "2020-12-29T04:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MDE0Ng==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549350146", "bodyText": "can you please name the argument of the withConfiguration methods consistently everywhere as configuration instead of flags or hadoopConfigFlags", "author": "iemejia", "createdAt": "2020-12-28T13:35:23Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -332,6 +340,10 @@ public Read withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .build();\n     }\n \n+    public Read withConfiguration(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MTE2MA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549351160", "bodyText": "Rename to withConfiguration to be consistent with the other methods + s/configurationFlags/configuration", "author": "iemejia", "createdAt": "2020-12-28T13:38:39Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -532,6 +581,12 @@ public ReadFiles withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .setSplittable(true)\n           .build();\n     }\n+\n+    /** Specify Hadoop configuration for ParquetReader. */\n+    public ReadFiles withHadoopConfiguration(Map<String, String> configurationFlags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352679", "bodyText": "We should probably define a default value inside of the builders (read, readFiles, parseGenericRecords, parseFilesGenericRecords)  .setConfiguration(...) and since we define a default value we won't need this if", "author": "iemejia", "createdAt": "2020-12-28T13:43:46Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -835,13 +906,18 @@ public void processElement(ProcessContext processContext) throws Exception {\n \n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n-        AvroParquetReader.Builder builder =\n-            AvroParquetReader.<GenericRecord>builder(new BeamParquetInputFile(seekableByteChannel));\n+        AvroParquetReader.Builder<GenericRecord> builder =\n+            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n+        if (hadoopBaseConfig != null) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTY2MA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561660", "bodyText": "looking at the SerializableConfiguration#91 , It seems null is the expected value for building a default configuration.\nI've also replaced all configuration != null checks with SerializableConfiguration.newConfiguration(configuration) for consistency and to avoid NPE. What do you think?", "author": "anantdamle", "createdAt": "2020-12-29T04:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjkwOA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352908", "bodyText": "s/Hadoop {@link Configuration}/{@link SerializableConfiguration}", "author": "iemejia", "createdAt": "2020-12-28T13:44:33Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356033", "bodyText": "Test with new Configuration(), this should not be nullable", "author": "iemejia", "createdAt": "2020-12-28T13:54:44Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIwOQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562209", "bodyText": "As I'm using SerializableConfiguration#newConfiguration it can be null.\nDo you want to me to add a test where a non-null configuration is tested?", "author": "anantdamle", "createdAt": "2020-12-29T04:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4ODcwNA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r550488704", "bodyText": "I would prefer it to not be Nullable but since this is internal I suppose we can adjust this later, on the other hand if someday Parquet finally gets rid of its Hadoop dependencies probably the null value would align better.", "author": "iemejia", "createdAt": "2020-12-31T13:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjkwNg==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356906", "bodyText": "\ud83d\udc4d", "author": "iemejia", "createdAt": "2020-12-28T13:57:21Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -682,7 +747,7 @@ public void processElement(\n       }\n \n       public Configuration getConfWithModelClass() throws Exception {\n-        Configuration conf = new Configuration();\n+        Configuration conf = SerializableConfiguration.newConfiguration(hadoopBaseConfig);", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ff5a094be93d41e14391600b071bdcb1369391bb", "url": "https://github.com/apache/beam/commit/ff5a094be93d41e14391600b071bdcb1369391bb", "message": "Consistency improvements and other fixes", "committedDate": "2020-12-29T04:08:25Z", "type": "commit"}]}