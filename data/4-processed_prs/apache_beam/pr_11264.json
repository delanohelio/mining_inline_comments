{"pr_number": 11264, "pr_title": "[BEAM-9496] Add to_dataframe and to_pcollection APIs.", "pr_createdAt": "2020-03-30T19:58:45Z", "pr_url": "https://github.com/apache/beam/pull/11264", "timeline": [{"oid": "272aa65113cb359a45a35b82a096976d11ae9e94", "url": "https://github.com/apache/beam/commit/272aa65113cb359a45a35b82a096976d11ae9e94", "message": "[BEAM-9496] Add to_dataframe and to_pcollection APIs.\n\nThis may be a more natural interface for mixing pandas-style code with Beam.\n\nIt does have the downside of being inefficient if dataframes are converted\nback to PCollections one at a time.", "committedDate": "2020-03-31T01:19:10Z", "type": "forcePushed"}, {"oid": "02c525bb5cfdf5cabed727f514e354a01ed11ae2", "url": "https://github.com/apache/beam/commit/02c525bb5cfdf5cabed727f514e354a01ed11ae2", "message": "[BEAM-9496] Add to_dataframe and to_pcollection APIs.\n\nThis may be a more natural interface for mixing pandas-style code with Beam.\n\nIt does have the downside of being inefficient if dataframes are converted\nback to PCollections one at a time.", "committedDate": "2020-03-31T05:55:49Z", "type": "forcePushed"}, {"oid": "1bbac0f5d700718fb98ea3b8496ac87857867d87", "url": "https://github.com/apache/beam/commit/1bbac0f5d700718fb98ea3b8496ac87857867d87", "message": "[BEAM-9496] Add to_dataframe and to_pcollection APIs.\n\nThis may be a more natural interface for mixing pandas-style code with Beam.\n\nIt does have the downside of being inefficient if dataframes are converted\nback to PCollections one at a time.", "committedDate": "2020-03-31T06:02:32Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE5NTc3NA==", "url": "https://github.com/apache/beam/pull/11264#discussion_r404195774", "bodyText": "Could you add something like \".. by retrieving the name of these variables in the calling context\"", "author": "TheNeuralBit", "createdAt": "2020-04-06T15:46:21Z", "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -0,0 +1,71 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import inspect\n+\n+from apache_beam import pvalue\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import transforms\n+\n+\n+def to_dataframe(pc):\n+  pass\n+\n+\n+# TODO: Or should this be called as_dataframe?\n+def to_dataframe(pcoll, proxy):\n+  return frame_base.DeferredFrame.wrap(\n+      expressions.PlaceholderExpression(proxy, pcoll))\n+\n+\n+# TODO: Or should this be called from_dataframe?\n+def to_pcollection(*dataframes, **kwargs):\n+  label = kwargs.pop('label', None)\n+  assert not kwargs  # TODO(Py3): Use PEP 3102\n+  if label is None:\n+    # Attempt to come up with a reasonable, stable label.", "originalCommit": "3f96077e14233b23390e64ce15fcaabf1d2efeb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIwMDQzOQ==", "url": "https://github.com/apache/beam/pull/11264#discussion_r404200439", "bodyText": "This looks like something that should be \"package-private\". Should it get an underscore prefix?", "author": "TheNeuralBit", "createdAt": "2020-04-06T15:52:39Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,255 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = {\n+        placeholders[key]._expr: pcoll\n+        for key, pcoll in input_dict.items()\n+    } | 'Eval' >> DataframeExpressionsTransform(\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+\n+class DataframeExpressionsTransform(transforms.PTransform):", "originalCommit": "3f96077e14233b23390e64ce15fcaabf1d2efeb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMDE0OQ==", "url": "https://github.com/apache/beam/pull/11264#discussion_r404210149", "bodyText": "I like as_dataframe if it were a method on PCollection since it's fluent - df = pcol.as_dataframe()\nSimilarly, below from_dataframe would be fluent as a static method on PCollection, but PCollection.from_dataframe feels too verbose.\nWould you agree we want something like the following to be as easy and intuitive as possible?\npcol = p | \"Read from Source\" >> beam.io.SomeSchemaSource(foo)\n\ndf = pcol.as_dataframe()\ndf_agg = df[df[\"measurement\" > threshold]].groupby(\"id\").count()\n\nPCollection.from_dataframe(df_agg) | \"Write to Sink\" >> beam.io.SomeSink(bar)", "author": "TheNeuralBit", "createdAt": "2020-04-06T16:05:58Z", "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -0,0 +1,71 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import inspect\n+\n+from apache_beam import pvalue\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import transforms\n+\n+\n+def to_dataframe(pc):\n+  pass\n+\n+\n+# TODO: Or should this be called as_dataframe?", "originalCommit": "3f96077e14233b23390e64ce15fcaabf1d2efeb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc0OTE2Mw==", "url": "https://github.com/apache/beam/pull/11264#discussion_r407749163", "bodyText": "So far we haven't added any methods to PCollection, but I'm open to the idea (thought it'd be a big change to the API that should be done wholistically, see https://lists.apache.org/thread.html/fcb422d61437a634662b24100d4e2d46a940ee766848b699023081d9%40%3Cdev.beam.apache.org%3E ) For now, at least, it seems a bit much to make dataframe-methods on PCollection itself.\nShort of that, can you think of any fluent styles for\nfrom apache_beam import dataframe as ???\n...\npcol = p | \"Read from Source\" >> beam.io.SomeSchemaSource(foo)\ndf = ???", "author": "robertwb", "createdAt": "2020-04-13T22:07:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMDE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUyNzk1Ng==", "url": "https://github.com/apache/beam/pull/11264#discussion_r411527956", "bodyText": "Yeah definitely fair to not make changes to PCollection yet. Another option would be methods on DeferredDataframe, like #as_pcollection():\npcol = p | \"Read from Source\" >> beam.io.SomeSchemaSource(foo)\n\ndf = to_dataframe(pcol)\ndf_agg = df[df[\"measurement\" > threshold]].groupby(\"id\").count()\n\ndf_agg.as_pcollection() | \"Write to Sink\" >> beam.io.SomeSink(bar)\nThis is just a bikeshed though. This is still very experimental so I don't think we're locking ourselves into anything.", "author": "TheNeuralBit", "createdAt": "2020-04-20T16:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMDE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU4MjU0OQ==", "url": "https://github.com/apache/beam/pull/11264#discussion_r411582549", "bodyText": "Yeah, it's all experimental at this point. We can keep churning on the best API while we flesh out functionality.", "author": "robertwb", "createdAt": "2020-04-20T18:03:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMDE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMDU5MA==", "url": "https://github.com/apache/beam/pull/11264#discussion_r404210590", "bodyText": "Could you add typehints and docstrings here and on to_dataframe?", "author": "TheNeuralBit", "createdAt": "2020-04-06T16:06:38Z", "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -0,0 +1,71 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import inspect\n+\n+from apache_beam import pvalue\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import transforms\n+\n+\n+def to_dataframe(pc):\n+  pass\n+\n+\n+# TODO: Or should this be called as_dataframe?\n+def to_dataframe(pcoll, proxy):\n+  return frame_base.DeferredFrame.wrap(\n+      expressions.PlaceholderExpression(proxy, pcoll))\n+\n+\n+# TODO: Or should this be called from_dataframe?\n+def to_pcollection(*dataframes, **kwargs):", "originalCommit": "3f96077e14233b23390e64ce15fcaabf1d2efeb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMjM4Mg==", "url": "https://github.com/apache/beam/pull/11264#discussion_r404212382", "bodyText": "Could this be streamlined with calls to to_dataframe and to_pcollection?", "author": "TheNeuralBit", "createdAt": "2020-04-06T16:08:59Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,255 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = {\n+        placeholders[key]._expr: pcoll\n+        for key, pcoll in input_dict.items()\n+    } | 'Eval' >> DataframeExpressionsTransform(\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]", "originalCommit": "3f96077e14233b23390e64ce15fcaabf1d2efeb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc0NTAyNw==", "url": "https://github.com/apache/beam/pull/11264#discussion_r407745027", "bodyText": "Yes. Done.", "author": "robertwb", "createdAt": "2020-04-13T21:57:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIxMjM4Mg=="}], "type": "inlineReview"}, {"oid": "9c402a454b52c7646638b0259026a82fc745a2be", "url": "https://github.com/apache/beam/commit/9c402a454b52c7646638b0259026a82fc745a2be", "message": "[BEAM-9496] Add to_dataframe and to_pcollection APIs.\n\nThis may be a more natural interface for mixing pandas-style code with Beam.\n\nIt does have the downside of being inefficient if dataframes are converted\nback to PCollections one at a time.", "committedDate": "2020-04-13T19:59:07Z", "type": "commit"}, {"oid": "9363e924f24e8136870b3cea64c0aba4c6064685", "url": "https://github.com/apache/beam/commit/9363e924f24e8136870b3cea64c0aba4c6064685", "message": "lint", "committedDate": "2020-04-13T19:59:07Z", "type": "commit"}, {"oid": "80f44114644aee64ab28240dcae945f7b09174cd", "url": "https://github.com/apache/beam/commit/80f44114644aee64ab28240dcae945f7b09174cd", "message": "Rewrite DataframeTransform in terms of to/from_pcollection.", "committedDate": "2020-04-13T21:53:00Z", "type": "commit"}, {"oid": "80f44114644aee64ab28240dcae945f7b09174cd", "url": "https://github.com/apache/beam/commit/80f44114644aee64ab28240dcae945f7b09174cd", "message": "Rewrite DataframeTransform in terms of to/from_pcollection.", "committedDate": "2020-04-13T21:53:00Z", "type": "forcePushed"}, {"oid": "758678c742c97722cbac161db72ade740118b6af", "url": "https://github.com/apache/beam/commit/758678c742c97722cbac161db72ade740118b6af", "message": "typing, docs", "committedDate": "2020-04-13T22:27:46Z", "type": "commit"}, {"oid": "758678c742c97722cbac161db72ade740118b6af", "url": "https://github.com/apache/beam/commit/758678c742c97722cbac161db72ade740118b6af", "message": "typing, docs", "committedDate": "2020-04-13T22:27:46Z", "type": "forcePushed"}, {"oid": "582b64ce4cbacc4972720a406c22c5d7344e6331", "url": "https://github.com/apache/beam/commit/582b64ce4cbacc4972720a406c22c5d7344e6331", "message": "lint, yapf, py2", "committedDate": "2020-04-14T16:35:33Z", "type": "commit"}]}