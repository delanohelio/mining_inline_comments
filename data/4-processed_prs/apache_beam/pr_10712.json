{"pr_number": 10712, "pr_title": "[BEAM-7246] Added Google Spanner Write Transform", "pr_createdAt": "2020-01-29T18:08:17Z", "pr_url": "https://github.com/apache/beam/pull/10712", "timeline": [{"oid": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "url": "https://github.com/apache/beam/commit/c8831cbcc13de6e701269d7ed7721e7aee70e482", "message": "Added Spanner Write Transform", "committedDate": "2020-01-30T07:44:41Z", "type": "commit"}, {"oid": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "url": "https://github.com/apache/beam/commit/c8831cbcc13de6e701269d7ed7721e7aee70e482", "message": "Added Spanner Write Transform", "committedDate": "2020-01-30T07:44:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjgyODEyOA==", "url": "https://github.com/apache/beam/pull/10712#discussion_r372828128", "bodyText": "As per java implementation, we can not determine the size of delete mutation, so we simply mark them as unbatchable mutations.", "author": "mszb", "createdAt": "2020-01-30T09:04:13Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -581,3 +644,369 @@ def display_data(self):\n                                            label='transaction')\n \n     return res\n+\n+\n+@experimental(extra_message=\"No backwards-compatibility guarantees.\")\n+class WriteToSpanner(PTransform):\n+\n+  def __init__(self, project_id, instance_id, database_id, pool=None,\n+               credentials=None, max_batch_size_bytes=1048576):\n+    \"\"\"\n+    A PTransform to write onto Google Cloud Spanner.\n+\n+    Args:\n+      project_id: Cloud spanner project id. Be sure to use the Project ID,\n+        not the Project Number.\n+      instance_id: Cloud spanner instance id.\n+      database_id: Cloud spanner database id.\n+      max_batch_size_bytes: (optional) Split the mutation into batches to\n+        reduce the number of transaction sent to Spanner. By default it is\n+        set to 1 MB (1048576 Bytes).\n+    \"\"\"\n+    self._configuration = _BeamSpannerConfiguration(\n+        project=project_id, instance=instance_id, database=database_id,\n+        credentials=credentials, pool=pool, snapshot_read_timestamp=None,\n+        snapshot_exact_staleness=None\n+    )\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._database_id = database_id\n+    self._project_id = project_id\n+    self._instance_id = instance_id\n+    self._pool = pool\n+\n+  def display_data(self):\n+    res = {\n+        'project_id': DisplayDataItem(self._project_id, label='Project Id'),\n+        'instance_id': DisplayDataItem(self._instance_id, label='Instance Id'),\n+        'pool': DisplayDataItem(str(self._pool), label='Pool'),\n+        'database': DisplayDataItem(self._database_id, label='Database'),\n+        'batch_size': DisplayDataItem(self._max_batch_size_bytes,\n+                                      label=\"Batch Size\"),\n+    }\n+    return res\n+\n+  def expand(self, pcoll):\n+    return (pcoll\n+            | \"make batches\" >>\n+            _WriteGroup(max_batch_size_bytes=self._max_batch_size_bytes)\n+            | 'Writing to spanner' >> ParDo(\n+                _WriteToSpannerDoFn(self._configuration)))\n+\n+\n+class _Mutator(namedtuple('_Mutator', [\"mutation\", \"operation\", \"kwargs\"])):\n+  __slots__ = ()\n+\n+  @property\n+  def byte_size(self):\n+    return self.mutation.ByteSize()\n+\n+\n+class MutationGroup(deque):\n+  \"\"\"\n+  A Bundle of Spanner Mutations (_Mutator).\n+  \"\"\"\n+\n+  @property\n+  def byte_size(self):\n+    s = 0\n+    for m in self.__iter__():\n+      s += m.byte_size\n+    return s\n+\n+  def primary(self):\n+    return next(self.__iter__())\n+\n+\n+class WriteMutation(object):\n+\n+  _OPERATION_DELETE = \"delete\"\n+  _OPERATION_INSERT = \"insert\"\n+  _OPERATION_INSERT_OR_UPDATE = \"insert_or_update\"\n+  _OPERATION_REPLACE = \"replace\"\n+  _OPERATION_UPDATE = \"update\"\n+\n+  def __init__(self,\n+               insert=None,\n+               update=None,\n+               insert_or_update=None,\n+               replace=None,\n+               delete=None,\n+               columns=None,\n+               values=None,\n+               keyset=None):\n+    \"\"\"\n+    A convenient class to create Spanner Mutations for Write. User can provide\n+    the operation via constructor or via static methods.\n+\n+    Note: If a user passing the operation via construction, make sure that it\n+    will only accept one operation at a time. For example, if a user passing\n+    a table name in the `insert` parameter, and he also passes the `update`\n+    parameter value, this will cause an error.\n+\n+    Args:\n+      insert: (Optional) Name of the table in which rows will be inserted.\n+      update: (Optional) Name of the table in which existing rows will be\n+        updated.\n+      insert_or_update: (Optional) Table name in which rows will be written.\n+        Like insert, except that if the row already exists, then its column\n+        values are overwritten with the ones provided. Any column values not\n+        explicitly written are preserved.\n+      replace: (Optional) Table name in which rows will be replaced. Like\n+        insert, except that if the row already exists, it is deleted, and the\n+        column values provided are inserted instead. Unlike `insert_or_update`,\n+        this means any values not explicitly written become `NULL`.\n+      delete: (Optional) Table name from which rows will be deleted. Succeeds\n+        whether or not the named rows were present.\n+      columns: The names of the columns in table to be written. The list of\n+        columns must contain enough columns to allow Cloud Spanner to derive\n+        values for all primary key columns in the row(s) to be modified.\n+      values: The values to be written. `values` can contain more than one\n+        list of values. If it does, then multiple rows are written, one for\n+        each entry in `values`. Each list in `values` must have exactly as\n+        many entries as there are entries in columns above. Sending multiple\n+        lists is equivalent to sending multiple Mutations, each containing one\n+        `values` entry and repeating table and columns.\n+      keyset: (Optional) The primary keys of the rows within table to delete.\n+        Delete is idempotent. The transaction will succeed even if some or\n+        all rows do not exist.\n+    \"\"\"\n+    self._columns = columns\n+    self._values = values\n+    self._keyset = keyset\n+\n+    self._insert = insert\n+    self._update = update\n+    self._insert_or_update = insert_or_update\n+    self._replace = replace\n+    self._delete = delete\n+\n+    if sum([\n+        1 for x in [self._insert, self._update, self._insert_or_update,\n+                    self._replace, self._delete]\n+        if x is not None\n+    ]) != 1:\n+      raise ValueError(\"No or more than one write mutation operation \"\n+                       \"provided: <%s: %s>\" % (self.__class__.__name__,\n+                                               str(self.__dict__)))\n+\n+  def __call__(self, *args, **kwargs):\n+    if self._insert is not None:\n+      return WriteMutation.insert(\n+          table=self._insert, columns=self._columns, values=self._values)\n+    elif self._update is not None:\n+      return WriteMutation.update(\n+          table=self._update, columns=self._columns, values=self._values)\n+    elif self._insert_or_update is not None:\n+      return WriteMutation.insert_or_update(\n+          table=self._insert_or_update,\n+          columns=self._columns,\n+          values=self._values)\n+    elif self._replace is not None:\n+      return WriteMutation.replace(\n+          table=self._replace, columns=self._columns, values=self._values)\n+    elif self._delete is not None:\n+      return WriteMutation.delete(table=self._delete, keyset=self._keyset)\n+\n+  @staticmethod\n+  def insert(table, columns, values):\n+    \"\"\"Insert one or more new table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(insert=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def update(table, columns, values):\n+    \"\"\"Update one or more existing table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+  @staticmethod\n+  def insert_or_update(table, columns, values):\n+    \"\"\"Insert/update one or more table rows.\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(\n+            insert_or_update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT_OR_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def replace(table, columns, values):\n+    \"\"\"Replace one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(replace=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_REPLACE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def delete(table, keyset):\n+    \"\"\"Delete one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      keyset: Keys/ranges identifying rows to delete.\n+    \"\"\"\n+    delete = Mutation.Delete(table=table, key_set=keyset._to_pb())\n+    return _Mutator(mutation=Mutation(delete=delete),\n+                    operation=WriteMutation._OPERATION_DELETE,\n+                    kwargs={\"table\": table, \"keyset\": keyset})\n+\n+\n+@with_input_types(typing.Union[MutationGroup, TaggedOutput])\n+@with_output_types(MutationGroup)\n+class _BatchFn(DoFn):\n+  \"\"\"\n+  Batches mutations together.\n+  \"\"\"\n+\n+  def __init__(self, max_batch_size_bytes):\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+\n+  def start_bundle(self):\n+    self._batch = MutationGroup()\n+    self._size_in_bytes = 0\n+\n+  def process(self, element):\n+    _max_bytes = self._max_batch_size_bytes\n+    mg_size = element.byte_size  # total size of the mutation group.\n+\n+    if mg_size + self._size_in_bytes > _max_bytes:\n+      # Batch is full, output the batch and resetting the count.\n+      yield self._batch\n+      self._size_in_bytes = 0\n+      self._batch = MutationGroup()\n+\n+    self._batch.extend(element)\n+    self._size_in_bytes += mg_size\n+\n+  def finish_bundle(self):\n+    if self._batch is not None:\n+      yield window.GlobalWindows.windowed_value(self._batch)\n+      self._batch = None\n+\n+\n+@with_input_types(MutationGroup)\n+@with_output_types(MutationGroup)\n+class _BatchableFilterFn(DoFn):\n+  \"\"\"\n+  Filters MutationGroups larger than the batch size to the output tagged with\n+  OUTPUT_TAG_UNBATCHABLE.\n+  \"\"\"\n+  OUTPUT_TAG_UNBATCHABLE = 'unbatchable'\n+\n+  def __init__(self, max_batch_size_bytes):\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._batchable = None\n+    self._unbatchable = None\n+\n+  def process(self, element):\n+    if element.primary().operation == 'delete':\n+      # As delete mutations are not batchable.", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjgzMTE5MA==", "url": "https://github.com/apache/beam/pull/10712#discussion_r372831190", "bodyText": "Since in python we don't use builder pattern, I use this magic method to construct the mutation object! This class also have the static method which I thought is more convenient for the user to use.", "author": "mszb", "createdAt": "2020-01-30T09:11:27Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -581,3 +644,369 @@ def display_data(self):\n                                            label='transaction')\n \n     return res\n+\n+\n+@experimental(extra_message=\"No backwards-compatibility guarantees.\")\n+class WriteToSpanner(PTransform):\n+\n+  def __init__(self, project_id, instance_id, database_id, pool=None,\n+               credentials=None, max_batch_size_bytes=1048576):\n+    \"\"\"\n+    A PTransform to write onto Google Cloud Spanner.\n+\n+    Args:\n+      project_id: Cloud spanner project id. Be sure to use the Project ID,\n+        not the Project Number.\n+      instance_id: Cloud spanner instance id.\n+      database_id: Cloud spanner database id.\n+      max_batch_size_bytes: (optional) Split the mutation into batches to\n+        reduce the number of transaction sent to Spanner. By default it is\n+        set to 1 MB (1048576 Bytes).\n+    \"\"\"\n+    self._configuration = _BeamSpannerConfiguration(\n+        project=project_id, instance=instance_id, database=database_id,\n+        credentials=credentials, pool=pool, snapshot_read_timestamp=None,\n+        snapshot_exact_staleness=None\n+    )\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._database_id = database_id\n+    self._project_id = project_id\n+    self._instance_id = instance_id\n+    self._pool = pool\n+\n+  def display_data(self):\n+    res = {\n+        'project_id': DisplayDataItem(self._project_id, label='Project Id'),\n+        'instance_id': DisplayDataItem(self._instance_id, label='Instance Id'),\n+        'pool': DisplayDataItem(str(self._pool), label='Pool'),\n+        'database': DisplayDataItem(self._database_id, label='Database'),\n+        'batch_size': DisplayDataItem(self._max_batch_size_bytes,\n+                                      label=\"Batch Size\"),\n+    }\n+    return res\n+\n+  def expand(self, pcoll):\n+    return (pcoll\n+            | \"make batches\" >>\n+            _WriteGroup(max_batch_size_bytes=self._max_batch_size_bytes)\n+            | 'Writing to spanner' >> ParDo(\n+                _WriteToSpannerDoFn(self._configuration)))\n+\n+\n+class _Mutator(namedtuple('_Mutator', [\"mutation\", \"operation\", \"kwargs\"])):\n+  __slots__ = ()\n+\n+  @property\n+  def byte_size(self):\n+    return self.mutation.ByteSize()\n+\n+\n+class MutationGroup(deque):\n+  \"\"\"\n+  A Bundle of Spanner Mutations (_Mutator).\n+  \"\"\"\n+\n+  @property\n+  def byte_size(self):\n+    s = 0\n+    for m in self.__iter__():\n+      s += m.byte_size\n+    return s\n+\n+  def primary(self):\n+    return next(self.__iter__())\n+\n+\n+class WriteMutation(object):\n+\n+  _OPERATION_DELETE = \"delete\"\n+  _OPERATION_INSERT = \"insert\"\n+  _OPERATION_INSERT_OR_UPDATE = \"insert_or_update\"\n+  _OPERATION_REPLACE = \"replace\"\n+  _OPERATION_UPDATE = \"update\"\n+\n+  def __init__(self,\n+               insert=None,\n+               update=None,\n+               insert_or_update=None,\n+               replace=None,\n+               delete=None,\n+               columns=None,\n+               values=None,\n+               keyset=None):\n+    \"\"\"\n+    A convenient class to create Spanner Mutations for Write. User can provide\n+    the operation via constructor or via static methods.\n+\n+    Note: If a user passing the operation via construction, make sure that it\n+    will only accept one operation at a time. For example, if a user passing\n+    a table name in the `insert` parameter, and he also passes the `update`\n+    parameter value, this will cause an error.\n+\n+    Args:\n+      insert: (Optional) Name of the table in which rows will be inserted.\n+      update: (Optional) Name of the table in which existing rows will be\n+        updated.\n+      insert_or_update: (Optional) Table name in which rows will be written.\n+        Like insert, except that if the row already exists, then its column\n+        values are overwritten with the ones provided. Any column values not\n+        explicitly written are preserved.\n+      replace: (Optional) Table name in which rows will be replaced. Like\n+        insert, except that if the row already exists, it is deleted, and the\n+        column values provided are inserted instead. Unlike `insert_or_update`,\n+        this means any values not explicitly written become `NULL`.\n+      delete: (Optional) Table name from which rows will be deleted. Succeeds\n+        whether or not the named rows were present.\n+      columns: The names of the columns in table to be written. The list of\n+        columns must contain enough columns to allow Cloud Spanner to derive\n+        values for all primary key columns in the row(s) to be modified.\n+      values: The values to be written. `values` can contain more than one\n+        list of values. If it does, then multiple rows are written, one for\n+        each entry in `values`. Each list in `values` must have exactly as\n+        many entries as there are entries in columns above. Sending multiple\n+        lists is equivalent to sending multiple Mutations, each containing one\n+        `values` entry and repeating table and columns.\n+      keyset: (Optional) The primary keys of the rows within table to delete.\n+        Delete is idempotent. The transaction will succeed even if some or\n+        all rows do not exist.\n+    \"\"\"\n+    self._columns = columns\n+    self._values = values\n+    self._keyset = keyset\n+\n+    self._insert = insert\n+    self._update = update\n+    self._insert_or_update = insert_or_update\n+    self._replace = replace\n+    self._delete = delete\n+\n+    if sum([\n+        1 for x in [self._insert, self._update, self._insert_or_update,\n+                    self._replace, self._delete]\n+        if x is not None\n+    ]) != 1:\n+      raise ValueError(\"No or more than one write mutation operation \"\n+                       \"provided: <%s: %s>\" % (self.__class__.__name__,\n+                                               str(self.__dict__)))\n+\n+  def __call__(self, *args, **kwargs):", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjgzMzE4OQ==", "url": "https://github.com/apache/beam/pull/10712#discussion_r372833189", "bodyText": "Since spanner package wont expose the Mutation and batch objects, so this is the only way to import it.", "author": "mszb", "createdAt": "2020-01-30T09:15:59Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -131,13 +187,18 @@\n try:\n   from google.cloud.spanner import Client\n   from google.cloud.spanner import KeySet\n+  from google.cloud.spanner_v1 import batch\n   from google.cloud.spanner_v1.database import BatchSnapshot\n+  from google.cloud.spanner_v1.proto.mutation_pb2 import Mutation", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg0Mjc0NQ==", "url": "https://github.com/apache/beam/pull/10712#discussion_r372842745", "bodyText": "Since we need to calculate the mutation size to create the batches, we need to create Mutation object which has the method ByteSize() and we uses its value to determine how many mutations we can have in one batch on _BatchFn  dofn. We are not passing these mutation object to spanner or anywhere, just using them to calculate the bytesize for write transform!", "author": "mszb", "createdAt": "2020-01-30T09:35:51Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -581,3 +644,369 @@ def display_data(self):\n                                            label='transaction')\n \n     return res\n+\n+\n+@experimental(extra_message=\"No backwards-compatibility guarantees.\")\n+class WriteToSpanner(PTransform):\n+\n+  def __init__(self, project_id, instance_id, database_id, pool=None,\n+               credentials=None, max_batch_size_bytes=1048576):\n+    \"\"\"\n+    A PTransform to write onto Google Cloud Spanner.\n+\n+    Args:\n+      project_id: Cloud spanner project id. Be sure to use the Project ID,\n+        not the Project Number.\n+      instance_id: Cloud spanner instance id.\n+      database_id: Cloud spanner database id.\n+      max_batch_size_bytes: (optional) Split the mutation into batches to\n+        reduce the number of transaction sent to Spanner. By default it is\n+        set to 1 MB (1048576 Bytes).\n+    \"\"\"\n+    self._configuration = _BeamSpannerConfiguration(\n+        project=project_id, instance=instance_id, database=database_id,\n+        credentials=credentials, pool=pool, snapshot_read_timestamp=None,\n+        snapshot_exact_staleness=None\n+    )\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._database_id = database_id\n+    self._project_id = project_id\n+    self._instance_id = instance_id\n+    self._pool = pool\n+\n+  def display_data(self):\n+    res = {\n+        'project_id': DisplayDataItem(self._project_id, label='Project Id'),\n+        'instance_id': DisplayDataItem(self._instance_id, label='Instance Id'),\n+        'pool': DisplayDataItem(str(self._pool), label='Pool'),\n+        'database': DisplayDataItem(self._database_id, label='Database'),\n+        'batch_size': DisplayDataItem(self._max_batch_size_bytes,\n+                                      label=\"Batch Size\"),\n+    }\n+    return res\n+\n+  def expand(self, pcoll):\n+    return (pcoll\n+            | \"make batches\" >>\n+            _WriteGroup(max_batch_size_bytes=self._max_batch_size_bytes)\n+            | 'Writing to spanner' >> ParDo(\n+                _WriteToSpannerDoFn(self._configuration)))\n+\n+\n+class _Mutator(namedtuple('_Mutator', [\"mutation\", \"operation\", \"kwargs\"])):\n+  __slots__ = ()\n+\n+  @property\n+  def byte_size(self):\n+    return self.mutation.ByteSize()\n+\n+\n+class MutationGroup(deque):\n+  \"\"\"\n+  A Bundle of Spanner Mutations (_Mutator).\n+  \"\"\"\n+\n+  @property\n+  def byte_size(self):\n+    s = 0\n+    for m in self.__iter__():\n+      s += m.byte_size\n+    return s\n+\n+  def primary(self):\n+    return next(self.__iter__())\n+\n+\n+class WriteMutation(object):\n+\n+  _OPERATION_DELETE = \"delete\"\n+  _OPERATION_INSERT = \"insert\"\n+  _OPERATION_INSERT_OR_UPDATE = \"insert_or_update\"\n+  _OPERATION_REPLACE = \"replace\"\n+  _OPERATION_UPDATE = \"update\"\n+\n+  def __init__(self,\n+               insert=None,\n+               update=None,\n+               insert_or_update=None,\n+               replace=None,\n+               delete=None,\n+               columns=None,\n+               values=None,\n+               keyset=None):\n+    \"\"\"\n+    A convenient class to create Spanner Mutations for Write. User can provide\n+    the operation via constructor or via static methods.\n+\n+    Note: If a user passing the operation via construction, make sure that it\n+    will only accept one operation at a time. For example, if a user passing\n+    a table name in the `insert` parameter, and he also passes the `update`\n+    parameter value, this will cause an error.\n+\n+    Args:\n+      insert: (Optional) Name of the table in which rows will be inserted.\n+      update: (Optional) Name of the table in which existing rows will be\n+        updated.\n+      insert_or_update: (Optional) Table name in which rows will be written.\n+        Like insert, except that if the row already exists, then its column\n+        values are overwritten with the ones provided. Any column values not\n+        explicitly written are preserved.\n+      replace: (Optional) Table name in which rows will be replaced. Like\n+        insert, except that if the row already exists, it is deleted, and the\n+        column values provided are inserted instead. Unlike `insert_or_update`,\n+        this means any values not explicitly written become `NULL`.\n+      delete: (Optional) Table name from which rows will be deleted. Succeeds\n+        whether or not the named rows were present.\n+      columns: The names of the columns in table to be written. The list of\n+        columns must contain enough columns to allow Cloud Spanner to derive\n+        values for all primary key columns in the row(s) to be modified.\n+      values: The values to be written. `values` can contain more than one\n+        list of values. If it does, then multiple rows are written, one for\n+        each entry in `values`. Each list in `values` must have exactly as\n+        many entries as there are entries in columns above. Sending multiple\n+        lists is equivalent to sending multiple Mutations, each containing one\n+        `values` entry and repeating table and columns.\n+      keyset: (Optional) The primary keys of the rows within table to delete.\n+        Delete is idempotent. The transaction will succeed even if some or\n+        all rows do not exist.\n+    \"\"\"\n+    self._columns = columns\n+    self._values = values\n+    self._keyset = keyset\n+\n+    self._insert = insert\n+    self._update = update\n+    self._insert_or_update = insert_or_update\n+    self._replace = replace\n+    self._delete = delete\n+\n+    if sum([\n+        1 for x in [self._insert, self._update, self._insert_or_update,\n+                    self._replace, self._delete]\n+        if x is not None\n+    ]) != 1:\n+      raise ValueError(\"No or more than one write mutation operation \"\n+                       \"provided: <%s: %s>\" % (self.__class__.__name__,\n+                                               str(self.__dict__)))\n+\n+  def __call__(self, *args, **kwargs):\n+    if self._insert is not None:\n+      return WriteMutation.insert(\n+          table=self._insert, columns=self._columns, values=self._values)\n+    elif self._update is not None:\n+      return WriteMutation.update(\n+          table=self._update, columns=self._columns, values=self._values)\n+    elif self._insert_or_update is not None:\n+      return WriteMutation.insert_or_update(\n+          table=self._insert_or_update,\n+          columns=self._columns,\n+          values=self._values)\n+    elif self._replace is not None:\n+      return WriteMutation.replace(\n+          table=self._replace, columns=self._columns, values=self._values)\n+    elif self._delete is not None:\n+      return WriteMutation.delete(table=self._delete, keyset=self._keyset)\n+\n+  @staticmethod\n+  def insert(table, columns, values):\n+    \"\"\"Insert one or more new table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(insert=batch._make_write_pb(table, columns, values)),", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg0NTU1Ng==", "url": "https://github.com/apache/beam/pull/10712#discussion_r372845556", "bodyText": "In this DoFn, we are just filtering the objects whos mutation size is greater the max_batch_size provided by the user. This helps us to focus on only batchable mutation which will further passed on the _BatchFn dofn to make batches.", "author": "mszb", "createdAt": "2020-01-30T09:41:35Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -581,3 +644,369 @@ def display_data(self):\n                                            label='transaction')\n \n     return res\n+\n+\n+@experimental(extra_message=\"No backwards-compatibility guarantees.\")\n+class WriteToSpanner(PTransform):\n+\n+  def __init__(self, project_id, instance_id, database_id, pool=None,\n+               credentials=None, max_batch_size_bytes=1048576):\n+    \"\"\"\n+    A PTransform to write onto Google Cloud Spanner.\n+\n+    Args:\n+      project_id: Cloud spanner project id. Be sure to use the Project ID,\n+        not the Project Number.\n+      instance_id: Cloud spanner instance id.\n+      database_id: Cloud spanner database id.\n+      max_batch_size_bytes: (optional) Split the mutation into batches to\n+        reduce the number of transaction sent to Spanner. By default it is\n+        set to 1 MB (1048576 Bytes).\n+    \"\"\"\n+    self._configuration = _BeamSpannerConfiguration(\n+        project=project_id, instance=instance_id, database=database_id,\n+        credentials=credentials, pool=pool, snapshot_read_timestamp=None,\n+        snapshot_exact_staleness=None\n+    )\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._database_id = database_id\n+    self._project_id = project_id\n+    self._instance_id = instance_id\n+    self._pool = pool\n+\n+  def display_data(self):\n+    res = {\n+        'project_id': DisplayDataItem(self._project_id, label='Project Id'),\n+        'instance_id': DisplayDataItem(self._instance_id, label='Instance Id'),\n+        'pool': DisplayDataItem(str(self._pool), label='Pool'),\n+        'database': DisplayDataItem(self._database_id, label='Database'),\n+        'batch_size': DisplayDataItem(self._max_batch_size_bytes,\n+                                      label=\"Batch Size\"),\n+    }\n+    return res\n+\n+  def expand(self, pcoll):\n+    return (pcoll\n+            | \"make batches\" >>\n+            _WriteGroup(max_batch_size_bytes=self._max_batch_size_bytes)\n+            | 'Writing to spanner' >> ParDo(\n+                _WriteToSpannerDoFn(self._configuration)))\n+\n+\n+class _Mutator(namedtuple('_Mutator', [\"mutation\", \"operation\", \"kwargs\"])):\n+  __slots__ = ()\n+\n+  @property\n+  def byte_size(self):\n+    return self.mutation.ByteSize()\n+\n+\n+class MutationGroup(deque):\n+  \"\"\"\n+  A Bundle of Spanner Mutations (_Mutator).\n+  \"\"\"\n+\n+  @property\n+  def byte_size(self):\n+    s = 0\n+    for m in self.__iter__():\n+      s += m.byte_size\n+    return s\n+\n+  def primary(self):\n+    return next(self.__iter__())\n+\n+\n+class WriteMutation(object):\n+\n+  _OPERATION_DELETE = \"delete\"\n+  _OPERATION_INSERT = \"insert\"\n+  _OPERATION_INSERT_OR_UPDATE = \"insert_or_update\"\n+  _OPERATION_REPLACE = \"replace\"\n+  _OPERATION_UPDATE = \"update\"\n+\n+  def __init__(self,\n+               insert=None,\n+               update=None,\n+               insert_or_update=None,\n+               replace=None,\n+               delete=None,\n+               columns=None,\n+               values=None,\n+               keyset=None):\n+    \"\"\"\n+    A convenient class to create Spanner Mutations for Write. User can provide\n+    the operation via constructor or via static methods.\n+\n+    Note: If a user passing the operation via construction, make sure that it\n+    will only accept one operation at a time. For example, if a user passing\n+    a table name in the `insert` parameter, and he also passes the `update`\n+    parameter value, this will cause an error.\n+\n+    Args:\n+      insert: (Optional) Name of the table in which rows will be inserted.\n+      update: (Optional) Name of the table in which existing rows will be\n+        updated.\n+      insert_or_update: (Optional) Table name in which rows will be written.\n+        Like insert, except that if the row already exists, then its column\n+        values are overwritten with the ones provided. Any column values not\n+        explicitly written are preserved.\n+      replace: (Optional) Table name in which rows will be replaced. Like\n+        insert, except that if the row already exists, it is deleted, and the\n+        column values provided are inserted instead. Unlike `insert_or_update`,\n+        this means any values not explicitly written become `NULL`.\n+      delete: (Optional) Table name from which rows will be deleted. Succeeds\n+        whether or not the named rows were present.\n+      columns: The names of the columns in table to be written. The list of\n+        columns must contain enough columns to allow Cloud Spanner to derive\n+        values for all primary key columns in the row(s) to be modified.\n+      values: The values to be written. `values` can contain more than one\n+        list of values. If it does, then multiple rows are written, one for\n+        each entry in `values`. Each list in `values` must have exactly as\n+        many entries as there are entries in columns above. Sending multiple\n+        lists is equivalent to sending multiple Mutations, each containing one\n+        `values` entry and repeating table and columns.\n+      keyset: (Optional) The primary keys of the rows within table to delete.\n+        Delete is idempotent. The transaction will succeed even if some or\n+        all rows do not exist.\n+    \"\"\"\n+    self._columns = columns\n+    self._values = values\n+    self._keyset = keyset\n+\n+    self._insert = insert\n+    self._update = update\n+    self._insert_or_update = insert_or_update\n+    self._replace = replace\n+    self._delete = delete\n+\n+    if sum([\n+        1 for x in [self._insert, self._update, self._insert_or_update,\n+                    self._replace, self._delete]\n+        if x is not None\n+    ]) != 1:\n+      raise ValueError(\"No or more than one write mutation operation \"\n+                       \"provided: <%s: %s>\" % (self.__class__.__name__,\n+                                               str(self.__dict__)))\n+\n+  def __call__(self, *args, **kwargs):\n+    if self._insert is not None:\n+      return WriteMutation.insert(\n+          table=self._insert, columns=self._columns, values=self._values)\n+    elif self._update is not None:\n+      return WriteMutation.update(\n+          table=self._update, columns=self._columns, values=self._values)\n+    elif self._insert_or_update is not None:\n+      return WriteMutation.insert_or_update(\n+          table=self._insert_or_update,\n+          columns=self._columns,\n+          values=self._values)\n+    elif self._replace is not None:\n+      return WriteMutation.replace(\n+          table=self._replace, columns=self._columns, values=self._values)\n+    elif self._delete is not None:\n+      return WriteMutation.delete(table=self._delete, keyset=self._keyset)\n+\n+  @staticmethod\n+  def insert(table, columns, values):\n+    \"\"\"Insert one or more new table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(insert=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def update(table, columns, values):\n+    \"\"\"Update one or more existing table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+  @staticmethod\n+  def insert_or_update(table, columns, values):\n+    \"\"\"Insert/update one or more table rows.\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(\n+            insert_or_update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT_OR_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def replace(table, columns, values):\n+    \"\"\"Replace one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(replace=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_REPLACE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def delete(table, keyset):\n+    \"\"\"Delete one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      keyset: Keys/ranges identifying rows to delete.\n+    \"\"\"\n+    delete = Mutation.Delete(table=table, key_set=keyset._to_pb())\n+    return _Mutator(mutation=Mutation(delete=delete),\n+                    operation=WriteMutation._OPERATION_DELETE,\n+                    kwargs={\"table\": table, \"keyset\": keyset})\n+\n+\n+@with_input_types(typing.Union[MutationGroup, TaggedOutput])\n+@with_output_types(MutationGroup)\n+class _BatchFn(DoFn):\n+  \"\"\"\n+  Batches mutations together.\n+  \"\"\"\n+\n+  def __init__(self, max_batch_size_bytes):\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+\n+  def start_bundle(self):\n+    self._batch = MutationGroup()\n+    self._size_in_bytes = 0\n+\n+  def process(self, element):\n+    _max_bytes = self._max_batch_size_bytes\n+    mg_size = element.byte_size  # total size of the mutation group.\n+\n+    if mg_size + self._size_in_bytes > _max_bytes:\n+      # Batch is full, output the batch and resetting the count.\n+      yield self._batch\n+      self._size_in_bytes = 0\n+      self._batch = MutationGroup()\n+\n+    self._batch.extend(element)\n+    self._size_in_bytes += mg_size\n+\n+  def finish_bundle(self):\n+    if self._batch is not None:\n+      yield window.GlobalWindows.windowed_value(self._batch)\n+      self._batch = None\n+\n+\n+@with_input_types(MutationGroup)\n+@with_output_types(MutationGroup)\n+class _BatchableFilterFn(DoFn):", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fc0c5cfdd734e3f2215e353e4078339e36dff659", "url": "https://github.com/apache/beam/commit/fc0c5cfdd734e3f2215e353e4078339e36dff659", "message": "Fix typo in _WriteGroup class.", "committedDate": "2020-01-30T10:59:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5NTY0Mw==", "url": "https://github.com/apache/beam/pull/10712#discussion_r374795643", "bodyText": "There is one other batching parameter which is important -- the maximum number of cells being mutated. Spanner has a hard 20K limit here, so a batch must have less than 20K mutated cells, including cells being mutated in indexes.\nJava version sets this to 5K by default.\nA third parameter max_number_rows was also added recently to java, limiting the total number of rows in a batch.", "author": "nielm", "createdAt": "2020-02-04T16:54:42Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -109,20 +111,74 @@\n \n ReadFromSpanner takes this transform in the constructor and pass this to the\n read pipeline as the singleton side input.\n+\n+Writing Data to Cloud Spanner.\n+\n+The WriteToSpanner transform writes to Cloud Spanner by executing a\n+collection a input rows (WriteMutation). The mutations are grouped into\n+batches for efficiency.\n+\n+WriteToSpanner transform relies on the WriteMutation objects which is exposed\n+by the SpannerIO API. WriteMutation have five static methods (insert, update,\n+insert_or_update, replace, delete). These methods returns the instance of the\n+_Mutator object which contains the mutation type and the Spanner Mutation\n+object. For more details, review the docs of the class SpannerIO.WriteMutation.\n+For example:::\n+\n+  mutations = [\n+                WriteMutation.insert(table='user', columns=('name', 'email'),\n+                values=[('sara'. 'sara@dev.com')])\n+              ]\n+  _ = (p\n+       | beam.Create(mutations)\n+       | WriteToSpanner(\n+          project_id=SPANNER_PROJECT_ID,\n+          instance_id=SPANNER_INSTANCE_ID,\n+          database_id=SPANNER_DATABASE_NAME)\n+        )\n+\n+You can also create WriteMutation via calling its constructor. For example:::\n+\n+  mutations = [\n+      WriteMutation(insert='users', columns=('name', 'email'),\n+                    values=[('sara\", 'sara@example.com')])\n+  ]\n+\n+For more information, review the docs available on WriteMutation class.\n+\n+WriteToSpanner transform also takes 'max_batch_size_bytes' param which is set\n+to 1MB (1048576 bytes) by default. This parameter used to reduce the number of", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTg5MjY4NA==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375892684", "bodyText": "For my understanding, maximum_number_cells would the (total number of columns * total number of rows)\nFor Example:\nWriteMutation.insert(\"roles\", (\"key\", \"rolename\"), [('abc1', \"test-1\"), ('abc2', \"test-2\"), ('abc3', \"test-3\")])\n\nin this case... the max_number_cells would be 2 * 3 = 6\nAnd for the max_rows_number will be 2 in the below case.\nMutationGroup([\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WriteMutation.insert(\"roles\", (\"key\", \"rolename\"),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0[('abc1', \"test1\")]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WriteMutation.insert(\"roles\", (\"key\", \"rolename\"),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0[('abc2', \"test2\")])\n\u00a0 \u00a0 \u00a0 \u00a0 ])\n\nPlease corrent me if I am mistaken!", "author": "mszb", "createdAt": "2020-02-06T15:13:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5NTY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyNjg3Mw==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375926873", "bodyText": "Yes, you are correct.", "author": "nielm", "createdAt": "2020-02-06T16:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5NTY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkzMDA5Mg==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375930092", "bodyText": "Also, given that in this transform the batches are not pre-sorted, I would make the defaults a lot smaller than the Java equivalent: say max 500 cells per batch, and max 50 rows.", "author": "nielm", "createdAt": "2020-02-06T16:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5NTY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk0NTYzOQ==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375945639", "bodyText": "Thanks. I'll update the code!", "author": "mszb", "createdAt": "2020-02-06T16:35:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5NTY0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwMDE2Mw==", "url": "https://github.com/apache/beam/pull/10712#discussion_r374800163", "bodyText": "Please add a note to the following\n\nUnlike the Java connector, this connector  does not create batches of transactions sorted by table and primary key.\n\nThis can be a feature which is added later, I would not let it block this PR.\nSee https://medium.com/google-cloud/cloud-spanner-maximizing-data-load-throughput-23a0fc064b6d for more info.", "author": "nielm", "createdAt": "2020-02-04T17:02:10Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -109,20 +111,74 @@\n \n ReadFromSpanner takes this transform in the constructor and pass this to the\n read pipeline as the singleton side input.\n+\n+Writing Data to Cloud Spanner.\n+\n+The WriteToSpanner transform writes to Cloud Spanner by executing a\n+collection a input rows (WriteMutation). The mutations are grouped into\n+batches for efficiency.\n+\n+WriteToSpanner transform relies on the WriteMutation objects which is exposed\n+by the SpannerIO API. WriteMutation have five static methods (insert, update,\n+insert_or_update, replace, delete). These methods returns the instance of the\n+_Mutator object which contains the mutation type and the Spanner Mutation\n+object. For more details, review the docs of the class SpannerIO.WriteMutation.\n+For example:::\n+\n+  mutations = [\n+                WriteMutation.insert(table='user', columns=('name', 'email'),\n+                values=[('sara'. 'sara@dev.com')])\n+              ]\n+  _ = (p\n+       | beam.Create(mutations)\n+       | WriteToSpanner(\n+          project_id=SPANNER_PROJECT_ID,\n+          instance_id=SPANNER_INSTANCE_ID,\n+          database_id=SPANNER_DATABASE_NAME)\n+        )\n+\n+You can also create WriteMutation via calling its constructor. For example:::\n+\n+  mutations = [\n+      WriteMutation(insert='users', columns=('name', 'email'),\n+                    values=[('sara\", 'sara@example.com')])\n+  ]\n+\n+For more information, review the docs available on WriteMutation class.\n+\n+WriteToSpanner transform also takes 'max_batch_size_bytes' param which is set\n+to 1MB (1048576 bytes) by default. This parameter used to reduce the number of\n+transactions sent to spanner by grouping the mutation into batches. Setting\n+this either to smaller value or zero to disable batching.\n+", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwMjUzMA==", "url": "https://github.com/apache/beam/pull/10712#discussion_r374802530", "bodyText": "What is the error handling here?\nSay a mutation fails to be written, what will happen to the pipeline?\n(The Java code has a option choosing between failing the entire pipeline, or outputting failed mutations to a separate output for later analysis.)", "author": "nielm", "createdAt": "2020-02-04T17:06:18Z", "path": "sdks/python/apache_beam/io/gcp/experimental/spannerio.py", "diffHunk": "@@ -581,3 +644,369 @@ def display_data(self):\n                                            label='transaction')\n \n     return res\n+\n+\n+@experimental(extra_message=\"No backwards-compatibility guarantees.\")\n+class WriteToSpanner(PTransform):\n+\n+  def __init__(self, project_id, instance_id, database_id, pool=None,\n+               credentials=None, max_batch_size_bytes=1048576):\n+    \"\"\"\n+    A PTransform to write onto Google Cloud Spanner.\n+\n+    Args:\n+      project_id: Cloud spanner project id. Be sure to use the Project ID,\n+        not the Project Number.\n+      instance_id: Cloud spanner instance id.\n+      database_id: Cloud spanner database id.\n+      max_batch_size_bytes: (optional) Split the mutation into batches to\n+        reduce the number of transaction sent to Spanner. By default it is\n+        set to 1 MB (1048576 Bytes).\n+    \"\"\"\n+    self._configuration = _BeamSpannerConfiguration(\n+        project=project_id, instance=instance_id, database=database_id,\n+        credentials=credentials, pool=pool, snapshot_read_timestamp=None,\n+        snapshot_exact_staleness=None\n+    )\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._database_id = database_id\n+    self._project_id = project_id\n+    self._instance_id = instance_id\n+    self._pool = pool\n+\n+  def display_data(self):\n+    res = {\n+        'project_id': DisplayDataItem(self._project_id, label='Project Id'),\n+        'instance_id': DisplayDataItem(self._instance_id, label='Instance Id'),\n+        'pool': DisplayDataItem(str(self._pool), label='Pool'),\n+        'database': DisplayDataItem(self._database_id, label='Database'),\n+        'batch_size': DisplayDataItem(self._max_batch_size_bytes,\n+                                      label=\"Batch Size\"),\n+    }\n+    return res\n+\n+  def expand(self, pcoll):\n+    return (pcoll\n+            | \"make batches\" >>\n+            _WriteGroup(max_batch_size_bytes=self._max_batch_size_bytes)\n+            | 'Writing to spanner' >> ParDo(\n+                _WriteToSpannerDoFn(self._configuration)))\n+\n+\n+class _Mutator(namedtuple('_Mutator', [\"mutation\", \"operation\", \"kwargs\"])):\n+  __slots__ = ()\n+\n+  @property\n+  def byte_size(self):\n+    return self.mutation.ByteSize()\n+\n+\n+class MutationGroup(deque):\n+  \"\"\"\n+  A Bundle of Spanner Mutations (_Mutator).\n+  \"\"\"\n+\n+  @property\n+  def byte_size(self):\n+    s = 0\n+    for m in self.__iter__():\n+      s += m.byte_size\n+    return s\n+\n+  def primary(self):\n+    return next(self.__iter__())\n+\n+\n+class WriteMutation(object):\n+\n+  _OPERATION_DELETE = \"delete\"\n+  _OPERATION_INSERT = \"insert\"\n+  _OPERATION_INSERT_OR_UPDATE = \"insert_or_update\"\n+  _OPERATION_REPLACE = \"replace\"\n+  _OPERATION_UPDATE = \"update\"\n+\n+  def __init__(self,\n+               insert=None,\n+               update=None,\n+               insert_or_update=None,\n+               replace=None,\n+               delete=None,\n+               columns=None,\n+               values=None,\n+               keyset=None):\n+    \"\"\"\n+    A convenient class to create Spanner Mutations for Write. User can provide\n+    the operation via constructor or via static methods.\n+\n+    Note: If a user passing the operation via construction, make sure that it\n+    will only accept one operation at a time. For example, if a user passing\n+    a table name in the `insert` parameter, and he also passes the `update`\n+    parameter value, this will cause an error.\n+\n+    Args:\n+      insert: (Optional) Name of the table in which rows will be inserted.\n+      update: (Optional) Name of the table in which existing rows will be\n+        updated.\n+      insert_or_update: (Optional) Table name in which rows will be written.\n+        Like insert, except that if the row already exists, then its column\n+        values are overwritten with the ones provided. Any column values not\n+        explicitly written are preserved.\n+      replace: (Optional) Table name in which rows will be replaced. Like\n+        insert, except that if the row already exists, it is deleted, and the\n+        column values provided are inserted instead. Unlike `insert_or_update`,\n+        this means any values not explicitly written become `NULL`.\n+      delete: (Optional) Table name from which rows will be deleted. Succeeds\n+        whether or not the named rows were present.\n+      columns: The names of the columns in table to be written. The list of\n+        columns must contain enough columns to allow Cloud Spanner to derive\n+        values for all primary key columns in the row(s) to be modified.\n+      values: The values to be written. `values` can contain more than one\n+        list of values. If it does, then multiple rows are written, one for\n+        each entry in `values`. Each list in `values` must have exactly as\n+        many entries as there are entries in columns above. Sending multiple\n+        lists is equivalent to sending multiple Mutations, each containing one\n+        `values` entry and repeating table and columns.\n+      keyset: (Optional) The primary keys of the rows within table to delete.\n+        Delete is idempotent. The transaction will succeed even if some or\n+        all rows do not exist.\n+    \"\"\"\n+    self._columns = columns\n+    self._values = values\n+    self._keyset = keyset\n+\n+    self._insert = insert\n+    self._update = update\n+    self._insert_or_update = insert_or_update\n+    self._replace = replace\n+    self._delete = delete\n+\n+    if sum([\n+        1 for x in [self._insert, self._update, self._insert_or_update,\n+                    self._replace, self._delete]\n+        if x is not None\n+    ]) != 1:\n+      raise ValueError(\"No or more than one write mutation operation \"\n+                       \"provided: <%s: %s>\" % (self.__class__.__name__,\n+                                               str(self.__dict__)))\n+\n+  def __call__(self, *args, **kwargs):\n+    if self._insert is not None:\n+      return WriteMutation.insert(\n+          table=self._insert, columns=self._columns, values=self._values)\n+    elif self._update is not None:\n+      return WriteMutation.update(\n+          table=self._update, columns=self._columns, values=self._values)\n+    elif self._insert_or_update is not None:\n+      return WriteMutation.insert_or_update(\n+          table=self._insert_or_update,\n+          columns=self._columns,\n+          values=self._values)\n+    elif self._replace is not None:\n+      return WriteMutation.replace(\n+          table=self._replace, columns=self._columns, values=self._values)\n+    elif self._delete is not None:\n+      return WriteMutation.delete(table=self._delete, keyset=self._keyset)\n+\n+  @staticmethod\n+  def insert(table, columns, values):\n+    \"\"\"Insert one or more new table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(insert=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def update(table, columns, values):\n+    \"\"\"Update one or more existing table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+  @staticmethod\n+  def insert_or_update(table, columns, values):\n+    \"\"\"Insert/update one or more table rows.\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(\n+            insert_or_update=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_INSERT_OR_UPDATE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def replace(table, columns, values):\n+    \"\"\"Replace one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      columns: Name of the table columns to be modified.\n+      values: Values to be modified.\n+    \"\"\"\n+    return _Mutator(\n+        mutation=Mutation(replace=batch._make_write_pb(table, columns, values)),\n+        operation=WriteMutation._OPERATION_REPLACE, kwargs={\n+            \"table\": table, \"columns\": columns, \"values\": values})\n+\n+  @staticmethod\n+  def delete(table, keyset):\n+    \"\"\"Delete one or more table rows.\n+\n+    Args:\n+      table: Name of the table to be modified.\n+      keyset: Keys/ranges identifying rows to delete.\n+    \"\"\"\n+    delete = Mutation.Delete(table=table, key_set=keyset._to_pb())\n+    return _Mutator(mutation=Mutation(delete=delete),\n+                    operation=WriteMutation._OPERATION_DELETE,\n+                    kwargs={\"table\": table, \"keyset\": keyset})\n+\n+\n+@with_input_types(typing.Union[MutationGroup, TaggedOutput])\n+@with_output_types(MutationGroup)\n+class _BatchFn(DoFn):\n+  \"\"\"\n+  Batches mutations together.\n+  \"\"\"\n+\n+  def __init__(self, max_batch_size_bytes):\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+\n+  def start_bundle(self):\n+    self._batch = MutationGroup()\n+    self._size_in_bytes = 0\n+\n+  def process(self, element):\n+    _max_bytes = self._max_batch_size_bytes\n+    mg_size = element.byte_size  # total size of the mutation group.\n+\n+    if mg_size + self._size_in_bytes > _max_bytes:\n+      # Batch is full, output the batch and resetting the count.\n+      yield self._batch\n+      self._size_in_bytes = 0\n+      self._batch = MutationGroup()\n+\n+    self._batch.extend(element)\n+    self._size_in_bytes += mg_size\n+\n+  def finish_bundle(self):\n+    if self._batch is not None:\n+      yield window.GlobalWindows.windowed_value(self._batch)\n+      self._batch = None\n+\n+\n+@with_input_types(MutationGroup)\n+@with_output_types(MutationGroup)\n+class _BatchableFilterFn(DoFn):\n+  \"\"\"\n+  Filters MutationGroups larger than the batch size to the output tagged with\n+  OUTPUT_TAG_UNBATCHABLE.\n+  \"\"\"\n+  OUTPUT_TAG_UNBATCHABLE = 'unbatchable'\n+\n+  def __init__(self, max_batch_size_bytes):\n+    self._max_batch_size_bytes = max_batch_size_bytes\n+    self._batchable = None\n+    self._unbatchable = None\n+\n+  def process(self, element):\n+    if element.primary().operation == 'delete':\n+      # As delete mutations are not batchable.\n+      yield TaggedOutput(_BatchableFilterFn.OUTPUT_TAG_UNBATCHABLE, element)\n+    else:\n+      _max_bytes = self._max_batch_size_bytes\n+      mg = element\n+      mg_size = mg.byte_size\n+      if mg_size > _max_bytes:\n+        yield TaggedOutput(_BatchableFilterFn.OUTPUT_TAG_UNBATCHABLE, element)\n+      else:\n+        yield element\n+\n+\n+class _WriteToSpannerDoFn(DoFn):\n+\n+  def __init__(self, spanner_configuration):\n+    self._spanner_configuration = spanner_configuration\n+    self._db_instance = None\n+    self.batches = Metrics.counter(self.__class__, 'SpannerBatches')\n+\n+  def setup(self):\n+    spanner_client = Client(self._spanner_configuration.project)\n+    instance = spanner_client.instance(self._spanner_configuration.instance)\n+    self._db_instance = instance.database(\n+        self._spanner_configuration.database,\n+        pool=self._spanner_configuration.pool)\n+\n+  def process(self, element):\n+    self.batches.inc()\n+    with self._db_instance.batch() as b:\n+      for m in element:\n+        if m.operation == WriteMutation._OPERATION_DELETE:\n+          batch_func = b.delete\n+        elif m.operation == WriteMutation._OPERATION_REPLACE:\n+          batch_func = b.replace\n+        elif m.operation == WriteMutation._OPERATION_INSERT_OR_UPDATE:\n+          batch_func = b.insert_or_update\n+        elif m.operation == WriteMutation._OPERATION_INSERT:\n+          batch_func = b.insert\n+        elif m.operation == WriteMutation._OPERATION_UPDATE:\n+          batch_func = b.update\n+        else:\n+          raise ValueError(\"Unknown operation action: %s\" % m.operation)\n+\n+        batch_func(**m.kwargs)", "originalCommit": "c8831cbcc13de6e701269d7ed7721e7aee70e482", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTg5MjQ2Mg==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375892462", "bodyText": "Since this write Write mutation code is tightly structured, that would be a very rare case (almost none) that the user will hit that exception. Here just making sure that the operation type should be one of the constant\u00a0set in the white mutation class. And for now, I haven't applied functionality\u00a0for the failed mutations to the separate\u00a0output. Do we need to add that functionality to this version?", "author": "mszb", "createdAt": "2020-02-06T15:12:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwMjUzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyNjQ0OQ==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375926449", "bodyText": "Sorry, by \"What is the error handling here\" I am asking what will happen if the Mutation fails to be written to Spanner\n\neg if the Mutation references a table or column that does not exist, or a value is not correct for the column type.\n\nIf a failure to write a single mutation would cause the entire pipeline to fail (which I think is the behavior now), then that should at least be documented, and perhaps something that should be introduced in a future version.", "author": "nielm", "createdAt": "2020-02-06T16:04:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwMjUzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk0NTkyMA==", "url": "https://github.com/apache/beam/pull/10712#discussion_r375945920", "bodyText": "Yes, you are right, that is the current behaviour. I'll update the docs for more clarification!", "author": "mszb", "createdAt": "2020-02-06T16:35:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgwMjUzMA=="}], "type": "inlineReview"}, {"oid": "0f5c57893940cf6717faffcd78806842aaaf6680", "url": "https://github.com/apache/beam/commit/0f5c57893940cf6717faffcd78806842aaaf6680", "message": "Merge branch 'master' into BEAM-7246_gcp_spannerio_write", "committedDate": "2020-02-10T08:11:30Z", "type": "commit"}, {"oid": "2e31bb0114a96ee27a1545e007eb4d44266ea4d9", "url": "https://github.com/apache/beam/commit/2e31bb0114a96ee27a1545e007eb4d44266ea4d9", "message": "added two batching parameters (max_number_rows, max_number_cells)", "committedDate": "2020-02-10T18:09:41Z", "type": "commit"}, {"oid": "c51d9a76f8b5c614aeafa41ac55f1d441eed5359", "url": "https://github.com/apache/beam/commit/c51d9a76f8b5c614aeafa41ac55f1d441eed5359", "message": "adds io note on CHANGES.md", "committedDate": "2020-02-10T18:32:16Z", "type": "commit"}, {"oid": "1f39f31ca9d730f2fb01ed50a427e4aaec475d04", "url": "https://github.com/apache/beam/commit/1f39f31ca9d730f2fb01ed50a427e4aaec475d04", "message": "Merge branch 'master' into BEAM-7246_gcp_spannerio_write", "committedDate": "2020-02-11T06:21:13Z", "type": "commit"}, {"oid": "fc3e056df25320cc608249b262bbf0df38a2882f", "url": "https://github.com/apache/beam/commit/fc3e056df25320cc608249b262bbf0df38a2882f", "message": "fix python code formate", "committedDate": "2020-02-11T14:13:14Z", "type": "commit"}, {"oid": "fdbd04b208543fa9e908c0056f11eba8890db4fc", "url": "https://github.com/apache/beam/commit/fdbd04b208543fa9e908c0056f11eba8890db4fc", "message": "Merge branch 'master' into BEAM-7246_gcp_spannerio_write", "committedDate": "2020-02-13T06:44:54Z", "type": "commit"}, {"oid": "989cf4a1cb5423070674b1606358a9d67bf723ea", "url": "https://github.com/apache/beam/commit/989cf4a1cb5423070674b1606358a9d67bf723ea", "message": "Added docs for spanner write.", "committedDate": "2020-02-13T14:14:03Z", "type": "commit"}, {"oid": "8bc522372bff4e848665977b15a5d921146c9d52", "url": "https://github.com/apache/beam/commit/8bc522372bff4e848665977b15a5d921146c9d52", "message": "Merge branch 'master' into BEAM-7246_gcp_spannerio_write", "committedDate": "2020-02-18T21:21:26Z", "type": "commit"}]}