{"pr_number": 10760, "pr_title": "[BEAM-9545] Dataframe transforms", "pr_createdAt": "2020-02-04T00:44:36Z", "pr_url": "https://github.com/apache/beam/pull/10760", "timeline": [{"oid": "6ad65298b5ec72fb48b5566fb9aa39705d38c606", "url": "https://github.com/apache/beam/commit/6ad65298b5ec72fb48b5566fb9aa39705d38c606", "message": "Evaluation of deferred dataframes via Beam operations.", "committedDate": "2020-03-13T20:43:13Z", "type": "forcePushed"}, {"oid": "ed4f26543597ae5fb889ef9f6fbbccd5b42ebbb4", "url": "https://github.com/apache/beam/commit/ed4f26543597ae5fb889ef9f6fbbccd5b42ebbb4", "message": "Evaluation of deferred dataframes via Beam operations.", "committedDate": "2020-03-14T05:29:24Z", "type": "forcePushed"}, {"oid": "ad804f153f0a96390c6328e7a118ce529a120d15", "url": "https://github.com/apache/beam/commit/ad804f153f0a96390c6328e7a118ce529a120d15", "message": "[BEAM-9496] Evaluation of deferred dataframes via Beam operations.", "committedDate": "2020-03-14T06:31:35Z", "type": "commit"}, {"oid": "ad804f153f0a96390c6328e7a118ce529a120d15", "url": "https://github.com/apache/beam/commit/ad804f153f0a96390c6328e7a118ce529a120d15", "message": "[BEAM-9496] Evaluation of deferred dataframes via Beam operations.", "committedDate": "2020-03-14T06:31:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU5ODA4NQ==", "url": "https://github.com/apache/beam/pull/10760#discussion_r394598085", "bodyText": "Is there a way to leverage pandas test suites for more complete testing?", "author": "TheNeuralBit", "createdAt": "2020-03-18T19:46:12Z", "path": "sdks/python/apache_beam/dataframe/transforms_test.py", "diffHunk": "@@ -0,0 +1,87 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import unittest\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import transforms\n+from apache_beam.testing.util import assert_that\n+\n+\n+class TransformTest(unittest.TestCase):\n+  def run_test(self, input, func):\n+    expected = func(input)\n+\n+    empty = input[0:0]\n+    input_placeholder = expressions.PlaceholderExpression(empty)\n+    input_deferred = frame_base.DeferredFrame.wrap(input_placeholder)\n+    actual_deferred = func(input_deferred)._expr.evaluate_at(\n+        expressions.Session({input_placeholder: input}))\n+\n+    def check_correct(actual):\n+      if actual is None:\n+        raise AssertionError('Empty frame but expected: \\n\\n%s' % (expected))\n+      sorted_actual = actual.sort_index()\n+      sorted_expected = expected.sort_index()\n+      if not sorted_actual.equals(sorted_expected):\n+        raise AssertionError(\n+            'Dataframes not equal: \\n\\n%s\\n\\n%s' %\n+            (sorted_actual, sorted_expected))\n+\n+    check_correct(actual_deferred)\n+\n+    with beam.Pipeline() as p:\n+      input_pcoll = p | beam.Create([input[::2], input[1::2]])\n+      output_pcoll = input_pcoll | transforms.DataframeTransform(\n+          func, proxy=empty)\n+      assert_that(\n+          output_pcoll,\n+          lambda actual: check_correct(pd.concat(actual) if actual else None))\n+\n+  def test_identity(self):\n+    df = pd.DataFrame({\n+        'Animal': ['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n+        'Speed': [380., 370., 24., 26.]\n+    })\n+    self.run_test(df, lambda x: x)\n+\n+  def test_sum_mean(self):\n+    df = pd.DataFrame({\n+        'Animal': ['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n+        'Speed': [380., 370., 24., 26.]\n+    })\n+    self.run_test(df, lambda df: df.groupby('Animal').sum())\n+    self.run_test(df, lambda df: df.groupby('Animal').mean())\n+\n+  def test_filter(self):\n+    df = pd.DataFrame({\n+        'Animal': ['Aardvark', 'Ant', 'Elephant', 'Zebra'],\n+        'Speed': [5, 2, 35, 40]\n+    })\n+    self.run_test(df, lambda df: df.filter(items=['Animal']))\n+    self.run_test(df, lambda df: df.filter(regex='A.*'))\n+    self.run_test(\n+        df, lambda df: df.set_index('Animal').filter(regex='F.*', axis='index'))\n+\n+\n+if __name__ == '__main__':\n+  unittest.main()", "originalCommit": "ad804f153f0a96390c6328e7a118ce529a120d15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI1NTM1OA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395255358", "bodyText": "I actually have some code that does just that: it books into the doctest framework to run vanilla pandas tests through these functions as beam pipelines. I've been meaning to get it into a PR, but didn't want to overwhelm you.", "author": "robertwb", "createdAt": "2020-03-19T19:03:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU5ODA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4MDE3Mw==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395380173", "bodyText": "Gotcha, I thought you had mentioned this at some point. I filed https://issues.apache.org/jira/browse/BEAM-9561 to track.", "author": "TheNeuralBit", "createdAt": "2020-03-19T23:47:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU5ODA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwMjI4Ng==", "url": "https://github.com/apache/beam/pull/10760#discussion_r394602286", "bodyText": "Would this example work? It doesn't set the proxy argument.", "author": "TheNeuralBit", "createdAt": "2020-03-18T19:54:06Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,244 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum())", "originalCommit": "ad804f153f0a96390c6328e7a118ce529a120d15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIyNTgzMA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395225830", "bodyText": "This is aspirational, hoping we can infer the proxy from the pcollection schema. Want to take that up :)?\nUpdated the doc for now.", "author": "robertwb", "createdAt": "2020-03-19T18:12:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwMjI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4MDQxNQ==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395380415", "bodyText": "Ah ok, yeah I can take that up :)", "author": "TheNeuralBit", "createdAt": "2020-03-19T23:48:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwMjI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYxMDQxMA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r394610410", "bodyText": "should this assign result_frames as well?", "author": "TheNeuralBit", "createdAt": "2020-03-18T20:10:10Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,244 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum())\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return dict\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      self._func(*(value for _, value in sorted(placeholders.items())))", "originalCommit": "ad804f153f0a96390c6328e7a118ce529a120d15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIyNjMxMw==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395226313", "bodyText": "Yeah. And now it's tested.", "author": "robertwb", "createdAt": "2020-03-19T18:13:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYxMDQxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYyMDExNQ==", "url": "https://github.com/apache/beam/pull/10760#discussion_r394620115", "bodyText": "We already have these defined in a couple of places (I see one in bundle_processor.py and one in apache_beam.transforms.external). It seems silly to redefine it everywhere, but the alternative (pulling it into some util file) also seems silly. I guess the best option is just replace them all with functools.lru_cache after we drop python 2.", "author": "TheNeuralBit", "createdAt": "2020-03-18T20:28:47Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,244 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum())\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return dict\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      self._func(*(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple(*(value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(_partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def _partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage\n+\n+    @memoize\n+    def expr_to_stages(expr):\n+      assert expr not in inputs\n+      # First attempt to compute this expression as part of an existing stage,\n+      # if possible.\n+      for stage in common_stages([expr_to_stages(arg) for arg in expr.args()\n+                                  if arg not in inputs]):\n+        if (not expr.requires_partition_by_index() or\n+            all(output_is_partitioned_by_index(arg, stage)\n+                for arg in expr.args())):\n+          break\n+      else:\n+        # Otherwise, compute this expression as part of a new stage.\n+        stage = Stage(expr.args(), expr.requires_partition_by_index())\n+        for arg in expr.args():\n+          if arg not in inputs:\n+            expr_to_stages(arg).append(stage)\n+            expr_to_stage(arg).outputs.add(arg)\n+      stage.ops.append(expr)\n+      # This is a list as given expression may be available in many stages.\n+      return [stage]\n+\n+    def expr_to_stage(expr):\n+      # Any will do; the first requires the fewest intermediate stages.\n+      return expr_to_stages(expr)[0]\n+\n+    # Ensure each output is computed.\n+    for expr in outputs.values():\n+      if expr not in inputs:\n+        expr_to_stage(expr).outputs.add(expr)\n+\n+    @memoize\n+    def stage_to_result(stage):\n+      return {expr._id: expr_to_pcoll(expr)\n+              for expr in stage.inputs} | ComputeStage(stage)\n+\n+    @memoize\n+    def expr_to_pcoll(expr):\n+      if expr in inputs:\n+        return inputs[expr]\n+      else:\n+        return stage_to_result(expr_to_stage(expr))[expr._id]\n+\n+    # Now we can compute and return the result.\n+    return {k: expr_to_pcoll(expr) for k, expr in outputs.items()}\n+\n+\n+def memoize(f):", "originalCommit": "ad804f153f0a96390c6328e7a118ce529a120d15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI1NDM5MQ==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395254391", "bodyText": "Yeah.", "author": "robertwb", "createdAt": "2020-03-19T19:02:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYyMDExNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYzMDMyMQ==", "url": "https://github.com/apache/beam/pull/10760#discussion_r394630321", "bodyText": "nit: none of the other functions defined here have an underscore prefix, can you drop this one?", "author": "TheNeuralBit", "createdAt": "2020-03-18T20:48:27Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,244 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum())\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return dict\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      self._func(*(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple(*(value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(_partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def _partition_by_index(df, levels=None, parts=10):", "originalCommit": "ad804f153f0a96390c6328e7a118ce529a120d15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI1Mzk3OA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395253978", "bodyText": "Done.", "author": "robertwb", "createdAt": "2020-03-19T19:01:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYzMDMyMQ=="}], "type": "inlineReview"}, {"oid": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "url": "https://github.com/apache/beam/commit/2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "message": "Fix and test tuple inputs and outputs.", "committedDate": "2020-03-19T19:04:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4MTQ5Mw==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395381493", "bodyText": "Is this for a future optimization? Unless I'm missing something, this function never returns more than a one-element list", "author": "TheNeuralBit", "createdAt": "2020-03-19T23:52:41Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage\n+\n+    @memoize\n+    def expr_to_stages(expr):\n+      assert expr not in inputs\n+      # First attempt to compute this expression as part of an existing stage,\n+      # if possible.\n+      for stage in common_stages([expr_to_stages(arg) for arg in expr.args()\n+                                  if arg not in inputs]):\n+        if (not expr.requires_partition_by_index() or\n+            all(output_is_partitioned_by_index(arg, stage)\n+                for arg in expr.args())):\n+          break\n+      else:\n+        # Otherwise, compute this expression as part of a new stage.\n+        stage = Stage(expr.args(), expr.requires_partition_by_index())\n+        for arg in expr.args():\n+          if arg not in inputs:\n+            expr_to_stages(arg).append(stage)\n+            expr_to_stage(arg).outputs.add(arg)\n+      stage.ops.append(expr)\n+      # This is a list as given expression may be available in many stages.", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkwMDkzNg==", "url": "https://github.com/apache/beam/pull/10760#discussion_r395900936", "bodyText": "This will be useful for future optimizations, but is still useful now (it can also accumulate more than one value due to the expr_to_stages(arg).append(stage) line above).", "author": "robertwb", "createdAt": "2020-03-20T21:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4MTQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODEyNzMyNw==", "url": "https://github.com/apache/beam/pull/10760#discussion_r398127327", "bodyText": "This is a set intersection correct? It looks like maybe you can't just use set.intersection here because order within the stage list is important?", "author": "TheNeuralBit", "createdAt": "2020-03-25T19:51:37Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNzgxNA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r403107814", "bodyText": "Would this be effectively the same as excluding PlaceholderExpression instances, or only including ComputedExpression instances?", "author": "TheNeuralBit", "createdAt": "2020-04-03T16:01:18Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage\n+\n+    @memoize\n+    def expr_to_stages(expr):\n+      assert expr not in inputs\n+      # First attempt to compute this expression as part of an existing stage,\n+      # if possible.\n+      for stage in common_stages([expr_to_stages(arg) for arg in expr.args()\n+                                  if arg not in inputs]):", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExMjEwMA==", "url": "https://github.com/apache/beam/pull/10760#discussion_r403112100", "bodyText": "A comment clarifying this logic would be helpful. My understanding: \"If expr does not require partitioning, just grab any stage, else grab the first stage where all of expr's inputs are partitioned as required. In either case, use the first such stage because earlier stages are closer to the inputs (have fewer intermediate stages)\"", "author": "TheNeuralBit", "createdAt": "2020-04-03T16:08:35Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage\n+\n+    @memoize\n+    def expr_to_stages(expr):\n+      assert expr not in inputs\n+      # First attempt to compute this expression as part of an existing stage,\n+      # if possible.\n+      for stage in common_stages([expr_to_stages(arg) for arg in expr.args()\n+                                  if arg not in inputs]):\n+        if (not expr.requires_partition_by_index() or\n+            all(output_is_partitioned_by_index(arg, stage)\n+                for arg in expr.args())):\n+          break", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzExMjk4Ng==", "url": "https://github.com/apache/beam/pull/10760#discussion_r403112986", "bodyText": "These two lines took me a while to grok (and I'm still not sure I totally get it). A comment would be helpful. My understanding:\nFor each upstream expr, append this new stage onto it's stage list.\nThe args need to be an output for that stage, so that we can shuffle it to go into this new stage. Here again we choose the first stage in the list because it's the closest to the inputs.", "author": "TheNeuralBit", "createdAt": "2020-04-03T16:10:02Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]\n+\n+    def common_stages(stage_lists):\n+      if stage_lists:\n+        for stage in stage_lists[0]:\n+          if all(stage in other for other in stage_lists[1:]):\n+            yield stage\n+\n+    @memoize\n+    def expr_to_stages(expr):\n+      assert expr not in inputs\n+      # First attempt to compute this expression as part of an existing stage,\n+      # if possible.\n+      for stage in common_stages([expr_to_stages(arg) for arg in expr.args()\n+                                  if arg not in inputs]):\n+        if (not expr.requires_partition_by_index() or\n+            all(output_is_partitioned_by_index(arg, stage)\n+                for arg in expr.args())):\n+          break\n+      else:\n+        # Otherwise, compute this expression as part of a new stage.\n+        stage = Stage(expr.args(), expr.requires_partition_by_index())\n+        for arg in expr.args():\n+          if arg not in inputs:\n+            expr_to_stages(arg).append(stage)\n+            expr_to_stage(arg).outputs.add(arg)", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE3NzAxMg==", "url": "https://github.com/apache/beam/pull/10760#discussion_r403177012", "bodyText": "Using k here and in ComputeStage's expand is confusing", "author": "TheNeuralBit", "createdAt": "2020-04-03T17:33:31Z", "path": "sdks/python/apache_beam/dataframe/transforms.py", "diffHunk": "@@ -0,0 +1,246 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import absolute_import\n+\n+import pandas as pd\n+\n+import apache_beam as beam\n+from apache_beam import transforms\n+from apache_beam.dataframe import expressions\n+from apache_beam.dataframe import frame_base\n+from apache_beam.dataframe import frames  # pylint: disable=unused-import\n+\n+\n+class DataframeTransform(transforms.PTransform):\n+  \"\"\"A PTransform for applying function that takes and returns dataframes\n+  to one or more PCollections.\n+\n+  For example, if pcoll is a PCollection of dataframes, one could write::\n+\n+      pcoll | DataframeTransform(lambda df: df.group_by('key').sum(), proxy=...)\n+\n+  To pass multiple PCollections, pass a tuple of PCollections wich will be\n+  passed to the callable as positional arguments, or a dictionary of\n+  PCollections, in which case they will be passed as keyword arguments.\n+  \"\"\"\n+  def __init__(self, func, proxy):\n+    self._func = func\n+    self._proxy = proxy\n+\n+  def expand(self, input_pcolls):\n+    def wrap_as_dict(values):\n+      if isinstance(values, dict):\n+        return values\n+      elif isinstance(values, tuple):\n+        return dict(enumerate(values))\n+      else:\n+        return {None: values}\n+\n+    # TODO: Infer the proxy from the input schema.\n+    def proxy(key):\n+      if key is None:\n+        return self._proxy\n+      else:\n+        return self._proxy[key]\n+\n+    # The input can be a dictionary, tuple, or plain PCollection.\n+    # Wrap as a dict for homogeneity.\n+    # TODO: Possibly inject batching here.\n+    input_dict = wrap_as_dict(input_pcolls)\n+    placeholders = {\n+        key: frame_base.DeferredFrame.wrap(\n+            expressions.PlaceholderExpression(proxy(key)))\n+        for key in input_dict.keys()\n+    }\n+\n+    # The calling convention of the user-supplied func varies according to the\n+    # type of the input.\n+    if isinstance(input_pcolls, dict):\n+      result_frames = self._func(**placeholders)\n+    elif isinstance(input_pcolls, tuple):\n+      result_frames = self._func(\n+          *(value for _, value in sorted(placeholders.items())))\n+    else:\n+      result_frames = self._func(placeholders[None])\n+\n+    # Likewise the output may be a dict, tuple, or raw (deferred) Dataframe.\n+    result_dict = wrap_as_dict(result_frames)\n+\n+    result_pcolls = self._apply_deferred_ops(\n+        {placeholders[key]._expr: pcoll\n+         for key, pcoll in input_dict.items()},\n+        {key: df._expr\n+         for key, df in result_dict.items()})\n+\n+    # Convert the result back into a set of PCollections.\n+    if isinstance(result_frames, dict):\n+      return result_pcolls\n+    elif isinstance(result_frames, tuple):\n+      return tuple((value for _, value in sorted(result_pcolls.items())))\n+    else:\n+      return result_pcolls[None]\n+\n+  def _apply_deferred_ops(\n+      self,\n+      inputs,  # type: Dict[PlaceholderExpr, PCollection]\n+      outputs,  # type: Dict[Any, Expression]\n+      ):  # -> Dict[Any, PCollection]\n+    \"\"\"Construct a Beam graph that evaluates a set of expressions on a set of\n+    input PCollections.\n+\n+    :param inputs: A mapping of placeholder expressions to PCollections.\n+    :param outputs: A mapping of keys to expressions defined in terms of the\n+        placeholders of inputs.\n+\n+    Returns a dictionary whose keys are those of outputs, and whose values are\n+    PCollections corresponding to the values of outputs evaluated at the\n+    values of inputs.\n+\n+    Logically, `_apply_deferred_ops({x: a, y: b}, {f: F(x, y), g: G(x, y)})`\n+    returns `{f: F(a, b), g: G(a, b)}`.\n+    \"\"\"\n+    class ComputeStage(beam.PTransform):\n+      \"\"\"A helper transform that computes a single stage of operations.\n+      \"\"\"\n+      def __init__(self, stage):\n+        self.stage = stage\n+\n+      def default_label(self):\n+        return '%s:%s' % (self.stage.ops, id(self))\n+\n+      def expand(self, pcolls):\n+        if self.stage.is_grouping:\n+          # Arrange such that partitioned_pcoll is properly partitioned.\n+          input_pcolls = {\n+              k: pcoll | 'Flat%s' % k >> beam.FlatMap(partition_by_index)\n+              for k,\n+              pcoll in pcolls.items()\n+          }\n+          partitioned_pcoll = input_pcolls | beam.CoGroupByKey(\n+          ) | beam.MapTuple(\n+              lambda _, inputs: {k: pd.concat(vs)\n+                                 for k, vs in inputs.items()})\n+        else:\n+          # Already partitioned, or no partitioning needed.\n+          (k, pcoll), = pcolls.items()\n+          partitioned_pcoll = pcoll | beam.Map(lambda df: {k: df})\n+\n+        # Actually evaluate the expressions.\n+        def evaluate(partition, stage=self.stage):\n+          session = expressions.Session(\n+              {expr: partition[expr._id]\n+               for expr in stage.inputs})\n+          for expr in stage.outputs:\n+            yield beam.pvalue.TaggedOutput(expr._id, expr.evaluate_at(session))\n+\n+        return partitioned_pcoll | beam.FlatMap(evaluate).with_outputs()\n+\n+    class Stage(object):\n+      \"\"\"Used to build up a set of operations that can be fused together.\n+      \"\"\"\n+      def __init__(self, inputs, is_grouping):\n+        self.inputs = set(inputs)\n+        self.is_grouping = is_grouping or len(self.inputs) > 1\n+        self.ops = []\n+        self.outputs = set()\n+\n+    # First define some helper functions.\n+    def output_is_partitioned_by_index(expr, stage):\n+      if expr in stage.inputs:\n+        return stage.is_grouping\n+      elif expr.preserves_partition_by_index():\n+        if expr.requires_partition_by_index():\n+          return True\n+        else:\n+          return all(\n+              output_is_partitioned_by_index(arg, stage) for arg in expr.args())\n+      else:\n+        return False\n+\n+    def partition_by_index(df, levels=None, parts=10):\n+      if levels is None:\n+        levels = list(range(df.index.nlevels))\n+      elif isinstance(levels, (int, str)):\n+        levels = [levels]\n+      hashes = sum(\n+          pd.util.hash_array(df.index.get_level_values(level))\n+          for level in levels)\n+      for k in range(parts):\n+        yield k, df[hashes % parts == k]", "originalCommit": "2197ba169edf06d2b98da23d7d8f35cbfcdf41f1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b557c127c93e2daaabc8d79f6e15677003e5b01b", "url": "https://github.com/apache/beam/commit/b557c127c93e2daaabc8d79f6e15677003e5b01b", "message": "Comments and clarification.", "committedDate": "2020-04-10T23:50:24Z", "type": "commit"}, {"oid": "b557c127c93e2daaabc8d79f6e15677003e5b01b", "url": "https://github.com/apache/beam/commit/b557c127c93e2daaabc8d79f6e15677003e5b01b", "message": "Comments and clarification.", "committedDate": "2020-04-10T23:50:24Z", "type": "forcePushed"}]}