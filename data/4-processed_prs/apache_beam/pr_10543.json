{"pr_number": 10543, "pr_title": "[BEAM-8941] Implement simple DSL for load tests", "pr_createdAt": "2020-01-09T14:28:10Z", "pr_url": "https://github.com/apache/beam/pull/10543", "timeline": [{"oid": "29804b0888dd3940632d6895d176283dbda4cf7d", "url": "https://github.com/apache/beam/commit/29804b0888dd3940632d6895d176283dbda4cf7d", "message": "[BEAM-8941] Implement simple DSL for load tests", "committedDate": "2020-01-09T14:30:11Z", "type": "forcePushed"}, {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481", "url": "https://github.com/apache/beam/commit/57fd8672c46e4fcb496867da907d1ac4f2676481", "message": "[BEAM-8941] Implement simple DSL for load tests", "committedDate": "2020-01-10T07:30:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTAyNQ==", "url": "https://github.com/apache/beam/pull/10543#discussion_r365295025", "bodyText": "There is also one more important pipeline option which is consumed by every test (both in Java and Python): streaming. Let's set its default value to False, so we don't have to manually specify it in all batch tests.", "author": "kamilwu", "createdAt": "2020-01-10T15:40:55Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName              [String]\n+     *         project              [String]\n+     *         publishToBigQuery    [boolean]\n+     *         metricsDataset       [String]\n+     *         metricsTable         [String]\n+     *         iterations           [int]\n+     *         fanout               [int]\n+     *         numWorkers           [int]\n+     *         parallelism          [int]\n+     *         tempLocation         [String]\n+     *         autoscalingAlgorithm [String]\n+     *         jobEndpoint          [String]\n+     *         environmentType      [String]\n+     *         environmentConfig    [String]\n+     *         inputOptions {\n+     *             numRecords       [int]\n+     *             keySize          [int]\n+     *             valueSize        [int]\n+     *             numHotKeys       [int]\n+     *             hotKeyFraction   [int]\n+     *         }\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private String _jobName\n+        private String _project\n+        private String  _publishToBigQuery\n+        private String  _metricsDataset\n+        private String  _metricsTable\n+        private InputOptions  _inputOptions\n+        private def  _iterations\n+        private def  _fanout\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+\n+        //dataflow only\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink only\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void iterations(final int itNumber) { _iterations = itNumber }\n+        void fanout(final int fanout) { _fanout = fanout }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }", "originalCommit": "57fd8672c46e4fcb496867da907d1ac4f2676481", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTEyNw==", "url": "https://github.com/apache/beam/pull/10543#discussion_r365295127", "bodyText": "Not every load test requires iterations parameter. There are also tests which require totally different parameters. For example, combine test requires top_count parameter and this parameter is absent here.", "author": "kamilwu", "createdAt": "2020-01-10T15:41:06Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName              [String]\n+     *         project              [String]\n+     *         publishToBigQuery    [boolean]\n+     *         metricsDataset       [String]\n+     *         metricsTable         [String]\n+     *         iterations           [int]\n+     *         fanout               [int]\n+     *         numWorkers           [int]\n+     *         parallelism          [int]\n+     *         tempLocation         [String]\n+     *         autoscalingAlgorithm [String]\n+     *         jobEndpoint          [String]\n+     *         environmentType      [String]\n+     *         environmentConfig    [String]\n+     *         inputOptions {\n+     *             numRecords       [int]\n+     *             keySize          [int]\n+     *             valueSize        [int]\n+     *             numHotKeys       [int]\n+     *             hotKeyFraction   [int]\n+     *         }\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private String _jobName\n+        private String _project\n+        private String  _publishToBigQuery\n+        private String  _metricsDataset\n+        private String  _metricsTable\n+        private InputOptions  _inputOptions\n+        private def  _iterations", "originalCommit": "57fd8672c46e4fcb496867da907d1ac4f2676481", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxNTk3OA==", "url": "https://github.com/apache/beam/pull/10543#discussion_r365315978", "bodyText": "@kamilwu Thanks for input, I'll look into current scripts and search for other parameters. Or, maybe, do you happen to have list of all possible parameters?", "author": "pawelpasterz", "createdAt": "2020-01-10T16:22:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTc3NDI5NQ==", "url": "https://github.com/apache/beam/pull/10543#discussion_r365774295", "bodyText": "I'm afraid I don't have such a list.\nI wonder whether we should explicitly specify all possible parameters. Every time we add a new parameter to one of the tests, we must also remember about this place.\nWhat about having a fixed list of parameters (publish_to_big_query, metrics_dataset, metrics_table, input_options, runner's parameters, etc.) that every test require and accepting all the other parameters without validation?", "author": "kamilwu", "createdAt": "2020-01-13T12:18:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTc5MTg0OA==", "url": "https://github.com/apache/beam/pull/10543#discussion_r365791848", "bodyText": "Will do, thanks !", "author": "pawelpasterz", "createdAt": "2020-01-13T13:03:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTEyNw=="}], "type": "inlineReview"}, {"oid": "58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "url": "https://github.com/apache/beam/commit/58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "message": "[BEAM-8941] Implement simple DSL for load tests", "committedDate": "2020-01-13T05:47:28Z", "type": "forcePushed"}, {"oid": "2561b48e203579e4e3f58281d327555d50d065b0", "url": "https://github.com/apache/beam/commit/2561b48e203579e4e3f58281d327555d50d065b0", "message": "[BEAM-8941] Implement simple DSL for load tests", "committedDate": "2020-01-13T12:34:23Z", "type": "forcePushed"}, {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "url": "https://github.com/apache/beam/commit/1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs", "committedDate": "2020-01-16T12:27:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNTQ1Mw==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368425453", "bodyText": "Do you have any load tests in Spark or Flink (non-portable)? I am asking because those two options can be added later. My main concern is here if we are sure if someone will actually run test with Spark/Flink option if it won't fail.", "author": "kkucharc", "createdAt": "2020-01-20T08:49:43Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }", "originalCommit": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ3OTM0OA==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368479348", "bodyText": "TBH I looked at CommonTestProperties class and picked Runner values. Since I am not that much familiar with code yet, my implementation (the one above) can be wrong. Should I limit it to dataflow and portable? (at least for the time being)", "author": "pawelpasterz", "createdAt": "2020-01-20T10:41:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNTQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNzQ1Ng==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368427456", "bodyText": "I am wondering if test if self-explanatory enough. My first guess was this is filename (or path) to test. But here I am not sure and how it's different from title. If it is impossible to change it I would document it better.", "author": "kkucharc", "createdAt": "2020-01-20T08:54:22Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'", "originalCommit": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ2NzE2MA==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368467160", "bodyText": "Good point, I need to change documentation, thanks!", "author": "pawelpasterz", "createdAt": "2020-01-20T10:16:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNzQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQzOTg5Mg==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368439892", "bodyText": "What do you think about changing it into notation \"'${...}'\"?", "author": "kkucharc", "createdAt": "2020-01-20T09:21:45Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName                  [String]\n+     *         project                  [String]\n+     *         publishToBigQuery        [boolean]\n+     *         metricsDataset (python)  [String]\n+     *         metricsTable (python)    [String]\n+     *         bigQueryDataset (java)   [String]\n+     *         bigQueryTable (java)     [String]\n+     *         numWorkers               [int]\n+     *         parallelism              [int]\n+     *         tempLocation             [String]\n+     *         autoscalingAlgorithm     [String]\n+     *         jobEndpoint              [String]\n+     *         environmentType          [String]\n+     *         environmentConfig        [String]\n+     *         inputOptions/coInputOptions (for python) {\n+     *             numRecords           [int]\n+     *             keySize              [int]\n+     *             valueSize            [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *         }\n+     *         sourceOptions/coSourceOptions (for java) {\n+     *             numRecords           [int]\n+     *             keySizeBytes         [int]\n+     *             valueSizeBytes       [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *             splitPointFrequencyRecords       [int]\n+     *         }\n+     *         stepOptions {\n+     *             outputRecordsPerInputRecord      [int]\n+     *             preservesInputKeyDistribution    [boolean]\n+     *         }\n+     *         specificParameters       [Map<String, Object>]\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private Map<String, Object> _specificParameters = new HashMap<>()\n+        private boolean _streaming = false\n+        private SourceOptions _coSourceOptions\n+        private InputOptions _coInputOptions\n+        private StepOptions _stepOptions\n+\n+        //required\n+        private String _jobName\n+        private String _project\n+        private String _publishToBigQuery\n+\n+        //java required\n+        private String _bigQueryDataset\n+        private String _bigQueryTable\n+        private SourceOptions _sourceOptions\n+\n+        //python required\n+        private String _metricsDataset\n+        private String _metricsTable\n+        private InputOptions _inputOptions\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_required = [\"_jobName\", \"_project\", \"_publishToBigQuery\"]\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+        private static final i_javaRequired = [\"_bigQueryDataset\", \"_bigQueryTable\", \"_sourceOptions\"]\n+        private static final i_pythonRequired = [\"_metricsDataset\", \"_metricsTable\", \"_inputOptions\"]\n+\n+        //dataflow required\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink required\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }\n+        void bigQueryDataset(final String dataset) { _bigQueryDataset = dataset }\n+        void bigQueryTable(final String table) { _bigQueryTable = table }\n+        void streaming(final boolean isStreaming) { _streaming = isStreaming }\n+        void sourceOptions(final Closure cl = {}) { _sourceOptions = makeSourceOptions {cl} }\n+        void coSourceOptions(final Closure cl = {}) { _coSourceOptions = makeSourceOptions(cl) }\n+        void inputOptions(final Closure cl = {}) { _inputOptions = makeInputOptions(cl) }\n+        void coInputOptions(final Closure cl = {}) { _coInputOptions = makeInputOptions(cl) }\n+        void stepOptions(final Closure cl = {}) { _stepOptions = makeStepOptions(cl) }\n+        void specificParameters(final Map<String, Object> map) { _specificParameters.putAll(map) }\n+\n+        //sdk -- snake_case vs camelCase\n+        void python() { i_sdk = SDK.PYTHON }\n+        void python37() { i_sdk = SDK.PYTHON_37 }\n+        void java() { i_sdk = SDK.JAVA }\n+\n+\n+        private InputOptions makeInputOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _inputOptions ?: InputOptions.withSDK(i_sdk))\n+        }\n+\n+        private SourceOptions makeSourceOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _sourceOptions ?: SourceOptions.withSDK(i_sdk))\n+        }\n+\n+        private StepOptions makeStepOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _stepOptions ?: StepOptions.withSDK(i_sdk))\n+        }\n+\n+        private <T> T makeOptions(final Closure cl = {}, final T options) {\n+            final def code = cl.rehydrate(options, this, this)\n+            code.resolveStrategy = Closure.DELEGATE_ONLY\n+            code()\n+            return options\n+        }\n+\n+        @Override\n+        Map<String, Serializable> toPrimitiveValues() {\n+            final def map = propertiesMap\n+            verifyPipelineProperties(map)\n+            return ConfigHelper.convertProperties(map, i_sdk)\n+        }\n+\n+        private void verifyPipelineProperties(final Map<String, Object> map) {\n+            verifyRequired(map)\n+            switch (i_runner) {\n+                case Runner.DATAFLOW:\n+                    verifyDataflowProperties(map)\n+                    break\n+                case Runner.FLINK:\n+                    verifyFlinkProperties(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private void verifyRequired(final Map<String, Object> map) {\n+            verifyCommonRequired(map)\n+            switch (i_sdk) {\n+                case SDK.PYTHON:\n+                case SDK.PYTHON_37:\n+                    verifyPythonRequired(map)\n+                    break\n+                case SDK.JAVA:\n+                    verifyJavaRequired(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private static void verifyCommonRequired(final Map<String, Object> map) {\n+            verify(map, \"\") { i_required.contains(it.key) }\n+        }\n+\n+        private static void verifyPythonRequired(final Map<String, Object> map) {\n+            verify(map, \"for Python SDK\") { i_pythonRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyJavaRequired(final Map<String, Object> map) {\n+            verify(map, \"for Java SDK\") { i_javaRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyDataflowProperties(final Map<String, Object> map) {\n+            verify(map, \"for Dataflow runner\") { i_dataflowRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyFlinkProperties(final Map<String, Object> map) {\n+            verify(map, \"for Flink runner\") { i_flinkRequired.contains(it.key) }\n+        }\n+\n+        private static void verify(final Map<String, Object> map, final String message, final Predicate<Map.Entry<String, Object>> predicate) {\n+            map.entrySet()\n+                    .stream()\n+                    .filter(predicate)\n+                    .forEach{ requireNonNull(it.value, \"${it.key.substring(1)} is required \" + message) }\n+        }\n+\n+        static PipelineOptions of(final PipelineOptions options) {\n+            final def newOptions = new PipelineOptions()\n+\n+            //primitive values\n+            InvokerHelper.setProperties(newOptions, options.propertiesMap)\n+\n+            //non-primitive\n+            newOptions._inputOptions = options._inputOptions ? InputOptions.of(options._inputOptions) : null\n+            newOptions._coInputOptions = options._coInputOptions ? InputOptions.of(options._coInputOptions) : null\n+            newOptions._sourceOptions = options._sourceOptions ? SourceOptions.of(options._sourceOptions) : null\n+            newOptions._coSourceOptions = options._coSourceOptions ? SourceOptions.of(options._coSourceOptions) : null\n+            newOptions._stepOptions = options._stepOptions ? StepOptions.of(options._stepOptions) : null\n+            newOptions._specificParameters = new HashMap<>(options._specificParameters)\n+\n+            return newOptions\n+        }\n+\n+        Map<String, Object> getPropertiesMap() {\n+            return [\n+                    i_sdk: i_sdk,\n+                    i_runner: i_runner,\n+                    _jobName: _jobName,\n+                    _project: _project,\n+                    _tempLocation: _tempLocation,\n+                    _publishToBigQuery: _publishToBigQuery,\n+                    _metricsDataset: _metricsDataset,\n+                    _metricsTable: _metricsTable,\n+                    _numWorkers: _numWorkers,\n+                    _autoscalingAlgorithm: _autoscalingAlgorithm,\n+                    _inputOptions: _inputOptions,\n+                    _coInputOptions: _coInputOptions,\n+                    _jobEndpoint: _jobEndpoint,\n+                    _environmentType: _environmentType,\n+                    _environmentConfig: _environmentConfig,\n+                    _parallelism: _parallelism,\n+                    _bigQueryDataset: _bigQueryDataset,\n+                    _bigQueryTable: _bigQueryTable,\n+                    _streaming: _streaming,\n+                    _sourceOptions: _sourceOptions,\n+                    _coSourceOptions: _coSourceOptions,\n+                    _stepOptions: _stepOptions\n+            ].putAll(_specificParameters.entrySet())\n+        }\n+\n+        private static class InputOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySize\n+            private def _valueSize\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private InputOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new InputOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySize(final int size) { _keySize = size }\n+            void valueSize(final int size) { _valueSize = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static InputOptions of(final InputOptions oldOptions) {\n+                final def newOptions = new InputOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            LinkedHashMap<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySize: _keySize,\n+                        _valueSize: _valueSize,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction\n+                ] as LinkedHashMap<String, Object>\n+            }\n+        }\n+\n+        private static class SourceOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySizeBytes\n+            private def _valueSizeBytes\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+            private def _splitPointFrequencyRecords\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private SourceOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new SourceOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySizeBytes(final int size) { _keySizeBytes = size }\n+            void valueSizeBytes(final int size) { _valueSizeBytes = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+            void splitPointFrequencyRecords(final int splitPoint) { _splitPointFrequencyRecords = splitPoint }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static SourceOptions of(final SourceOptions oldOptions) {\n+                final def newOptions = new SourceOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            Map<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySizeBytes: _keySizeBytes,\n+                        _valueSizeBytes: _valueSizeBytes,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction,\n+                        _splitPointFrequencyRecords: _splitPointFrequencyRecords\n+                ]\n+            }\n+        }\n+\n+        private static class StepOptions implements SerializableOption<String> {\n+            private def _outputRecordsPerInputRecord\n+            private boolean  _preservesInputKeyDistribution\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private StepOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def option = new StepOptions()\n+                option.i_sdk = sdk\n+                return option\n+            }\n+\n+            void outputRecordsPerInputRecord(final int records) { _outputRecordsPerInputRecord = records }\n+            void preservesInputKeyDistribution(final boolean  shouldPreserve) { _preservesInputKeyDistribution = shouldPreserve }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''", "originalCommit": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ2NzQ4NA==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368467484", "bodyText": "Thanks, will change it!", "author": "pawelpasterz", "createdAt": "2020-01-20T10:17:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQzOTg5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ0NDYxOQ==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368444619", "bodyText": "What do you think about changing it into notation \"'${...}'\"?", "author": "kkucharc", "createdAt": "2020-01-20T09:31:10Z", "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName                  [String]\n+     *         project                  [String]\n+     *         publishToBigQuery        [boolean]\n+     *         metricsDataset (python)  [String]\n+     *         metricsTable (python)    [String]\n+     *         bigQueryDataset (java)   [String]\n+     *         bigQueryTable (java)     [String]\n+     *         numWorkers               [int]\n+     *         parallelism              [int]\n+     *         tempLocation             [String]\n+     *         autoscalingAlgorithm     [String]\n+     *         jobEndpoint              [String]\n+     *         environmentType          [String]\n+     *         environmentConfig        [String]\n+     *         inputOptions/coInputOptions (for python) {\n+     *             numRecords           [int]\n+     *             keySize              [int]\n+     *             valueSize            [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *         }\n+     *         sourceOptions/coSourceOptions (for java) {\n+     *             numRecords           [int]\n+     *             keySizeBytes         [int]\n+     *             valueSizeBytes       [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *             splitPointFrequencyRecords       [int]\n+     *         }\n+     *         stepOptions {\n+     *             outputRecordsPerInputRecord      [int]\n+     *             preservesInputKeyDistribution    [boolean]\n+     *         }\n+     *         specificParameters       [Map<String, Object>]\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private Map<String, Object> _specificParameters = new HashMap<>()\n+        private boolean _streaming = false\n+        private SourceOptions _coSourceOptions\n+        private InputOptions _coInputOptions\n+        private StepOptions _stepOptions\n+\n+        //required\n+        private String _jobName\n+        private String _project\n+        private String _publishToBigQuery\n+\n+        //java required\n+        private String _bigQueryDataset\n+        private String _bigQueryTable\n+        private SourceOptions _sourceOptions\n+\n+        //python required\n+        private String _metricsDataset\n+        private String _metricsTable\n+        private InputOptions _inputOptions\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_required = [\"_jobName\", \"_project\", \"_publishToBigQuery\"]\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+        private static final i_javaRequired = [\"_bigQueryDataset\", \"_bigQueryTable\", \"_sourceOptions\"]\n+        private static final i_pythonRequired = [\"_metricsDataset\", \"_metricsTable\", \"_inputOptions\"]\n+\n+        //dataflow required\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink required\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }\n+        void bigQueryDataset(final String dataset) { _bigQueryDataset = dataset }\n+        void bigQueryTable(final String table) { _bigQueryTable = table }\n+        void streaming(final boolean isStreaming) { _streaming = isStreaming }\n+        void sourceOptions(final Closure cl = {}) { _sourceOptions = makeSourceOptions {cl} }\n+        void coSourceOptions(final Closure cl = {}) { _coSourceOptions = makeSourceOptions(cl) }\n+        void inputOptions(final Closure cl = {}) { _inputOptions = makeInputOptions(cl) }\n+        void coInputOptions(final Closure cl = {}) { _coInputOptions = makeInputOptions(cl) }\n+        void stepOptions(final Closure cl = {}) { _stepOptions = makeStepOptions(cl) }\n+        void specificParameters(final Map<String, Object> map) { _specificParameters.putAll(map) }\n+\n+        //sdk -- snake_case vs camelCase\n+        void python() { i_sdk = SDK.PYTHON }\n+        void python37() { i_sdk = SDK.PYTHON_37 }\n+        void java() { i_sdk = SDK.JAVA }\n+\n+\n+        private InputOptions makeInputOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _inputOptions ?: InputOptions.withSDK(i_sdk))\n+        }\n+\n+        private SourceOptions makeSourceOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _sourceOptions ?: SourceOptions.withSDK(i_sdk))\n+        }\n+\n+        private StepOptions makeStepOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _stepOptions ?: StepOptions.withSDK(i_sdk))\n+        }\n+\n+        private <T> T makeOptions(final Closure cl = {}, final T options) {\n+            final def code = cl.rehydrate(options, this, this)\n+            code.resolveStrategy = Closure.DELEGATE_ONLY\n+            code()\n+            return options\n+        }\n+\n+        @Override\n+        Map<String, Serializable> toPrimitiveValues() {\n+            final def map = propertiesMap\n+            verifyPipelineProperties(map)\n+            return ConfigHelper.convertProperties(map, i_sdk)\n+        }\n+\n+        private void verifyPipelineProperties(final Map<String, Object> map) {\n+            verifyRequired(map)\n+            switch (i_runner) {\n+                case Runner.DATAFLOW:\n+                    verifyDataflowProperties(map)\n+                    break\n+                case Runner.FLINK:\n+                    verifyFlinkProperties(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private void verifyRequired(final Map<String, Object> map) {\n+            verifyCommonRequired(map)\n+            switch (i_sdk) {\n+                case SDK.PYTHON:\n+                case SDK.PYTHON_37:\n+                    verifyPythonRequired(map)\n+                    break\n+                case SDK.JAVA:\n+                    verifyJavaRequired(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private static void verifyCommonRequired(final Map<String, Object> map) {\n+            verify(map, \"\") { i_required.contains(it.key) }\n+        }\n+\n+        private static void verifyPythonRequired(final Map<String, Object> map) {\n+            verify(map, \"for Python SDK\") { i_pythonRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyJavaRequired(final Map<String, Object> map) {\n+            verify(map, \"for Java SDK\") { i_javaRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyDataflowProperties(final Map<String, Object> map) {\n+            verify(map, \"for Dataflow runner\") { i_dataflowRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyFlinkProperties(final Map<String, Object> map) {\n+            verify(map, \"for Flink runner\") { i_flinkRequired.contains(it.key) }\n+        }\n+\n+        private static void verify(final Map<String, Object> map, final String message, final Predicate<Map.Entry<String, Object>> predicate) {\n+            map.entrySet()\n+                    .stream()\n+                    .filter(predicate)\n+                    .forEach{ requireNonNull(it.value, \"${it.key.substring(1)} is required \" + message) }\n+        }\n+\n+        static PipelineOptions of(final PipelineOptions options) {\n+            final def newOptions = new PipelineOptions()\n+\n+            //primitive values\n+            InvokerHelper.setProperties(newOptions, options.propertiesMap)\n+\n+            //non-primitive\n+            newOptions._inputOptions = options._inputOptions ? InputOptions.of(options._inputOptions) : null\n+            newOptions._coInputOptions = options._coInputOptions ? InputOptions.of(options._coInputOptions) : null\n+            newOptions._sourceOptions = options._sourceOptions ? SourceOptions.of(options._sourceOptions) : null\n+            newOptions._coSourceOptions = options._coSourceOptions ? SourceOptions.of(options._coSourceOptions) : null\n+            newOptions._stepOptions = options._stepOptions ? StepOptions.of(options._stepOptions) : null\n+            newOptions._specificParameters = new HashMap<>(options._specificParameters)\n+\n+            return newOptions\n+        }\n+\n+        Map<String, Object> getPropertiesMap() {\n+            return [\n+                    i_sdk: i_sdk,\n+                    i_runner: i_runner,\n+                    _jobName: _jobName,\n+                    _project: _project,\n+                    _tempLocation: _tempLocation,\n+                    _publishToBigQuery: _publishToBigQuery,\n+                    _metricsDataset: _metricsDataset,\n+                    _metricsTable: _metricsTable,\n+                    _numWorkers: _numWorkers,\n+                    _autoscalingAlgorithm: _autoscalingAlgorithm,\n+                    _inputOptions: _inputOptions,\n+                    _coInputOptions: _coInputOptions,\n+                    _jobEndpoint: _jobEndpoint,\n+                    _environmentType: _environmentType,\n+                    _environmentConfig: _environmentConfig,\n+                    _parallelism: _parallelism,\n+                    _bigQueryDataset: _bigQueryDataset,\n+                    _bigQueryTable: _bigQueryTable,\n+                    _streaming: _streaming,\n+                    _sourceOptions: _sourceOptions,\n+                    _coSourceOptions: _coSourceOptions,\n+                    _stepOptions: _stepOptions\n+            ].putAll(_specificParameters.entrySet())\n+        }\n+\n+        private static class InputOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySize\n+            private def _valueSize\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private InputOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new InputOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySize(final int size) { _keySize = size }\n+            void valueSize(final int size) { _valueSize = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static InputOptions of(final InputOptions oldOptions) {\n+                final def newOptions = new InputOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            LinkedHashMap<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySize: _keySize,\n+                        _valueSize: _valueSize,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction\n+                ] as LinkedHashMap<String, Object>\n+            }\n+        }\n+\n+        private static class SourceOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySizeBytes\n+            private def _valueSizeBytes\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+            private def _splitPointFrequencyRecords\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private SourceOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new SourceOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySizeBytes(final int size) { _keySizeBytes = size }\n+            void valueSizeBytes(final int size) { _valueSizeBytes = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+            void splitPointFrequencyRecords(final int splitPoint) { _splitPointFrequencyRecords = splitPoint }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''", "originalCommit": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ2NzUzOQ==", "url": "https://github.com/apache/beam/pull/10543#discussion_r368467539", "bodyText": "Thanks, will change it!", "author": "pawelpasterz", "createdAt": "2020-01-20T10:17:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ0NDYxOQ=="}], "type": "inlineReview"}, {"oid": "8a5bee2635ca00d200de05b3c86a36de93e74d9c", "url": "https://github.com/apache/beam/commit/8a5bee2635ca00d200de05b3c86a36de93e74d9c", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs", "committedDate": "2020-01-22T13:20:43Z", "type": "forcePushed"}, {"oid": "56644b169c0d5158aa5820694e7916f6a742f7e7", "url": "https://github.com/apache/beam/commit/56644b169c0d5158aa5820694e7916f6a742f7e7", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs", "committedDate": "2020-01-27T08:42:42Z", "type": "forcePushed"}, {"oid": "801a6bb5e4554af43b29dee00a1b7175c326b6be", "url": "https://github.com/apache/beam/commit/801a6bb5e4554af43b29dee00a1b7175c326b6be", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs", "committedDate": "2020-01-27T12:50:48Z", "type": "commit"}, {"oid": "801a6bb5e4554af43b29dee00a1b7175c326b6be", "url": "https://github.com/apache/beam/commit/801a6bb5e4554af43b29dee00a1b7175c326b6be", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs", "committedDate": "2020-01-27T12:50:48Z", "type": "forcePushed"}]}