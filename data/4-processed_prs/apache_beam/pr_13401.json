{"pr_number": 13401, "pr_title": "[BEAM-11324] Add additional verification in PartitioningSession", "pr_createdAt": "2020-11-21T00:39:36Z", "pr_url": "https://github.com/apache/beam/pull/13401", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0MDAxMA==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528040010", "bodyText": "Note this only stores the most recently computed result. We may want to also verify that each input partitioning yields equivalent results (modulo ordering).", "author": "TheNeuralBit", "createdAt": "2020-11-21T00:41:22Z", "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)", "originalCommit": "ac1e3a3705ecd00bfdafea8bda207c419bc42633", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Nzk2NQ==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528047965", "bodyText": "Well, it could be something different...", "author": "robertwb", "createdAt": "2020-11-21T01:25:11Z", "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)\n+\n+          if input_partitioning == partitionings.Nothing():\n+            input_partitioning = partitionings.Index()\n+          elif isinstance(input_partitioning, partitionings.Index):\n+            input_partitioning = partitionings.Singleton()\n+          else:  # partitionings.Singleton()", "originalCommit": "79a08776618b8e1474862479fe6fd83f381758dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODExMQ==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048111", "bodyText": "This loop isn't obvious to follow. Perhaps iterate over set([input_partitioning, Nothing(), Index(), Singleton()]) and continue in the cases where it's not a subpartition of index_partitioning.", "author": "robertwb", "createdAt": "2020-11-21T01:26:12Z", "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)\n+\n+          if input_partitioning == partitionings.Nothing():", "originalCommit": "79a08776618b8e1474862479fe6fd83f381758dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI3Mg==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048272", "bodyText": "Did yapf suggest this?", "author": "robertwb", "createdAt": "2020-11-21T01:27:13Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -1401,17 +1403,20 @@ def replace(self, limit, **kwargs):\n   def reset_index(self, level=None, **kwargs):\n     if level is not None and not isinstance(level, (tuple, list)):\n       level = [level]\n+\n     if level is None or len(level) == self._expr.proxy().index.nlevels:\n       # TODO: Could do distributed re-index with offsets.\n       requires_partition_by = partitionings.Singleton()\n     else:\n       requires_partition_by = partitionings.Nothing()\n+\n+", "originalCommit": "79a08776618b8e1474862479fe6fd83f381758dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNjY3OQ==", "url": "https://github.com/apache/beam/pull/13401#discussion_r529106679", "bodyText": "I think this was me, removed it", "author": "TheNeuralBit", "createdAt": "2020-11-24T01:26:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048447", "bodyText": "I wonder how expensive this will get. Hopefully not too bad. It could, however, mess up anything that depends on the random seed.", "author": "robertwb", "createdAt": "2020-11-21T01:28:35Z", "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -48,10 +48,15 @@ def lookup(self, expr):  #  type: (Expression) -> Any\n class PartitioningSession(Session):\n   \"\"\"An extension of Session that enforces actual partitioning of inputs.\n \n-  When evaluating an expression, inputs are partitioned according to its\n-  `requires_partition_by` specifications, the expression is evaluated on each\n-  partition separately, and the final result concatinated, as if this were\n-  actually executed in a parallel manner.\n+  Each expression is evaluated multiple times for various supported", "originalCommit": "79a08776618b8e1474862479fe6fd83f381758dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyMzc1Nw==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528923757", "bodyText": "Yeah I think the performance impact is worth it. We could probably reduce it by being a little more discerning in what gets re-executed. For example I don't think there's any value in doing this for expressions that have preserves=Nothing().\nReally good point about the random seed. I could bracket the runs with calls to random.getstate() and random.setstate()", "author": "TheNeuralBit", "createdAt": "2020-11-23T18:50:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTA4MjY4OQ==", "url": "https://github.com/apache/beam/pull/13401#discussion_r529082689", "bodyText": "Let's do that.", "author": "robertwb", "createdAt": "2020-11-24T00:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNjA3Nw==", "url": "https://github.com/apache/beam/pull/13401#discussion_r529106077", "bodyText": "Done", "author": "TheNeuralBit", "createdAt": "2020-11-24T01:26:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0OTMxMA==", "url": "https://github.com/apache/beam/pull/13401#discussion_r528049310", "bodyText": "This isn't a very strong check if the dataframes are fairly sparse (as they are in most examples). We could try concat + repartition and verify the results are the same.", "author": "robertwb", "createdAt": "2020-11-21T01:34:25Z", "path": "sdks/python/apache_beam/dataframe/partitionings.py", "diffHunk": "@@ -115,6 +115,23 @@ def partition_fn(self, df, num_partitions):\n     for key in range(num_partitions):\n       yield key, df[hashes % num_partitions == key]\n \n+  def check(self, dfs):", "originalCommit": "79a08776618b8e1474862479fe6fd83f381758dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNTY1NA==", "url": "https://github.com/apache/beam/pull/13401#discussion_r529105654", "bodyText": "Added a TODO to do this in a later PR", "author": "TheNeuralBit", "createdAt": "2020-11-24T01:25:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0OTMxMA=="}], "type": "inlineReview"}, {"oid": "9af3c3214e0abf8e11f4930b6223252d8b574600", "url": "https://github.com/apache/beam/commit/9af3c3214e0abf8e11f4930b6223252d8b574600", "message": "Default to distributed=True in frames_test.py:", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "d0ae355dacb9bd515e626d20a3a20ce91f9f562b", "url": "https://github.com/apache/beam/commit/d0ae355dacb9bd515e626d20a3a20ce91f9f562b", "message": "Add additional partitioning verification to PartitioningSession", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "8fabffc24af8143f9cff17bc72f1c330011150ee", "url": "https://github.com/apache/beam/commit/8fabffc24af8143f9cff17bc72f1c330011150ee", "message": "Series pre-agg doesn't preserve partitioning", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "fe213240ad70ae1bc82e6ccfe1203807c714e01d", "url": "https://github.com/apache/beam/commit/fe213240ad70ae1bc82e6ccfe1203807c714e01d", "message": "reset_index doesn't preserve partitioning", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "896c1143ee95abaab310302527e9909b450031b9", "url": "https://github.com/apache/beam/commit/896c1143ee95abaab310302527e9909b450031b9", "message": "remove whitespace", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "b0f3fc2d9a0bae457d00aabf88eb85ff2e598399", "url": "https://github.com/apache/beam/commit/b0f3fc2d9a0bae457d00aabf88eb85ff2e598399", "message": "store and recover random state, clarify loop", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "f1a46c2b722f4808772488cd8a9ec1a01efffd49", "url": "https://github.com/apache/beam/commit/f1a46c2b722f4808772488cd8a9ec1a01efffd49", "message": "add TODO", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "f672f3c2708d79880d4a89f4ee87500121224024", "url": "https://github.com/apache/beam/commit/f672f3c2708d79880d4a89f4ee87500121224024", "message": "map_index doesn't preserve partitioning", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "d2d8cf9a0bdeba75cdc4de520ddfb2d44967026f", "url": "https://github.com/apache/beam/commit/d2d8cf9a0bdeba75cdc4de520ddfb2d44967026f", "message": "lint", "committedDate": "2020-11-24T02:32:25Z", "type": "commit"}, {"oid": "9dfbadea3ab745a3fabf4ab0f355ed4e361e8d86", "url": "https://github.com/apache/beam/commit/9dfbadea3ab745a3fabf4ab0f355ed4e361e8d86", "message": "lint", "committedDate": "2020-11-24T02:34:41Z", "type": "commit"}, {"oid": "9dfbadea3ab745a3fabf4ab0f355ed4e361e8d86", "url": "https://github.com/apache/beam/commit/9dfbadea3ab745a3fabf4ab0f355ed4e361e8d86", "message": "lint", "committedDate": "2020-11-24T02:34:41Z", "type": "forcePushed"}, {"oid": "3c882faae889b88daffc77e496637b171bb8021b", "url": "https://github.com/apache/beam/commit/3c882faae889b88daffc77e496637b171bb8021b", "message": "lint", "committedDate": "2020-11-24T02:55:49Z", "type": "commit"}, {"oid": "5dcdc9c5432dc8123415a5f761b26f775ef84af5", "url": "https://github.com/apache/beam/commit/5dcdc9c5432dc8123415a5f761b26f775ef84af5", "message": "Disallow grouping by a series", "committedDate": "2020-11-24T18:31:25Z", "type": "commit"}, {"oid": "c73224e04940c5d2216942a17dadede7c1238592", "url": "https://github.com/apache/beam/commit/c73224e04940c5d2216942a17dadede7c1238592", "message": "lint", "committedDate": "2020-11-24T18:33:24Z", "type": "commit"}, {"oid": "0b917fd532d156c90bb50d876f2abc5009544247", "url": "https://github.com/apache/beam/commit/0b917fd532d156c90bb50d876f2abc5009544247", "message": "lint", "committedDate": "2020-11-24T18:47:18Z", "type": "commit"}, {"oid": "c86fca6c8f9bb296d1d0652ea0012c48113ed4fd", "url": "https://github.com/apache/beam/commit/c86fca6c8f9bb296d1d0652ea0012c48113ed4fd", "message": "import order", "committedDate": "2020-11-24T19:00:41Z", "type": "commit"}]}