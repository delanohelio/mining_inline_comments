{"pr_number": 13443, "pr_title": "[BEAM-11361] Dynamic splitting of SDF IOs.", "pr_createdAt": "2020-11-30T18:55:39Z", "pr_url": "https://github.com/apache/beam/pull/13443", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5NDE1NA==", "url": "https://github.com/apache/beam/pull/13443#discussion_r532994154", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              exactly 10 characters long (i.e. every 10th charcter was a newline), than this\n          \n          \n            \n              exactly 10 characters long (i.e. every 10th charcter was a newline), then this", "author": "boyuanzz", "createdAt": "2020-12-01T00:32:11Z", "path": "sdks/python/apache_beam/dataframe/io.py", "diffHunk": "@@ -181,39 +193,153 @@ def expand(self, root):\n                 self.args,\n                 self.kwargs,\n                 self.incremental,\n+                self.splittable,\n                 self.binary)))\n     from apache_beam.dataframe import convert\n     return convert.to_dataframe(\n         pcoll, proxy=_prefix_range_index_with(':', sample[:0]))\n \n \n-# TODO(robertwb): Actually make an SDF.\n-class _ReadFromPandasDoFn(beam.DoFn):\n-  def __init__(self, reader, args, kwargs, incremental, binary):\n+class _TruncatingFileHandle(object):\n+  \"\"\"A wrapper of a file-like object representing the restriction of the\n+  underling handle according to the given SDF restriction tracker, breaking\n+  the file only after the given delimiter.\n+\n+  For example, if the underling restriction is [103, 607) and each line were\n+  exactly 10 characters long (i.e. every 10th charcter was a newline), than this", "originalCommit": "adc4c01615fe8ba892bfe181298f2ca86333ecc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDM4MjM2MQ==", "url": "https://github.com/apache/beam/pull/13443#discussion_r534382361", "bodyText": "Done.", "author": "robertwb", "createdAt": "2020-12-02T18:17:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk5NDE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc3MzYxNw==", "url": "https://github.com/apache/beam/pull/13443#discussion_r533773617", "bodyText": "I want to make sure I understand how incremental and splittable work together:\n\nIf incremental == True and splittable == True, the reading is both splittable and we will have progress on that.\nIf incremental == True and splittable == False, the reading is not splittable but we will have progress on that.\nIf incremental ==False and splittable ==True, the reading is not splittable and no progress on it.\nIf incremental ==False and splittable == False, the reading is not splittable and no progress on it.\n\nDoes it correct?", "author": "boyuanzz", "createdAt": "2020-12-01T22:47:39Z", "path": "sdks/python/apache_beam/dataframe/io.py", "diffHunk": "@@ -181,39 +193,153 @@ def expand(self, root):\n                 self.args,\n                 self.kwargs,\n                 self.incremental,\n+                self.splittable,\n                 self.binary)))\n     from apache_beam.dataframe import convert\n     return convert.to_dataframe(\n         pcoll, proxy=_prefix_range_index_with(':', sample[:0]))\n \n \n-# TODO(robertwb): Actually make an SDF.\n-class _ReadFromPandasDoFn(beam.DoFn):\n-  def __init__(self, reader, args, kwargs, incremental, binary):\n+class _TruncatingFileHandle(object):\n+  \"\"\"A wrapper of a file-like object representing the restriction of the\n+  underling handle according to the given SDF restriction tracker, breaking\n+  the file only after the given delimiter.\n+\n+  For example, if the underling restriction is [103, 607) and each line were\n+  exactly 10 characters long (i.e. every 10th charcter was a newline), than this\n+  would give a view of a 500-byte file consisting of bytes bytes 110 to 609\n+  (inclusive) of the underlying file.\n+\n+  As with all SDF trackers, the endpoint may change dynamically during reading.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      underlying,\n+      tracker,\n+      delim=b'\\n',\n+      chunk_size=_DEFAULT_BYTES_CHUNKSIZE):\n+    self._underlying = underlying\n+    self._tracker = tracker\n+    self._buffer_start_pos = self._tracker.current_restriction().start\n+    self._delim = delim\n+    self._chunk_size = chunk_size\n+\n+    self._buffer = self._empty = self._delim[:0]\n+    self._done = False\n+    if self._buffer_start_pos > 0:\n+      # Seek to first delimiter after the start position.\n+      self._underlying.seek(self._buffer_start_pos)\n+      if self.buffer_to_delim():\n+        line_start = self._buffer.index(self._delim) + len(self._delim)\n+        self._buffer_start_pos += line_start\n+        self._buffer = self._buffer[line_start:]\n+      else:\n+        self._done = True\n+\n+  def readable(self):\n+    return True\n+\n+  def writable(self):\n+    return False\n+\n+  def seekable(self):\n+    return False\n+\n+  @property\n+  def closed(self):\n+    return False\n+\n+  def __iter__(self):\n+    # For pandas is_file_like.\n+    raise NotImplementedError()\n+\n+  def buffer_to_delim(self, offset=0):\n+    \"\"\"Read enough of the file such that the buffer contains the delimiter, or\n+    end-of-file is reached.\n+    \"\"\"\n+    if self._delim in self._buffer[offset:]:\n+      return True\n+    while True:\n+      chunk = self._underlying.read(self._chunk_size)\n+      self._buffer += chunk\n+      if self._delim in chunk:\n+        return True\n+      elif not chunk:\n+        return False\n+\n+  def read(self, size=-1):\n+    if self._done:\n+      return self._empty\n+    elif size == -1:\n+      self._buffer += self._underlying.read()\n+    elif not self._buffer:\n+      self._buffer = self._underlying.read(size)\n+\n+    if self._tracker.try_claim(self._buffer_start_pos + len(self._buffer)):\n+      res = self._buffer\n+      self._buffer = self._empty\n+      self._buffer_start_pos += len(res)\n+    else:\n+      offset = self._tracker.current_restriction().stop - self._buffer_start_pos\n+      if self.buffer_to_delim(offset):\n+        end_of_line = self._buffer.index(self._delim, offset)\n+        res = self._buffer[:end_of_line + len(self._delim)]\n+      else:\n+        res = self._buffer\n+      self._done = True\n+    return res\n+\n+\n+class _ReadFromPandasDoFn(beam.DoFn, beam.RestrictionProvider):\n+  def __init__(self, reader, args, kwargs, incremental, splittable, binary):\n     # avoid pickling issues\n     if reader.__module__.startswith('pandas.'):\n       reader = reader.__name__\n     self.reader = reader\n     self.args = args\n     self.kwargs = kwargs\n     self.incremental = incremental\n+    self.splittable = splittable", "originalCommit": "adc4c01615fe8ba892bfe181298f2ca86333ecc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDM4MDk1OA==", "url": "https://github.com/apache/beam/pull/13443#discussion_r534380958", "bodyText": "That is correct. This relates to the comment \"We could do this for all sources that are read linearly, as long as they don't try to seek.\" below, but for the moment we're simply not supporting progress there as it introduces additional complexity (and the progress might actually be a bit deceptive as we complete the full (typically large) read before going onto the next step).", "author": "robertwb", "createdAt": "2020-12-02T18:15:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc3MzYxNw=="}], "type": "inlineReview"}, {"oid": "9b5354c793969d3d296a20917aa6ffb98561cf99", "url": "https://github.com/apache/beam/commit/9b5354c793969d3d296a20917aa6ffb98561cf99", "message": "[BEAM-11361] Dynamic splitting of SDF IOs.\n\nFor now, only json lines are supported. CSV files are more complicated.", "committedDate": "2020-12-02T18:16:49Z", "type": "commit"}, {"oid": "156ece2db191f835e8b13a1aaf7a0e9213984c23", "url": "https://github.com/apache/beam/commit/156ece2db191f835e8b13a1aaf7a0e9213984c23", "message": "lint", "committedDate": "2020-12-02T18:16:49Z", "type": "commit"}, {"oid": "550b4e6f3de86385de0655b4c9d4194730c3e084", "url": "https://github.com/apache/beam/commit/550b4e6f3de86385de0655b4c9d4194730c3e084", "message": "yapf", "committedDate": "2020-12-02T18:16:58Z", "type": "commit"}, {"oid": "ec1b0b728bdce718a088e2156fe46975c04d82e8", "url": "https://github.com/apache/beam/commit/ec1b0b728bdce718a088e2156fe46975c04d82e8", "message": "more comments, fix typo", "committedDate": "2020-12-02T18:21:32Z", "type": "commit"}, {"oid": "ec1b0b728bdce718a088e2156fe46975c04d82e8", "url": "https://github.com/apache/beam/commit/ec1b0b728bdce718a088e2156fe46975c04d82e8", "message": "more comments, fix typo", "committedDate": "2020-12-02T18:21:32Z", "type": "forcePushed"}]}