{"pr_number": 12982, "pr_title": "[BEAM-9547] Dataframe covariance and correlation.", "pr_createdAt": "2020-10-01T00:02:25Z", "pr_url": "https://github.com/apache/beam/pull/12982", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMxMDc5OQ==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501310799", "bodyText": "nit: consider defining a variable for delta to make this easier to relate to the formula in https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm", "author": "TheNeuralBit", "createdAt": "2020-10-07T21:08:00Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -34,6 +36,124 @@ def __array__(self, dtype=None):\n \n   between = frame_base._elementwise_method('between')\n \n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def std(self, axis, skipna, level, ddof, **kwargs):\n+    if level is not None:\n+      raise NotImplementedError(\"per-level aggregation\")\n+    if skipna:\n+      self = self.dropna()\n+\n+    # See the online, numerically stable formulae at\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+    def compute_moments(x):\n+      n = len(x)\n+      m = x.std(ddof=0)**2 * n\n+      s = x.sum()\n+      return pd.DataFrame(dict(m=[m], s=[s], n=[n]))\n+\n+    def combine_moments(data):\n+      m = s = n = 0.0\n+      for datum in data.itertuples():\n+        if datum.n == 0:\n+          continue\n+        elif n == 0:\n+          m, s, n = datum.m, datum.s, datum.n\n+        else:\n+          m += datum.m + (s / n - datum.s / datum.n)**2 * n * datum.n / (", "originalCommit": "322b4ce3a4c2909e673704aebbe0aa964efaf417", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5Nzg5NQ==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502197895", "bodyText": "Done.", "author": "robertwb", "createdAt": "2020-10-09T05:32:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMxMDc5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMxNTMyNA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501315324", "bodyText": "Can this link directly to https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm and/or https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm?", "author": "TheNeuralBit", "createdAt": "2020-10-07T21:17:20Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -34,6 +36,124 @@ def __array__(self, dtype=None):\n \n   between = frame_base._elementwise_method('between')\n \n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def std(self, axis, skipna, level, ddof, **kwargs):\n+    if level is not None:\n+      raise NotImplementedError(\"per-level aggregation\")\n+    if skipna:\n+      self = self.dropna()\n+\n+    # See the online, numerically stable formulae at\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance", "originalCommit": "322b4ce3a4c2909e673704aebbe0aa964efaf417", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5NzIzOA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502197238", "bodyText": "Done. Even if the subheading change, they're descriptive enough to identify what is meant.", "author": "robertwb", "createdAt": "2020-10-09T05:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMxNTMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyMTgzNA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501321834", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          other: df.corr(other, method=method, DataFrame=min_periods)[\n          \n          \n            \n                          other: df.corr(other, method=method, min_periods=min_periods)[", "author": "TheNeuralBit", "createdAt": "2020-10-07T21:31:27Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -34,6 +36,124 @@ def __array__(self, dtype=None):\n \n   between = frame_base._elementwise_method('between')\n \n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def std(self, axis, skipna, level, ddof, **kwargs):\n+    if level is not None:\n+      raise NotImplementedError(\"per-level aggregation\")\n+    if skipna:\n+      self = self.dropna()\n+\n+    # See the online, numerically stable formulae at\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+    def compute_moments(x):\n+      n = len(x)\n+      m = x.std(ddof=0)**2 * n\n+      s = x.sum()\n+      return pd.DataFrame(dict(m=[m], s=[s], n=[n]))\n+\n+    def combine_moments(data):\n+      m = s = n = 0.0\n+      for datum in data.itertuples():\n+        if datum.n == 0:\n+          continue\n+        elif n == 0:\n+          m, s, n = datum.m, datum.s, datum.n\n+        else:\n+          m += datum.m + (s / n - datum.s / datum.n)**2 * n * datum.n / (\n+              n + datum.n)\n+          s += datum.s\n+          n += datum.n\n+      if n <= ddof:\n+        return float('nan')\n+      else:\n+        return math.sqrt(m / (n - ddof))\n+\n+    moments = expressions.ComputedExpression(\n+        'compute_moments',\n+        compute_moments, [self._expr],\n+        requires_partition_by=partitionings.Nothing())\n+    with expressions.allow_non_parallel_operations(True):\n+      return frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'combine_moments',\n+              combine_moments, [moments],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def corr(self, other, method, min_periods):\n+    if method == 'pearson':  # Note that this is the default.\n+      x = self.dropna()\n+      y = other.dropna()\n+\n+      # Do this first to filter to the entries that are present on both sides.\n+      def join(x, y):\n+        return pd.concat([x, y], axis=1, join='inner').rename(\n+            lambda c: 'xy'[c], axis=1)\n+\n+      # Use the formulae from\n+      # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Covariance\n+      def compute_co_moments(x, y):\n+        n = len(x)\n+        if n <= 1:\n+          c = 0\n+        else:\n+          c = x.corr(y) * x.std() * y.std() * (n - 1)\n+        sx = x.sum()\n+        sy = y.sum()\n+        return pd.DataFrame(dict(c=[c], sx=[sx], sy=[sy], n=[n]))\n+\n+      def combine_co_moments(data, std_x, std_y):\n+        c = sx = sy = n = 0.0\n+        for datum in data.itertuples():\n+          if datum.n == 0:\n+            continue\n+          elif n == 0:\n+            c, sx, sy, n = datum.c, datum.sx, datum.sy, datum.n\n+          else:\n+            c += (\n+                datum.c + (sx / n - datum.sx / datum.n) *\n+                (sy / n - datum.sy / datum.n) * n * datum.n / (n + datum.n))\n+            sx += datum.sx\n+            sy += datum.sy\n+            n += datum.n\n+        if n < max(2, min_periods or 0):\n+          return float('nan')\n+        else:\n+          return c / (n - 1) / std_x / std_y\n+\n+      joined = frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'join',\n+              join, [x._expr, y._expr],\n+              requires_partition_by=partitionings.Index()))\n+      std_x = joined.x.std()\n+      std_y = joined.y.std()\n+\n+      moments = expressions.ComputedExpression(\n+          'compute_co_moments',\n+          compute_co_moments, [joined.x._expr, joined.y._expr])\n+\n+      with expressions.allow_non_parallel_operations(True):\n+        return frame_base.DeferredFrame.wrap(\n+            expressions.ComputedExpression(\n+                'comnine_co_moments',\n+                combine_co_moments, [moments, std_x._expr, std_y._expr],\n+                requires_partition_by=partitionings.Singleton()))\n+\n+    else:\n+      # The rank-based correlations are not obviously parallelizable, though\n+      # perhaps an approximation could be done with a knowledge of quantiles\n+      # and custom partitioning.\n+      return frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'corr',\n+              lambda df,\n+              other: df.corr(other, method=method, DataFrame=min_periods)[", "originalCommit": "322b4ce3a4c2909e673704aebbe0aa964efaf417", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM0ODUxNA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501348514", "bodyText": "Nevermind looks like you corrected this later", "author": "TheNeuralBit", "createdAt": "2020-10-07T22:35:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyMTgzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzNTI3MA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501335270", "bodyText": "It would help to be more specific here, I spent a while trying to find references. It looks like combine_co_moments  is the formula for combining covariance from two sets hidden at the bottom of the Online Covariance section.\nThe other critical piece is the translation between co-moment and pearson correlation coefficient. Is there something we can reference for that? It seems to follow from the last definition here. IIUC the co-moment is the numerator in that definition, and that fits with your code.", "author": "TheNeuralBit", "createdAt": "2020-10-07T22:02:17Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -34,6 +36,124 @@ def __array__(self, dtype=None):\n \n   between = frame_base._elementwise_method('between')\n \n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def std(self, axis, skipna, level, ddof, **kwargs):\n+    if level is not None:\n+      raise NotImplementedError(\"per-level aggregation\")\n+    if skipna:\n+      self = self.dropna()\n+\n+    # See the online, numerically stable formulae at\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+    def compute_moments(x):\n+      n = len(x)\n+      m = x.std(ddof=0)**2 * n\n+      s = x.sum()\n+      return pd.DataFrame(dict(m=[m], s=[s], n=[n]))\n+\n+    def combine_moments(data):\n+      m = s = n = 0.0\n+      for datum in data.itertuples():\n+        if datum.n == 0:\n+          continue\n+        elif n == 0:\n+          m, s, n = datum.m, datum.s, datum.n\n+        else:\n+          m += datum.m + (s / n - datum.s / datum.n)**2 * n * datum.n / (\n+              n + datum.n)\n+          s += datum.s\n+          n += datum.n\n+      if n <= ddof:\n+        return float('nan')\n+      else:\n+        return math.sqrt(m / (n - ddof))\n+\n+    moments = expressions.ComputedExpression(\n+        'compute_moments',\n+        compute_moments, [self._expr],\n+        requires_partition_by=partitionings.Nothing())\n+    with expressions.allow_non_parallel_operations(True):\n+      return frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'combine_moments',\n+              combine_moments, [moments],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def corr(self, other, method, min_periods):\n+    if method == 'pearson':  # Note that this is the default.\n+      x = self.dropna()\n+      y = other.dropna()\n+\n+      # Do this first to filter to the entries that are present on both sides.\n+      def join(x, y):\n+        return pd.concat([x, y], axis=1, join='inner').rename(\n+            lambda c: 'xy'[c], axis=1)\n+\n+      # Use the formulae from", "originalCommit": "322b4ce3a4c2909e673704aebbe0aa964efaf417", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MjY4Ng==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501362686", "bodyText": "I think the reference to pearson correlation coefficient is no longer necessary after this was moved to _cov_aligned, which can be understood just from https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online. It would still be helpful to be more specific here though.", "author": "TheNeuralBit", "createdAt": "2020-10-07T23:16:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzNTI3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM1MTAwNw==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501351007", "bodyText": "nit: this could become x.cov(y) * (n-1) which makes this more easily relatable to the wiki link", "author": "TheNeuralBit", "createdAt": "2020-10-07T22:42:12Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -150,8 +115,72 @@ def combine_co_moments(data, std_x, std_y):\n           expressions.ComputedExpression(\n               'corr',\n               lambda df,\n-              other: df.corr(other, method=method, DataFrame=min_periods)[\n-                  self._expr, other._expr],\n+              other: df.corr(other, method=method, min_periods=min_periods),\n+              [self._expr, other._expr],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  def _corr_aligned(self, other, method, min_periods):\n+    std_x = self.std()\n+    std_y = other.std()\n+    cov = self._cov_aligned(other, min_periods)\n+    with expressions.allow_non_parallel_operations(True):\n+      return frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'normalize',\n+              lambda cov,\n+              std_x,\n+              std_y: cov / (std_x * std_y),\n+              [cov._expr, std_x._expr, std_y._expr],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def cov(self, other, min_periods, ddof):\n+    x, y = self.dropna().align(other.dropna(), 'inner')\n+    return x._cov_aligned(y, min_periods, ddof)\n+\n+  def _cov_aligned(self, other, min_periods, ddof=1):\n+    # Use the formulae from\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Covariance\n+    def compute_co_moments(x, y):\n+      n = len(x)\n+      if n <= 1:\n+        c = 0\n+      else:\n+        c = x.corr(y) * x.std() * y.std() * (n - 1)", "originalCommit": "f6ef962ad62f77bda8782b82498e3770bac45fa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5NzkzOA==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502197938", "bodyText": "Good point. Done.", "author": "robertwb", "createdAt": "2020-10-09T05:32:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM1MTAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM1Mzk5MQ==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501353991", "bodyText": "Should this also check against ddof like std?", "author": "TheNeuralBit", "createdAt": "2020-10-07T22:49:56Z", "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -150,8 +115,72 @@ def combine_co_moments(data, std_x, std_y):\n           expressions.ComputedExpression(\n               'corr',\n               lambda df,\n-              other: df.corr(other, method=method, DataFrame=min_periods)[\n-                  self._expr, other._expr],\n+              other: df.corr(other, method=method, min_periods=min_periods),\n+              [self._expr, other._expr],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  def _corr_aligned(self, other, method, min_periods):\n+    std_x = self.std()\n+    std_y = other.std()\n+    cov = self._cov_aligned(other, min_periods)\n+    with expressions.allow_non_parallel_operations(True):\n+      return frame_base.DeferredFrame.wrap(\n+          expressions.ComputedExpression(\n+              'normalize',\n+              lambda cov,\n+              std_x,\n+              std_y: cov / (std_x * std_y),\n+              [cov._expr, std_x._expr, std_y._expr],\n+              requires_partition_by=partitionings.Singleton()))\n+\n+  @frame_base.args_to_kwargs(pd.Series)\n+  @frame_base.populate_defaults(pd.Series)\n+  def cov(self, other, min_periods, ddof):\n+    x, y = self.dropna().align(other.dropna(), 'inner')\n+    return x._cov_aligned(y, min_periods, ddof)\n+\n+  def _cov_aligned(self, other, min_periods, ddof=1):\n+    # Use the formulae from\n+    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Covariance\n+    def compute_co_moments(x, y):\n+      n = len(x)\n+      if n <= 1:\n+        c = 0\n+      else:\n+        c = x.corr(y) * x.std() * y.std() * (n - 1)\n+      sx = x.sum()\n+      sy = y.sum()\n+      return pd.DataFrame(dict(c=[c], sx=[sx], sy=[sy], n=[n]))\n+\n+    def combine_co_moments(data):\n+      c = sx = sy = n = 0.0\n+      for datum in data.itertuples():\n+        if datum.n == 0:\n+          continue\n+        elif n == 0:\n+          c, sx, sy, n = datum.c, datum.sx, datum.sy, datum.n\n+        else:\n+          c += (\n+              datum.c + (sx / n - datum.sx / datum.n) *\n+              (sy / n - datum.sy / datum.n) * n * datum.n / (n + datum.n))\n+          sx += datum.sx\n+          sy += datum.sy\n+          n += datum.n\n+      if n < max(2, min_periods or 0):\n+        return float('nan')", "originalCommit": "f6ef962ad62f77bda8782b82498e3770bac45fa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5ODE4Mw==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502198183", "bodyText": "Yes. Done.", "author": "robertwb", "createdAt": "2020-10-09T05:33:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM1Mzk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MDk2NQ==", "url": "https://github.com/apache/beam/pull/12982#discussion_r501360965", "bodyText": "Why revert this?", "author": "TheNeuralBit", "createdAt": "2020-10-07T23:10:49Z", "path": "sdks/python/apache_beam/dataframe/frames_test.py", "diffHunk": "@@ -36,7 +36,7 @@ def _run_test(self, func, *args):\n             expressions.ConstantExpression(arg, arg[0:0])) for arg in args\n     ]\n     expected = func(*args)\n-    actual = expressions.PartitioningSession({}).evaluate(\n+    actual = expressions.Session({}).evaluate(", "originalCommit": "bfa633a581f5efc5097c50f68dd0dac0d91d4449", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5ODMzOQ==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502198339", "bodyText": "Some of the other tests in this file were relying on the ordering.", "author": "robertwb", "createdAt": "2020-10-09T05:34:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MDk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjE5OTg3Mw==", "url": "https://github.com/apache/beam/pull/12982#discussion_r502199873", "bodyText": "I've made this an option, we can try to switch it in the future.", "author": "robertwb", "createdAt": "2020-10-09T05:40:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MDk2NQ=="}], "type": "inlineReview"}, {"oid": "e5b9a39c78efe8cafcbd6efa084fc27e94308d2d", "url": "https://github.com/apache/beam/commit/e5b9a39c78efe8cafcbd6efa084fc27e94308d2d", "message": "Scalar apply method.", "committedDate": "2020-10-09T18:48:28Z", "type": "forcePushed"}, {"oid": "766039917cc43424a1a6f6232caa46e7a19c84b1", "url": "https://github.com/apache/beam/commit/766039917cc43424a1a6f6232caa46e7a19c84b1", "message": "[BEAM-9547] Implement covariance and correlation.", "committedDate": "2020-10-09T23:51:24Z", "type": "commit"}, {"oid": "c4c61423f4eb2d44067c05c63a7e0def2d16b5c3", "url": "https://github.com/apache/beam/commit/c4c61423f4eb2d44067c05c63a7e0def2d16b5c3", "message": "[BEAM-9547] Dataframe covariance and correlation.", "committedDate": "2020-10-09T23:51:24Z", "type": "commit"}, {"oid": "83c90581b226cc1c6ece1aec7ba3c48a3c1a7053", "url": "https://github.com/apache/beam/commit/83c90581b226cc1c6ece1aec7ba3c48a3c1a7053", "message": "Scalar apply method.", "committedDate": "2020-10-09T23:51:24Z", "type": "commit"}, {"oid": "83c90581b226cc1c6ece1aec7ba3c48a3c1a7053", "url": "https://github.com/apache/beam/commit/83c90581b226cc1c6ece1aec7ba3c48a3c1a7053", "message": "Scalar apply method.", "committedDate": "2020-10-09T23:51:24Z", "type": "forcePushed"}]}