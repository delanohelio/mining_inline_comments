{"pr_number": 13353, "pr_title": "[BEAM-11267] Remove unnecessary reshuffle for stateful ParDo after key\u2026", "pr_createdAt": "2020-11-16T14:56:16Z", "pr_url": "https://github.com/apache/beam/pull/13353", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNDgxMA==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524334810", "bodyText": "Need to check Kryo internals whether this is breaking change \ud83e\udd14", "author": "dmvk", "createdAt": "2020-11-16T15:04:37Z", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "originalCommit": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMzU3MA==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524333570", "bodyText": "Should this be turned to Precondition.checkState?", "author": "je-ik", "createdAt": "2020-11-16T15:02:52Z", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java", "diffHunk": "@@ -84,6 +85,17 @@ public void setOutputDataStream(PValue value, DataStream<?> set) {\n     }\n   }\n \n+  <T extends PValue> void setProducer(T value, PTransform<?, T> producer) {\n+    if (!producers.containsKey(value)) {", "originalCommit": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU0NTk4OQ==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524545989", "bodyText": "\ud83d\udc4d makes sense", "author": "dmvk", "createdAt": "2020-11-16T20:17:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMzU3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524335827", "bodyText": "This looks unrelated, can you just explain this modification? I suppose it is correct, just wonder why it was GenericTypeInfo before.", "author": "je-ik", "createdAt": "2020-11-16T15:05:59Z", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "originalCommit": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU0NTMxNg==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524545316", "bodyText": "Not sure if this is necessary. I wanted to ensure that the new \"reinterpreted partitioning\" is compatible with the one used by GBK / Combine.\nThe idea was if partitioning is not compatible, it may result in some state partitioning related glitches (eg. you wouldn't have local state for a key-group you need).\nSecond thoughts, flink selects target partition (key group) based on \"pojo hash code\" (not based on binary representation), so the previous version was probably compatible enough \ud83e\udd14\n@mxm WDYT?", "author": "dmvk", "createdAt": "2020-11-16T20:15:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU5NzUzMg==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524597532", "bodyText": "I would think that the partitioning is ensured by the reinterpretAsKeyedStream and will therefore be preserved from the previous shuffle phase. Is this not enough?", "author": "je-ik", "createdAt": "2020-11-16T21:22:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM4MzU0Mg==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526383542", "bodyText": "I think that reinterpretAsKeyedStream just trusts the user and there are no safety nets in place.", "author": "dmvk", "createdAt": "2020-11-18T20:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDU4NA==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634584", "bodyText": "It would be nice if these explicit calls wouldn't be required. I believe context.setOutputDataStream internally has the current transform available. So we could update the producer internally in the context.", "author": "mxm", "createdAt": "2020-11-16T21:52:24Z", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -971,7 +987,9 @@ public void translateNode(\n               .transform(fullName, outputTypeInfo, (OneInputStreamOperator) doFnOperator)\n               .uid(fullName);\n \n-      context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+      final PCollection<KV<K, Iterable<InputT>>> output = context.getOutput(transform);\n+      context.setOutputDataStream(output, outDataStream);\n+      context.setProducer(output, transform);", "originalCommit": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM4MjMxNQ==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526382315", "bodyText": "nice catch \ud83d\udc4d", "author": "dmvk", "createdAt": "2020-11-18T20:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDkzNw==", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634937", "bodyText": "Same here.", "author": "mxm", "createdAt": "2020-11-16T21:52:37Z", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -1127,7 +1148,9 @@ public void translateNode(\n \n         keyedWorkItemStream.getExecutionEnvironment().addOperator(rawFlinkTransform);\n \n-        context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+        final PCollection<KV<K, OutputT>> output = context.getOutput(transform);\n+        context.setOutputDataStream(output, outDataStream);\n+        context.setProducer(output, transform);", "originalCommit": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MTg3Ng==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526471876", "bodyText": "I would call this parameter forcesShuffle or something similar.", "author": "je-ik", "createdAt": "2020-11-18T22:41:26Z", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {", "originalCommit": "0d44f87259aac9a088b9e014c50e1429acfb42f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MjUyNQ==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526472525", "bodyText": "Instead of the StatelessIdentityDoFn we could use MapElements.into(...).via(e -> KV.of(\"\", e.getValue()), which would enforce shuffle semantically. That might improve readability a bit.", "author": "je-ik", "createdAt": "2020-11-18T22:42:49Z", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));", "originalCommit": "0d44f87259aac9a088b9e014c50e1429acfb42f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MzMwMg==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526473302", "bodyText": "Do we need to set runner if we enforce the translator?", "author": "je-ik", "createdAt": "2020-11-18T22:44:25Z", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);", "originalCommit": "0d44f87259aac9a088b9e014c50e1429acfb42f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY0MDE5NA==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526640194", "bodyText": "unfortunately yes, otherwise pipeline.create() would fail", "author": "dmvk", "createdAt": "2020-11-19T07:16:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MzMwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3NDEzMg==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526474132", "bodyText": "Can we merge the two methods, that seem to differ only by this PTransform applied here?", "author": "je-ik", "createdAt": "2020-11-18T22:46:08Z", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));\n+    }\n+    aggregate.apply(ParDo.of(new StatefulNoopDoFn<>()));\n+    translator.translate(pipeline);\n+    return env.getStreamGraph().getJobGraph();\n+  }\n+\n+  @Test\n+  public void testStatefulParDoAfterGroupByKeyChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterGroupByKeyChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Iterable<Long>>> aggregate =\n+        pipeline\n+            .apply(\n+                Create.of(KV.of(\"foo\", 1L), KV.of(\"bar\", 1L))\n+                    .withCoder(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of())))\n+            .apply(GroupByKey.create());", "originalCommit": "0d44f87259aac9a088b9e014c50e1429acfb42f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY0MTA1NQ==", "url": "https://github.com/apache/beam/pull/13353#discussion_r526641055", "bodyText": "I thought about it, but the methods are really small so it's worth decreasing readability here", "author": "dmvk", "createdAt": "2020-11-19T07:18:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3NDEzMg=="}], "type": "inlineReview"}, {"oid": "d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "url": "https://github.com/apache/beam/commit/d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation.", "committedDate": "2020-12-16T09:22:10Z", "type": "forcePushed"}, {"oid": "f1c1514dc7b33841d59da22e00980163dd7e0474", "url": "https://github.com/apache/beam/commit/f1c1514dc7b33841d59da22e00980163dd7e0474", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation.", "committedDate": "2020-12-16T10:09:19Z", "type": "commit"}, {"oid": "f1c1514dc7b33841d59da22e00980163dd7e0474", "url": "https://github.com/apache/beam/commit/f1c1514dc7b33841d59da22e00980163dd7e0474", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation.", "committedDate": "2020-12-16T10:09:19Z", "type": "forcePushed"}]}