{"pr_number": 13282, "pr_title": "[BEAM-11172] Enable KafkaIO performance test for Dataflow runner v2 with SDF.", "pr_createdAt": "2020-11-07T01:37:04Z", "pr_url": "https://github.com/apache/beam/pull/13282", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520677337", "bodyText": "What actually \"runner v2\" is?", "author": "aromanenko-dev", "createdAt": "2020-11-10T16:01:20Z", "path": ".test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy", "diffHunk": "@@ -61,14 +61,38 @@ job(jobName) {\n     autoscalingAlgorithm         : 'NONE'\n   ]\n \n+  Map runnerV2SdfWrapperPipelineOptions = pipelineOptions + [\n+    kafkaTopic                   : 'beam-runnerv2',\n+    bigQueryTable                : 'kafkaioit_results_sdf_wrapper',\n+    influxMeasurement            : 'kafkaioit_results_sdf_wrapper',\n+    experiments                  : 'beam_fn_api,use_runner_v2,use_unified_worker',\n+  ]\n+\n+  Map runnerV2SdfPipelineOptions = pipelineOptions + [\n+    kafkaTopic                   : 'beam-sdf',\n+    bigQueryTable                : 'kafkaioit_results_runner_v2',\n+    influxMeasurement            : 'kafkaioit_results_runner_v2',\n+    experiments                  : 'beam_fn_api,use_runner_v2,use_unified_worker,use_sdf_kafka_read',\n+  ]\n+\n   steps {\n     gradle {\n       rootBuildScriptDir(common.checkoutDir)\n       common.setGradleSwitches(delegate)\n       switches(\"--info\")\n-      switches(\"-DintegrationTestPipelineOptions=\\'${common.joinOptionsWithNestedJsonValues(pipelineOptions)}\\'\")\n+      switches(\"-DintegrationTestPipelineOptions=\\'${common.joinOptionsWithNestedJsonValues(runnerV2SdfWrapperPipelineOptions)}\\'\")\n+      switches(\"-DintegrationTestRunner=dataflow\")\n+      switches(\"-Dexperiment=use_runner_v2\")", "originalCommit": "cbdcd5a110bc2f38ddf014d8886154637fc89cb7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk3ODEwNA==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520978104", "bodyText": "runner v2 is a new version of Dataflow. We are under migration from old Dataflow runner to this new runner. We(dataflow) are more interested in making sure SDF and KafkaIO work well on the new runner.", "author": "boyuanzz", "createdAt": "2020-11-11T01:04:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjEzNDAzMg==", "url": "https://github.com/apache/beam/pull/13282#discussion_r522134032", "bodyText": "Hmm, maybe it's not related to this PR, but the value use_runner_v2 doesn't reflect that it's related only to Dataflow, I'd assume mostly Runner v2 API looking on this. Do you know if it's already defined term?", "author": "aromanenko-dev", "createdAt": "2020-11-12T14:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE2MTM1Nw==", "url": "https://github.com/apache/beam/pull/13282#discussion_r522161357", "bodyText": "That's a good point. The Dataflow backend expects that exact string but for Beam users dataflow_v2 makes more sense. It is just an experiment and temporary, though. Hopefully most users will never need to set it. We could add the additional string and just swap it out when we submit the job.", "author": "kennknowles", "createdAt": "2020-11-12T14:47:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE2OTczNg==", "url": "https://github.com/apache/beam/pull/13282#discussion_r522169736", "bodyText": "Imho, it's fine if it's just temporary but if it's going to be exposed to user or even for the sake of clarity, I'd prefer to have additional string value, like dataflow_v2 for instance.", "author": "aromanenko-dev", "createdAt": "2020-11-12T14:58:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMyOTEzNw==", "url": "https://github.com/apache/beam/pull/13282#discussion_r522329137", "bodyText": "For testing, we need this string to decided whether to build the sdk container. In the latest change, I have removed this argument and checked pipeline options string only.", "author": "boyuanzz", "createdAt": "2020-11-12T18:36:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NzMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3OTA0Mw==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520679043", "bodyText": "Why do we need this function? Is it possible to have a timestamp out of window bounds?", "author": "aromanenko-dev", "createdAt": "2020-11-10T16:03:33Z", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -407,4 +408,13 @@ public double getTotalSize(double numRecords) {\n       return avgRecordSize.get() * numRecords / (1 + avgRecordGap.get());\n     }\n   }\n+\n+  private static Instant ensureTimestampWithinBounds(Instant timestamp) {", "originalCommit": "cbdcd5a110bc2f38ddf014d8886154637fc89cb7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk4NDY3NQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520984675", "bodyText": "The MIN_TIMESTAMP produced in Dataflow is smaller than BoundedWindow.MIN_TIMESTAMP.", "author": "boyuanzz", "createdAt": "2020-11-11T01:14:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3OTA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4MTQ4MQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520681481", "bodyText": "Why not to use metrics for counting?", "author": "aromanenko-dev", "createdAt": "2020-11-10T16:06:46Z", "path": "sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java", "diffHunk": "@@ -115,6 +122,43 @@ public static void setup() throws IOException {\n             .get();\n   }\n \n+  @Test\n+  public void testKafkaIOWithRunnerV2() throws IOException {\n+    writePipeline\n+        .apply(\"Generate records\", Read.from(new SyntheticBoundedSource(sourceOptions)))\n+        .apply(\"Measure write time\", ParDo.of(new TimeMonitor<>(NAMESPACE, WRITE_TIME_METRIC_NAME)))\n+        .apply(\"Write to Kafka\", writeToKafka());\n+\n+    readPipeline.getOptions().as(Options.class).setStreaming(true);\n+    PCollection<Integer> elementCount =\n+        readPipeline\n+            .apply(\"Read from Runner V2 Kafka\", readFromKafka())\n+            .apply(\n+                \"Measure read time\", ParDo.of(new TimeMonitor<>(NAMESPACE, READ_TIME_METRIC_NAME)))\n+            .apply(\"Map records to strings\", MapElements.via(new MapKafkaRecordsToStrings()))\n+            .apply(\n+                \"Keyed by empty key\",\n+                MapElements.into(new TypeDescriptor<KV<byte[], String>>() {})\n+                    .via(element -> KV.of(new byte[0], element)))\n+            .apply(\n+                \"Counting elements\", ParDo.of(new CountingElementFn(options.getNumberOfRecords())));", "originalCommit": "cbdcd5a110bc2f38ddf014d8886154637fc89cb7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk4NTE4Nw==", "url": "https://github.com/apache/beam/pull/13282#discussion_r520985187", "bodyText": "Yes, counter works as well. I think we can use existing element counter as well.", "author": "boyuanzz", "createdAt": "2020-11-11T01:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4MTQ4MQ=="}], "type": "inlineReview"}, {"oid": "013a8784b6740bba589daa28cb3d1c4e4ac07937", "url": "https://github.com/apache/beam/commit/013a8784b6740bba589daa28cb3d1c4e4ac07937", "message": "Update tests.", "committedDate": "2020-11-11T04:30:32Z", "type": "forcePushed"}, {"oid": "c8729f3589903700f3e266833e1ffc391cb7ec35", "url": "https://github.com/apache/beam/commit/c8729f3589903700f3e266833e1ffc391cb7ec35", "message": "Update test setup", "committedDate": "2020-11-11T18:39:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYwMzc3NA==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521603774", "bodyText": "Maybe including the word 'Dataflow' would be informative for these configs.", "author": "tysonjh", "createdAt": "2020-11-11T19:57:39Z", "path": ".test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy", "diffHunk": "@@ -61,14 +61,60 @@ job(jobName) {\n     autoscalingAlgorithm         : 'NONE'\n   ]\n \n+  Map runnerV2SdfWrapperPipelineOptions = pipelineOptions + [", "originalCommit": "c8729f3589903700f3e266833e1ffc391cb7ec35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYxODkxOQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521618919", "bodyText": "Adding an 'experiment' flag here could be confused with the pipelineOptions experiment flag. Maybe instead of this, it would be better to inspect the pipelineOptions instead?", "author": "tysonjh", "createdAt": "2020-11-11T20:27:31Z", "path": "buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy", "diffHunk": "@@ -233,6 +233,8 @@ class BeamModulePlugin implements Plugin<Project> {\n \n     // Required. Pipeline options to be used by the tested pipeline.\n     String integrationTestPipelineOptions = System.getProperty('integrationTestPipelineOptions')\n+\n+    String experiment = System.getProperty('experiment', '')", "originalCommit": "c8729f3589903700f3e266833e1ffc391cb7ec35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYyMzU1OQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521623559", "bodyText": "I'm not very familiar with this file, but it seems like we're now configuring runnerv2 tasks (e.g. cleanup tasks, building container images) based on various settings (e.g. pipelineOptions, the gradle 'experiment' flag) here and also in various build.gradle files.\nShould we be picking one over the other? For example, the PR you're reviewing of mine that contains IT tests for examples, configures the tests in the build.gradle file. It seems strikingly similar to what is going on here though.", "author": "tysonjh", "createdAt": "2020-11-11T20:36:48Z", "path": "buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy", "diffHunk": "@@ -1442,7 +1447,7 @@ class BeamModulePlugin implements Plugin<Project> {\n       JavaPerformanceTestConfiguration configuration = it ? it as JavaPerformanceTestConfiguration : new JavaPerformanceTestConfiguration()\n \n       // Task for running integration tests\n-      project.task('integrationTest', type: Test) {\n+      def itTask = project.task('integrationTest', type: Test) {", "originalCommit": "c8729f3589903700f3e266833e1ffc391cb7ec35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYzNTI1NQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521635255", "bodyText": "This file defines several common project configurations, which can be applied to any subproject.\nComparing to the solution we have for dataflow build, one benefit of using such configuration to create IT is that we can have one centralized place for all IO to set up ITs instead of having them one by one and it's easier to use one test configuration to invoke tests on different runners. Taking this KafkaIO IT as an example, if one day Flink is interested in this suite, Flink  can just use the same configuration in https://github.com/apache/beam/blob/master/.test-infra/jenkins/job_PerformanceTests_KafkaIO_IT.groovy.", "author": "boyuanzz", "createdAt": "2020-11-11T20:59:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYyMzU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NzI4NQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521647285", "bodyText": "Instead of naming the test after the runner, is it possible to name the test after the behavior that is being tested and use the configuration to ensure it runs on the appropriate runners? Is there a reason to have both this test and testKafkaReadsAndWritesProperly?\nIf so, it would be good to understand the difference through the test naming. From what I can tell the material difference is that this test uses a streaming pipeline to read where testKafkaReadsAndWritesProperly uses a batch one.", "author": "tysonjh", "createdAt": "2020-11-11T21:24:44Z", "path": "sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java", "diffHunk": "@@ -115,16 +116,55 @@ public static void setup() throws IOException {\n             .get();\n   }\n \n+  @Test\n+  public void testKafkaIOWithRunnerV2() throws IOException {", "originalCommit": "c8729f3589903700f3e266833e1ffc391cb7ec35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY4MDc1OQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r521680759", "bodyText": "That's correct. I'll do the renaming. Thanks for catching that!", "author": "boyuanzz", "createdAt": "2020-11-11T22:38:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NzI4NQ=="}], "type": "inlineReview"}, {"oid": "d32629a17b3cda161f6be81459f6749a6d64499e", "url": "https://github.com/apache/beam/commit/d32629a17b3cda161f6be81459f6749a6d64499e", "message": "Enable KafkaIO performance test for runner v2 with SDF.", "committedDate": "2020-11-12T01:06:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE4NTIyNQ==", "url": "https://github.com/apache/beam/pull/13282#discussion_r522185225", "bodyText": "nit: Please, remove \"Runner V2\"", "author": "aromanenko-dev", "createdAt": "2020-11-12T15:18:23Z", "path": "sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOIT.java", "diffHunk": "@@ -116,15 +117,54 @@ public static void setup() throws IOException {\n   }\n \n   @Test\n-  public void testKafkaIOReadsAndWritesCorrectly() throws IOException {\n+  public void testKafkaIOReadsAndWritesCorrectlyInStreaming() throws IOException {\n+    // Use batch pipeline to write records.\n+    writePipeline\n+        .apply(\"Generate records\", Read.from(new SyntheticBoundedSource(sourceOptions)))\n+        .apply(\"Measure write time\", ParDo.of(new TimeMonitor<>(NAMESPACE, WRITE_TIME_METRIC_NAME)))\n+        .apply(\"Write to Kafka\", writeToKafka());\n+\n+    // Use streaming pipeline to read Kafka records.\n+    readPipeline.getOptions().as(Options.class).setStreaming(true);\n+    readPipeline\n+        .apply(\"Read from Runner V2 Kafka\", readFromKafka())", "originalCommit": "d32629a17b3cda161f6be81459f6749a6d64499e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "730e7c064a878facbb6b5d3560445d2c513742ae", "url": "https://github.com/apache/beam/commit/730e7c064a878facbb6b5d3560445d2c513742ae", "message": "[BEAM-11172] Enable KafkaIO performance test for runner v2 with SDF.", "committedDate": "2020-11-12T19:13:49Z", "type": "commit"}, {"oid": "730e7c064a878facbb6b5d3560445d2c513742ae", "url": "https://github.com/apache/beam/commit/730e7c064a878facbb6b5d3560445d2c513742ae", "message": "[BEAM-11172] Enable KafkaIO performance test for runner v2 with SDF.", "committedDate": "2020-11-12T19:13:49Z", "type": "forcePushed"}]}