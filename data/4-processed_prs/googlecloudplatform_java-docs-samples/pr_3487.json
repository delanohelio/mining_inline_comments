{"pr_number": 3487, "pr_title": "[WIP] Spark Applications for Cloud Bigtable", "pr_createdAt": "2020-08-09T13:24:36Z", "pr_url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNTc1OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468905759", "bodyText": "Can you group like dependencies together?\n\norg.apache.spark deps\nhbase deps\nbigtable deps\nworkaround deps\n\nAlso please extract the group versions into a const (sparkVersion, hbaseVersion, bigtableVersion)", "author": "igorbernstein2", "createdAt": "2020-08-11T22:44:30Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDg0Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468914842", "bodyText": "Also, please leave comments as to why the versions were chosen.\nie dataproc version x uses spark y and hbase z\nThere is a version conflict between hbase & spark for jackson, so we force the version to match spark", "author": "igorbernstein2", "createdAt": "2020-08-11T23:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNTc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwMzM4MQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472203381", "bodyText": "done", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:38:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNTc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjY3NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468906674", "bodyText": "Should this be moved to dependencyOverrides?", "author": "igorbernstein2", "createdAt": "2020-08-11T22:47:00Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDM3MQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468914371", "bodyText": "Also, add a comment as to where this version came from", "author": "igorbernstein2", "createdAt": "2020-08-11T23:10:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjY3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwNDUzMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472204533", "bodyText": "Left a FIXME and a comment. Considered done.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:39:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjk3Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468906972", "bodyText": "I'm a bit confused by this: this dependency is provided so it should be on the classpath already, so how could this cause a runtime error?", "author": "igorbernstein2", "createdAt": "2020-08-11T22:47:57Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwNTk2Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472205962", "bodyText": "This is required to run the samples from within IDEA. Once we're done with the main requirements of the samples we'd have to revisit this. Leaving in fixes val for now (and considered done).", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:41:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNjk3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468907133", "bodyText": "This needs a bit of explanation", "author": "igorbernstein2", "createdAt": "2020-08-11T22:48:27Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwNjkzMQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472206931", "bodyText": "Left a comment. Done.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:42:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxNDk5NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472314994", "bodyText": "Needs more explanation: which deps already provide this?\nWithout this information how will we know that this can be removed when updating versions", "author": "igorbernstein2", "createdAt": "2020-08-18T16:12:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAxNjExOQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473016119", "bodyText": "I didn't care what deps provide each and every excluded dependency. It was required to get assembly working (it failed due to multiple dependencies providing the same packages and classes). It was kind of trial and error way of solving it. Once it worked I crossed that task out.\nDo you want me to dig deeper and describe the dependency \"chains\"? I'd argue if that helped much as \"when updating versions\" other deps may have to be excluded. WDYT?", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NDg0NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473064844", "bodyText": "At the very least I would like a comment explaining what kind of issues these are causing? is it at build time? or run time? So that when we need to upgrade dependencies I know if these exclusions can be removed.", "author": "igorbernstein2", "createdAt": "2020-08-19T14:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE3NjYyMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473176623", "bodyText": "There are the comments already:\n\n// Excluding duplicates for the uber-jar\n// There are other deps to provide necessary packages\n\nDo you want me to elaborate further?", "author": "jaceklaskowski", "createdAt": "2020-08-19T16:49:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI0NzI5MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473247290", "bodyText": "Yes, I saw those comments, but I need more specific information so that we know if we can remove these exclusion in future. I need to know what issue or error this is causing and when that error is occuring and why is sbt is not doing the right thing for me out of the box\nRight now I can remove the entire exclusion list and everything looks like its working fine. How will I know in the future that these manual interventions are not necessary?", "author": "igorbernstein2", "createdAt": "2020-08-19T18:47:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwNzEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwODIzMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468908232", "bodyText": "Can you look into passing project, instance id in during runtime programmatically?  There should be a way to pass the configuration options. Maybe via the options()?\nIdeally the project id & instance id can be passed in as parameters alongside the table id", "author": "igorbernstein2", "createdAt": "2020-08-11T22:51:40Z", "path": "bigtable/spark/src/main/resources/hbase-site.xml", "diffHunk": "@@ -0,0 +1,49 @@\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<!--\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+-->\n+<configuration>\n+    <property>\n+        <name>google.bigtable.project.id</name>\n+        <value>your-google-cloud-project</value>\n+    </property>\n+    <property>\n+        <name>google.bigtable.instance.id</name>\n+        <value>your-bigtable-instance</value>\n+    </property>", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwNzg0NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472207845", "bodyText": "It's already handled. The missing piece is to offer an option to switch between command line and xml file. It's on its way. Considered done for now.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:44:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwODIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NjAyMA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473066020", "bodyText": "I'm closing this comment in favor of the discussion above (\"removing the xml altogether\")", "author": "igorbernstein2", "createdAt": "2020-08-19T14:19:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwODIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMDc4NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468910784", "bodyText": "Can this code be reorganized into a setup phase and query phase? Currently the data operations interwoven with the configuration.\nMaybe something like this:\n// Spark representation of the row\ncase class BigtableRecord (...)\n\n// Map  the scala representation to Bigtable\nval catalog = s\"\"\"....\"\"\"\n\n\n// configure the output sink\nval tableOpts = Map( ... )\n\n// Generate some data in memory\nval numRecords = Try(args(1).toInt).getOrElse(10)\nval records = (0 until numRecords).map(BigtableRecord.apply).toDF\n\n// write it to bigtable\nrecords.write....", "author": "igorbernstein2", "createdAt": "2020-08-11T22:59:18Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxMjYxOQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472212619", "bodyText": "think I've got it covered.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:51:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMDc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTcxNA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468911714", "bodyText": "Not sure if this is a scala thing, but why are the imports sprinkled throughout the file?\n\nHBaseTableCatalog is import for the entire file\nSparkSession is in the function scope\nspark implicits are in the middle\n\nCan you group them? Maybe move the non-implicits to the file level and the implicits to the top of the function", "author": "igorbernstein2", "createdAt": "2020-08-11T23:01:57Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNDA4Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472214083", "bodyText": "import spark.implicits._ is a Scala thing and needs to be after val spark to import values from the spark object.\nOther imports are close to the place they're needed (and would not be surprised if represent part of the Scala code style of mine).", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:53:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMjIwMA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468912200", "bodyText": "Why is the launcher necessary? it doesnt seem to be used", "author": "igorbernstein2", "createdAt": "2020-08-11T23:03:32Z", "path": "bigtable/spark/src/test/scala/example/WordcountLuncher.scala", "diffHunk": "@@ -0,0 +1,7 @@\n+package example", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNDY3OA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472214678", "bodyText": "It's to act as a test without any testing framework. It should migrate to real tests in next iteration.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMjIwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzcyMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913722", "bodyText": "This seems like a workaround similar to some of the options set in the xml. Can we keep all of them together?", "author": "igorbernstein2", "createdAt": "2020-08-11T23:08:24Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    \"hbase.spark.use.hbasecontext\" -> \"false\")", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNTA0Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472215047", "bodyText": "Please expand on this.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:54:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk2Mjk4NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r482962985", "bodyText": "nvm, I think I misunderstood what hbase.spark.use.hbasecontext is for", "author": "igorbernstein2", "createdAt": "2020-09-03T13:07:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzgxMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913813", "bodyText": "What is \"5\" here?", "author": "igorbernstein2", "createdAt": "2020-08-11T23:08:43Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNjQzNw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472216437", "bodyText": "3 seems to be the minimum to really create a HBase/Bigtable table. Not sure what it means exactly. I'll explain the options once the code's ready.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:56:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzkxNA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468913914", "bodyText": "Please add a comment pointing users where they can find more options they can use", "author": "igorbernstein2", "createdAt": "2020-08-11T23:09:02Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,59 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog\n+\n+import scala.util.Try\n+\n+object Wordcount extends App {\n+\n+  println(\"Starting up...\")\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+\n+  println(\"Spark version: \" + spark.version)\n+\n+  val table = Try(args(0)).getOrElse(\"wordcount\")\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  val numRecords = Try(args(1).toInt).getOrElse(10)\n+  println(s\"Saving $numRecords records to $table\")\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+  val opts = Map(", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNzEwNw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472217107", "bodyText": "Left a FIXME.", "author": "jaceklaskowski", "createdAt": "2020-08-18T13:57:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMzkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTAwNA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468915004", "bodyText": "here and everywhere else, please add trailing newlines", "author": "igorbernstein2", "createdAt": "2020-08-11T23:12:16Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,42 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.5\" % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  // Exception: Incompatible Jackson 2.9.2\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame (!)\n+  \"org.apache.spark\" %% \"spark-streaming\" % \"2.4.5\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % \"1.15.0\",\n+  // NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % \"2.2.3\",\n+  \"org.apache.hbase\" % \"hbase-client\" % \"2.2.3\"\n+)\n+\n+excludeDependencies ++= Seq(\n+  ExclusionRule(organization = \"asm\", \"asm\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils-core\"),\n+  ExclusionRule(organization = \"org.mortbay.jetty\", \"servlet-api\")\n+)\n+\n+assemblyMergeStrategy in assembly := {\n+  case PathList(\"META-INF\", \"io.netty.versions.properties\") => MergeStrategy.first\n+  case PathList(\"META-INF\", \"MANIFEST.MF\") => MergeStrategy.discard\n+  case PathList(\"mozilla\", \"public-suffix-list.txt\") => MergeStrategy.first\n+  case PathList(\"google\", xs @ _*) => xs match {\n+    case ps @ (x :: xs) if ps.last.endsWith(\".proto\") => MergeStrategy.first\n+    case _ => MergeStrategy.deduplicate\n+  }\n+  case x =>\n+    val oldStrategy = (assemblyMergeStrategy in assembly).value\n+    oldStrategy(x)\n+    // FIXME\n+    MergeStrategy.first\n+}", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxNTM5Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472315393", "bodyText": "did you forget to push a commit? that newline is still missing", "author": "igorbernstein2", "createdAt": "2020-08-18T16:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTAwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAxODE0MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473018140", "bodyText": "Should be OK now.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:13:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNjYxOA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468916618", "bodyText": "Can you add a paragraph explaining how all of the project interact?\nsomething along the lines of: Spark is the execution environment that can parallelize data processing. The spark provides an api for storage system to plug into this environment. The HBase Spark Connector implements an adapter for Hbase. bigtable-hbase-2.x-hadoop provides a bridge from the HBase api to cloud bigtable. Thus allowing spark pipelines to interact with bigtable using the native spark api.", "author": "igorbernstein2", "createdAt": "2020-08-11T23:17:13Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIyMTM0NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472221345", "bodyText": "done", "author": "jaceklaskowski", "createdAt": "2020-08-18T14:02:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNjYxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTE1OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468919159", "bodyText": "All of these steps should be under a single section \"Test using the Bigtable emulator\"\nSomething like (note I'm using 4 backticks so that this comment would render)\n## Test using the Bigtable emulator\n``\n# Build an uber/fat jar\nsbt clean assembly\n\n# Start the emulator\ngcloud beta emulators bigtable start\n$(gcloud beta emulators bigtable env-init)\n\n# Start the job\n$SPARK_HOME/bin/spark-submit \\\n  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n  $BIGTABLE_TABLE $NUMBER_OF_ROWS\n\n# Verify \n$ cbt \\\n  -project=your-google-cloud-project \\\n  -instance=your-bigtable-instance \\\n  ls\nwordcount\n``\n\n\nThen we can add a section in the future for dataproc and real bigtable", "author": "igorbernstein2", "createdAt": "2020-08-11T23:25:13Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Example\n+\n+The Spark application can be assembled into an uber/fat jar with all of its dependencies and configuration. To build use `sbt` as follows:\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+**NOTE**: Since the Cloud Bigtable configuration is included in the (fat) jar, any changes will require re-assembling it.\n+\n+## Start Bigtable Emulator", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIyNDk2NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472224964", "bodyText": "considered done", "author": "jaceklaskowski", "createdAt": "2020-08-18T14:08:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTE1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTQ1MQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r468919451", "bodyText": "why wordcount?", "author": "igorbernstein2", "createdAt": "2020-08-11T23:26:11Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,85 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark) . \n+\n+The project uses [sbt](https://www.scala-sbt.org/) as the build tool.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Example\n+\n+The Spark application can be assembled into an uber/fat jar with all of its dependencies and configuration. To build use `sbt` as follows:\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+**NOTE**: Since the Cloud Bigtable configuration is included in the (fat) jar, any changes will require re-assembling it.\n+\n+## Start Bigtable Emulator\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+## Configure the Example\n+\n+Create environment variables for the following commands:\n+\n+```\n+BIGTABLE_TABLE=wordcount\n+NUMBER_OF_ROWS=5\n+```\n+\n+## Run\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $BIGTABLE_TABLE $NUMBER_OF_ROWS\n+```\n+\n+## Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=your-google-cloud-project \\\n+  -instance=your-bigtable-instance \\\n+  ls\n+wordcount", "originalCommit": "85dee393bb9c2c52b027c4d75f633e05db70ba2e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIyNTQyMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472225423", "bodyText": "any name would work. It's configured using an environment var. There's Wordcount sample already.", "author": "jaceklaskowski", "createdAt": "2020-08-18T14:08:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTQ1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwOTg3Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472309872", "bodyText": "Now that the sample is counting words it makes more sense.\nHowever the previous 2 steps will create 2 tables named \"wordcount-dataframe\" and \"wordcount-rdd\". So please update this section appropriately", "author": "igorbernstein2", "createdAt": "2020-08-18T16:04:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxOTQ1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzY0Ng==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472237646", "bodyText": "typo in 'Launcher'", "author": "kolea2", "createdAt": "2020-08-18T14:25:23Z", "path": "bigtable/spark/src/test/scala/example/DataFrameDemoLuncher.scala", "diffHunk": "@@ -0,0 +1,7 @@\n+package example\n+\n+object DataFrameDemoLuncher extends App {", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyMDU1MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473020550", "bodyText": "Time to rewrite it to be a full-blown test, isn't it? I'm adding a testing framework to the project. In next commit...", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:16:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwNjMwMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r476906303", "bodyText": "SGTM - if presubmit checks cannot run the test for now, we will manually run and verify instead", "author": "kolea2", "createdAt": "2020-08-26T00:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMwMzA1Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r481303052", "bodyText": "Added a simple test that gets rid of this launcher. Considered resolved.", "author": "jaceklaskowski", "createdAt": "2020-09-01T17:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzY0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzOTIxNg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472239216", "bodyText": "if these are meant to be uncommented by the developer, please add that as a comment", "author": "kolea2", "createdAt": "2020-08-18T14:27:30Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyMTk2OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473021969", "bodyText": "They're going to be part of a switch to allow for command line options or xml for configuration. Soon...", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:18:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzOTIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NzE2NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473067165", "bodyText": "Lets merge this discussion to the comment above. I think the xml should be removed altogether and the example should focus on single maintainable approach", "author": "igorbernstein2", "createdAt": "2020-08-19T14:21:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzOTIxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwNTQ4NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472305484", "bodyText": "The pricing sentence seems extraneous...none of the other samples mention that", "author": "igorbernstein2", "createdAt": "2020-08-18T15:57:43Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwNzU0OA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472307548", "bodyText": "I would like to avoid specifying dependencies at runtime. This is create a lot of headaches for support engineers. Please move it to sbt file (same for below)", "author": "igorbernstein2", "createdAt": "2020-08-18T16:00:43Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyMzgxMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473023813", "bodyText": "Tried, but for some reason it didn't work. The uber-jar didn't work with dataproc (haven't tried with the other envs). Marked to be resolved.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:20:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMwNzU0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472310712", "bodyText": "the number 5 needs an explanation", "author": "igorbernstein2", "createdAt": "2020-08-18T16:05:36Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMTgwMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472311803", "bodyText": "Also, why is the table called wordcount? the job is not counting words", "author": "igorbernstein2", "createdAt": "2020-08-18T16:07:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyNTI2Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473025262", "bodyText": "I'm going to use scopt for command-line option parsing that will get rid of this and some other comand-line issues.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:23:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1NzcxOQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473057719", "bodyText": "I'd prefer to avoid adding another dependency here. I think it will be a lot more clear if either remove the 5 or assign it to a named env var or just add a simple comment explaining what it is", "author": "igorbernstein2", "createdAt": "2020-08-19T14:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMDcxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472312982", "bodyText": "I'm a bit confused why this wasnt done for the emulator? Which will confuse users if the connector autocreates tables or not. Please be consistent between the 2 examples", "author": "igorbernstein2", "createdAt": "2020-08-18T16:09:18Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,162 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to load or save data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+- Apache Spark is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+- The Apache HBase\u2122 Spark Connector implements the DataSource API for Apache HBase.\n+- `bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+## Prerequisites\n+\n+1. A [Google Cloud project](https://console.cloud.google.com/) with billing enabled.\n+Please be aware of [Cloud Bigtable](https://cloud.google.com/bigtable/pricing) pricing.\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Test Examples using Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+```\n+APP_NAME=example.Wordcount\n+WORDCOUNT_BIGTABLE_TABLE=wordcount-rdd\n+WORDCOUNT_FILE=README.md\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $WORDCOUNT_BIGTABLE_TABLE", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMzU3Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472313577", "bodyText": "Also, to makes this a bit shorter, please use the families= parameter of createtable", "author": "igorbernstein2", "createdAt": "2020-08-18T16:10:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyNjc3Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473026772", "bodyText": "I'm a bit confused why this wasnt done for the emulator?\n\nI've been asking the very same question myself too. I don't know. It worked before without pre-creating the table yet Bigtable required it. No idea why. I'll work on it.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIzNDg2OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473234869", "bodyText": "Ok, I found it. It was always required. I simply used the older samples that did work with the emulator and the real Bigtable instance and created tables by themselves. When I created new samples I didn't copy that part. With the recent changes I made sure tables are created when not available. I'll make it optional (via a command-line option).", "author": "jaceklaskowski", "createdAt": "2020-08-19T18:23:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk2NTAyMQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r482965021", "bodyText": "Please avoid making things optional, just create the table in emulator", "author": "igorbernstein2", "createdAt": "2020-09-03T13:10:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMjk4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMzgzMQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472313831", "bodyText": "which version of data proc?", "author": "igorbernstein2", "createdAt": "2020-08-18T16:10:27Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,55 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+// Versions to match Dataproc", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTAyNjkzOQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r491026939", "bodyText": "As discussed offline, we will target dataproc 1.4\nAccording to:\nhttps://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.4\nsparkVersion = 2.4.6\nhbaseVersion = 1.3.6\nscala = 2.11.12", "author": "igorbernstein2", "createdAt": "2020-09-18T15:28:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxMzgzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMDU0Ng==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472320546", "bodyText": "After a bit more thought, I think we should get rid of hbase-site.xml altogether. Settings things like com.google.cloud.bigtable.hbase2_x.BigtableConnection should be considered an internal implementation detail of BigtableConfiguration. I'm in particular worried about newer versions needing other shims injected. For each if the hbase-connector decided to start using async apis then we would have to start setting the async connection class name as well. I'd rather this be maintained java-bigtable-hbase than in a customer's handwritten xml file.\nPlease remove the xml file altogether and only use BigtableConfiguration here", "author": "igorbernstein2", "createdAt": "2020-08-18T16:21:00Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMTM0Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472321347", "bodyText": "please remove these comments ( google.bigtable.{project,instance}.id )", "author": "igorbernstein2", "createdAt": "2020-08-18T16:22:17Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMzY5MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472323690", "bodyText": "That issue is supposed to be fixed in spark 2.2.1 & 2.3.0...this example uses 2.4.x. So I'm not sure that is correct or we should reopened that bug", "author": "igorbernstein2", "createdAt": "2020-08-18T16:25:48Z", "path": "bigtable/spark/src/main/resources/hbase-site.xml", "diffHunk": "@@ -0,0 +1,49 @@\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<!--\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+-->\n+<configuration>\n+    <property>\n+        <name>google.bigtable.project.id</name>\n+        <value>your-google-cloud-project</value>\n+    </property>\n+    <property>\n+        <name>google.bigtable.instance.id</name>\n+        <value>your-bigtable-instance</value>\n+    </property>\n+    <property>\n+        <name>hbase.client.connection.impl</name>\n+        <value>com.google.cloud.bigtable.hbase2_x.BigtableConnection</value>\n+    </property>\n+\n+    <!-- shc uses namespaces, which bigtable doesn't support. This has the\n+    Bigtable client log warns rather than throw -->\n+    <property>\n+        <name>google.bigtable.namespace.warnings</name>\n+        <value>true</value>\n+    </property>\n+\n+    <!--  workaround: https://issues.apache.org/jira/browse/SPARK-21549 -->", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAyOTAyOA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473029028", "bodyText": "I simply copied it from bigtable-shc. I'll look into it (and make sure that the file is as succinct as possible).", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyMzY5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNDUzOQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472324539", "bodyText": "HBaseTableCatalog.newTable -> \"5\", needs an explanation before this PR can be merged\nThe other options can be left as a FIXME for now", "author": "igorbernstein2", "createdAt": "2020-08-18T16:27:07Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjA4Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326083", "bodyText": "Lets make this a bit more useful. As opposed to generating a sequence, writing it and then reading. Why not make the a CopyTable example which reads the wordcount table generated from the RDD example and writes it to a new table", "author": "igorbernstein2", "createdAt": "2020-08-18T16:29:28Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  val hbaseContext = new HBaseContext(spark.sparkContext, conf)\n+  val opts_nouse = opts.filterNot { case (k, _) => k == HBaseSparkConf.USE_HBASECONTEXT }\n+  // END\n+\n+  records\n+    .write\n+    .format(\"org.apache.hadoop.hbase.spark\")\n+    .options(opts_nouse)\n+    .save\n+\n+  println(s\"Writing to $table...DONE\")\n+\n+  println(s\"Loading $table\")\n+  spark\n+    .read\n+    .format(\"org.apache.hadoop.hbase.spark\")\n+    .options(opts_nouse)\n+    .load\n+    .show(truncate = false)\n+  println(s\"Loading $table...DONE\")", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAzMTQ5OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473031499", "bodyText": "OK. We've discussed it and decided to use Google Cloud Storage, no? Seems we've got two useful examples of sample applications! I'll work on this and the one with GCS.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:32:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjUxMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326512", "bodyText": "javadoc explaining what this job does", "author": "igorbernstein2", "createdAt": "2020-08-18T16:30:05Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,55 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.hbase.client.Put\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+object Wordcount extends App {", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjg4Ng==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472326886", "bodyText": "I thought we are using hbase 2.x?", "author": "igorbernstein2", "createdAt": "2020-08-18T16:30:41Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,55 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.hbase.client.Put\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  import org.apache.spark.SparkConf\n+  val sparkConf = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat in HBase 1.6.0", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAzNDQyOA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473034428", "bodyText": "think it's a leftover from the days I used 1.x. Left FIXME to verify it.", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:36:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMyNjg4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNjcwMw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472336703", "bodyText": "I'm confused by this line:\n\nwhy is it separate from the opts definition above?\nwhy is the option being removed rather then reset to true?", "author": "igorbernstein2", "createdAt": "2020-08-18T16:46:11Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF\n+\n+  // TODO Use a command-line option to switch between command line params and xml\n+\n+  // Hack to specify HBase properties on command line\n+  // BEGIN\n+  // import org.apache.hadoop.hbase.HBaseConfiguration\n+  // val conf = HBaseConfiguration.create()\n+  // conf.set(\"google.bigtable.project.id\", projectId)\n+  // conf.set(\"google.bigtable.instance.id\", instanceId)\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  val hbaseContext = new HBaseContext(spark.sparkContext, conf)\n+  val opts_nouse = opts.filterNot { case (k, _) => k == HBaseSparkConf.USE_HBASECONTEXT }", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzAzNjcwMA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473036700", "bodyText": "This is a mixture of two \"conflicting\" approaches to hbase/bigtable configuration - command line vs xml file. The whole block is to support configuration on command line while the comment a few lines above (that I quoted below) shows how to do it with hbase-site.xml config.\nHBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only", "author": "jaceklaskowski", "createdAt": "2020-08-19T13:39:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNjcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzA2OA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472337068", "bodyText": "move data generation to after setup", "author": "igorbernstein2", "createdAt": "2020-08-18T16:46:47Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")\n+\n+  // FIXME Explain the options\n+  // FIXME Where to find more options supported? Any docs?\n+  val opts = Map(\n+    HBaseTableCatalog.tableCatalog -> catalog,\n+    HBaseTableCatalog.newTable -> \"5\",\n+    HBaseSparkConf.USE_HBASECONTEXT -> \"false\") // accepts xml configs only\n+\n+  import spark.implicits._\n+  val records = (0 until numRecords).map(BigtableRecord.apply).toDF", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzYyNg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r472337626", "bodyText": "please move the println to where the write is actually happening", "author": "igorbernstein2", "createdAt": "2020-08-18T16:47:44Z", "path": "bigtable/spark/src/main/scala/example/DataFrameDemo.scala", "diffHunk": "@@ -0,0 +1,92 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+\n+import scala.util.Try\n+\n+object DataFrameDemo extends App {\n+\n+  println(\"Starting up...\")\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = Try(args(2)).getOrElse(\"dataframe-demo\")\n+  val numRecords = Try(args(3).toInt).getOrElse(10)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\n+  println(\"Spark version: \" + spark.version)\n+\n+  val catalog =\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"key\",\n+       |\"columns\":{\n+       |\"col0\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\n+       |\"col1\":{\"cf\":\"cf1\", \"col\":\"col1\", \"type\":\"boolean\"},\n+       |\"col2\":{\"cf\":\"cf2\", \"col\":\"col2\", \"type\":\"double\"},\n+       |\"col3\":{\"cf\":\"cf3\", \"col\":\"col3\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  println(s\"Writing $numRecords records to $table\")", "originalCommit": "0b155d03315891f9a44a451177b69a63154acb6a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIzMzEzMA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473233130", "bodyText": "fixed", "author": "jaceklaskowski", "createdAt": "2020-08-19T18:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTIwMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473059202", "bodyText": "nit: the path has to be an absolute path\nGOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json", "author": "igorbernstein2", "createdAt": "2020-08-19T14:10:32Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json", "originalCommit": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIzMzA0MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473233040", "bodyText": "fixed", "author": "jaceklaskowski", "createdAt": "2020-08-19T18:20:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTk1NA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473059954", "bodyText": "why overwrite the assignments in Configure Environment?", "author": "igorbernstein2", "createdAt": "2020-08-19T14:11:37Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=tuesday\n+BIGTABLE_SPARK_CLUSTER_ID=tuesday-c1\n+BIGTABLE_SPARK_CLUSTER_ZONE=europe-west3-a", "originalCommit": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE3OTIxNw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473179217", "bodyText": "This is to keep them close to the commands to test it out with Dataproc. They're grouped together once we settle on what and how. OK?", "author": "jaceklaskowski", "createdAt": "2020-08-19T16:53:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA1OTk1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2MDA3MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473060070", "bodyText": "why overwrite the assignments in Configure Environment?", "author": "igorbernstein2", "createdAt": "2020-08-19T14:11:47Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,327 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Use a library (scopt) to handle command line options\n+- [ ] Use scalatest for testing (and remove `DataFrameDemoLuncher.scala`)\n+- [ ] Use a command-line option to switch between command line params and xml for Bigtable configuration\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+- [ ] Use the `families=` parameter of `cbt createtable`\n+- [ ] Review [hbase-site.xml](src/main/resources/hbase-site.xml) so it contains the required properties only\n+- [ ] Migrate [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) to a `CopyTable` example which reads the wordcount table generated from the RDD example and writes it to a new table\n+- [ ] Create another example that uses files in Google Cloud Storage and saves the content to a Bigtable table\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+GOOGLE_CLOUD_PROJECT=your-project-id\n+BIGTABLE_INSTANCE=your-bigtable-instance\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+Use one of the Spark sample applications as the `--class` parameter.\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-rdd README.md\n+```\n+\n+### DataFrameDemo\n+\n+The following `spark-submit` uses [example.DataFrameDemo](src/main/scala/example/DataFrameDemo.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.DataFrameDemo \\\n+  target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  wordcount-dataframe 5\n+```\n+\n+### Verify\n+\n+There should be one table.\n+\n+```\n+$ cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+wordcount\n+```\n+\n+There should be the number of rows that you requested on command line.\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_TABLE\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.Wordcount\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount-rdd\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  createfamily $BIGTABLE_SPARK_WORDCOUNT_TABLE cf\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class $BIGTABLE_SPARK_CLASS \\\n+  $BIGTABLE_SPARK_JAR \\\n+  $GOOGLE_CLOUD_PROJECT $BIGTABLE_INSTANCE \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  -instance=$BIGTABLE_INSTANCE \\\n+  read $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  listinstances\n+```\n+\n+```\n+cbt \\\n+  -project=$GOOGLE_CLOUD_PROJECT \\\n+  deleteinstance $BIGTABLE_INSTANCE\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/DataFrameDemo.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=your-service-account-json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=tuesday\n+BIGTABLE_SPARK_CLUSTER_ID=tuesday-c1\n+BIGTABLE_SPARK_CLUSTER_ZONE=europe-west3-a\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo", "originalCommit": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE3OTkxOA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473179918", "bodyText": "Same as above. Close to the commands that need them. I'll make sure they're in a more proper place once we know what exactly we need and where. OK?", "author": "jaceklaskowski", "createdAt": "2020-08-19T16:54:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2MDA3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NTI1OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473065259", "bodyText": "fix what? please be descriptive, so that we know what needs to be fixed", "author": "igorbernstein2", "createdAt": "2020-08-19T14:18:51Z", "path": "bigtable/spark/build.sbt", "diffHunk": "@@ -0,0 +1,55 @@\n+name := \"bigtable-spark-samples\"\n+\n+version := \"0.1\"\n+\n+scalaVersion := \"2.11.12\"\n+\n+// Versions to match Dataproc\n+val sparkVersion = \"2.4.5\"\n+val hbaseVersion = \"2.2.3\"\n+val bigtableVersion = \"1.15.0\"\n+libraryDependencies ++= Seq(\n+  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % Provided,\n+  \"org.apache.hbase.connectors.spark\" % \"hbase-spark\" % \"1.0.0\" % Provided,\n+  \"com.google.cloud.bigtable\" % \"bigtable-hbase-2.x-hadoop\" % bigtableVersion\n+)\n+\n+val fixes = Seq(\n+  // Fix for Exception: Incompatible Jackson 2.9.2\n+  // Version conflict between HBase and Spark\n+  // Forcing the version to match Spark\n+  // FIXME Would that work with dependencyOverrides?\n+  \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.9.10\",\n+  // Fix for NoClassDefFoundError: org/apache/spark/streaming/dstream/DStream\n+  // when saving a DataFrame\n+  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion % Provided,\n+  // Fix for NoClassDefFoundError: org/apache/hadoop/hbase/fs/HFileSystem\n+  // Why?!?! The example does NOT use them directly!\n+  \"org.apache.hbase\" % \"hbase-server\" % hbaseVersion,\n+  \"org.apache.hbase\" % \"hbase-client\" % hbaseVersion\n+)\n+libraryDependencies ++= fixes\n+\n+// Excluding duplicates for the uber-jar\n+// There are other deps to provide necessary packages\n+excludeDependencies ++= Seq(\n+  ExclusionRule(organization = \"asm\", \"asm\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils\"),\n+  ExclusionRule(organization = \"commons-beanutils\", \"commons-beanutils-core\"),\n+  ExclusionRule(organization = \"org.mortbay.jetty\", \"servlet-api\")\n+)\n+\n+assemblyMergeStrategy in assembly := {\n+  case PathList(\"META-INF\", \"io.netty.versions.properties\") => MergeStrategy.first\n+  case PathList(\"META-INF\", \"MANIFEST.MF\") => MergeStrategy.discard\n+  case PathList(\"mozilla\", \"public-suffix-list.txt\") => MergeStrategy.first\n+  case PathList(\"google\", xs @ _*) => xs match {\n+    case ps @ (x :: xs) if ps.last.endsWith(\".proto\") => MergeStrategy.first\n+    case _ => MergeStrategy.deduplicate\n+  }\n+  case x =>\n+    val oldStrategy = (assemblyMergeStrategy in assembly).value\n+    oldStrategy(x)\n+    // FIXME", "originalCommit": "7d94b3c8e78e6bfba65bdf5e58cf9f357e54a71e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE4MDk4Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r473180983", "bodyText": "Fixed. Left a comment.", "author": "jaceklaskowski", "createdAt": "2020-08-19T16:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzA2NTI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NzU3OA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485897578", "bodyText": "because this will be used as a sample, I'd prefer to keep this a little less verbose. I think it's good to mention it's a workaround, but I would leave out the stack trace.", "author": "kolea2", "createdAt": "2020-09-09T20:18:40Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,61 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.client._\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+// FIXME Explain the purpose of the app\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  var hConf = BigtableConfiguration.configure(projectId, instanceId)\n+  hConf.set(TableOutputFormat.OUTPUT_TABLE, table)\n+\n+  import org.apache.hadoop.mapreduce.Job\n+  val job = Job.getInstance(hConf)\n+  job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n+  hConf = job.getConfiguration\n+\n+  import org.apache.spark.SparkConf\n+  val config = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat\n+  // See https://stackoverflow.com/a/51959451/1305344\n+  // Without the property:", "originalCommit": "ab7c4ee2a76bb1c8027df712999454efdfc72207", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjM1NjM0Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r486356347", "bodyText": "As discussed offline, move this info into a CONTRIBUTING.md doc", "author": "kolea2", "createdAt": "2020-09-10T13:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NzU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5ODE1Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485898152", "bodyText": "it may be hard to maintain this if we upgrade the hbase version, as well as the static line numbers. Is this link necessary?", "author": "kolea2", "createdAt": "2020-09-09T20:19:50Z", "path": "bigtable/spark/src/main/scala/example/Wordcount.scala", "diffHunk": "@@ -0,0 +1,61 @@\n+package example\n+\n+import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+import org.apache.hadoop.hbase.client._\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n+import org.apache.hadoop.hbase.mapreduce.TableOutputFormat\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.spark.SparkContext\n+\n+// FIXME Explain the purpose of the app\n+object Wordcount extends App {\n+\n+  val projectId = args(0)\n+  val instanceId = args(1)\n+  val table = args(2)\n+  val file = args(3)\n+\n+  val ColumnFamily = \"cf\"\n+  val ColumnFamilyBytes = Bytes.toBytes(ColumnFamily)\n+  val ColumnNameBytes = Bytes.toBytes(\"Count\")\n+\n+  var hConf = BigtableConfiguration.configure(projectId, instanceId)\n+  hConf.set(TableOutputFormat.OUTPUT_TABLE, table)\n+\n+  import org.apache.hadoop.mapreduce.Job\n+  val job = Job.getInstance(hConf)\n+  job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n+  hConf = job.getConfiguration\n+\n+  import org.apache.spark.SparkConf\n+  val config = new SparkConf()\n+\n+  // Workaround for a bug in TableOutputFormat\n+  // See https://stackoverflow.com/a/51959451/1305344\n+  // Without the property:\n+  // org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.\n+  // at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:138)\n+  // at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.assertConf(SparkHadoopWriter.scala:393)\n+  // at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n+  // at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)\n+  config.set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n+\n+  val sc = SparkContext.getOrCreate(config)\n+  val wordCounts = sc\n+    .textFile(file)\n+    .flatMap(_.split(\"\\\\W+\"))\n+    .filter(!_.isEmpty)\n+    .map { word => (word, 1) }\n+    .reduceByKey(_ + _)\n+    .map { case (word, count) =>\n+      val put = new Put(Bytes.toBytes(word))\n+        .addColumn(ColumnFamilyBytes, ColumnNameBytes, Bytes.toBytes(count))\n+      // The KEY is ignored while the output value must be either a Put or a Delete instance\n+      // https://github.com/apache/hbase/blob/rel/2.2.3/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java#L46-L48", "originalCommit": "ab7c4ee2a76bb1c8027df712999454efdfc72207", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5ODc2Ng==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485898766", "bodyText": "why --quiet?", "author": "kolea2", "createdAt": "2020-09-09T20:21:00Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,392 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+Set the following environment variable to reference the assembly file.\n+\n+```\n+BIGTABLE_SPARK_ASSEMBLY_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+```\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+SPARK_HOME=your-spark-home\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance\n+\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount\n+BIGTABLE_SPARK_WORDCOUNT_FILE=README.md\n+\n+BIGTABLE_SPARK_COPYTABLE_TABLE=copytable\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+### Create Tables\n+\n+Create the tables using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_COPYTABLE_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+copytable\n+wordcount\n+```\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+324\n+```\n+\n+**TIP** For details about using the `cbt` tool, including a list of available commands, see the [cbt Reference](https://cloud.google.com/bigtable/docs/cbt-reference).\n+\n+### CopyTable\n+\n+The following `spark-submit` uses [example.CopyTable](src/main/scala/example/CopyTable.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.CopyTable \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_COPYTABLE_TABLE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_COPYTABLE_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_COPYTABLE_TABLE\n+324\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Create Cloud Bigtable Instance\n+\n+Create a Cloud Bigtable instance using the Google Cloud Console (as described in the [Create a Cloud Bigtable instance](https://cloud.google.com/bigtable/docs/quickstart-cbt#create-instance)) or `gcloud beta bigtable instances`.\n+\n+```\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-zone-id\n+BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME=your-display-name\n+\n+gcloud beta bigtable instances \\\n+  create $BIGTABLE_SPARK_INSTANCE_ID \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+  --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+  --display-name=$BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME \\\n+  --instance-type=DEVELOPMENT\n+```\n+\n+Check out the available Cloud Bigtable instances using `gcloud beta bigtable instances list` command.\n+\n+```\n+gcloud beta bigtable instances list\n+```\n+\n+### Create Table\n+\n+Create a table using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table. There should be \n+324 rows.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+324\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+Use `cbt listinstances` to list existing Bigtable instances.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+There should be at least `BIGTABLE_SPARK_INSTANCE_ID` instance. Delete it using `cbt deleteinstance`.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/CopyTable.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=your-region\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  --quiet\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=your-instance-id\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-cluster-zone\n+\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE rowkey\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf1\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf2\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf3\n+```\n+\n+### Submit Wordcount Job\n+\n+Submit the Wordcount job to a Cloud Dataproc instance.\n+\n+```\n+gcloud dataproc jobs submit spark \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --class=$BIGTABLE_SPARK_CLASS \\\n+  --jars=$BIGTABLE_SPARK_JAR \\\n+  --properties=spark.jars.packages='org.apache.hbase.connectors.spark:hbase-spark:1.0.0' \\\n+  -- \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+### Verify\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  read $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+### Clean Up\n+\n+Delete the Bigtable instance.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+Delete the Dataproc cluster.\n+\n+```\n+gcloud dataproc clusters delete $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  --quiet", "originalCommit": "ab7c4ee2a76bb1c8027df712999454efdfc72207", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5OTQ2OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485899469", "bodyText": "same concern here with static links", "author": "kolea2", "createdAt": "2020-09-09T20:22:14Z", "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+object CopyTable extends App {\n+\n+  val appName = this.getClass.getSimpleName.replace(\"$\", \"\")\n+  println(s\"$appName Spark application is starting up...\")\n+\n+  val (projectId, instanceId, fromTable, toTable) = parse(args)\n+  println(\n+    s\"\"\"\n+      |Parameters:\n+      |projectId: $projectId\n+      |instanceId: $instanceId\n+      |copy from $fromTable to $toTable\n+      |\"\"\".stripMargin)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().getOrCreate()\n+  println(s\"Spark version: ${spark.version}\")\n+\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  // Creating HBaseContext explicitly to use the conf above\n+  // That's how to use command-line arguments for projectId and instanceId\n+  // Otherwise, we'd have to use hbase-site.xml\n+  // See HBaseSparkConf.USE_HBASECONTEXT option in hbase-connectors project\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala#L44", "originalCommit": "ab7c4ee2a76bb1c8027df712999454efdfc72207", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5OTkzNQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r485899935", "bodyText": "I'd add some additional comments here as to what we're doing (specifically around createCatalogJSON)", "author": "kolea2", "createdAt": "2020-09-09T20:23:13Z", "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+package example\n+\n+import org.apache.hadoop.hbase.spark.datasources.{HBaseSparkConf, HBaseTableCatalog}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+object CopyTable extends App {\n+\n+  val appName = this.getClass.getSimpleName.replace(\"$\", \"\")\n+  println(s\"$appName Spark application is starting up...\")\n+\n+  val (projectId, instanceId, fromTable, toTable) = parse(args)\n+  println(\n+    s\"\"\"\n+      |Parameters:\n+      |projectId: $projectId\n+      |instanceId: $instanceId\n+      |copy from $fromTable to $toTable\n+      |\"\"\".stripMargin)\n+\n+  import org.apache.spark.sql.SparkSession\n+  val spark = SparkSession.builder().getOrCreate()\n+  println(s\"Spark version: ${spark.version}\")\n+\n+  import com.google.cloud.bigtable.hbase.BigtableConfiguration\n+  val conf = BigtableConfiguration.configure(projectId, instanceId)\n+  import org.apache.hadoop.hbase.spark.HBaseContext\n+  // Creating HBaseContext explicitly to use the conf above\n+  // That's how to use command-line arguments for projectId and instanceId\n+  // Otherwise, we'd have to use hbase-site.xml\n+  // See HBaseSparkConf.USE_HBASECONTEXT option in hbase-connectors project\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala#L44\n+  new HBaseContext(spark.sparkContext, conf)\n+\n+  def createCatalogJSON(table: String): String = {\n+    s\"\"\"{\n+       |\"table\":{\"namespace\":\"default\", \"name\":\"$table\", \"tableCoder\":\"PrimitiveType\"},\n+       |\"rowkey\":\"word\",\n+       |\"columns\":{\n+       |  \"word\":{\"cf\":\"rowkey\", \"col\":\"word\", \"type\":\"string\"},\n+       |  \"count\":{\"cf\":\"cf\", \"col\":\"Count\", \"type\":\"int\"}\n+       |}\n+       |}\"\"\".stripMargin\n+  }\n+\n+  // The HBaseTableCatalog options are described in the sources themselves only\n+  // https://github.com/apache/hbase-connectors/blob/rel/1.0.0/spark/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala\n+\n+  println(s\"Loading records from $fromTable\")", "originalCommit": "ab7c4ee2a76bb1c8027df712999454efdfc72207", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzA4Mzc4MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r487083780", "bodyText": "looks good! Only thing I think needs to be added is a cleanup function to remove the data added to the table after it's finished with the assertions.", "author": "kolea2", "createdAt": "2020-09-11T14:30:03Z", "path": "bigtable/spark/src/test/scala/example/IntegrationTest.scala", "diffHunk": "@@ -0,0 +1,43 @@\n+package example\n+\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.models.Query\n+import com.google.bigtable.repackaged.com.google.cloud.bigtable.data.v2.{BigtableDataClient, BigtableDataSettings}\n+import org.scalatest.flatspec._\n+import org.scalatest.matchers._\n+\n+class IntegrationTest extends AnyFlatSpec\n+    with should.Matchers {\n+\n+  def getOrThrowException(envName: String): String = {\n+    sys.env.getOrElse(\n+      envName,\n+      throw new IllegalStateException(s\"Environment variable '$envName' is required to perform this integration test.\"))\n+  }\n+  val projectId = getOrThrowException(\"BIGTABLE_SPARK_PROJECT_ID\")\n+  val instanceId = getOrThrowException(\"BIGTABLE_SPARK_INSTANCE_ID\")\n+  val table_wordcount = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_TABLE\")\n+  val file = getOrThrowException(\"BIGTABLE_SPARK_WORDCOUNT_FILE\")\n+  val table_copytable = getOrThrowException(\"BIGTABLE_SPARK_COPYTABLE_TABLE\")\n+  val rowCount = getOrThrowException(\"BIGTABLE_SPARK_ROW_COUNT\").toInt\n+\n+  \"IntegrationTest\" should \"write records to Bigtable, copy them between tables\" in {\n+    import org.apache.spark.{SparkConf, SparkContext}\n+    val appName = getClass.getSimpleName.replace(\"$\", \"\")\n+    val config = new SparkConf().setMaster(\"local[*]\").setAppName(appName)\n+    SparkContext.getOrCreate(config)\n+\n+    val wordcountArgs = Array(projectId, instanceId, table_wordcount, file)\n+    Wordcount.main(wordcountArgs)\n+    val copytableArgs = Array(projectId, instanceId, table_wordcount, table_copytable)\n+    CopyTable.main(copytableArgs)\n+\n+    val settings =\n+      BigtableDataSettings.newBuilder().setProjectId(projectId).setInstanceId(instanceId).build()\n+    val dataClient = BigtableDataClient.create(settings)\n+    import collection.JavaConverters._\n+    val wordcountRowCount = dataClient.readRows(Query.create(table_wordcount)).iterator().asScala.length\n+    val copytableRowCount = dataClient.readRows(Query.create(table_copytable)).iterator().asScala.length\n+    wordcountRowCount should be(rowCount)\n+    wordcountRowCount should be(copytableRowCount)", "originalCommit": "167dc628b0ebb5f1855a125e3818f959d494df0e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "24f1ebda25fe6e6c66b57b04f9e4306045b1ee1a", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/24f1ebda25fe6e6c66b57b04f9e4306045b1ee1a", "message": "[WIP] Spark Applications for Cloud Bigtable", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "226ec5b00ad8c860ce2c772fa9e3bda2771e935b", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/226ec5b00ad8c860ce2c772fa9e3bda2771e935b", "message": "Loading and saving using DataFrame API", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "2720e7176bc8a8495ee8e4af8421baa7514445a1", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2720e7176bc8a8495ee8e4af8421baa7514445a1", "message": "Writing data using RDD API to Bigtable", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "326f31e087813f8e941ac29e52dbbfdf81a87d50", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/326f31e087813f8e941ac29e52dbbfdf81a87d50", "message": "Addressing comments after a code review", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "b5c505960de92c2b518767c7fb9faa52dda0b868", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b5c505960de92c2b518767c7fb9faa52dda0b868", "message": "Submit DataFrameDemo to Cloud Dataproc", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "b0f61a24ad9c6755199640e5efa60fd61c1babbb", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b0f61a24ad9c6755199640e5efa60fd61c1babbb", "message": "After code review", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "7cad5dd859f72bc15157caf136d26c3f865751ec", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7cad5dd859f72bc15157caf136d26c3f865751ec", "message": "After code review", "committedDate": "2020-09-17T12:48:16Z", "type": "commit"}, {"oid": "212becb859c29f308cea96790061bef22539c774", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/212becb859c29f308cea96790061bef22539c774", "message": "[Wordcount] Use Admin API to create tables", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "f08e91054d204786b6f6ff98baa379ecace992b2", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/f08e91054d204786b6f6ff98baa379ecace992b2", "message": "Command-line options + testing framework", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "55be0bf3d55f30ddbbbafc7e4c1530fa46ca2f22", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/55be0bf3d55f30ddbbbafc7e4c1530fa46ca2f22", "message": "Add the other cmd options + exclude tests in assembly", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "c76467280531af29c247ca810f1b89c2acfb46cd", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/c76467280531af29c247ca810f1b89c2acfb46cd", "message": "Tasts marked done (and removed)", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "46679884dae9a338c6701a852f9ae07522b5e763", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/46679884dae9a338c6701a852f9ae07522b5e763", "message": "No need for extra dep. Bye, bye scopt", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "70657d61a871b9e213e74e7aa2729746003cadc3", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/70657d61a871b9e213e74e7aa2729746003cadc3", "message": "No need for xml config. Command-line arguments enough", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "b269353d9222c19e013ec62b14890a831d1dbd21", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/b269353d9222c19e013ec62b14890a831d1dbd21", "message": "Create Tables before running demos", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "8761674d8b42194fffeb3f39defbc8cf430b76d1", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/8761674d8b42194fffeb3f39defbc8cf430b76d1", "message": "CopyTable - Copying tables", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "2ee3dfcce550907b8b2251e900b55801e965ddd9", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2ee3dfcce550907b8b2251e900b55801e965ddd9", "message": "Integration test", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "409ca69856f4398021806002bf02c054dda31799", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/409ca69856f4398021806002bf02c054dda31799", "message": "Assert row count in integration test", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "a3d096fee60175181d78a32b6595874f318b489b", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/a3d096fee60175181d78a32b6595874f318b489b", "message": "BIGTABLE_SPARK_ROW_COUNT for IT test to assert the row count", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "21ff69dd42fbbd1d01db3512f9bf87f7ad0790d4", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/21ff69dd42fbbd1d01db3512f9bf87f7ad0790d4", "message": "Use \"stable\" file for integration test", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "20393ed52ba8d41397ac252bad7a85eb6f857f3d", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/20393ed52ba8d41397ac252bad7a85eb6f857f3d", "message": "Running Integration Test", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "fe3c6e5371fe3f5fca88eaac37144841d350a6c9", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fe3c6e5371fe3f5fca88eaac37144841d350a6c9", "message": "License headers", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fd8027b9f4767610c446f99ff0006d267ec0401c", "message": "Fix the last round of style comments", "committedDate": "2020-09-17T12:48:17Z", "type": "commit"}, {"oid": "fd8027b9f4767610c446f99ff0006d267ec0401c", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fd8027b9f4767610c446f99ff0006d267ec0401c", "message": "Fix the last round of style comments", "committedDate": "2020-09-17T12:48:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMyODY1NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r492328655", "bodyText": "Please find the format for headers here (same comment for other files): https://github.com/GoogleCloudPlatform/java-docs-samples#source-code-headers", "author": "kolea2", "createdAt": "2020-09-21T20:31:38Z", "path": "bigtable/spark/src/main/scala/example/CopyTable.scala", "diffHunk": "@@ -0,0 +1,115 @@\n+/*", "originalCommit": "fd8027b9f4767610c446f99ff0006d267ec0401c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUyNzQ4Mg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r492527482", "bodyText": "Made some changes. Can you check out the latest version of the PR? Thanks.", "author": "jaceklaskowski", "createdAt": "2020-09-22T07:32:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMyODY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMzMDEwMQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r492330101", "bodyText": "this is not left as an instruction on what to set as - maybe switch this to BIGTABLE_SPARK_CLUSTER_ID, which is already defined elsewhere?", "author": "kolea2", "createdAt": "2020-09-21T20:34:33Z", "path": "bigtable/spark/README.md", "diffHunk": "@@ -0,0 +1,458 @@\n+# Spark Applications for Cloud Bigtable\n+\n+## Overview\n+\n+The project shows how to read data from or write data to [Cloud Bigtable](https://cloud.google.com/bigtable) using [Apache Spark](https://spark.apache.org/) and [Apache HBase\u2122 Spark Connector](https://github.com/apache/hbase-connectors/tree/master/spark).\n+\n+**Apache Spark** is the execution environment that can distribute and parallelize data processing (loading data from and writing data to various data sources).\n+Apache Spark provides DataSource API for external systems to plug into as data sources (also known as data providers).\n+\n+**Apache HBase\u2122 Spark Connector** implements the DataSource API for Apache HBase and allows executing relational queries on data stored in Cloud Bigtable.\n+\n+**Google Cloud Bigtable** is a fully-managed cloud service for a NoSQL database of petabyte-scale and large analytical and operational workloads.\n+`bigtable-hbase-2.x-hadoop` provides a bridge from the HBase API to Cloud Bigtable that allows Spark queries to interact with Bigtable using the native Spark API.\n+\n+**Google Cloud Dataproc** is a fully-managed cloud service for running [Apache Spark](https://spark.apache.org/) applications and [Apache Hadoop](https://hadoop.apache.org/) clusters.\n+\n+## Tasks\n+\n+FIXME Remove the section once all tasks done.\n+\n+- [ ] Avoid specifying dependencies at runtime (remove `--packages` option for `spark-submit`)\n+- [ ] Make sure README.md is up-to-date before claiming the PR done\n+\n+## Prerequisites\n+\n+1. [Google Cloud project](https://console.cloud.google.com/)\n+\n+1. [Google Cloud SDK](https://cloud.google.com/sdk/) installed.\n+\n+1. [sbt](https://www.scala-sbt.org/) installed.\n+\n+1. [Apache Spark](https://spark.apache.org/) installed. Download Spark built for Scala 2.11.\n+\n+1. A basic familiarity with [Apache Spark](https://spark.apache.org/) and [Scala](https://www.scala-lang.org/).\n+\n+## Assemble the Examples\n+\n+Execute the following `sbt` command to assemble the sample applications as a single uber/fat jar (with all of its dependencies and configuration).\n+\n+```\n+sbt clean assembly\n+```\n+\n+The above command should build `target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar` file.\n+\n+Set the following environment variable to reference the assembly file.\n+\n+```\n+BIGTABLE_SPARK_ASSEMBLY_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+```\n+\n+## Run Examples with Bigtable Emulator\n+\n+Start the Bigtable Emulator.\n+\n+```\n+gcloud beta emulators bigtable start\n+```\n+\n+Set the following environment variables for the sample applications to use:\n+\n+```\n+SPARK_HOME=your-spark-home\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance\n+\n+BIGTABLE_SPARK_WORDCOUNT_TABLE=wordcount\n+BIGTABLE_SPARK_WORDCOUNT_FILE=src/test/resources/Romeo-and-Juliet-prologue.txt\n+\n+BIGTABLE_SPARK_COPYTABLE_TABLE=copytable\n+```\n+\n+Initialize the environment to point to the Bigtable Emulator.\n+\n+```\n+$(gcloud beta emulators bigtable env-init)\n+```\n+\n+### Create Tables\n+\n+Create the tables using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_COPYTABLE_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+copytable\n+wordcount\n+```\n+\n+### Wordcount\n+\n+The following `spark-submit` uses [example.Wordcount](src/main/scala/example/Wordcount.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+88\n+```\n+\n+**TIP** For details about using the `cbt` tool, including a list of available commands, see the [cbt Reference](https://cloud.google.com/bigtable/docs/cbt-reference).\n+\n+### CopyTable\n+\n+The following `spark-submit` uses [example.CopyTable](src/main/scala/example/CopyTable.scala).\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.CopyTable \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_COPYTABLE_TABLE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_COPYTABLE_TABLE` table.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_COPYTABLE_TABLE\n+88\n+```\n+\n+## Run Wordcount with Cloud Bigtable\n+\n+### Create Cloud Bigtable Instance\n+\n+Create a Cloud Bigtable instance using the Google Cloud Console (as described in the [Create a Cloud Bigtable instance](https://cloud.google.com/bigtable/docs/quickstart-cbt#create-instance)) or `gcloud beta bigtable instances`.\n+\n+```\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-zone-id\n+BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME=your-display-name\n+\n+gcloud beta bigtable instances \\\n+  create $BIGTABLE_SPARK_INSTANCE_ID \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+  --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+  --display-name=$BIGTABLE_SPARK_INSTANCE_DISPLAY_NAME \\\n+  --instance-type=DEVELOPMENT\n+```\n+\n+Check out the available Cloud Bigtable instances and make sure yours is listed.\n+\n+```\n+gcloud beta bigtable instances list\n+```\n+\n+### Create Table\n+\n+Create a table using `cbt createtable` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_WORDCOUNT_TABLE \\\n+  \"families=cf\"\n+```\n+\n+List tables using `cbt ls` command.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+### Submit Wordcount\n+\n+```\n+$SPARK_HOME/bin/spark-submit \\\n+  --packages org.apache.hbase.connectors.spark:hbase-spark:1.0.0 \\\n+  --class example.Wordcount \\\n+  $BIGTABLE_SPARK_ASSEMBLY_JAR \\\n+  $BIGTABLE_SPARK_PROJECT_ID $BIGTABLE_SPARK_INSTANCE_ID \\\n+  $BIGTABLE_SPARK_WORDCOUNT_TABLE $BIGTABLE_SPARK_WORDCOUNT_FILE\n+```\n+\n+### Verify\n+\n+Use `cbt count` to count the number of rows in the `BIGTABLE_SPARK_WORDCOUNT_TABLE` table. There should be \n+88 rows.\n+\n+```\n+$ cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  count $BIGTABLE_SPARK_WORDCOUNT_TABLE\n+88\n+```\n+\n+### Delete Cloud Bigtable Instance\n+\n+Use `cbt listinstances` to list existing Bigtable instances.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  listinstances\n+```\n+\n+There should be at least `BIGTABLE_SPARK_INSTANCE_ID` instance. Delete it using `cbt deleteinstance`.\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  deleteinstance $BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+## Submit DataFrameDemo to Cloud Dataproc\n+\n+This section describes the steps to submit [DataFrameDemo](src/main/scala/example/CopyTable.scala) application to [Google Cloud Dataproc](https://cloud.google.com/dataproc/).\n+\n+**TIP** Read [Quickstart using the gcloud command-line tool](https://cloud.google.com/dataproc/docs/quickstarts/quickstart-gcloud) that shows how to use the Google Cloud SDK `gcloud` command-line tool to create a Google Cloud Dataproc cluster and more.\n+\n+### Configure Environment\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+BIGTABLE_SPARK_INSTANCE_ID=your-bigtable-instance-id\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=europe-west4\n+BIGTABLE_SPARK_JAR=target/scala-2.11/bigtable-spark-samples-assembly-0.1.jar\n+BIGTABLE_SPARK_CLASS=example.DataFrameDemo\n+BIGTABLE_SPARK_TABLE=DataFrameDemo\n+```\n+\n+**NOTE** `BIGTABLE_SPARK_REGION` should point to your region. Read [Available regions and zones](https://cloud.google.com/compute/docs/regions-zones#available) in the official documentation.\n+\n+### Authenticate\n+\n+Authenticate to a Google Cloud Platform API using service or user accounts.\n+Learn about [authenticating to a GCP API](https://cloud.google.com/docs/authentication/) in the Google Cloud documentation.\n+\n+**NOTE**: In most situations, we recommend [authenticating as a service account](https://cloud.google.com/docs/authentication/production) to a Google Cloud Platform (GCP) API.\n+\n+```\n+GOOGLE_APPLICATION_CREDENTIALS=/your/service/account.json\n+```\n+\n+### Create Google Cloud Dataproc Cluster\n+\n+```\n+BIGTABLE_SPARK_DATAPROC_CLUSTER=spark-cluster\n+BIGTABLE_SPARK_REGION=your-region\n+BIGTABLE_SPARK_PROJECT_ID=your-project-id\n+\n+gcloud dataproc clusters create $BIGTABLE_SPARK_DATAPROC_CLUSTER \\\n+  --region=$BIGTABLE_SPARK_REGION \\\n+  --project=$BIGTABLE_SPARK_PROJECT_ID\n+```\n+\n+### Configure Cloud Bigtable\n+\n+```\n+BIGTABLE_SPARK_INSTANCE_ID=your-instance-id\n+BIGTABLE_SPARK_CLUSTER_ID=your-cluster-id\n+BIGTABLE_SPARK_CLUSTER_ZONE=your-cluster-zone\n+\n+gcloud bigtable instances create $BIGTABLE_SPARK_INSTANCE_ID \\\n+    --cluster=$BIGTABLE_SPARK_CLUSTER_ID \\\n+    --cluster-zone=$BIGTABLE_SPARK_CLUSTER_ZONE \\\n+    --display-name=$BIGTABLE_SPARK_INSTANCE_ID\n+```\n+\n+```\n+BIGTABLE_SPARK_PROJECT_ID=bigtable-spark-connector\n+BIGTABLE_SPARK_DataFrameDemo_TABLE=DataFrameDemo\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createtable $BIGTABLE_SPARK_DataFrameDemo_TABLE\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  ls\n+```\n+\n+```\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE rowkey\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf1\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf2\n+\n+cbt \\\n+  -project=$BIGTABLE_SPARK_PROJECT_ID \\\n+  -instance=$BIGTABLE_SPARK_INSTANCE_ID \\\n+  createfamily $BIGTABLE_SPARK_DataFrameDemo_TABLE cf3\n+```\n+\n+### Submit Wordcount Job\n+\n+Submit the Wordcount job to a Cloud Dataproc instance.\n+\n+```\n+gcloud dataproc jobs submit spark \\\n+  --cluster=$BIGTABLE_SPARK_CLUSTER \\", "originalCommit": "fd8027b9f4767610c446f99ff0006d267ec0401c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2c30372a2af6058b5bc6a375ab45d54f19003e46", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/2c30372a2af6058b5bc6a375ab45d54f19003e46", "message": "Deps to match Dataproc 1.4", "committedDate": "2020-09-21T20:37:03Z", "type": "commit"}, {"oid": "4ac6d5994a9ddfcbde0623ef09006da3e7912ad2", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/4ac6d5994a9ddfcbde0623ef09006da3e7912ad2", "message": "No need for supersafe", "committedDate": "2020-09-22T07:16:37Z", "type": "commit"}, {"oid": "6535386614043e901f45ee696ce90ca0345c7f69", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/6535386614043e901f45ee696ce90ca0345c7f69", "message": "Source Code Headers\n\nAs per the official doc at https://github.com/GoogleCloudPlatform/java-docs-samples#source-code-headers", "committedDate": "2020-09-22T07:29:26Z", "type": "commit"}, {"oid": "ed22800dffc4866a9bda93afda6e11f217fc778e", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/ed22800dffc4866a9bda93afda6e11f217fc778e", "message": "Run Wordcount with Cloud Bigtable verified to work", "committedDate": "2020-09-22T08:52:20Z", "type": "commit"}, {"oid": "179badeab9fc212d909ccda649217386fa02fa56", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/179badeab9fc212d909ccda649217386fa02fa56", "message": "Run Wordcount with Cloud Dataproc", "committedDate": "2020-09-22T10:13:50Z", "type": "commit"}, {"oid": "fbc31e14cce818656ec096c888f95e409feb847d", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/fbc31e14cce818656ec096c888f95e409feb847d", "message": "Removing HBase version dep", "committedDate": "2020-09-22T15:30:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NjM0Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r493976347", "bodyText": "please fix extra space here (after https://)", "author": "kolea2", "createdAt": "2020-09-24T00:46:10Z", "path": "bigtable/spark/src/test/resources/log4j.properties", "diffHunk": "@@ -0,0 +1,18 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https:// www.apache.org/licenses/LICENSE-2.0", "originalCommit": "fbc31e14cce818656ec096c888f95e409feb847d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3NjUxMA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/3487#discussion_r493976510", "bodyText": "please fix extra space here (after https://)", "author": "kolea2", "createdAt": "2020-09-24T00:46:46Z", "path": "bigtable/spark/project/build.properties", "diffHunk": "@@ -0,0 +1,15 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https:// www.apache.org/licenses/LICENSE-2.0", "originalCommit": "fbc31e14cce818656ec096c888f95e409feb847d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0c2fff88b0e05faa57f3a01a5ca3701367e5d9c6", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/0c2fff88b0e05faa57f3a01a5ca3701367e5d9c6", "message": "Licence headers", "committedDate": "2020-09-24T08:27:16Z", "type": "commit"}, {"oid": "e3fd0ae2913ee8cba85a2ed19843e9f7785b1d4e", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/e3fd0ae2913ee8cba85a2ed19843e9f7785b1d4e", "message": "README", "committedDate": "2020-09-24T08:29:14Z", "type": "commit"}]}