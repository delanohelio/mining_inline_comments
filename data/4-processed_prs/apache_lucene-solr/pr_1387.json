{"pr_number": 1387, "pr_title": "SOLR-14210: Include replica health in healtcheck handler", "pr_createdAt": "2020-03-29T01:15:53Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1387", "timeline": [{"oid": "24e73f7849b64d568722ee505a3bbdae93c3b044", "url": "https://github.com/apache/lucene-solr/commit/24e73f7849b64d568722ee505a3bbdae93c3b044", "message": "SOLR-14210: Include replica health in healtcheck handler", "committedDate": "2020-03-29T01:10:13Z", "type": "commit"}, {"oid": "4f10225e5fac1f4ad3aaf1b37b293f145b9b105e", "url": "https://github.com/apache/lucene-solr/commit/4f10225e5fac1f4ad3aaf1b37b293f145b9b105e", "message": "Never cache", "committedDate": "2020-03-29T01:14:59Z", "type": "commit"}, {"oid": "49752a8716ade9451b6eccb627672ffd12e67f1c", "url": "https://github.com/apache/lucene-solr/commit/49752a8716ade9451b6eccb627672ffd12e67f1c", "message": "Check Replicas, not Slices", "committedDate": "2020-03-29T20:29:57Z", "type": "commit"}, {"oid": "e09ef6510a8a44d7b5694c350d13ddbbb668a56a", "url": "https://github.com/apache/lucene-solr/commit/e09ef6510a8a44d7b5694c350d13ddbbb668a56a", "message": "Move HealthCheckHandlerTest to correct package\nCreate a test for findUnhealthyCores, factoring it into a separate method\nRename param as 'requireHealthyCores'", "committedDate": "2020-03-29T21:52:27Z", "type": "commit"}, {"oid": "1464a253d5a768c31d592e8cf5974e0548af6103", "url": "https://github.com/apache/lucene-solr/commit/1464a253d5a768c31d592e8cf5974e0548af6103", "message": "Add explicit message to response to state that all cores are healthy", "committedDate": "2020-03-29T23:00:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r400009116", "bodyText": "Not sure about performance here if you have hundreds of collections and thousands of replicas in a large cluster.\nThe alternative is to instead iterate cores on current node, and consult with clusterState their overall state.", "author": "janhoy", "createdAt": "2020-03-30T08:22:56Z", "path": "solr/core/src/java/org/apache/solr/handler/admin/HealthCheckHandler.java", "diffHunk": "@@ -88,15 +95,42 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw\n       return;\n     }\n \n-    // Set status to true if this node is in live_nodes\n-    if (clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n-      rsp.add(STATUS, OK);\n-    } else {\n+    // Fail if not in live_nodes\n+    if (!clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n       rsp.add(STATUS, FAILURE);\n       rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Host Unavailable: Not in live nodes as per zk\"));\n+      return;\n     }\n \n-    rsp.setHttpCaching(false);\n+    // Optionally require that all cores on this node are active if param 'failWhenRecovering=true'\n+    if (req.getParams().getBool(PARAM_REQUIRE_HEALTHY_CORES, false)) {\n+      List<String> unhealthyCores = findUnhealthyCores(clusterState, cores.getNodeConfig().getNodeName());\n+      if (unhealthyCores.size() > 0) {\n+          rsp.add(STATUS, FAILURE);\n+          rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE,\n+                  \"Replica(s) \" + unhealthyCores + \" are currently initializing or recovering\"));\n+          return;\n+      }\n+      rsp.add(\"MESSAGE\", \"All cores are healthy\");\n+    }\n+\n+    // All lights green, report healthy\n+    rsp.add(STATUS, OK);\n+  }\n+\n+  /**\n+   * Find replicas DOWN or RECOVERING\n+   * @param clusterState clusterstate from ZK\n+   * @param nodeName this node name\n+   * @return list of core names that are either DOWN ore RECOVERING on 'nodeName'\n+   */\n+  static List<String> findUnhealthyCores(ClusterState clusterState, String nodeName) {\n+    return clusterState.getCollectionsMap().values().stream()", "originalCommit": "1464a253d5a768c31d592e8cf5974e0548af6103", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyNjAxOQ==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r400326019", "bodyText": "This should still be relatively fast with hundreds of collections and thousands of replicas.\nBut it would be nice to get some performance tests before this gets merged in.\nOne question I have, since I'm not too familiar with \"active\" slices. \"Inactive\" slices are the new shards from a shard split that has not completed yet, right? If so maybe we want to return false if there are any replicas from inactive slices on the node. Otherwise taking the node down could possibly hamper a shard-split.\nPlease correct me if I'm wrong on any of those statements.", "author": "HoustonPutman", "createdAt": "2020-03-30T16:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyODM0MQ==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r400328341", "bodyText": "Also if the clusterState thinks that cores live on this node, but the core directories do not exist, then I think that this handler should respond not healthy. Therefore I think we need to go with the clusterState method, not iterate cores on the current node.", "author": "HoustonPutman", "createdAt": "2020-03-30T16:29:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQyNDUyNg==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401424526", "bodyText": "maybe we want to return false if there are any replicas from inactive slices on the node\n\nInactive shards are not searched, so we should not care about those. A shard split will not clean up the old shard, but instead mark it inactive, until user manually deletes those shards, or the Autoscaling framework rules go reap them. That is why I chose to check active shards only. We should be ok if the active shard(s) only are up and active, then k8s can go restart the next node.\nIf a shard split is currently running (could be long running), on a node being restarted, the split would be aborted but when the node comes up again I believe the overseer might try again??", "author": "janhoy", "createdAt": "2020-04-01T07:58:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQyNTE4MA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401425180", "bodyText": "Also if the clusterState thinks that cores live on this node, but the core directories do not exist, then I think that this handler should respond not healthy.\n\nYes, we can add an extra check that for each replica in clusterstate for node, we check that it exists locally?", "author": "janhoy", "createdAt": "2020-04-01T07:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY5MTg3NA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401691874", "bodyText": "Also if the clusterState thinks that cores live on this node, but the core directories do not exist, then I think that this handler should respond not healthy.\n\nYes, we can add an extra check that for each replica in clusterstate for node, we check that it exists locally?\n\nI think that the logic as it stands now should work, because the cluster state will report the replica as \"DOWN\" (From my experience, but we can also add tests around this). The comment was meant to validate the current approach over the other one you mentioned:\n\nThe alternative is to instead iterate cores on current node, and consult with clusterState their overall state.", "author": "HoustonPutman", "createdAt": "2020-04-01T15:11:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTcwODk4MA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401708980", "bodyText": "If a shard split is currently running (could be long running), on a node being restarted, the split would be aborted but when the node comes up again I believe the overseer might try again??\n\nAnd I understand your decision for the active shards better now. As long as the overseer thing is true, then we should be fine. And if in the future if we need to, we can add another parameter to fail on inactive slices as well.", "author": "HoustonPutman", "createdAt": "2020-04-01T15:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAwOTExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyNjA0OQ==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r400326049", "bodyText": "probably easier to use this in a try with resources block, so that it closes even if something fails.", "author": "HoustonPutman", "createdAt": "2020-03-30T16:26:10Z", "path": "solr/core/src/test/org/apache/solr/handler/admin/HealthCheckHandlerTest.java", "diffHunk": "@@ -177,4 +183,13 @@ public void testHealthCheckV2Api() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testFindUnhealthyCores() throws Exception {\n+    ZkStateReader reader = ClusterStateMockUtil.buildClusterState(\"csrr2rDcsr2rR\", 1, 1, \"node1\", \"node2\");", "originalCommit": "1464a253d5a768c31d592e8cf5974e0548af6103", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQzMTMxMA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401431310", "bodyText": "Will do", "author": "janhoy", "createdAt": "2020-04-01T08:10:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyNjA0OQ=="}], "type": "inlineReview"}, {"oid": "7ae227a036f69cf31b1e5d793969caf0e485c890", "url": "https://github.com/apache/lucene-solr/commit/7ae227a036f69cf31b1e5d793969caf0e485c890", "message": "Merge branch 'master' into solr14210-replica-health", "committedDate": "2020-04-01T07:51:51Z", "type": "commit"}, {"oid": "4258c5b8dc44f03eda45edb241d1f2a819d432a1", "url": "https://github.com/apache/lucene-solr/commit/4258c5b8dc44f03eda45edb241d1f2a819d432a1", "message": "Review comments", "committedDate": "2020-04-01T08:11:37Z", "type": "commit"}, {"oid": "80c5bfa06f059d74cc35641863901db3b4397553", "url": "https://github.com/apache/lucene-solr/commit/80c5bfa06f059d74cc35641863901db3b4397553", "message": "Javadoc", "committedDate": "2020-04-01T12:05:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTczNzIwMA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r401737200", "bodyText": "requireHealthyCores not failWhenRecovering", "author": "HoustonPutman", "createdAt": "2020-04-01T16:13:09Z", "path": "solr/core/src/java/org/apache/solr/handler/admin/HealthCheckHandler.java", "diffHunk": "@@ -88,15 +96,46 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw\n       return;\n     }\n \n-    // Set status to true if this node is in live_nodes\n-    if (clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n-      rsp.add(STATUS, OK);\n-    } else {\n+    // Fail if not in live_nodes\n+    if (!clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n       rsp.add(STATUS, FAILURE);\n       rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Host Unavailable: Not in live nodes as per zk\"));\n+      return;\n     }\n \n-    rsp.setHttpCaching(false);\n+    // Optionally require that all cores on this node are active if param 'failWhenRecovering=true'", "originalCommit": "80c5bfa06f059d74cc35641863901db3b4397553", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "643e26e4f7d94b5654dd00e73e3f1b2127fd84c8", "url": "https://github.com/apache/lucene-solr/commit/643e26e4f7d94b5654dd00e73e3f1b2127fd84c8", "message": "Merge branch 'master' into solr14210-replica-health", "committedDate": "2020-04-01T22:42:48Z", "type": "commit"}, {"oid": "d625eed7dba297487569541c8d57be4958cf5387", "url": "https://github.com/apache/lucene-solr/commit/d625eed7dba297487569541c8d57be4958cf5387", "message": "Fix review comment", "committedDate": "2020-04-01T22:43:41Z", "type": "commit"}, {"oid": "5abe3bf14d9d0b1be28be96105c5d6b0ddde5f80", "url": "https://github.com/apache/lucene-solr/commit/5abe3bf14d9d0b1be28be96105c5d6b0ddde5f80", "message": "lowercase message key", "committedDate": "2020-04-01T22:54:44Z", "type": "commit"}, {"oid": "4e82a75c00efd0eeaf45d51abcb8c7e8f1328b1b", "url": "https://github.com/apache/lucene-solr/commit/4e82a75c00efd0eeaf45d51abcb8c7e8f1328b1b", "message": "Credit to Houston", "committedDate": "2020-04-01T23:01:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjMwMzk2Ng==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r402303966", "bodyText": "Why does this need to go to cluster state? This becomes a very expensive method when you have a lot of collections because it fetches collection states for all collections from ZK even if those collections have no replicas on the current node. Imagine having 300 collections and doing 299 zk read operations just for a health check if the node hosts replicas for one collection only.\nI think this method should be rewritten to iterate over the CloudDescriptors of all local cores and check for hasRegistered == true and lastPublished == ACTIVE. Those two should be sufficient for a health check functionality. We don't even need to consult the cluster state.", "author": "shalinmangar", "createdAt": "2020-04-02T13:16:50Z", "path": "solr/core/src/java/org/apache/solr/handler/admin/HealthCheckHandler.java", "diffHunk": "@@ -88,15 +96,46 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw\n       return;\n     }\n \n-    // Set status to true if this node is in live_nodes\n-    if (clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n-      rsp.add(STATUS, OK);\n-    } else {\n+    // Fail if not in live_nodes\n+    if (!clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n       rsp.add(STATUS, FAILURE);\n       rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Host Unavailable: Not in live nodes as per zk\"));\n+      return;\n     }\n \n-    rsp.setHttpCaching(false);\n+    // Optionally require that all cores on this node are active if param 'requireHealthyCores=true'\n+    if (req.getParams().getBool(PARAM_REQUIRE_HEALTHY_CORES, false)) {\n+      List<String> unhealthyCores = findUnhealthyCores(clusterState,\n+              cores.getNodeConfig().getNodeName(),\n+              cores.getAllCoreNames());\n+      if (unhealthyCores.size() > 0) {\n+          rsp.add(STATUS, FAILURE);\n+          rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE,\n+                  \"Replica(s) \" + unhealthyCores + \" are currently initializing or recovering\"));\n+          return;\n+      }\n+      rsp.add(\"message\", \"All cores are healthy\");\n+    }\n+\n+    // All lights green, report healthy\n+    rsp.add(STATUS, OK);\n+  }\n+\n+  /**\n+   * Find replicas DOWN or RECOVERING, or replicas in clusterstate that do not exist on local node\n+   * @param clusterState clusterstate from ZK\n+   * @param nodeName this node name\n+   * @param allCoreNames list of all core names on current node\n+   * @return list of core names that are either DOWN ore RECOVERING on 'nodeName'\n+   */\n+  static List<String> findUnhealthyCores(ClusterState clusterState, String nodeName, Collection<String> allCoreNames) {\n+    return clusterState.getCollectionsMap().values().stream()", "originalCommit": "4e82a75c00efd0eeaf45d51abcb8c7e8f1328b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjM0NDAyNQ==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r402344025", "bodyText": "I assumed ClusterState object in each node is cached on the node and iterating it will not incur any new ZK calls, but it is uptated by watches? If it incurs connections then I agree with you!\nI want to exclude replicas of inactive shards from the check. The only place I could find that info was in Slice inside Clusterstate. Sure, I can iterate each core on local host, find its Slice-ID and then go lookup the Slice in clusterstate to find whether it's active, that was my other alternative but more code.", "author": "janhoy", "createdAt": "2020-04-02T14:11:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjMwMzk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjM3MjE2OA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r402372168", "bodyText": "The cluster state is a shell object that holds individual collection states that each live in different znodes. Each node watches only those collection states for which it hosts a replica. The rest of the collections exist as a lazy reference which is populated by a live read. The getCollectionsMap() method calls CollectionRef.get() for all collections so it will cause a live read to zk for all lazy references. The lazy reference can optionally cache the fetched state for 2 seconds (if you call CollectionRef.get(true)) but that too is too short an interval for a health check.\n\nI want to exclude replicas of inactive shards from the check. The only place I could find that info was in Slice inside Clusterstate.\n\nIt's more code but it is a good idea for sure. Your idea of skipping recovery_failed cores from the health check is also sound.\nThanks for taking this up!", "author": "shalinmangar", "createdAt": "2020-04-02T14:47:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjMwMzk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjM4MjA0MA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r402382040", "bodyText": "Thanks, this is super valuable. Will switch the loop to only fetch info for collection/replicas that reside on the node!", "author": "janhoy", "createdAt": "2020-04-02T14:59:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjMwMzk2Ng=="}], "type": "inlineReview"}, {"oid": "996da9cae71d9c35ea98a674854f0f315dccdfd0", "url": "https://github.com/apache/lucene-solr/commit/996da9cae71d9c35ea98a674854f0f315dccdfd0", "message": "Optimize testFindUnhealthyCores() to consult local state and only check clusterstate for local collections", "committedDate": "2020-04-03T13:01:24Z", "type": "commit"}, {"oid": "25b25a64579360822bf7cee19bdcec6f8c2b0629", "url": "https://github.com/apache/lucene-solr/commit/25b25a64579360822bf7cee19bdcec6f8c2b0629", "message": "Typo", "committedDate": "2020-04-03T14:26:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403045311", "bodyText": "@shalinmangar Please have a look at the new logic. If all local cores are active and registered, then we do not consult clusterstate at all. And we only consult clusterstate now to filter out replicas from inactive shards.\nThe only thing I'm unsure of now is whether I feed the correct slice ID to .getActiveSlicesMap().containsKey(**HERE**). Is this slice ID the same as c.getShardId() on CloudDescriptor? My unit test is a mock, so I cannot be sure :)", "author": "janhoy", "createdAt": "2020-04-03T14:28:25Z", "path": "solr/core/src/java/org/apache/solr/handler/admin/HealthCheckHandler.java", "diffHunk": "@@ -88,15 +98,45 @@ public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throw\n       return;\n     }\n \n-    // Set status to true if this node is in live_nodes\n-    if (clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n-      rsp.add(STATUS, OK);\n-    } else {\n+    // Fail if not in live_nodes\n+    if (!clusterState.getLiveNodes().contains(cores.getZkController().getNodeName())) {\n       rsp.add(STATUS, FAILURE);\n       rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, \"Host Unavailable: Not in live nodes as per zk\"));\n+      return;\n     }\n \n-    rsp.setHttpCaching(false);\n+    // Optionally require that all cores on this node are active if param 'requireHealthyCores=true'\n+    if (req.getParams().getBool(PARAM_REQUIRE_HEALTHY_CORES, false)) {\n+      Collection<CloudDescriptor> coreDescriptors = cores.getCores().stream()\n+          .map(c -> c.getCoreDescriptor().getCloudDescriptor()).collect(Collectors.toList());\n+      List<String> unhealthyCores = findUnhealthyCores(coreDescriptors, clusterState);\n+      if (unhealthyCores.size() > 0) {\n+          rsp.add(STATUS, FAILURE);\n+          rsp.setException(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE,\n+                  \"Replica(s) \" + unhealthyCores + \" are currently initializing or recovering\"));\n+          return;\n+      }\n+      rsp.add(\"message\", \"All cores are healthy\");\n+    }\n+\n+    // All lights green, report healthy\n+    rsp.add(STATUS, OK);\n+  }\n+\n+  /**\n+   * Find replicas DOWN or RECOVERING, or replicas in clusterstate that do not exist on local node.\n+   * We first find local cores which are either not registered or unhealthy, and check each of these against\n+   * the clusterstate, and return a list of unhealthy replicas that are part of an active shard for an existing collection\n+   * @param cores list of core descriptors to iterate\n+   * @param clusterState clusterstate from ZK\n+   * @return list of core names that are either DOWN ore RECOVERING on 'nodeName'\n+   */\n+  static List<String> findUnhealthyCores(Collection<CloudDescriptor> cores, ClusterState clusterState) {", "originalCommit": "996da9cae71d9c35ea98a674854f0f315dccdfd0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA3MTM0Mg==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403071342", "bodyText": "Looks good to me. Yes, the c.getShardId() will give the slice ID so checking that against the active slice map is fine. One gotcha is that the c.getCoreNodeName() can return null if a core is not fully loaded yet so you might want to return a count from this method instead of a list of core node names where some of them may be null depending on the current state of the system.", "author": "shalinmangar", "createdAt": "2020-04-03T15:05:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwNTczNw==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403105737", "bodyText": "Ok. Since I want a more informative error message, I instead did a null-check on coreNodeName and use <collectionName>_<shardId> as a fallback.", "author": "janhoy", "createdAt": "2020-04-03T15:58:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NzUxMg==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403147512", "bodyText": "I think it might be helpful to name the fallback unknown:<collectionName>_<shardId> or code-loading:<collectionName>_<shardId> to distinguish from the rest of the list which contains coreNodeNames. Otherwise people might be confused why a core that they can't find in their list is registering as unhealthy.", "author": "HoustonPutman", "createdAt": "2020-04-03T16:59:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1MTk3NA==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403151974", "bodyText": "Also this isn't a blocker, but I was wondering if you might have some insight @shalinmangar. When the core directory no longer exists (say for example the disk was wiped before starting the node), the clusterState will register the missing replica(s) as DOWN and the logs will error saying that those cores cannot be found. Will those missing cores still be returned within the cores.getCores() call? If so then this is a non-issue, but if they aren't included as I suspect, then this will return healthy even when there are replicas in the clusterState scheduled on the node that are not healthy.\nMaybe this just requires a fix in a different part of solr to auto-delete replicas that have cores that are missing on startup. Solr used to auto-recreate the cores, but I think that functionality was removed in Solr 6 or 7.", "author": "HoustonPutman", "createdAt": "2020-04-03T17:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzI5MTA0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403291043", "bodyText": "I think it might be helpful to name the fallback unknown:<collectionName>_<shardId> or code-loading:<collectionName>_<shardId> to distinguish...\n\nI think I have changed my mind and agree with shalin that it is enough to return a count, and if that count > 0 include in the error msg \"N out of M cores are still not healthy\". That will avoid the confusion and give a clear and short state to caller. Imagine a node with 3000 cores just having been started but not yet recovered, that list of RECOVERING cores would be huge :) If you really need to know which cores are unhealthy, there are ways to find that elsewhere.", "author": "janhoy", "createdAt": "2020-04-03T20:04:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzQ1MzUxNg==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403453516", "bodyText": "When the core directory no longer exists (say for example the disk was wiped before starting the node), the clusterState will register the missing replica(s) as DOWN and the logs will error saying that those cores cannot be found. Will those missing cores still be returned within the cores.getCores() call?\n\nNo, if the core directories themselves have been wiped off then the node cannot return that core in cores.getCores().\n\nbut if they aren't included as I suspect, then this will return healthy even when there are replicas in the clusterState scheduled on the node that are not healthy.\n\nYes but that should be okay? The node itself is in fact healthy.\n\nMaybe this just requires a fix in a different part of solr to auto-delete replicas that have cores that are missing on startup.\n\nIs this a common case? i.e. wiping disks and putting the nodes back in rotation? It is more common to have nodes which have cores that are not in the cluster state and those are unloaded automatically if those nodes come back up. Also, there's node lost trigger which can be used to delete replicas from cluster state for nodes that go away for a long time.", "author": "shalinmangar", "createdAt": "2020-04-04T10:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzcxOTU2Mw==", "url": "https://github.com/apache/lucene-solr/pull/1387#discussion_r403719563", "bodyText": "So I changed logic to report number of unhealthy cores instead of list of names.\nI agree that the corner case of wiping disk is not within scope of this issue.", "author": "janhoy", "createdAt": "2020-04-05T15:48:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzA0NTMxMQ=="}], "type": "inlineReview"}, {"oid": "0c5d95c6bf4d40666b5ddee2c7277d6915a74f37", "url": "https://github.com/apache/lucene-solr/commit/0c5d95c6bf4d40666b5ddee2c7277d6915a74f37", "message": "Avoid null for coreNodeName", "committedDate": "2020-04-03T15:56:59Z", "type": "commit"}, {"oid": "d9e7cefb8b7e4b104a8c60c9080e84a2f2d9bfc5", "url": "https://github.com/apache/lucene-solr/commit/d9e7cefb8b7e4b104a8c60c9080e84a2f2d9bfc5", "message": "credit shalin", "committedDate": "2020-04-03T15:59:04Z", "type": "commit"}, {"oid": "1f715c4aaf10473b445ed8d88fafd8dc92de7d6c", "url": "https://github.com/apache/lucene-solr/commit/1f715c4aaf10473b445ed8d88fafd8dc92de7d6c", "message": "Merge branch 'master' into solr14210-replica-health", "committedDate": "2020-04-05T15:45:31Z", "type": "commit"}, {"oid": "92b617c693581b38623cb08c9d54390a5ef64803", "url": "https://github.com/apache/lucene-solr/commit/92b617c693581b38623cb08c9d54390a5ef64803", "message": "Return count of unhealthy cores rather than a list of names", "committedDate": "2020-04-05T15:46:28Z", "type": "commit"}, {"oid": "1c141b721148b7ec5c89432983cf7853e2e13161", "url": "https://github.com/apache/lucene-solr/commit/1c141b721148b7ec5c89432983cf7853e2e13161", "message": "num_cores_unhealthy instead of num_unhealth", "committedDate": "2020-04-05T15:50:45Z", "type": "commit"}]}