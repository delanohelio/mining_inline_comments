{"pr_number": 1473, "pr_title": "LUCENE-9353: Move terms metadata to its own file.", "pr_createdAt": "2020-05-01T07:44:52Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1473", "timeline": [{"oid": "6edfcfad0ded142fbce158704b17fc9b02139b77", "url": "https://github.com/apache/lucene-solr/commit/6edfcfad0ded142fbce158704b17fc9b02139b77", "message": "LUCENE-9353: Move terms metadata to its own file.\n\nSee https://issues.apache.org/jira/browse/LUCENE-9353.", "committedDate": "2020-05-01T07:41:40Z", "type": "commit"}, {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42", "url": "https://github.com/apache/lucene-solr/commit/9a4df99cc3f2611315aa016192b921985ea94e42", "message": "Merge branch 'master' into separate_terms_meta_file", "committedDate": "2020-05-01T07:45:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418558656", "bodyText": "do we have a test that tickles these cases?", "author": "msokolov", "createdAt": "2020-05-01T14:13:17Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);", "originalCommit": "9a4df99cc3f2611315aa016192b921985ea94e42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2MzM2OA==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418963368", "bodyText": "Not directly, and these things are hard to test, though I agree we could do better. I opened https://issues.apache.org/jira/browse/LUCENE-9356 to try to improve the coverage of these code paths.", "author": "jpountz", "createdAt": "2020-05-02T14:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTAzOTA5OQ==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419039099", "bodyText": "Thanks, Adrien", "author": "msokolov", "createdAt": "2020-05-03T03:08:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1OTQzMA==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418559430", "bodyText": "again, I don't know if we have test coverage for the corrputed metadata?", "author": "msokolov", "createdAt": "2020-05-01T14:14:49Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);\n+            }\n+            if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n+              throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsMetaIn);\n+            }\n+            if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n+              throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsMetaIn);\n+            }\n+            final long indexStartFP = indexMetaIn.readVLong();\n+            FieldReader previous = fieldMap.put(fieldInfo.name,\n+                new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n+                    indexStartFP, indexIn, minTerm, maxTerm));\n+            if (previous != null) {\n+              throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsMetaIn);\n+            }\n+          }\n+        } catch (Throwable exception) {\n+          priorE = exception;\n+        } finally {\n+          if (metaIn != null) {\n+            CodecUtil.checkFooter(metaIn, priorE);", "originalCommit": "9a4df99cc3f2611315aa016192b921985ea94e42", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419337819", "bodyText": "@jpountz here I see the same lack of serializer write/read code, could it be possible to have such thing, It would improve readability and unit testing by only mocking fieldMetadatas and check serialization is correctly applied.", "author": "juanka588", "createdAt": "2020-05-04T10:17:40Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {\n+      CodecUtil.writeIndexHeader(metaOut, BlockTreeTermsReader.TERMS_META_CODEC_NAME, BlockTreeTermsReader.VERSION_CURRENT,\n+          state.segmentInfo.getId(), state.segmentSuffix);\n \n-      termsOut.writeVInt(fields.size());\n+      metaOut.writeVInt(fields.size());", "originalCommit": "9a4df99cc3f2611315aa016192b921985ea94e42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5MTcyMQ==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419391721", "bodyText": "Your proposal sounds orthogonal to this pull request to me?", "author": "jpountz", "createdAt": "2020-05-04T12:15:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0OTg2MQ==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419449861", "bodyText": "yes but maybe is the opportunity to move the code and add more testability in BlockTreeTermsReader.java", "author": "juanka588", "createdAt": "2020-05-04T13:48:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzNjM2Ng==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r421336366", "bodyText": "I prefer decoupling changes when possible.", "author": "jpountz", "createdAt": "2020-05-07T08:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0ODI3Nw==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419448277", "bodyText": "why this file is not created at the same time with the indexOut, termOut?", "author": "juanka588", "createdAt": "2020-05-04T13:46:38Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {", "originalCommit": "9a4df99cc3f2611315aa016192b921985ea94e42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NDA0NQ==", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419694045", "bodyText": "That would work too. I like keeping the index output open for as little time as possible when it doesn't make things worse otherwise.", "author": "jpountz", "createdAt": "2020-05-04T20:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0ODI3Nw=="}], "type": "inlineReview"}, {"oid": "9faa6304a4c1655bfce2497eb243d51669c2cf48", "url": "https://github.com/apache/lucene-solr/commit/9faa6304a4c1655bfce2497eb243d51669c2cf48", "message": "Merge remote-tracking branch 'origin/master' into separate_terms_meta_file", "committedDate": "2020-06-08T14:43:03Z", "type": "commit"}, {"oid": "10b8c13161098494ba1492aa6bb9e9527c43343f", "url": "https://github.com/apache/lucene-solr/commit/10b8c13161098494ba1492aa6bb9e9527c43343f", "message": "Merge branch 'master' into separate_terms_meta_file", "committedDate": "2020-06-10T06:29:46Z", "type": "commit"}, {"oid": "7587f1c8835a4b47d5a1759438a9c5d27d8bce85", "url": "https://github.com/apache/lucene-solr/commit/7587f1c8835a4b47d5a1759438a9c5d27d8bce85", "message": "Use separate file for FST metadata", "committedDate": "2020-06-10T07:47:36Z", "type": "commit"}, {"oid": "06e381af374146772c8268dd4e591f2f360b97e6", "url": "https://github.com/apache/lucene-solr/commit/06e381af374146772c8268dd4e591f2f360b97e6", "message": "Improve CHANGES entry", "committedDate": "2020-06-10T08:07:27Z", "type": "commit"}, {"oid": "84208aa9a68900fd30a6513e9b0528db7fe2278f", "url": "https://github.com/apache/lucene-solr/commit/84208aa9a68900fd30a6513e9b0528db7fe2278f", "message": "iter", "committedDate": "2020-06-10T08:13:49Z", "type": "commit"}, {"oid": "db5a8f2052ea424938693f2ac8522b2d5f162f24", "url": "https://github.com/apache/lucene-solr/commit/db5a8f2052ea424938693f2ac8522b2d5f162f24", "message": "Merge branch 'master' into separate_terms_meta_file", "committedDate": "2020-06-16T10:19:36Z", "type": "commit"}, {"oid": "3e225d8215d3f8caab74fdb370b59b9e6e66f95f", "url": "https://github.com/apache/lucene-solr/commit/3e225d8215d3f8caab74fdb370b59b9e6e66f95f", "message": "Improve truncation detection.", "committedDate": "2020-06-16T12:35:07Z", "type": "commit"}, {"oid": "8b8417ee3aff7dd726a518c816100452fb7d618c", "url": "https://github.com/apache/lucene-solr/commit/8b8417ee3aff7dd726a518c816100452fb7d618c", "message": "iter", "committedDate": "2020-06-16T12:35:54Z", "type": "commit"}]}