{"pr_number": 1573, "pr_title": "Cleanup TermsHashPerField", "pr_createdAt": "2020-06-12T15:18:21Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1573", "timeline": [{"oid": "a17d9c098d6cf3709ea435536af8a49d008fdadb", "url": "https://github.com/apache/lucene-solr/commit/a17d9c098d6cf3709ea435536af8a49d008fdadb", "message": "first cut", "committedDate": "2020-06-03T16:16:33Z", "type": "commit"}, {"oid": "26d204b23f3072dfd8b82e1b6be1127594f2ecc5", "url": "https://github.com/apache/lucene-solr/commit/26d204b23f3072dfd8b82e1b6be1127594f2ecc5", "message": "fo", "committedDate": "2020-06-10T14:52:00Z", "type": "commit"}, {"oid": "103740fceb32f5f697f945127a2f9369c4f31510", "url": "https://github.com/apache/lucene-solr/commit/103740fceb32f5f697f945127a2f9369c4f31510", "message": " test", "committedDate": "2020-06-12T14:55:52Z", "type": "commit"}, {"oid": "4b2b7c9df1ac6fc9f97077d979d40e07afd30399", "url": "https://github.com/apache/lucene-solr/commit/4b2b7c9df1ac6fc9f97077d979d40e07afd30399", "message": "foo\n\n:", "committedDate": "2020-06-12T15:11:27Z", "type": "commit"}, {"oid": "2810b5b02c07cb0f9a94fa02c9d89166c7534483", "url": "https://github.com/apache/lucene-solr/commit/2810b5b02c07cb0f9a94fa02c9d89166c7534483", "message": "Merge branch 'master' into cleanup_terms_hash_per_field", "committedDate": "2020-06-12T15:12:18Z", "type": "commit"}, {"oid": "9930367ddef17f17b379caba92dee68ff2323f1b", "url": "https://github.com/apache/lucene-solr/commit/9930367ddef17f17b379caba92dee68ff2323f1b", "message": "Merge branch 'master' into cleanup_terms_hash_per_field", "committedDate": "2020-06-13T12:07:37Z", "type": "commit"}, {"oid": "57d00439a91e79dc22803908267cbb44ef0ec825", "url": "https://github.com/apache/lucene-solr/commit/57d00439a91e79dc22803908267cbb44ef0ec825", "message": "remove unnecessary member", "committedDate": "2020-06-13T12:23:52Z", "type": "commit"}, {"oid": "d3d20dc60c8f3c502810c8867cdb30642e851cd8", "url": "https://github.com/apache/lucene-solr/commit/d3d20dc60c8f3c502810c8867cdb30642e851cd8", "message": "add more docs and rename vars to improve readability", "committedDate": "2020-06-13T13:39:51Z", "type": "commit"}, {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "url": "https://github.com/apache/lucene-solr/commit/acacaabcab06cf2d92057c0f8ced57431eb3964b", "message": "fix typo", "committedDate": "2020-06-13T13:45:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM0Nw==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751347", "bodyText": "Hmm why not keep this assertion (to confirm that if the field is not somehow indexed we are not accidentally/incorrectly running this code)?", "author": "mikemccand", "createdAt": "2020-06-13T16:31:50Z", "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -207,8 +202,6 @@ public void newPostingsArray() {\n \n   @Override\n   ParallelPostingsArray createPostingsArray(int size) {\n-    IndexOptions indexOptions = fieldInfo.getIndexOptions();\n-    assert indexOptions != IndexOptions.NONE;", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwMzkxMA==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439803910", "bodyText": "I moved it in a better place in the ctor of the base class. I think that's enough?", "author": "s1monw", "createdAt": "2020-06-14T08:13:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM3Ng==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751376", "bodyText": "s/terms text/terms's text", "author": "mikemccand", "createdAt": "2020-06-13T16:32:21Z", "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQxMQ==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751411", "bodyText": "+1 for this renaming!", "author": "mikemccand", "createdAt": "2020-06-13T16:32:45Z", "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash\n+  final int[] addressOffset; // maps term ID to current stream address", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751436", "bodyText": "Hmm, did these aggregations move somewhere else?  Oh, they look entirely removed?  Were they redundant (computed elsewhere) and these ones were unused?", "author": "mikemccand", "createdAt": "2020-06-13T16:33:30Z", "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -56,12 +56,6 @@ public FreqProxTermsWriterPerField(FieldInvertState invertState, TermsHash terms\n   @Override\n   void finish() throws IOException {\n     super.finish();\n-    sumDocFreq += fieldState.uniqueTermCount;\n-    sumTotalTermFreq += fieldState.length;", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDMxNQ==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804315", "bodyText": "sumDocFreq and sumTotalTermFreq are unused. They were used in FreqProxFields in the past but not anymore for a while now. I removed their commented out usage so you can see it in a followup commit", "author": "s1monw", "createdAt": "2020-06-14T08:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNTM4MQ==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439805381", "bodyText": "see this https://github.com/apache/lucene-solr/pull/1573/files#diff-aa6c5376b6b755262430916164fd0088L84", "author": "s1monw", "createdAt": "2020-06-14T08:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTYwNw==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751607", "bodyText": "Hmm docID is unused in this method?  But I guess the other impl (normal postings) needs it?", "author": "mikemccand", "createdAt": "2020-06-13T16:36:04Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java", "diffHunk": "@@ -222,7 +234,7 @@ void writeProx(TermVectorsPostingsArray postings, int termID) {\n   }\n \n   @Override\n-  void newTerm(final int termID) {\n+  void newTerm(final int termID, final int docID) {", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDM5MA==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804390", "bodyText": "that's correct.", "author": "s1monw", "createdAt": "2020-06-14T08:19:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTcxMg==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751712", "bodyText": "Thank you for the javadocs/comments for such cryptic and ancient code ;)\nMaybe just stores instead of allows to store?", "author": "mikemccand", "createdAt": "2020-06-13T16:37:43Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTc3OA==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751778", "bodyText": "Ahh I see, we just moved the assertion to a better place, awesome.", "author": "mikemccand", "createdAt": "2020-06-13T16:39:14Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDQ0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804443", "bodyText": "yeah :)", "author": "s1monw", "createdAt": "2020-06-14T08:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1NzUxNg==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439757516", "bodyText": "s/allocated/allocate", "author": "mikemccand", "createdAt": "2020-06-13T18:06:47Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}\n+   * or {@link #reinitHash()} was called. */\n+  final void sortTerms() {\n+    assert sortedTermIDs == null;\n     sortedTermIDs = bytesHash.sort();\n+  }\n+\n+  /**\n+   * Returns the sorted term IDs. {@link #sortTerms()} must be called before\n+   */\n+  final int[] getSortedTermIDs() {\n+    assert sortedTermIDs != null;\n     return sortedTermIDs;\n   }\n \n+  final void reinitHash() {\n+    sortedTermIDs = null;\n+    bytesHash.reinit();\n+  }\n+\n   private boolean doNextCall;\n \n   // Secondary entry point (for 2nd & subsequent TermsHash),\n   // because token text has already been \"interned\" into\n   // textStart, so we hash by textStart.  term vectors use\n   // this API.\n-  public void add(int textStart) throws IOException {\n+  private void add(int textStart, final int docID) throws IOException {\n     int termID = bytesHash.addByPoolOffset(textStart);\n     if (termID >= 0) {      // New posting\n       // First time we are seeing this token since we last\n       // flushed the hash.\n-      // Init stream slices\n-      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n-        intPool.nextBuffer();\n-      }\n-\n-      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n-        bytePool.nextBuffer();\n-      }\n+      initStreamSlices(termID, docID);\n+    } else {\n+      positionStreamSlice(termID, docID);\n+    }\n+  }\n \n-      intUptos = intPool.buffer;\n-      intUptoStart = intPool.intUpto;\n-      intPool.intUpto += streamCount;\n+  private void initStreamSlices(int termID, int docID) throws IOException {\n+    // Init stream slices\n+    // TODO: figure out why this is 2*streamCount here. streamCount should be enough?\n+    if ((2*streamCount) + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n+      // can we fit all the streams in the current buffer?\n+      intPool.nextBuffer();\n+    }\n \n-      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n+    if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < (2*streamCount) * ByteBlockPool.FIRST_LEVEL_SIZE) {\n+      // can we fit at least one byte per stream in the current buffer, if not allocated a new one", "originalCommit": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "url": "https://github.com/apache/lucene-solr/commit/6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "message": "apply feedback", "committedDate": "2020-06-14T08:21:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1NzkyMQ==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439857921", "bodyText": "this the?", "author": "dweiss", "createdAt": "2020-06-14T18:55:42Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]", "originalCommit": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODA0Ng==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858046", "bodyText": "should not -> must not perhaps?", "author": "dweiss", "createdAt": "2020-06-14T18:57:14Z", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}", "originalCommit": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODE3Mw==", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858173", "bodyText": "extra space?", "author": "dweiss", "createdAt": "2020-06-14T18:58:52Z", "path": "lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.lucene.index;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import com.carrotsearch.randomizedtesting.generators.RandomPicks;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.Counter;\n+import org.apache.lucene.util.IntBlockPool;\n+import org.apache.lucene.util.LuceneTestCase;\n+\n+public class TestTermsHashPerField extends LuceneTestCase  {\n+\n+  private static TermsHashPerField createNewHash(AtomicInteger newCalled, AtomicInteger addCalled) {\n+    IntBlockPool intBlockPool = new IntBlockPool();\n+    ByteBlockPool byteBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+    ByteBlockPool termBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+\n+    TermsHashPerField hash = new TermsHashPerField(1, intBlockPool, byteBlockPool, termBlockPool, Counter.newCounter(),\n+        null, \"testfield\", IndexOptions.DOCS_AND_FREQS) {\n+\n+      private FreqProxTermsWriterPerField.FreqProxPostingsArray freqProxPostingsArray;\n+\n+      @Override\n+      void newTerm(int termID, int docID) {\n+        newCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        postings.lastDocIDs[termID] = docID;\n+        postings.lastDocCodes[termID] = docID << 1;\n+        postings.termFreqs[termID] = 1;\n+      }\n+\n+      @Override\n+      void addTerm(int termID, int docID) {\n+        addCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        if (docID != postings.lastDocIDs[termID]) {\n+          if (1 == postings.termFreqs[termID]) {\n+            writeVInt(0, postings.lastDocCodes[termID]|1);\n+          } else {\n+            writeVInt(0, postings.lastDocCodes[termID]);\n+            writeVInt(0, postings.termFreqs[termID]);\n+          }\n+          postings.termFreqs[termID] = 1;\n+          postings.lastDocCodes[termID] = (docID - postings.lastDocIDs[termID]) << 1;\n+          postings.lastDocIDs[termID] = docID;\n+        } else {\n+          postings.termFreqs[termID] = Math.addExact(postings.termFreqs[termID], 1);\n+        }\n+      }\n+\n+      @Override\n+      void newPostingsArray() {\n+        freqProxPostingsArray = (FreqProxTermsWriterPerField.FreqProxPostingsArray) postingsArray;\n+", "originalCommit": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4ef7fcb44d51cfb0ab6563f2db05709d182b1764", "url": "https://github.com/apache/lucene-solr/commit/4ef7fcb44d51cfb0ab6563f2db05709d182b1764", "message": "apply feedback", "committedDate": "2020-06-15T18:13:49Z", "type": "commit"}, {"oid": "e04aa190757b12bd93b0c71b4f5b721997700320", "url": "https://github.com/apache/lucene-solr/commit/e04aa190757b12bd93b0c71b4f5b721997700320", "message": "Merge branch 'master' into cleanup_terms_hash_per_field", "committedDate": "2020-06-15T18:14:11Z", "type": "commit"}, {"oid": "5ebbfa1d068140f006f1c8103a2bb541d501a39c", "url": "https://github.com/apache/lucene-solr/commit/5ebbfa1d068140f006f1c8103a2bb541d501a39c", "message": "Merge branch 'master' into cleanup_terms_hash_per_field", "committedDate": "2020-06-16T12:31:15Z", "type": "commit"}]}