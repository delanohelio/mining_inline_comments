{"pr_number": 1188, "pr_title": "SOLR-14044: Support collection and shard deletion in shared storage", "pr_createdAt": "2020-01-20T21:46:48Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1188", "timeline": [{"oid": "a7e9d68c5d408650477010dc294eebb162828e4d", "url": "https://github.com/apache/lucene-solr/commit/a7e9d68c5d408650477010dc294eebb162828e4d", "message": "Support collection and shard deletion in shared storage", "committedDate": "2020-01-20T21:42:36Z", "type": "commit"}, {"oid": "fcfc41a4e84758a3609f218356efc634844526c0", "url": "https://github.com/apache/lucene-solr/commit/fcfc41a4e84758a3609f218356efc634844526c0", "message": "Add end to end collection api delete tests and fix local client test", "committedDate": "2020-01-22T07:12:19Z", "type": "commit"}, {"oid": "523dd2c3f91ddea7988e9af6980031f5aafe36a1", "url": "https://github.com/apache/lucene-solr/commit/523dd2c3f91ddea7988e9af6980031f5aafe36a1", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete", "committedDate": "2020-01-27T19:08:17Z", "type": "commit"}, {"oid": "523dd2c3f91ddea7988e9af6980031f5aafe36a1", "url": "https://github.com/apache/lucene-solr/commit/523dd2c3f91ddea7988e9af6980031f5aafe36a1", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete", "committedDate": "2020-01-27T19:08:17Z", "type": "forcePushed"}, {"oid": "45e58815651e6fe955ff6fa3fb4f9b6b9dcf09bf", "url": "https://github.com/apache/lucene-solr/commit/45e58815651e6fe955ff6fa3fb4f9b6b9dcf09bf", "message": "Fix timestamps", "committedDate": "2020-02-01T00:29:36Z", "type": "commit"}, {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "url": "https://github.com/apache/lucene-solr/commit/b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "message": "Remove debug log line and fix timestamps", "committedDate": "2020-02-03T19:33:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0MjQ1OA==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374842458", "bodyText": "I only see new imports. Is there any functional change in this file?", "author": "mbwaheed", "createdAt": "2020-02-04T18:23:14Z", "path": "solr/core/src/java/org/apache/solr/cloud/Overseer.java", "diffHunk": "@@ -70,6 +70,9 @@\n import org.apache.solr.handler.admin.CollectionsHandler;", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374844537", "bodyText": "Shouldn't this be warning instead of exception? Same as line#167. I believe intention is to not block deletion and leave orphan files behind, correct?", "author": "mbwaheed", "createdAt": "2020-02-04T18:27:08Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/DeleteCollectionCmd.java", "diffHunk": "@@ -142,6 +148,34 @@ public void call(ClusterState state, ZkNodeProps message, NamedList results) thr\n           break;\n         }\n       }\n+      \n+      // Delete the collection files from shared store. We want to delete all of the files before we delete\n+      // the collection state from ZooKeeper.\n+      DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collection);\n+      if (docCollection != null && docCollection.getSharedIndex()) {\n+        SharedStoreManager sharedStoreManager = ocmh.overseer.getCoreContainer().getSharedStoreManager();\n+        BlobDeleteManager deleteManager = sharedStoreManager.getBlobDeleteManager();\n+        BlobDeleteProcessor deleteProcessor = deleteManager.getOverseerDeleteProcessor();\n+        // deletes all files belonging to this collection\n+        CompletableFuture<BlobDeleterTaskResult> deleteFuture = \n+            deleteProcessor.deleteCollection(collection, false);\n+        \n+        try {\n+          // TODO: Find a reasonable timeout value\n+          BlobDeleterTaskResult result = deleteFuture.get(60, TimeUnit.SECONDS);\n+          if (!result.isSuccess()) {\n+            log.warn(\"Deleting all files belonging to shared collection \" + collection + \n+                \" was not successful! Files belonging to this collection may be orphaned.\");\n+          }\n+        } catch (TimeoutException tex) {\n+          // We can orphan files here if we don't delete everything in time but what matters for potentially\n+          // reusing the collection name is that the zookeeper state of the collection gets deleted which \n+          // will happen in the finally block\n+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not complete deleting collection\" + ", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NTE1OQ==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374845159", "bodyText": "Same comment for DeleteShardCmd.java.", "author": "mbwaheed", "createdAt": "2020-02-04T18:28:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ0MTUyNg==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r375441526", "bodyText": "I throw an exception in both cases so the client calling is aware if the command fails and files are orphaned. In DeleteCollection the files are \"truly orphaned\" because even if we error out here, the collection will always be deleted from zookeeper in the finally block and it's effectively gone from Solr's perspective. The DeleteShard will fail the whole command without doing the same delete from zookeeper action and a subsequent delete shard command can be called to try again which isn't the case in the former (unless the same collection name is created again).", "author": "andyvuong", "createdAt": "2020-02-05T18:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ1OTExNQ==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r375459115", "bodyText": "There is inconsistency. In case of timeout we throw and in case of failures we log warning. I am fine with throwing exception but it needs to be consistent for all failures. Is there a reason for this inconsistency?", "author": "mbwaheed", "createdAt": "2020-02-05T19:24:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg5NTM3NQ==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374895375", "bodyText": "Can we put this conversion in some utility and put a reason why we use nanoTime and not currentTimeMillis?", "author": "mbwaheed", "createdAt": "2020-02-04T20:07:58Z", "path": "solr/core/src/java/org/apache/solr/store/blob/metadata/CorePushPull.java", "diffHunk": "@@ -137,7 +139,7 @@ public BlobCoreMetadata pushToBlobStore(String currentMetadataSuffix, String new\n          */\n         for (BlobCoreMetadata.BlobFile d : resolvedMetadataResult.getFilesToDelete()) {\n             bcmBuilder.removeFile(d);\n-            BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(d, System.currentTimeMillis());\n+            BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(d, System.nanoTime() / 1000000);", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkwODY0OQ==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374908649", "bodyText": "I don't see CorePushTest anymore. If that is the case, then we should delete this note. And if some test needs a non-default value, it should override.", "author": "mbwaheed", "createdAt": "2020-02-04T20:36:30Z", "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteManager.java", "diffHunk": "@@ -18,40 +18,77 @@\n package org.apache.solr.store.blob.process;\n \n import java.lang.invoke.MethodHandles;\n-import java.util.Set;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.LinkedBlockingDeque;\n-import java.util.concurrent.RejectedExecutionException;\n-import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n-import org.apache.lucene.util.NamedThreadFactory;\n-import org.apache.solr.common.util.ExecutorUtil.MDCAwareThreadPoolExecutor;\n+import org.apache.solr.core.SolrCore;\n import org.apache.solr.store.blob.client.CoreStorageClient;\n import org.apache.solr.store.blob.metadata.CorePushPull;\n+import org.apache.solr.store.blob.metadata.ServerSideMetadata;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.annotations.VisibleForTesting;\n+\n /**\n- * Manager of blobs (files) to delete, putting them in a queue (if space left on the queue) then consumed and processed\n- * by {@link BlobDeleterTask}\n+ * This class manages the deletion machinery required by shared storage enabled collections. Its responsibilities\n+ * include the allocation and management of bounded deletion task queues and their consumers. \n+ * \n+ * Deletion of blob files from shared store happen on two paths:\n+ *  1. In the indexing path, the local {@link SolrCore}'s index files represented by an instance of a\n+ *  {@link ServerSideMetadata} object is resolved against the blob store's core.metadata file, or the\n+ *  the source of truth for what index files a {@link SolrCore} should have. As the difference between \n+ *  these two metadata instances are resolved, we add files to be deleted to the BlobDeleteManager which\n+ *  enqueues a {@link BlobDeleterTask} for asynchronous processing.\n+ *  2. In the collection admin API, we may delete a collection or collection shard. In the former, all index\n+ *  files belonging to the specified collection on shared storage should be deleted while in the latter \n+ *  all index files belonging to a particular collection/shard pair should be deleted.   \n+ * \n+ * Shard leaders are the only replicas receiving indexing traffic and pushing to shared store in a shared collection\n+ * so all Solr nodes in a cluster may be sending deletion requests to the shared storage provider at a given moment.\n+ * Collection commands are only processed by the Overseer and therefore only the Overseer should be deleting entire\n+ * collections or shard files from shared storage.\n+ * \n+ * The BlobDeleteManager maintains two queues to prevent any potential starvation, one for the incremental indexing \n+ * deletion path that is always initiated when a Solr node with shared collections starts up and one that is only\n+ * used when the current node is Overseer and handles Overseer specific actions.\n  */\n public class BlobDeleteManager {\n \n   private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n   \n+  /**\n+   * Identifier for a BlobDeleteProcessor that runs on all Solr nodes containing shared collections\n+   */\n+  public static final String BLOB_FILE_DELETER = \"BlobFileDeleter\";\n+  \n+  /**\n+   * Identifier for a BlobDeleteProcessor that runs on the Overseer if the Solr cluster contains\n+   * any shared collection\n+   */\n+  public static final String OVERSEER_BLOB_FILE_DELETER = \"OverseerBlobFileDeleter\";\n+  \n   /**\n    * Limit to the number of blob files to delete accepted on the delete queue (and lost in case of server crash). When\n    * the queue reaches that size, no more deletes are accepted (will be retried later for a core, next time it is pushed).\n    * (note that tests in searchserver.blobstore.metadata.CorePushTest trigger a merge that enqueues more than 100 files to", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkwOTU0NA==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374909544", "bodyText": "minor: deleteDelayMs", "author": "mbwaheed", "createdAt": "2020-02-04T20:38:29Z", "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteManager.java", "diffHunk": "@@ -71,92 +108,81 @@\n    * delete until we know for sure the file can be resuscitated...\n    */\n   private final long deleteDelayMs;\n+  \n+  private AtomicBoolean isShutdown; \n \n   /**\n-   * TODO : Creates a default delete client, should have config based one  \n+   * Creates a new BlobDeleteManager with the provided {@link CoreStorageClient} and instantiates\n+   * it with a default deletedelayMs, queue size, and thread pool size. A default {@link BlobDeleteProcessor}", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkyNjc2Mw==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374926763", "bodyText": "default prefix in the name does not make sense. I guess it can just be maxDeleteAttempts.", "author": "mbwaheed", "createdAt": "2020-02-04T21:15:23Z", "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteProcessor.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.solr.store.blob.process;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.lucene.util.NamedThreadFactory;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.util.ExecutorUtil.MDCAwareThreadPoolExecutor;\n+import org.apache.solr.store.blob.client.BlobClientUtils;\n+import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobFileDeletionTask;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobPrefixedFileDeletionTask;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * A generic deletion processor used for deleting object files from shared\n+ * storage. Each processor manages its own task bounded thread pool for processing\n+ * {@link BlobDeleterTask} asynchronously. Processors support retrying tasks if \n+ * necessary but retry decisions are left to the individual task implementations.  \n+ * \n+ * Instances of {@link BlobDeleteProcessor} are managed by the {@link BlobDeleteManager}.\n+ */\n+public class BlobDeleteProcessor {\n+  \n+  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n+  \n+  private final String name;\n+  private final int almostMaxQueueSize;\n+  /**\n+   * Note we sleep() after each failed attempt, so multiply this value by {@link #fixedRetryDelay} to find\n+   * out how long we'll retry (at least) if Blob access fails for some reason (\"at least\" because we\n+   * re-enqueue at the tail of the queue ({@link BlobDeleteManager} creates a list), so there might be additional\n+   * processing delay if the queue is not empty and is processed before the enqueued retry is processed).\n+   */\n+  private final int defaultMaxDeleteAttempts;", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkyOTc2Mg==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374929762", "bodyText": "typo: gaurantee", "author": "mbwaheed", "createdAt": "2020-02-04T21:21:41Z", "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleterTask.java", "diffHunk": "@@ -18,94 +18,244 @@\n package org.apache.solr.store.blob.process;\n \n import java.lang.invoke.MethodHandles;\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Locale;\n import java.util.Set;\n-import java.util.concurrent.ThreadPoolExecutor;\n-import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * Task in charge of deleting Blobs (files) from blob store.\n+ * Generic deletion task for files located on shared storage\n  */\n-class BlobDeleterTask implements Runnable {\n+public abstract class BlobDeleterTask implements Callable<BlobDeleterTaskResult> {\n \n   private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n-\n-  /**\n-   * Note we sleep() after each failed attempt, so multiply this value by {@link #SLEEP_MS_FAILED_ATTEMPT} to find\n-   * out how long we'll retry (at least) if Blob access fails for some reason (\"at least\" because we\n-   * re-enqueue at the tail of the queue ({@link BlobDeleteManager} creates a list), so there might be additional\n-   * processing delay if the queue is not empty and is processed before the enqueued retry is processed).\n-   */\n-  private static int MAX_DELETE_ATTEMPTS = 50;\n-  private static long SLEEP_MS_FAILED_ATTEMPT = TimeUnit.SECONDS.toMillis(10);\n-\n+  \n   private final CoreStorageClient client;\n-  private final String sharedBlobName;\n-  private final Set<String> blobNames;\n+  private final String collectionName;\n   private final AtomicInteger attempt;\n-  private final ThreadPoolExecutor executor;\n+  \n   private final long queuedTimeMs;\n+  private final int maxAttempts;\n+  private final boolean allowRetry;\n+  private Throwable err;\n \n-  BlobDeleterTask(CoreStorageClient client, String sharedBlobName, Set<String> blobNames, ThreadPoolExecutor executor) {\n-    this.client = client; \n-    this.sharedBlobName = sharedBlobName;\n-    this.blobNames = blobNames;\n+  public BlobDeleterTask(CoreStorageClient client, String collectionName, boolean allowRetry,\n+      int maxAttempts) {\n+    this.client = client;\n+    this.collectionName = collectionName;\n     this.attempt = new AtomicInteger(0);\n-    this.executor = executor;\n-    this.queuedTimeMs = System.nanoTime();\n+    this.queuedTimeMs = System.nanoTime() / 1000000;\n+    this.allowRetry = allowRetry;\n+    this.maxAttempts = maxAttempts;\n   }\n-\n+  \n+  /**\n+   * Performs a deletion action and request against the shared storage for the given collection\n+   * and returns the list of file paths deleted\n+   */\n+  public abstract Collection<String> doDelete() throws Exception;\n+  \n+  /**\n+   * Return a String representing the action performed by the BlobDeleterTask for logging purposes\n+   */\n+  public abstract String getActionName();\n+  \n   @Override\n-  public void run() {\n-    final long startTimeMs = System.nanoTime();\n+  public BlobDeleterTaskResult call() {\n+    List<String> filesDeleted = new LinkedList<>();\n+    final long startTimeMs = System.nanoTime() / 1000000;\n     boolean isSuccess = true;\n-      \n+    boolean shouldRetry = false;\n     try {\n+      filesDeleted.addAll(doDelete());\n+      attempt.incrementAndGet();\n+      return new BlobDeleterTaskResult(this, filesDeleted, isSuccess, shouldRetry, err);\n+    } catch (Exception ex) {\n+      if (err == null) {\n+        err = ex;\n+      } else {\n+        err.addSuppressed(ex);\n+      }\n+      int attempts = attempt.incrementAndGet();\n+      isSuccess = false;\n+      log.warn(\"BlobDeleterTask failed on attempt=\" + attempts  + \" collection=\" + collectionName\n+          + \" task=\" + toString(), ex);\n+      if (allowRetry) {\n+        if (attempts < maxAttempts) {\n+          shouldRetry = true;\n+        } else {\n+          log.warn(\"Reached \" + maxAttempts + \" attempt limit for deletion task \" + toString() + \n+              \". This task won't be retried.\");\n+        }\n+      }\n+    } finally {\n+      long now = System.nanoTime() / 1000000;\n+      long runTime = now - startTimeMs;\n+      long startLatency = now - this.queuedTimeMs;\n+      log(getActionName(), collectionName, runTime, startLatency, isSuccess, getAdditionalLogMessage());\n+    }\n+    return new BlobDeleterTaskResult(this, filesDeleted, isSuccess, shouldRetry, err);\n+  }\n+  \n+  /**\n+   * Override-able by deletion tasks to provide additional action specific logging\n+   */\n+  public String getAdditionalLogMessage() {\n+    return \"\";\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    return \"collectionName=\" + collectionName + \" allowRetry=\" + allowRetry + \n+        \" queuedTimeMs=\" + queuedTimeMs + \" attemptsTried=\" + attempt.get();\n+  }\n+  \n+  public int getAttempts() {\n+    return attempt.get();\n+  }\n+\n+  public void log(String action, String collectionName, long runTime, long startLatency, boolean isSuccess, \n+      String additionalMessage) {\n+    String message = String.format(Locale.ROOT, \n+        \"action=%s storageProvider=%s bucketRegion=%s bucketName=%s, runTime=%s \"\n+        + \"startLatency=%s attempt=%s isSuccess=%s %s\",\n+        action, client.getStorageProvider().name(), client.getBucketRegion(), client.getBucketName(),\n+        runTime, startLatency, attempt.get(), isSuccess, additionalMessage);\n+    log.info(message);\n+  }\n+  \n+  /**\n+   * Represents the result of a deletion task\n+   */\n+  public static class BlobDeleterTaskResult {\n+    private final BlobDeleterTask task;\n+    private final Collection<String> filesDeleted;\n+    private final boolean isSuccess;\n+    private final boolean shouldRetry;\n+    private final Throwable err;\n+    \n+    public BlobDeleterTaskResult(BlobDeleterTask task, Collection<String> filesDeleted, \n+        boolean isSuccess, boolean shouldRetry, Throwable errs) {\n+      this.task = task;\n+      this.filesDeleted = filesDeleted;\n+      this.isSuccess = isSuccess;\n+      this.shouldRetry = shouldRetry;\n+      this.err = errs;\n+    }\n+    \n+    public boolean isSuccess() {\n+      return isSuccess;\n+    }\n+    \n+    public boolean shouldRetry() {\n+      return shouldRetry;\n+    }\n+    \n+    public BlobDeleterTask getTask() {\n+      return task;\n+    }\n+    \n+    /**\n+     * @return the files that are being deleted. Note if the task wasn't successful there is no gaurantee", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDk0NA==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374934944", "bodyText": "To improve it little further, maybe add one non-matching prefix entry and make sure we don't get that in the result.", "author": "mbwaheed", "createdAt": "2020-02-04T21:32:59Z", "path": "solr/core/src/test/org/apache/solr/store/blob/client/CoreStorageClientTest.java", "diffHunk": "@@ -87,6 +88,22 @@ public void testPushStreamReturnsPath() throws Exception {\n     int expectedBlobKeyLength = TEST_CORE_NAME_1.length() + uuid4length + 1 + 4;\n     Assert.assertEquals(blobPath.length(), expectedBlobKeyLength);\n   }\n+  \n+  @Test\n+  public void testListBlobFiles() throws Exception {", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NjU5OA==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374946598", "bodyText": "This needs to be non-null otherwise it will through NPE in BlobDeleterTaskResult#call.\nBecause of this, testRetryableTaskSucceeds is failing. It gets enqueued 5 time instead of 4.\nAlso, the failed test is leaking a thread. To fix that processor.shutdown needs to be inside finally block.\ncom.carrotsearch.randomizedtesting.ThreadLeakError: There are still zombie threads that couldn't be terminated:\n\nThread[id=24, name=DeleterForTest-1-thread-1, state=WAITING, group=TGRP-BlobDeleteProcessorTest]\nat sun.misc.Unsafe.park(Native Method)\nat java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\nat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\nat java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)\nat java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)\nat java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)", "author": "mbwaheed", "createdAt": "2020-02-04T21:58:34Z", "path": "solr/core/src/test/org/apache/solr/store/blob/process/BlobDeleteProcessorTest.java", "diffHunk": "@@ -0,0 +1,472 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.solr.store.blob.process;\n+\n+import java.nio.file.Path;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.solr.SolrTestCaseJ4;\n+import org.apache.solr.store.blob.client.BlobException;\n+import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.client.LocalStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobFileDeletionTask;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobPrefixedFileDeletionTask;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Unit tests for {@link BlobDeleteProcessor}\n+ */\n+public class BlobDeleteProcessorTest extends SolrTestCaseJ4 {\n+  \n+  private static String DEFAULT_PROCESSOR_NAME = \"DeleterForTest\";\n+  private static Path sharedStoreRootPath;\n+  private static CoreStorageClient blobClient;\n+  \n+  private static List<BlobDeleterTask> enqueuedTasks;\n+\n+  @BeforeClass\n+  public static void setupTestClass() throws Exception {\n+    sharedStoreRootPath = createTempDir(\"tempDir\");\n+    System.setProperty(LocalStorageClient.BLOB_STORE_LOCAL_FS_ROOT_DIR_PROPERTY, sharedStoreRootPath.resolve(\"LocalBlobStore/\").toString());\n+    blobClient = new LocalStorageClient() {\n+       \n+      // no ops for BlobFileDeletionTask and BlobPrefixedFileDeletionTask to execute successfully\n+      @Override\n+      public void deleteBlobs(Collection<String> paths) throws BlobException {\n+        return;\n+      }\n+\n+      // no ops for BlobFileDeletionTask and BlobPrefixedFileDeletionTask to execute successfully\n+      @Override\n+      public List<String> listCoreBlobFiles(String prefix) throws BlobException {\n+        return new LinkedList<>();\n+      }\n+    };\n+  }\n+  \n+  @Before\n+  public void setup() {\n+    enqueuedTasks = new LinkedList<BlobDeleterTask>();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteFilesEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    Set<String> names = new HashSet<>();\n+    names.add(\"test1\");\n+    names.add(\"test2\");\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteFiles(name, names, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobPrefixedFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteShardEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteCollection(name, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobPrefixedFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteCollectionEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteShard(name, name, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that we don't retry tasks that are not configured to be retried\n+   * and end up failing\n+   */\n+  @Test\n+  public void testNonRetryableTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried \n+\n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    \n+    // enqueue a task that fails and is not retryable\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildFailingTaskForTest(blobClient, name, totalAttempts, false), isRetry);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    \n+    // the first fails\n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(false, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+\n+    // initial error + 0 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(0, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that the retry logic kicks in for tasks configured to retry\n+   * and subsequent retry succeeds\n+   */\n+  @Test\n+  public void testRetryableTaskSucceeds() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried \n+    int totalFails = 3; // total number of times the task should fail\n+    \n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // enqueue a task that fails totalFails number of times before succeeding\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildScheduledFailingTaskForTest(blobClient, name, totalAttempts, true, totalFails), isRetry);\n+    \n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    \n+    // the first 3 fail and last one succeeds\n+    assertEquals(4, enqueuedTasks.size());\n+    \n+    assertNotNull(res);\n+    assertEquals(4, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    \n+    // initial error + 2 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(2, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that after all task attempts are exhausted we bail out\n+   */\n+  @Test\n+  public void testRetryableTaskFails() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried\n+    \n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // enqueue a task that fails every time it runs but is configured to retry\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildFailingTaskForTest(blobClient, name, totalAttempts, true), isRetry);\n+    \n+    // wait for this task and all its potential retries to finish \n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    // 1 initial enqueue + 4 retries\n+    assertEquals(5, enqueuedTasks.size());\n+    \n+    assertNotNull(res);\n+    assertEquals(5, res.getTask().getAttempts());\n+    assertEquals(false, res.isSuccess());\n+    // circuit breaker should be false after all attempts are exceeded\n+    assertEquals(false, res.shouldRetry());\n+    \n+    // initial error + 4 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(4, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that we cannot add more deletion tasks to the processor if the work queue\n+   * is at its target max but that we can re-add tasks that are retries to the queue\n+   */\n+  @Test\n+  public void testWorkQueueFull() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1;\n+    int retryDelay = 1000;\n+    \n+    String name = \"testName\";\n+    boolean allowRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // numThreads is 1 and we'll enqueue a blocking task that ensures our pool\n+    // will be occupied while we add new tasks subsequently to test enqueue rejection\n+    CountDownLatch tasklatch = new CountDownLatch(1);\n+    processor.enqueue(buildBlockingTaskForTest(tasklatch), allowRetry);\n+\n+    // Fill the internal work queue beyond the maxQueueSize, the internal queue size is not \n+    // approximate so we'll just add beyond the max\n+    for (int i = 0; i < maxQueueSize*2; i++) {\n+      try {\n+        processor.deleteCollection(name, allowRetry);\n+      } catch (Exception ex) {\n+        // ignore\n+      }\n+    }\n+    \n+    // verify adding a new task is rejected\n+    try {\n+      processor.deleteCollection(name, allowRetry);\n+      fail(\"Task should have been rejected\");\n+    } catch (Exception ex) {\n+      assertTrue(ex.getMessage().contains(\"Unable to enqueue deletion\"));\n+    }\n+    CompletableFuture<BlobDeleterTaskResult> cf = null;\n+    try {\n+      // verify adding a task that is marked as a retry is not rejected \n+       cf = processor.enqueue(buildFailingTaskForTest(blobClient, name, 5, true), /* isRetry */ true);\n+    } catch (Exception ex) {\n+      fail(\"Task should not have been rejected\");\n+    }\n+    \n+    // clean up and unblock the task\n+    tasklatch.countDown();\n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that with a continuous stream of delete tasks being enqueued, all eventually complete\n+   * successfully in the face of failing tasks and retries without locking up our pool anywhere\n+   */\n+  @Test\n+  public void testSimpleConcurrentDeletionEnqueues() throws Exception {\n+    int maxQueueSize = 200;\n+    int numThreads = 5;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 100;\n+    int numberOfTasks = 200;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    List<BlobDeleterTask> tasks = generateRandomTasks(defaultMaxAttempts, numberOfTasks);\n+    List<CompletableFuture<BlobDeleterTaskResult>> taskResultsFutures = new LinkedList<>();\n+    List<BlobDeleterTaskResult> results = new LinkedList<>();\n+    for (BlobDeleterTask t : tasks) {\n+      taskResultsFutures.add(processor.enqueue(t, false));\n+    }\n+    \n+    taskResultsFutures.forEach(cf -> {\n+      try {\n+        results.add(cf.get(20000, TimeUnit.MILLISECONDS));\n+      } catch (Exception ex) {\n+        fail(\"We timed out on some task!\");\n+      }\n+    });\n+    \n+    // we shouldn't enqueue more than (numberOfTasks * defaultMaxAttempts) tasks to the pool \n+    assertTrue(enqueuedTasks.size() < (numberOfTasks * defaultMaxAttempts));\n+    assertEquals(numberOfTasks, results.size());\n+    int totalAttempts = 0;\n+    for (BlobDeleterTaskResult res : results) {\n+      assertNotNull(res);\n+      assertNotNull(res.getTask());\n+      assertEquals(\"scheduledFailingTask\", res.getTask().getActionName());\n+      totalAttempts += res.getTask().getAttempts();\n+    }\n+    // total task attempts should be consistent with our test scaffolding\n+    assertTrue(totalAttempts < (numberOfTasks * defaultMaxAttempts));\n+      \n+    processor.shutdown();\n+  }\n+  \n+  private List<BlobDeleterTask> generateRandomTasks(int defaultMaxAttempts, int taskCount) {\n+    List<BlobDeleterTask> tasks = new LinkedList<>();\n+    for (int i = 0; i < taskCount; i++) {\n+      BlobDeleterTask task = null;\n+      int totalAttempts = random().nextInt(defaultMaxAttempts);\n+      int totalFails = random().nextInt(defaultMaxAttempts + 1);\n+      task = buildScheduledFailingTaskForTest(blobClient, \"test\"+i, totalAttempts, true, totalFails);\n+      tasks.add(task);\n+    }\n+    return tasks;\n+  }\n+  \n+  /**\n+   * Returns a test-only task for just holding onto a resource for test purposes\n+   */\n+  private BlobDeleterTask buildBlockingTaskForTest(CountDownLatch latch) {\n+    return new BlobDeleterTask(null, null, false, 0) {\n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        // block until something forces this latch to count down\n+        latch.await();\n+        return null;\n+      }\n+      \n+      @Override\n+      public String getActionName() { return \"blockingTask\"; }\n+    };\n+  }\n+  \n+  /**\n+   * Returns a test-only task that always fails on action execution by throwing an\n+   * exception\n+   */\n+  private BlobDeleterTask buildFailingTaskForTest(CoreStorageClient client, \n+      String collectionName, int maxRetries, boolean allowRetries) {\n+    return new BlobDeleterTask(client, collectionName, allowRetries, maxRetries) {\n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        throw new Exception(\"\");\n+      }\n+      \n+      @Override\n+      public String getActionName() { return \"failingTask\"; }\n+    };\n+  }\n+  \n+  /**\n+   * Returns a test-only task that fails a specified number of times before succeeding\n+   */\n+  private BlobDeleterTask buildScheduledFailingTaskForTest(CoreStorageClient client, \n+      String collectionName, int maxRetries, boolean allowRetries, int failTotal) {\n+    return new BlobDeleterTask(client, collectionName, allowRetries, maxRetries) {\n+      private AtomicInteger failCount = new AtomicInteger(0);\n+      \n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        while (failCount.get() < failTotal) {\n+          failCount.incrementAndGet();\n+          throw new Exception(\"\");\n+        }\n+        return null;", "originalCommit": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ0MjMxOA==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r375442318", "bodyText": "The thread leak is due to the bug in the test which should be addressed now", "author": "andyvuong", "createdAt": "2020-02-05T18:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NjU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ2MDYzNQ==", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r375460635", "bodyText": "Even with test fixed, tests cleanup should always run (failure or no-failures). Otherwise, any future regression that causes a test to fail will start leaking threads. From past experience we know that, leaking threads negatively impacts the whole test run, not just the tests causing the leak.", "author": "mbwaheed", "createdAt": "2020-02-05T19:27:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NjU5OA=="}], "type": "inlineReview"}, {"oid": "c03e85e35fe859ed3339e589a58ecd7a4bc7c749", "url": "https://github.com/apache/lucene-solr/commit/c03e85e35fe859ed3339e589a58ecd7a4bc7c749", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete", "committedDate": "2020-02-05T18:10:30Z", "type": "commit"}, {"oid": "e822f98f7a50c0281b3ea3db43a0930556a0ab25", "url": "https://github.com/apache/lucene-solr/commit/e822f98f7a50c0281b3ea3db43a0930556a0ab25", "message": "Address review comments and fix test", "committedDate": "2020-02-05T18:48:15Z", "type": "commit"}, {"oid": "cd034879d7892223ec57db75d5df1314839f95de", "url": "https://github.com/apache/lucene-solr/commit/cd034879d7892223ec57db75d5df1314839f95de", "message": "Close resource and throw exception on failure", "committedDate": "2020-02-05T22:08:39Z", "type": "commit"}]}