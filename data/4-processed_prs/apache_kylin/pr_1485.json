{"pr_number": 1485, "pr_title": "KYLIN-4818 Support Cube Planner Phase One in Kylin 4", "pr_createdAt": "2020-11-23T13:48:46Z", "pr_url": "https://github.com/apache/kylin/pull/1485", "timeline": [{"oid": "cad8705cdbe814e764e56cd96dd32dab7be32041", "url": "https://github.com/apache/kylin/commit/cad8705cdbe814e764e56cd96dd32dab7be32041", "message": "KYLIN-4818 Calculate cuboid rowcount via HLL", "committedDate": "2020-11-24T03:20:44Z", "type": "forcePushed"}, {"oid": "e750fffe071444a4dad8c375b9419cbcb3564eec", "url": "https://github.com/apache/kylin/commit/e750fffe071444a4dad8c375b9419cbcb3564eec", "message": "KYLIN-4818 Calculate cuboid rowcount via HLL", "committedDate": "2020-11-24T06:11:42Z", "type": "forcePushed"}, {"oid": "909ba43fc31fd53d6e019924dc2fa63f5553b2aa", "url": "https://github.com/apache/kylin/commit/909ba43fc31fd53d6e019924dc2fa63f5553b2aa", "message": "KYLIN-4818 Persist metadata in SparkExecutable", "committedDate": "2020-12-03T10:38:03Z", "type": "forcePushed"}, {"oid": "d5e2a587ecbbb28c3007305c8709c6fa8fb25c6e", "url": "https://github.com/apache/kylin/commit/d5e2a587ecbbb28c3007305c8709c6fa8fb25c6e", "message": "KYLIN-4818 Calculate cuboid rowcount via HLL", "committedDate": "2020-12-05T09:36:12Z", "type": "commit"}, {"oid": "40f04ca5156a2b72c2094ba7d8b4a433afa80c41", "url": "https://github.com/apache/kylin/commit/40f04ca5156a2b72c2094ba7d8b4a433afa80c41", "message": "KYLIN-4818 Support Cube Planner Phase One in Kylin 4\n\n- Use Spark to calculate cuboid's HllCounter for the first segment\n- Re-enable Cube planner by default\n- Change default precision from 10 to 14 for HLLCounter", "committedDate": "2020-12-05T09:36:12Z", "type": "commit"}, {"oid": "3291a3553e7a866e8d05f47a7f4aad1ebf772c7d", "url": "https://github.com/apache/kylin/commit/3291a3553e7a866e8d05f47a7f4aad1ebf772c7d", "message": "KYLIN-4818 Persist metadata in SparkExecutable", "committedDate": "2020-12-05T09:36:12Z", "type": "commit"}, {"oid": "2e13c8857700fd4d1c4e4daede6600562c62d494", "url": "https://github.com/apache/kylin/commit/2e13c8857700fd4d1c4e4daede6600562c62d494", "message": "KYLIN-4818 Performance profile for CuboidStatisticsJob", "committedDate": "2020-12-07T13:30:43Z", "type": "commit"}, {"oid": "2e13c8857700fd4d1c4e4daede6600562c62d494", "url": "https://github.com/apache/kylin/commit/2e13c8857700fd4d1c4e4daede6600562c62d494", "message": "KYLIN-4818 Performance profile for CuboidStatisticsJob", "committedDate": "2020-12-07T13:30:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY0MzIyNQ==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r537643225", "bodyText": "If we have 10 thousand cuboid to be statistics, we will call Long.toString 10 thousand times and create 10 thousand String objects. Is it the cause of bad performance ?", "author": "hit-lacus", "createdAt": "2020-12-07T16:27:21Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(String, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[String, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i.toString, AggInfo(i.toString)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }\n+      info(ids(idx).toString).cuboid.counter.addHashDirectly(value)", "originalCommit": "2e13c8857700fd4d1c4e4daede6600562c62d494", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "url": "https://github.com/apache/kylin/commit/22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "message": "KYLIN-4818 Reduce toString method call", "committedDate": "2020-12-08T02:58:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4Njc4MA==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539286780", "bodyText": "please add repartition operation : inputDs.rdd.repartition(inputDs.sparkSession.sparkContext.defaultParallelism)", "author": "zzcclp", "createdAt": "2020-12-09T13:01:05Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MjM4MQ==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539992381", "bodyText": "Thanks, it is reasonable.", "author": "hit-lacus", "createdAt": "2020-12-10T09:04:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4Njc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4ODE0NQ==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539288145", "bodyText": "convert 'List' to 'Array': seg.getAllLayout.map(x => x.getId).toArray", "author": "zzcclp", "createdAt": "2020-12-09T13:03:09Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTA2Ng==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539289066", "bodyText": "change ids: List[Long] to ids: Array[Long]", "author": "zzcclp", "createdAt": "2020-12-09T13:04:33Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTU4Mw==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539289583", "bodyText": "uses mutable.LongMap[AggInfo]() instead of mutable.Map[Long, AggInfo]() here", "author": "zzcclp", "createdAt": "2020-12-09T13:05:18Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MzE0NA==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539993144", "bodyText": "Great !", "author": "hit-lacus", "createdAt": "2020-12-10T09:05:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTU4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5MjM2OA==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539292368", "bodyText": "some suggestion:\nval currCuboidBitSet = allCuboidsBitSet(idx)\nval currCuboidLength = currCuboidBitSet.length\nwhile (position < currCuboidLength) {\n         value += rowHashCodesLong(currCuboidBitSet(position))\n         position += 1\n}\n\nreduce the time of calling 'allCuboidsBitSet(idx)'", "author": "zzcclp", "createdAt": "2020-12-09T13:09:32Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5OTAzOQ==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539999039", "bodyText": "Good advice.", "author": "hit-lacus", "createdAt": "2020-12-10T09:14:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5MjM2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5NDMyNQ==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539294325", "bodyText": "uses cuboidIds: Array[Long] instead of cuboidIds: List[Long]", "author": "zzcclp", "createdAt": "2020-12-09T13:12:32Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }\n+      info(ids(idx)).cuboid.counter.addHashDirectly(value)\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter2 += (endMills - startMills)\n+  }\n+\n+  def getCuboidBitSet(cuboidIds: List[Long], nRowKey: Int): Array[Array[Integer]] = {", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc5NTkxNw==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539795917", "bodyText": "If r.get(idx) == null, r.get(idx).toString will throw NPE, please change to :\nvar colValue = if (r.get(idx) == null) \"0\" else r.get(idx).toString", "author": "zzcclp", "createdAt": "2020-12-10T02:26:10Z", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTg0OTUyNA==", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539849524", "bodyText": "What's the difference between '.seq' directory and '.json' directory? Do we need to delete which one after running job successfully?", "author": "zzcclp", "createdAt": "2020-12-10T05:09:49Z", "path": "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java", "diffHunk": "@@ -530,11 +537,19 @@ public void setSnapshots(ConcurrentHashMap<String, String> snapshots) {\n     }\n \n     public String getStatisticsResourcePath() {\n-        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid());\n+        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".seq\");\n+    }\n+\n+    public String getPreciseStatisticsResourcePath() {\n+        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".json\");", "originalCommit": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4ebccc6c63bb02874d79d91cbae85877540881c4", "url": "https://github.com/apache/kylin/commit/4ebccc6c63bb02874d79d91cbae85877540881c4", "message": "KYLIN-4818 Refine CuboidStatisticsJob to improve performance", "committedDate": "2020-12-10T10:31:30Z", "type": "commit"}, {"oid": "4ebccc6c63bb02874d79d91cbae85877540881c4", "url": "https://github.com/apache/kylin/commit/4ebccc6c63bb02874d79d91cbae85877540881c4", "message": "KYLIN-4818 Refine CuboidStatisticsJob to improve performance", "committedDate": "2020-12-10T10:31:30Z", "type": "forcePushed"}]}