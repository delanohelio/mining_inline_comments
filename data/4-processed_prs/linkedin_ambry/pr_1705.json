{"pr_number": 1705, "pr_title": "Fix null replica in HelixClusterMap when decommissioning replicas.", "pr_createdAt": "2020-11-24T23:22:48Z", "pr_url": "https://github.com/linkedin/ambry/pull/1705", "timeline": [{"oid": "a07dfea5a76fa8c942fbf140ab9aa1541b9116a2", "url": "https://github.com/linkedin/ambry/commit/a07dfea5a76fa8c942fbf140ab9aa1541b9116a2", "message": "Fix null replica in HelixClusterMap when decommissing replicas.\n\nAmbry frontend now allows GET operation to try on OFFLINE replicas. During replica decommission, the first step is to\ndisable replica. This will eventually bring replica to OFFLINE state and remove its entry from DataNodeConfig. All listeners\n(both frontends and servers) will receive the DataNodeConfig change and remove this replica from in-mem clustermap. However,\nat this point of time, the IdealState in Helix hasn't been changed and this replica still exists in external view. So when\nrouter calls getReplicaIdByState(), the RoutingTableProvider still returns instance(server) associated with this replica. This\ncauses a null replica when router queries in-mem clustermap by this instance and our code doesn't invalidate the null value in\nthe return result, which results in NPEs.\nThis PR makes two changes:\n1. Add a config to control whether to add down replicas to the replica pool in operation tracker. If yes, down(offline) replicas\n   will be added to the end.\n2. Preclude null replicas in the method of getReplicaIdsByState().", "committedDate": "2020-11-24T22:54:51Z", "type": "commit"}, {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497", "url": "https://github.com/linkedin/ambry/commit/47d0793fcbec5876b483d5f31bbe6661b7b40497", "message": "fix test failure", "committedDate": "2020-11-25T18:02:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0ODkzOQ==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535548939", "bodyText": "nit: it looks like this comment should still go above getResourcesInClusterWithTag?", "author": "cgtz", "createdAt": "2020-12-03T20:05:56Z", "path": "ambry-clustermap/src/test/java/com/github/ambry/clustermap/MockHelixAdmin.java", "diffHunk": "@@ -407,36 +446,15 @@ int getSetInstanceConfigCallCount() {\n     return setInstanceConfigCallCount;\n   }\n \n-  /**\n-   * Private class that holds partition state infos from one data node.\n-   */\n-  class ReplicaStateInfos {\n-    Map<String, Map<String, String>> replicaStateMap;\n-\n-    ReplicaStateInfos() {\n-      replicaStateMap = new HashMap<>();\n-    }\n-\n-    void setReplicaState(String partition, String state) {\n-      Map<String, String> stateMap = new HashMap<>();\n-      stateMap.put(CurrentState.CurrentStateProperty.CURRENT_STATE.name(), state);\n-      replicaStateMap.put(partition, stateMap);\n-    }\n-\n-    Map<String, Map<String, String>> getReplicaStateMap() {\n-      return replicaStateMap;\n-    }\n+  @Override\n+  public List<String> getResourcesInClusterWithTag(String clusterName, String tag) {\n+    throw new IllegalStateException(\"Not implemented\");\n   }\n \n   // ***************************************", "originalCommit": "47d0793fcbec5876b483d5f31bbe6661b7b40497", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NzIxOA==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535577218", "bodyText": "So we have removed the option to exclude cross colo replicas that aren't in the originating DC? I thought that we used this option in production to improve latency in the not found case?", "author": "cgtz", "createdAt": "2020-12-03T20:33:41Z", "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -265,19 +258,29 @@\n     }\n     List<ReplicaId> backupReplicasToCheck = new ArrayList<>(backupReplicas);\n     List<ReplicaId> downReplicasToCheck = new ArrayList<>(downReplicas);\n-    if (includeNonOriginatingDcReplicas || this.originatingDcName == null) {\n-      backupReplicas.forEach(this::addToEndOfPool);\n+\n+    // Add replicas that are neither in local dc nor in originating dc.\n+    backupReplicas.forEach(this::addToEndOfPool);", "originalCommit": "47d0793fcbec5876b483d5f31bbe6661b7b40497", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4MDM4NQ==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535680385", "bodyText": "Right, with this change, we always allows GET request to also try on non-originating replicas. Since these replicas will be tried with lower priority and we have failOnNotFound logic to terminate operation before exhausting all replicas, I think we don't really sacrifice the latency in \"NotFound\" case.", "author": "jsjtzyy", "createdAt": "2020-12-03T22:10:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NzIxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY5NTMwOQ==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535695309", "bodyText": "(The intention of this change is to simplify some logic in operation tracker. We already have many configs to control its behavior and some of them may not be needed anymore. )", "author": "jsjtzyy", "createdAt": "2020-12-03T22:38:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NzIxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY5OTM1MQ==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535699351", "bodyText": "Ah that makes sense that hasFailedOnNotFound() will catch this first. I forgot about that. It is good to simplify the configuration as much as possible.", "author": "cgtz", "createdAt": "2020-12-03T22:45:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NzIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2MzYyMw==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535663623", "bodyText": "It may be better to do a loop here. Mutating the offlineReplicas list was a little confusing to me and a loop can save a couple list iterations:\nSet remoteOfflineReplicas\nfor (replica in offlineReplicas) {\n  if (replica in originating DC) {\n    numReplicasInOriginatingDc++;\n  }\n  if (replica in local DC) {\n    addToEndOfPool\n  } else {\n    remoteOfflineReplicas.add(replica)\n  }\n}\nremoteOfflineReplicas.forEach(addToEndOfPool);", "author": "cgtz", "createdAt": "2020-12-03T21:50:51Z", "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -265,19 +258,29 @@\n     }\n     List<ReplicaId> backupReplicasToCheck = new ArrayList<>(backupReplicas);\n     List<ReplicaId> downReplicasToCheck = new ArrayList<>(downReplicas);\n-    if (includeNonOriginatingDcReplicas || this.originatingDcName == null) {\n-      backupReplicas.forEach(this::addToEndOfPool);\n+\n+    // Add replicas that are neither in local dc nor in originating dc.\n+    backupReplicas.forEach(this::addToEndOfPool);\n+\n+    if (routerConfig.routerOperationTrackerIncludeDownReplicas) {\n+      // Add those replicas deemed by native failure detector to be down\n       downReplicas.forEach(this::addToEndOfPool);\n-    } else {\n-      // This is for get request only. Take replicasRequired copy of replicas to do the request\n-      // Please note replicasRequired is 6 because total number of local and originating replicas is always <= 6.\n-      // This may no longer be true with partition classes and flexible replication.\n-      // Don't do this if originatingDcName is unknown.\n-      while (replicaPool.size() < numOfReplicasRequired && backupReplicas.size() > 0) {\n-        addToEndOfPool(backupReplicas.pollFirst());\n-      }\n-      while (replicaPool.size() < numOfReplicasRequired && downReplicas.size() > 0) {\n-        addToEndOfPool(downReplicas.pollFirst());\n+      // Add those replicas deemed by Helix to be down (offline). This only applies to GET operation.\n+      // Adding this logic to mitigate situation where one or more Zookeeper clusters are suddenly unavailable while\n+      // ambry servers are still up.\n+      if (routerOperation == RouterOperation.GetBlobOperation\n+          || routerOperation == RouterOperation.GetBlobInfoOperation) {\n+        Set<ReplicaId> offlineReplicas =\n+            new HashSet<>(getEligibleReplicas(partitionId, null, EnumSet.of(ReplicaState.OFFLINE)));\n+        Set<ReplicaId> originatingReplicas = offlineReplicas.stream().filter(r -> r.getDataNodeId().getDatacenterName().equals(this.originatingDcName)).collect(", "originalCommit": "47d0793fcbec5876b483d5f31bbe6661b7b40497", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY5NDE0Mg==", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535694142", "bodyText": "Sure, this looks more straightforward.", "author": "jsjtzyy", "createdAt": "2020-12-03T22:36:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2MzYyMw=="}], "type": "inlineReview"}, {"oid": "3d3e6c46732448941b37bfd6cec4a438d195c064", "url": "https://github.com/linkedin/ambry/commit/3d3e6c46732448941b37bfd6cec4a438d195c064", "message": "address Casey's comment", "committedDate": "2020-12-03T22:57:08Z", "type": "commit"}, {"oid": "dfe191909742d6304de1b2ec631e6a5b7bf931a8", "url": "https://github.com/linkedin/ambry/commit/dfe191909742d6304de1b2ec631e6a5b7bf931a8", "message": "fix the race condition in ReplicaSyncUpManager (minor refactoring)", "committedDate": "2020-12-06T22:11:17Z", "type": "commit"}]}