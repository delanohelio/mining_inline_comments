{"pr_number": 1559, "pr_title": "Support Hybrid Compaction Policy", "pr_createdAt": "2020-06-11T20:49:19Z", "pr_url": "https://github.com/linkedin/ambry/pull/1559", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzODIzMA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439138230", "bodyText": "we can make this static.", "author": "justinlin-linkedin", "createdAt": "2020-06-12T00:12:48Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy{\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private final Logger logger = LoggerFactory.getLogger(getClass());", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwMjE5OQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r441202199", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-16T23:46:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzODIzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzODY3MA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439138670", "bodyText": "nit: need to format this file.", "author": "justinlin-linkedin", "createdAt": "2020-06-12T00:13:36Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy{", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwMzg1Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r441203853", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-16T23:52:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzODY3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzOTgyMg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439139822", "bodyText": "info logs seem to verbose. Can you change it to trace?", "author": "justinlin-linkedin", "createdAt": "2020-06-12T00:15:39Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy{\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private final Logger logger = LoggerFactory.getLogger(getClass());\n+  private final SafeCounterWithoutLock counter;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    counter = new SafeCounterWithoutLock();\n+  }\n+\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats)\n+      throws StoreException {\n+    return selectCompactionPolicy().getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity,\n+        segmentHeaderSize, logSegmentsNotInJournal, blobStoreStats);\n+  }\n+\n+  /**\n+   * Selects which compaction policy needs to be used for current compaction cycle.\n+   * @return CompactAllPolicy if the round number of compaction reach to storeConfig.storeCompactionPolicySwitchPeriod.\n+   * Othewise @return StatsBasedCompactionPolicy.\n+   */\n+  CompactionPolicy selectCompactionPolicy(){\n+    if (counter.incrementAndGet() == 0) {\n+      logger.info(\"Return CompactAllPolicy this round\");", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwNDExNg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r441204116", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-16T23:52:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzOTgyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0MDIxOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439140219", "bodyText": "nit: Select which compaction policy to use ...", "author": "justinlin-linkedin", "createdAt": "2020-06-12T00:16:22Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy{\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private final Logger logger = LoggerFactory.getLogger(getClass());\n+  private final SafeCounterWithoutLock counter;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    counter = new SafeCounterWithoutLock();\n+  }\n+\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats)\n+      throws StoreException {\n+    return selectCompactionPolicy().getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity,\n+        segmentHeaderSize, logSegmentsNotInJournal, blobStoreStats);\n+  }\n+\n+  /**\n+   * Selects which compaction policy needs to be used for current compaction cycle.", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwNDI3Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r441204277", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-16T23:53:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0MDIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0MzgzMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439143833", "bodyText": "I don't feel good about the way we count the compaction round. All blobStore(partition) uses the same CompactionPolicy instance, they would interface each other and in some cases causes unexpected result.\nAssume we have 7 partitions in one host, and compaction manager trigger compaction on each one of them in the same order alway, p1, p2, p3...p7, p1, p2, p3... p7. In this case, P7 would always use CompactionAllPolicy and the other partitions would always use statsbased compaction policy.\nI think what we need is a per partition HybridCompactionPolicy. It can't be shared by partitions, because it's no longer a stateless object.", "author": "justinlin-linkedin", "createdAt": "2020-06-12T00:23:38Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy{", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg4OTg5OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r441889898", "bodyText": "Thanks for the advise. Implemented with first approach that maintains a map which key is replicaId and value is the counter.", "author": "SophieGuo410", "createdAt": "2020-06-17T23:37:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0MzgzMw=="}], "type": "inlineReview"}, {"oid": "11ae1f010e007d329f2a6da7b11dfd141dcceae7", "url": "https://github.com/linkedin/ambry/commit/11ae1f010e007d329f2a6da7b11dfd141dcceae7", "message": "support per replica compaction policy", "committedDate": "2020-06-16T08:20:52Z", "type": "forcePushed"}, {"oid": "f4f49ee6630105534f58a53de8846ef808ba321f", "url": "https://github.com/linkedin/ambry/commit/f4f49ee6630105534f58a53de8846ef808ba321f", "message": "support per replica compaction policy", "committedDate": "2020-06-16T16:17:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTAwOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443099009", "bodyText": "we actually don't have to differentiate the compaction policy factory when creating and incrementing the couter. We can just increment it for every policy.", "author": "justinlin-linkedin", "createdAt": "2020-06-20T04:03:56Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionManager.java", "diffHunk": "@@ -173,15 +178,30 @@ boolean removeBlobStore(BlobStore store) {\n     return true;\n   }\n \n+  /**\n+   * Get the current replicaToCounterMap policy.\n+   * @return {@link this.replicaToCounterMap}\n+   */\n+  Map<ReplicaId, CompactionPolicyCounter> getReplicaToCounterMap(){\n+    return this.replicaToCounterMap;\n+  }\n+\n   /**\n    * Get compaction details for a given {@link BlobStore} if any\n    * @param blobStore the {@link BlobStore} for which compaction details are requested\n    * @return the {@link CompactionDetails} containing the details about log segments that needs to be compacted.\n    * {@code null} if compaction is not required\n    * @throws StoreException when {@link BlobStore} is not started\n    */\n-  private CompactionDetails getCompactionDetails(BlobStore blobStore) throws StoreException {\n-    return blobStore.getCompactionDetails(compactionPolicy);\n+  CompactionDetails getCompactionDetails(BlobStore blobStore) throws StoreException {\n+    ReplicaId replicaId = blobStore.getReplicaId();\n+    if (compactionPolicyFactory != null && compactionPolicyFactory instanceof HybridCompactionPolicyFactory) {\n+      CompactionPolicyCounter compactionPolicyCounter =\n+          replicaToCounterMap.getOrDefault(replicaId, new CompactionPolicyCounter(storeConfig));\n+      compactionPolicyCounter.increment();\n+      replicaToCounterMap.put(replicaId, compactionPolicyCounter);\n+    }", "originalCommit": "2ebd942e2fcfdc1f780c8f39ddca7677bcfad106", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjYyMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443972623", "bodyText": "Move map inside Hybrid compaction policy.", "author": "SophieGuo410", "createdAt": "2020-06-23T05:36:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTAwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTE0Ng==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443099146", "bodyText": "let's set a limit here by using getInt(name, default, min, max). This would make sure that period would be 0 or negative number.", "author": "justinlin-linkedin", "createdAt": "2020-06-20T04:06:02Z", "path": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java", "diffHunk": "@@ -339,6 +346,7 @@ public StoreConfig(VerifiableProperties verifiableProperties) {\n     storeMaxNumberOfEntriesToReturnFromJournal =\n         verifiableProperties.getIntInRange(\"store.max.number.of.entries.to.return.from.journal\", 5000, 1, 10000);\n     storeDeletedMessageRetentionDays = verifiableProperties.getInt(\"store.deleted.message.retention.days\", 7);\n+    storeCompactionPolicySwitchPeriod = verifiableProperties.getInt(\"store.compaction.policy.switch.period\", 7);", "originalCommit": "2ebd942e2fcfdc1f780c8f39ddca7677bcfad106", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjY2Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443972667", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-23T05:36:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTI2Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443099267", "bodyText": "not used.", "author": "justinlin-linkedin", "createdAt": "2020-06-20T04:08:36Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicyCounter.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import java.util.concurrent.atomic.AtomicInteger;", "originalCommit": "2ebd942e2fcfdc1f780c8f39ddca7677bcfad106", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjY5Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443972697", "bodyText": "Removed.", "author": "SophieGuo410", "createdAt": "2020-06-23T05:36:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTcyNg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443099726", "bodyText": "I was wonder if you would consider this way. Using BlobStoreStats to pass store id, which is essentially the same as replica id number, to CompactionPolicy. And in CompactionPolicy, we keep a map from store id to a counter? In this way, we don't have to change interface here.", "author": "justinlin-linkedin", "createdAt": "2020-06-20T04:18:04Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicy.java", "diffHunk": "@@ -37,6 +37,6 @@\n    * @throws StoreException\n    */\n   CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n-      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats)\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, CompactionPolicyCounter compactionPolicyCounter)", "originalCommit": "2ebd942e2fcfdc1f780c8f39ddca7677bcfad106", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3Mjk3OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443972978", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-23T05:37:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTc0OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443099748", "bodyText": "nit: drop public and make this class package-private.", "author": "justinlin-linkedin", "createdAt": "2020-06-20T04:18:36Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicyCounter.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+\n+/**\n+ * A counter used to switch {@link CompactAllPolicy}.\n+ */\n+public class CompactionPolicyCounter {", "originalCommit": "2ebd942e2fcfdc1f780c8f39ddca7677bcfad106", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjkwMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r443972903", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-23T05:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5OTc0OA=="}], "type": "inlineReview"}, {"oid": "1d7e07bba09f5382aae90529afb62d58dac5b739", "url": "https://github.com/linkedin/ambry/commit/1d7e07bba09f5382aae90529afb62d58dac5b739", "message": "maintain map inside compactionPolicy", "committedDate": "2020-06-22T23:56:06Z", "type": "forcePushed"}, {"oid": "3d80a04f6baec2b65fbab26492daf637320f1110", "url": "https://github.com/linkedin/ambry/commit/3d80a04f6baec2b65fbab26492daf637320f1110", "message": "maintain map inside compactionPolicy", "committedDate": "2020-06-23T05:31:34Z", "type": "forcePushed"}, {"oid": "294aaf326878a6f232abd0a123c23cd0bef33d6c", "url": "https://github.com/linkedin/ambry/commit/294aaf326878a6f232abd0a123c23cd0bef33d6c", "message": "Support Hybrid Compaction Policy", "committedDate": "2020-06-26T16:18:04Z", "type": "commit"}, {"oid": "0c3ca1f649486fa1be6196bf9709ba75c8c5d3ea", "url": "https://github.com/linkedin/ambry/commit/0c3ca1f649486fa1be6196bf9709ba75c8c5d3ea", "message": "support per replica compaction policy", "committedDate": "2020-06-26T16:18:04Z", "type": "commit"}, {"oid": "32c91b6d72b663931f6fb58d21220403cf7c2571", "url": "https://github.com/linkedin/ambry/commit/32c91b6d72b663931f6fb58d21220403cf7c2571", "message": "format change", "committedDate": "2020-06-26T16:18:04Z", "type": "commit"}, {"oid": "b7548a3e5720c29f31539d35b837245f37dc9aa0", "url": "https://github.com/linkedin/ambry/commit/b7548a3e5720c29f31539d35b837245f37dc9aa0", "message": "update to regular counter", "committedDate": "2020-06-26T16:18:04Z", "type": "commit"}, {"oid": "58e53947adc7d484b3568134078ee31a47c6f471", "url": "https://github.com/linkedin/ambry/commit/58e53947adc7d484b3568134078ee31a47c6f471", "message": "maintain map inside compactionPolicy", "committedDate": "2020-06-26T16:18:04Z", "type": "commit"}, {"oid": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "url": "https://github.com/linkedin/ambry/commit/769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "message": "upgrade HybridCompactionPolicy", "committedDate": "2020-06-26T16:22:10Z", "type": "commit"}, {"oid": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "url": "https://github.com/linkedin/ambry/commit/769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "message": "upgrade HybridCompactionPolicy", "committedDate": "2020-06-26T16:22:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1NDIwMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r446354203", "bodyText": "why not just use jackson's objectmapper for de-serilization http://tutorials.jenkov.com/java-json/jackson-objectmapper.html#jackson-objectmapper-example. It would be just oneliner.", "author": "justinlin-linkedin", "createdAt": "2020-06-26T18:50:46Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACTION_POLICY_COUNTER = \"compactionPolicyCounter\";\n+  private static final String VALUE = \"value\";\n+  private static final String LAST_COMPACT_ALL_TIME = \"lastCompactAllTime\";\n+  private static final String COMPACT_POLICY_INFO_PATH = \"/compactionPolicyInfo.\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   * @throws StoreException\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = blobToCompactionPolicySwitchInfoMap.getOrDefault(storeId,\n+        new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig), INIT_COMPACT_ALL_TIME));\n+    blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, null);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactionTime\" : 1593128052284\n+   * }\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private void recoverCompactionPolicySwitchInfo(CompactionPolicySwitchInfo compactionPolicySwitchInfo, File file) {", "originalCommit": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE0NDk2Mg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447144962", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T17:46:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1NDIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2MjkxOA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r446362918", "bodyText": "logger.error", "author": "justinlin-linkedin", "createdAt": "2020-06-26T19:10:45Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACTION_POLICY_COUNTER = \"compactionPolicyCounter\";\n+  private static final String VALUE = \"value\";\n+  private static final String LAST_COMPACT_ALL_TIME = \"lastCompactAllTime\";\n+  private static final String COMPACT_POLICY_INFO_PATH = \"/compactionPolicyInfo.\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   * @throws StoreException\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = blobToCompactionPolicySwitchInfoMap.getOrDefault(storeId,\n+        new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig), INIT_COMPACT_ALL_TIME));\n+    blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, null);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactionTime\" : 1593128052284\n+   * }\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private void recoverCompactionPolicySwitchInfo(CompactionPolicySwitchInfo compactionPolicySwitchInfo, File file) {\n+    try {\n+      JSONObject jsonObject = new JSONObject(readStringFromFile(file.toString()));\n+      JSONObject compactionPolicyCounter =\n+          jsonObject.has(COMPACTION_POLICY_COUNTER) ? jsonObject.getJSONObject(COMPACTION_POLICY_COUNTER) : null;\n+      int compactionPolicyCounterValue = INIT_COUNTER_VALUE;\n+      if (compactionPolicyCounter != null) {\n+        compactionPolicyCounterValue =\n+            compactionPolicyCounter.has(VALUE) ? compactionPolicyCounter.getInt(VALUE) : INIT_COUNTER_VALUE;\n+      } else {\n+        logger.trace(\"CompactionPolicyCounter is null\");\n+      }\n+      long lastCompactAllTime =\n+          jsonObject.has(LAST_COMPACT_ALL_TIME) ? jsonObject.getLong(LAST_COMPACT_ALL_TIME) : INIT_COMPACT_ALL_TIME;\n+      compactionPolicySwitchInfo.setLastCompactAllTime(lastCompactAllTime);\n+      compactionPolicySwitchInfo.getCompactionPolicyCounter().setValue(compactionPolicyCounterValue);\n+    } catch (IOException e) {\n+      logger.trace(\"tempFile : {} is not exist\", file);", "originalCommit": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE0NTAzOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447145039", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T17:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2MjkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNTgxOA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r446415818", "bodyText": "we can make logic a bit better without checking file every time selecting a policy. Something like\nif (!blobToCompactionPolicySwitchInfos.contain(storeId)) {\n    File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH))\n    if (file.exists()) {\n        ObjectMapper objectMapper = new ObjectMapper();\n        CompactionPolicySwitchInfo switchInfo = objectMapper.readValue(file, CompactionPolicySwithInfo.class);\n        blobToCompactionPolicySwtichInfos.put(storeId, switchInfo);\n    } else {\n        blobToCompactionPolicySwtichInfos.put(storeId, new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig), INIT_COMPACT_ALL_TIME));\n    }\n}", "author": "justinlin-linkedin", "createdAt": "2020-06-26T21:22:14Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACTION_POLICY_COUNTER = \"compactionPolicyCounter\";\n+  private static final String VALUE = \"value\";\n+  private static final String LAST_COMPACT_ALL_TIME = \"lastCompactAllTime\";\n+  private static final String COMPACT_POLICY_INFO_PATH = \"/compactionPolicyInfo.\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   * @throws StoreException\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = blobToCompactionPolicySwitchInfoMap.getOrDefault(storeId,\n+        new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig), INIT_COMPACT_ALL_TIME));\n+    blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(dataDir, storeId, compactionPolicySwitchInfo);", "originalCommit": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE0NTExNQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447145115", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T17:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNTgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNzIzMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r446417233", "bodyText": "you can omit leading slash for two reasons\n\nIt's less cross-platform compatible, we can always use File.separator. Not that it matters here, just a nice pattern.\nUse Paths.get is probably a better way to concatenate different parts of the file path.\n\nAlso, dataDir already has the partition id/replica id/store id as the last part of the filepath, we don't need to suffix storeId in the filename here. But we can add .json as extension, just to indicate this is a json formatted file.", "author": "justinlin-linkedin", "createdAt": "2020-06-26T21:26:20Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACTION_POLICY_COUNTER = \"compactionPolicyCounter\";\n+  private static final String VALUE = \"value\";\n+  private static final String LAST_COMPACT_ALL_TIME = \"lastCompactAllTime\";\n+  private static final String COMPACT_POLICY_INFO_PATH = \"/compactionPolicyInfo.\";", "originalCommit": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE0NTE4Mg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447145182", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T17:46:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNzIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNzYyOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r446417629", "bodyText": "please pass dataDir here.", "author": "justinlin-linkedin", "createdAt": "2020-06-26T21:27:23Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.json.JSONObject;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACTION_POLICY_COUNTER = \"compactionPolicyCounter\";\n+  private static final String VALUE = \"value\";\n+  private static final String LAST_COMPACT_ALL_TIME = \"lastCompactAllTime\";\n+  private static final String COMPACT_POLICY_INFO_PATH = \"/compactionPolicyInfo.\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   * @throws StoreException\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = blobToCompactionPolicySwitchInfoMap.getOrDefault(storeId,\n+        new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig), INIT_COMPACT_ALL_TIME));\n+    blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, null);", "originalCommit": "769d92403ae7200c7fe1f3ea2ab5a116a6a7cac7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE0NTI2MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447145261", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T17:46:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQxNzYyOQ=="}], "type": "inlineReview"}, {"oid": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "url": "https://github.com/linkedin/ambry/commit/876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "message": "address comments", "committedDate": "2020-06-29T07:21:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyOTExMA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447229110", "bodyText": "typo: swith-> switch", "author": "zzmao", "createdAt": "2020-06-29T20:19:21Z", "path": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java", "diffHunk": "@@ -81,6 +81,13 @@\n   @Default(\"7\")\n   public final int storeDeletedMessageRetentionDays;\n \n+  /**\n+   * How often the HybridCompactionPolicy swith from StatsBasedCompactionPolicy to CompactAllPolicy.", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjcwNQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282705", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:09:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyOTExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyOTYzOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447229639", "bodyText": "Looks like this method is not used.", "author": "zzmao", "createdAt": "2020-06-29T20:20:23Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -1081,6 +1081,13 @@ public long getEndPositionOfLastPut() throws StoreException {\n     return replicaStatusDelegates;\n   }\n \n+  /**\n+   * @return a {@link ReplicaId} associated with this store\n+   */\n+  public ReplicaId getReplicaId() {", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjYxMw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282613", "bodyText": "Removed.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:09:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyOTYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjY1Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447232653", "bodyText": "not used.", "author": "zzmao", "createdAt": "2020-06-29T20:25:58Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionManager.java", "diffHunk": "@@ -13,12 +13,15 @@\n  */\n package com.github.ambry.store;\n \n+import com.github.ambry.clustermap.ReplicaId;", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjY5NA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447232694", "bodyText": "not used.", "author": "zzmao", "createdAt": "2020-06-29T20:26:04Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionManager.java", "diffHunk": "@@ -13,12 +13,15 @@\n  */\n package com.github.ambry.store;\n \n+import com.github.ambry.clustermap.ReplicaId;\n import com.github.ambry.config.StoreConfig;\n import com.github.ambry.utils.Time;\n import com.github.ambry.utils.Utils;\n import java.util.Collection;\n import java.util.EnumSet;\n+import java.util.HashMap;", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjgwMQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282801", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjc0OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447232748", "bodyText": "not used.", "author": "zzmao", "createdAt": "2020-06-29T20:26:10Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionManager.java", "diffHunk": "@@ -13,12 +13,15 @@\n  */\n package com.github.ambry.store;\n \n+import com.github.ambry.clustermap.ReplicaId;\n import com.github.ambry.config.StoreConfig;\n import com.github.ambry.utils.Time;\n import com.github.ambry.utils.Utils;\n import java.util.Collection;\n import java.util.EnumSet;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjQ2Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282467", "bodyText": "Removed.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjk0Ng==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447232946", "bodyText": "java doc", "author": "zzmao", "createdAt": "2020-06-29T20:26:34Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicy.java", "diffHunk": "@@ -33,10 +33,11 @@\n    *                                {@link Journal}\n    * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n    * {@link CompactionDetails} are requested\n+   * @param dataDir", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjM0OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282348", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:08:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzMjk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzNjM3MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447236371", "bodyText": "add a field called \"lastCompactionAllTime\"?", "author": "zzmao", "createdAt": "2020-06-29T20:32:50Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactionTime\" : 1593128052284", "originalCommit": "876ad65183dc1dc22fd1f9a5a13bccbeb8714715", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjI2Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447282267", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-29T22:08:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzNjM3MQ=="}], "type": "inlineReview"}, {"oid": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "url": "https://github.com/linkedin/ambry/commit/fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "message": "address comments", "committedDate": "2020-06-29T20:56:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNDc2MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r439534761", "bodyText": "Is this still a TODO?", "author": "lightningrob", "createdAt": "2020-06-12T16:53:42Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -209,6 +209,7 @@ private void getDeprecatedContainers() {\n           deprecatedContainers.add(new Pair<>(container.getParentAccountId(), container.getId()));\n         }\n       });\n+      //TODO: Filter out the INACTIVE containers from deprecatedContainers set if it's already been compacted.", "originalCommit": "2d93b812d7ed78f8c47c4f87fe346177cf0d6360", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4MzM1NA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447883354", "bodyText": "Yes. That's an optimize that I plan to implement in a separate pr.", "author": "SophieGuo410", "createdAt": "2020-06-30T18:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNDc2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMTA0Ng==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447331046", "bodyText": "Please add Days to end to be consistent (and clearer).", "author": "lightningrob", "createdAt": "2020-06-30T00:14:35Z", "path": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java", "diffHunk": "@@ -81,6 +81,13 @@\n   @Default(\"7\")\n   public final int storeDeletedMessageRetentionDays;\n \n+  /**\n+   * How often the HybridCompactionPolicy switch from StatsBasedCompactionPolicy to CompactAllPolicy.\n+   */\n+  @Config(\"store.compaction.policy.switch.period\")\n+  @Default(\"7\")\n+  public final int storeCompactionPolicySwitchPeriod;", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNTU5OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447415598", "bodyText": "Updated. And I separated the storeCompactionPolicySwitchTimestampDays and storeCompactionPolicySwitchCounterDays to make the tuning more flexible.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMTA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMTgwNA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447331804", "bodyText": "for this blob store.", "author": "lightningrob", "createdAt": "2020-06-30T00:17:01Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -1117,6 +1117,13 @@ void cancel() {\n     }\n   }\n \n+  /**\n+   * @return the storeId for this blob.", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNTc5Mg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447415792", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:19:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMTgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNTA3Ng==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447335076", "bodyText": "Does this refer to CompactionPolicySwitchInfo or something else?", "author": "lightningrob", "createdAt": "2020-06-30T00:25:47Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicy.java", "diffHunk": "@@ -33,10 +33,11 @@\n    *                                {@link Journal}\n    * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n    * {@link CompactionDetails} are requested\n+   * @param dataDir The directory to use to store compactionPolicyInfo", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjA1MA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416050", "bodyText": "Updated with CompactionPolicySwitchInfo. Sorry for the confusion.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNTA3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjE3Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447336173", "bodyText": "Reason for these methods to use Value instead of Counter?  Some convention required?", "author": "lightningrob", "createdAt": "2020-06-30T00:28:38Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicyCounter.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonPropertyOrder;\n+\n+\n+/**\n+ * A counter used to switch {@link CompactAllPolicy}.\n+ */\n+@JsonPropertyOrder({\"storeCompactionPolicySwitchPeriod\", \"counter\"})\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.ANY)\n+class CompactionPolicyCounter {\n+  private int storeCompactionPolicySwitchPeriod;\n+  private int counter;\n+\n+  CompactionPolicyCounter(int storeCompactionPolicySwitchPeriod) {\n+    this.storeCompactionPolicySwitchPeriod = storeCompactionPolicySwitchPeriod;\n+  }\n+\n+  //make sure objectMapper can work correctly\n+  CompactionPolicyCounter() {\n+  }\n+\n+  public int getValue() {", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNzUxOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447337519", "bodyText": "Also add method javadocs.", "author": "lightningrob", "createdAt": "2020-06-30T00:32:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjE3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjI0MA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416240", "bodyText": "Change to counter to be more clear and added the javadocs.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:21:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNjE3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODAyNg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447338026", "bodyText": "Javadocs", "author": "lightningrob", "createdAt": "2020-06-30T00:34:43Z", "path": "ambry-store/src/main/java/com/github/ambry/store/CompactionPolicySwitchInfo.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonPropertyOrder;\n+\n+\n+/**\n+ * The {@link CompactionPolicy} info to determine when to use which {@link CompactionPolicy}.\n+ */\n+@JsonPropertyOrder({\"compactionPolicyCounter\", \"lastCompactionTime\"})\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.ANY)\n+public class CompactionPolicySwitchInfo {\n+  private CompactionPolicyCounter compactionPolicyCounter;\n+  private long lastCompactAllTime;\n+\n+  CompactionPolicySwitchInfo(CompactionPolicyCounter compactionPolicyCounter, long lastCompactAllTime) {\n+    this.compactionPolicyCounter = compactionPolicyCounter;\n+    this.lastCompactAllTime = lastCompactAllTime;\n+  }\n+\n+  //make sure objectMapper can work correctly\n+  CompactionPolicySwitchInfo() {\n+  }\n+\n+  CompactionPolicyCounter getCompactionPolicyCounter() {", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjM4MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416381", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODAyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MDQ4Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447340483", "bodyText": "Minor: more like getOrRecover.  Or just get, since reading from file is an impl detail.", "author": "lightningrob", "createdAt": "2020-06-30T00:43:05Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjY4MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416681", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:23:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MDQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTA2OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447341068", "bodyText": "Do you actually have both counter and value?", "author": "lightningrob", "createdAt": "2020-06-30T00:45:11Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjc5NQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416795", "bodyText": "Updated to have counter only.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTM1NQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447341355", "bodyText": "Use static ObjectMapper.", "author": "lightningrob", "createdAt": "2020-06-30T00:46:06Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjg5Nw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416897", "bodyText": "Done.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTM1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTc4Mg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447341782", "bodyText": "If we're eating the exception and using default, should add an error metric since it likely indicates a bug.", "author": "lightningrob", "createdAt": "2020-06-30T00:47:27Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNjk5OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447416998", "bodyText": "Add the metrics.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:24:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTc4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzU4OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447343588", "bodyText": "What happens if we reset the info and host crashes before CompactAll compaction makes much progress?  After restart, will it go back to StatsBased?", "author": "lightningrob", "createdAt": "2020-06-30T00:53:57Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+    if (readyToTriggerCompactionAllPolicy(compactionPolicySwitchInfo)) {\n+      logger.trace(\"Return CompactAllPolicy this round\");\n+      updateCompactionInfo(compactionPolicySwitchInfo);\n+      return new CompactAllPolicy(storeConfig, time);\n+    } else {\n+      if (compactionPolicySwitchInfo.getCompactionPolicyCounter() == null) {\n+        logger.trace(\"Counter is null\");\n+      } else {\n+        logger.trace(\"Return StatsBasedCompactionPolicy this round\");\n+      }\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+  }\n+\n+  /**\n+   * Determine which compactionPolicy to use for current compaction cycle.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @return {@code true} if the counter value equals to 0 or it's storeCompactionPolicySwitchPeriod days past the start time of CompactAllPolicy.\n+   */\n+  private boolean readyToTriggerCompactionAllPolicy(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    return compactionPolicySwitchInfo.getCompactionPolicyCounter() != null\n+        && compactionPolicySwitchInfo.getCompactionPolicyCounter().getValue() == 0 ||\n+        compactionPolicySwitchInfo.getLastCompactAllTime() + TimeUnit.DAYS.toMillis(\n+            storeConfig.storeCompactionPolicySwitchPeriod) <= System.currentTimeMillis();\n+  }\n+\n+  /**\n+   * Update the {@link CompactionPolicySwitchInfo} before the start of {@link CompactAllPolicy}", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQyMDA3NQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447420075", "bodyText": "In this situation, after restart it will resume the compaction which handled in compaction design and after it finished, it will go to StatsBased.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:34:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzU4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzY3OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447343678", "bodyText": "Extra lines at end.", "author": "lightningrob", "createdAt": "2020-06-30T00:54:19Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+    if (readyToTriggerCompactionAllPolicy(compactionPolicySwitchInfo)) {\n+      logger.trace(\"Return CompactAllPolicy this round\");\n+      updateCompactionInfo(compactionPolicySwitchInfo);\n+      return new CompactAllPolicy(storeConfig, time);\n+    } else {\n+      if (compactionPolicySwitchInfo.getCompactionPolicyCounter() == null) {\n+        logger.trace(\"Counter is null\");\n+      } else {\n+        logger.trace(\"Return StatsBasedCompactionPolicy this round\");\n+      }\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+  }\n+\n+  /**\n+   * Determine which compactionPolicy to use for current compaction cycle.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @return {@code true} if the counter value equals to 0 or it's storeCompactionPolicySwitchPeriod days past the start time of CompactAllPolicy.\n+   */\n+  private boolean readyToTriggerCompactionAllPolicy(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    return compactionPolicySwitchInfo.getCompactionPolicyCounter() != null\n+        && compactionPolicySwitchInfo.getCompactionPolicyCounter().getValue() == 0 ||\n+        compactionPolicySwitchInfo.getLastCompactAllTime() + TimeUnit.DAYS.toMillis(\n+            storeConfig.storeCompactionPolicySwitchPeriod) <= System.currentTimeMillis();\n+  }\n+\n+  /**\n+   * Update the {@link CompactionPolicySwitchInfo} before the start of {@link CompactAllPolicy}\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   */\n+  private void updateCompactionInfo(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    compactionPolicySwitchInfo.setLastCompactAllTime(System.currentTimeMillis());\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().setValue(INIT_COUNTER_VALUE);\n+  }\n+\n+  /**\n+   * @return blobToCompactionPolicySwitchInfoMap which key is storeId and value is {@link CompactionPolicySwitchInfo}\n+   */\n+  Map<String, CompactionPolicySwitchInfo> getBlobToCompactionPolicySwitchInfoMap() {\n+    return this.blobToCompactionPolicySwitchInfoMap;\n+  }\n+\n+  /**\n+   * Back up {@link CompactionPolicySwitchInfo} in Json format for each {@link BlobStore}\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   */\n+  private void backUpCompactionPolicyInfo(String dataDir, String storeId,\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (dataDir != null && !dataDir.isEmpty()) {\n+      File tempFile = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      try {\n+        tempFile.createNewFile();\n+        mapper.defaultPrettyPrintingWriter().writeValue(tempFile, compactionPolicySwitchInfo);\n+      } catch (IOException e) {\n+        logger.error(\"Exception while store compaction policy info for local report. Output file path - {}\",\n+            tempFile.getAbsolutePath(), e);\n+      }\n+    }\n+  }\n+}\n+", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQyMDY0Mg==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447420642", "bodyText": "Removed.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:35:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NDYxOQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447344619", "bodyText": "Minor: saying \"for each store\" implies the method does something for every store, but it's only acting on one.", "author": "lightningrob", "createdAt": "2020-06-30T00:57:14Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+    if (readyToTriggerCompactionAllPolicy(compactionPolicySwitchInfo)) {\n+      logger.trace(\"Return CompactAllPolicy this round\");\n+      updateCompactionInfo(compactionPolicySwitchInfo);\n+      return new CompactAllPolicy(storeConfig, time);\n+    } else {\n+      if (compactionPolicySwitchInfo.getCompactionPolicyCounter() == null) {\n+        logger.trace(\"Counter is null\");\n+      } else {\n+        logger.trace(\"Return StatsBasedCompactionPolicy this round\");\n+      }\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+  }\n+\n+  /**\n+   * Determine which compactionPolicy to use for current compaction cycle.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @return {@code true} if the counter value equals to 0 or it's storeCompactionPolicySwitchPeriod days past the start time of CompactAllPolicy.\n+   */\n+  private boolean readyToTriggerCompactionAllPolicy(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    return compactionPolicySwitchInfo.getCompactionPolicyCounter() != null\n+        && compactionPolicySwitchInfo.getCompactionPolicyCounter().getValue() == 0 ||\n+        compactionPolicySwitchInfo.getLastCompactAllTime() + TimeUnit.DAYS.toMillis(\n+            storeConfig.storeCompactionPolicySwitchPeriod) <= System.currentTimeMillis();\n+  }\n+\n+  /**\n+   * Update the {@link CompactionPolicySwitchInfo} before the start of {@link CompactAllPolicy}\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   */\n+  private void updateCompactionInfo(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    compactionPolicySwitchInfo.setLastCompactAllTime(System.currentTimeMillis());\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().setValue(INIT_COUNTER_VALUE);\n+  }\n+\n+  /**\n+   * @return blobToCompactionPolicySwitchInfoMap which key is storeId and value is {@link CompactionPolicySwitchInfo}\n+   */\n+  Map<String, CompactionPolicySwitchInfo> getBlobToCompactionPolicySwitchInfoMap() {\n+    return this.blobToCompactionPolicySwitchInfoMap;\n+  }\n+\n+  /**\n+   * Back up {@link CompactionPolicySwitchInfo} in Json format for each {@link BlobStore}", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEwMzQ0NA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r448103444", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-07-01T04:07:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NDYxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzM2OQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447347369", "bodyText": "Don't you want to do the update either way?", "author": "lightningrob", "createdAt": "2020-06-30T01:06:18Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+    if (readyToTriggerCompactionAllPolicy(compactionPolicySwitchInfo)) {\n+      logger.trace(\"Return CompactAllPolicy this round\");\n+      updateCompactionInfo(compactionPolicySwitchInfo);", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQyMTcwOA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447421708", "bodyText": "I updated the method name to updateCompactionInfoWhenCompactAll to avoid confusion. This method is used to keep record of the last time when we run CompactAllPolicy, so if we passed \"storeCompactionPolicySwitchTimestampDays\" days since we run compactAllPolicy, we will re-run it again.", "author": "SophieGuo410", "createdAt": "2020-06-30T05:39:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzM2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzgxOA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447347818", "bodyText": "This method name is a mouthful.  Is there a downside to the caller first calling updateCompactionInfo() and then this method which could just be selectCompactionPolicy()?", "author": "lightningrob", "createdAt": "2020-06-30T01:07:49Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private final ObjectMapper mapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getAndRecoverCompactionPolicySwitchInfo(storeId, dataDir);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, storeId, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get and Recover the CompactionPolicySwitchInfo from file.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getAndRecoverCompactionPolicySwitchInfo(String storeId, String dataDir) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchPeriod\" : 3,\n+   *     \"counter\" : 1,\n+   *     \"value\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593463435900\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file) {\n+    try {\n+      ObjectMapper objectMapper = new ObjectMapper();\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchPeriod),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ1NDA5OQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447454099", "bodyText": "I will only update the compactionPolicySwitchInfo when we trigger the compactAllPolicy. Already re-named the method name to updateCompactionInfoWhenCompactAll() instead of updateCompactionInfo().", "author": "SophieGuo410", "createdAt": "2020-06-30T07:00:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0ODE3MQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447348171", "bodyText": "Minor: please move private utility methods to bottom.", "author": "lightningrob", "createdAt": "2020-06-30T01:09:05Z", "path": "ambry-store/src/test/java/com/github/ambry/store/CompactionPolicyTest.java", "diffHunk": "@@ -82,6 +94,135 @@ public CompactionPolicyTest(String compactionPolicyFactoryStr) throws Exception\n     mockBlobStoreStats = blobStore.getBlobStoreStats();\n     CompactionPolicyFactory compactionPolicyFactory = Utils.getObj(compactionPolicyFactoryStr, config, time);\n     compactionPolicy = compactionPolicyFactory.getCompactionPolicy();\n+    dataDirPath = Paths.get(MOUNT_PATH).toAbsolutePath();\n+  }\n+\n+  private void cleanupBackupFiles() throws IOException {", "originalCommit": "fb0055eeaaa4b6fe9a7b43fa92bb682084648645", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ1NDM0Ng==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447454346", "bodyText": "Updated.", "author": "SophieGuo410", "createdAt": "2020-06-30T07:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0ODE3MQ=="}], "type": "inlineReview"}, {"oid": "fd1255131f5ad909d4b5c25cfeeb0813a10fcfc8", "url": "https://github.com/linkedin/ambry/commit/fd1255131f5ad909d4b5c25cfeeb0813a10fcfc8", "message": "address comments", "committedDate": "2020-06-30T05:35:45Z", "type": "forcePushed"}, {"oid": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "url": "https://github.com/linkedin/ambry/commit/b9046a816f853978ca1e09cf85f9e17f41aab6c1", "message": "address comments", "committedDate": "2020-06-30T07:00:46Z", "type": "commit"}, {"oid": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "url": "https://github.com/linkedin/ambry/commit/b9046a816f853978ca1e09cf85f9e17f41aab6c1", "message": "address comments", "committedDate": "2020-06-30T07:00:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5ODQ2OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447898468", "bodyText": "shouldn't we increment the counter before select which policy to use?", "author": "justinlin-linkedin", "createdAt": "2020-06-30T18:36:24Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private static final ObjectMapper objectMapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getCompactionPolicySwitchInfo(storeId, dataDir, blobStoreStats);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();", "originalCommit": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzkwODc0Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447908743", "bodyText": "That's the tricky part. I'm planning to increment after the backup with two reasons:\n\nwhen counter equals to 0, it will run compactAllPolicy. which means if we don't have any back up files yet, we will run compact all policy.\nif we are running compactAll for a long time and host restart, the file backs up the current status and re-run compactAll instead of statsBased.\nLet me know if you have any other concern.", "author": "SophieGuo410", "createdAt": "2020-06-30T18:53:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5ODQ2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5ODc4NA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447898784", "bodyText": "please add datadir and store id in the info log.", "author": "justinlin-linkedin", "createdAt": "2020-06-30T18:37:00Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private static final ObjectMapper objectMapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getCompactionPolicySwitchInfo(storeId, dataDir, blobStoreStats);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);", "originalCommit": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk0MDk4MA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447940980", "bodyText": "Done.", "author": "SophieGuo410", "createdAt": "2020-06-30T19:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5ODc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5OTI0OA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447899248", "bodyText": "nit: please add datadir or store id in the log.", "author": "justinlin-linkedin", "createdAt": "2020-06-30T18:37:48Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private static final ObjectMapper objectMapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getCompactionPolicySwitchInfo(storeId, dataDir, blobStoreStats);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get the CompactionPolicySwitchInfo from file or blobToCompactionPolicySwitchInfoMap.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getCompactionPolicySwitchInfo(String storeId, String dataDir, BlobStoreStats blobStoreStats) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file, blobStoreStats);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchCounterDays),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchCounterDays\" : 3,\n+   *     \"counter\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593492962651\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file, BlobStoreStats blobStoreStats) {\n+    try {\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      blobStoreStats.getMetrics().blobStoreRecoverCompactionPolicySwitchInfoErrorCount.inc();\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchCounterDays),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");", "originalCommit": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk0MTA2Mw==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447941063", "bodyText": "Done.", "author": "SophieGuo410", "createdAt": "2020-06-30T19:53:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg5OTI0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzkwNDQ1NQ==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447904455", "bodyText": "nit: I encourage to create and write data to a temporary file and then rename the temporary file to COMPACT_POLICY_INFO_PATH file\nSomething like\nFile tempFile = new File(dataDir, COMPACT_POLICY_INFO_PATH + \".temp\");\nif (!tempFile.exists()) {\n    tempFile.createNewFile();\n}\nobjectMapper.defaultPrettyPrintingWriter().writeValue(tempFile, compactionPolicySwitchInfo);\ntempFile.renameTo(new File(dataDir, COMPACT_POLICY_INFO_PATH);\n\nThe reason to use a temporary file and rename it after is that POSIX guarantees file rename is an atomic operation, but not the file write. The json file is pretty small so that file writing should be atomic, but it's just a good pattern to follow.", "author": "justinlin-linkedin", "createdAt": "2020-06-30T18:46:50Z", "path": "ambry-store/src/main/java/com/github/ambry/store/HybridCompactionPolicy.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.utils.Time;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+\n+/**\n+ * HybridCompactionPolicy will run compaction with StatsBasedCompactionPolicy more frequently and with CompactAllPolicy\n+ * one time out of storeCompactionPolicySwitchPeriod.\n+ */\n+public class HybridCompactionPolicy implements CompactionPolicy {\n+  private final Time time;\n+  private final StoreConfig storeConfig;\n+  private static final Logger logger = LoggerFactory.getLogger(HybridCompactionPolicy.class);\n+  private final Map<String, CompactionPolicySwitchInfo> blobToCompactionPolicySwitchInfoMap;\n+  private static final ObjectMapper objectMapper = new ObjectMapper();\n+  private static final String COMPACT_POLICY_INFO_PATH = File.separator + \"compactionPolicyInfo.json\";\n+  private static final int INIT_COMPACT_ALL_TIME = 0;\n+  private static final int INIT_COUNTER_VALUE = 0;\n+\n+  HybridCompactionPolicy(StoreConfig storeConfig, Time time) {\n+    this.storeConfig = storeConfig;\n+    this.time = time;\n+    this.blobToCompactionPolicySwitchInfoMap = new HashMap<>();\n+  }\n+\n+  /**\n+   * @param totalCapacity Total capacity of the {@link BlobStore}\n+   * @param usedCapacity Used capacity of the {@link BlobStore}\n+   * @param segmentCapacity Segment capacity of a {@link LogSegment}\n+   * @param segmentHeaderSize Segment header size of a {@link LogSegment}\n+   * @param logSegmentsNotInJournal {@link List<String> } of log segment names which has non overlapping entries with\n+   *                                {@link Journal}\n+   * @param blobStoreStats {@link BlobStoreStats} pertaining to the {@link BlobStore} for which\n+   * {@link CompactionDetails} are requested\n+   * @param dataDir the dir to store {@link CompactionPolicySwitchInfo}\n+   * @return {@link CompactAllPolicy} or {@link StatsBasedCompactionPolicy}'s {@link CompactionDetails} depends on the switching rules.\n+   */\n+  @Override\n+  public CompactionDetails getCompactionDetails(long totalCapacity, long usedCapacity, long segmentCapacity,\n+      long segmentHeaderSize, List<String> logSegmentsNotInJournal, BlobStoreStats blobStoreStats, String dataDir)\n+      throws StoreException {\n+    String storeId = blobStoreStats.getStoreId();\n+    CompactionPolicySwitchInfo compactionPolicySwitchInfo = getCompactionPolicySwitchInfo(storeId, dataDir, blobStoreStats);\n+    CompactionPolicy selectCompactionPolicy =\n+        selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(compactionPolicySwitchInfo);\n+    logger.info(\"Current compaction policy  is : {}\", selectCompactionPolicy);\n+    backUpCompactionPolicyInfo(dataDir, compactionPolicySwitchInfo);\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().increment();\n+    return selectCompactionPolicy.getCompactionDetails(totalCapacity, usedCapacity, segmentCapacity, segmentHeaderSize,\n+        logSegmentsNotInJournal, blobStoreStats, dataDir);\n+  }\n+\n+  /**\n+   * Get the CompactionPolicySwitchInfo from file or blobToCompactionPolicySwitchInfoMap.\n+   * @param dataDir The directory to store the file.\n+   * @param storeId id of the BlobStore\n+   * @return {@link CompactionPolicySwitchInfo} gets from map or recover from file if needed.\n+   */\n+  private CompactionPolicySwitchInfo getCompactionPolicySwitchInfo(String storeId, String dataDir, BlobStoreStats blobStoreStats) {\n+    if (!blobToCompactionPolicySwitchInfoMap.containsKey(storeId)) {\n+      File file = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());\n+      if (file.exists()) {\n+        CompactionPolicySwitchInfo compactionPolicySwitchInfo = recoverCompactionPolicySwitchInfo(file, blobStoreStats);\n+        blobToCompactionPolicySwitchInfoMap.put(storeId, compactionPolicySwitchInfo);\n+      } else {\n+        blobToCompactionPolicySwitchInfoMap.put(storeId,\n+            new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchCounterDays),\n+                INIT_COMPACT_ALL_TIME));\n+      }\n+    }\n+    return blobToCompactionPolicySwitchInfoMap.get(storeId);\n+  }\n+\n+  /**\n+   * Recover the {@link CompactionPolicySwitchInfo} from backup file.\n+   * {\n+   *   \"compactionPolicyCounter\" : {\n+   *     \"storeCompactionPolicySwitchCounterDays\" : 3,\n+   *     \"counter\" : 1\n+   *   },\n+   *   \"lastCompactAllTime\" : 1593492962651\n+   * }\n+   * @param file the backup file stores {@link CompactionPolicySwitchInfo}\n+   */\n+  private CompactionPolicySwitchInfo recoverCompactionPolicySwitchInfo(File file, BlobStoreStats blobStoreStats) {\n+    try {\n+      return objectMapper.readValue(file, CompactionPolicySwitchInfo.class);\n+    } catch (IOException e) {\n+      logger.error(\"Could not deserialize file : {} into {} Object\", file, CompactionPolicySwitchInfo.class.getName());\n+      blobStoreStats.getMetrics().blobStoreRecoverCompactionPolicySwitchInfoErrorCount.inc();\n+      return new CompactionPolicySwitchInfo(new CompactionPolicyCounter(storeConfig.storeCompactionPolicySwitchCounterDays),\n+          INIT_COMPACT_ALL_TIME);\n+    }\n+  }\n+\n+  /**\n+   * Selects which compaction policy to use for current compaction cycle.\n+   * @return {@link CompactionPolicy} to use for current compaction cycle(Selects between {@link StatsBasedCompactionPolicy} and {@link CompactAllPolicy}).\n+   */\n+  CompactionPolicy selectCompactionPolicyAndUpdateCompactionPolicySwitchInfo(\n+      CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (compactionPolicySwitchInfo == null) {\n+      logger.trace(\"CompactionPolicySwitchInfo is null\");\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+    if (readyToTriggerCompactionAllPolicy(compactionPolicySwitchInfo)) {\n+      logger.trace(\"Return CompactAllPolicy this round\");\n+      updateCompactionInfoWhenCompactAll(compactionPolicySwitchInfo);\n+      return new CompactAllPolicy(storeConfig, time);\n+    } else {\n+      if (compactionPolicySwitchInfo.getCompactionPolicyCounter() == null) {\n+        logger.trace(\"Counter is null\");\n+      } else {\n+        logger.trace(\"Return StatsBasedCompactionPolicy this round\");\n+      }\n+      return new StatsBasedCompactionPolicy(storeConfig, time);\n+    }\n+  }\n+\n+  /**\n+   * Determine which compactionPolicy to use for current compaction cycle.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   * @return {@code true} if the counter value equals to 0 or it's storeCompactionPolicySwitchPeriod days past the start time of CompactAllPolicy.\n+   */\n+  private boolean readyToTriggerCompactionAllPolicy(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    return compactionPolicySwitchInfo.getCompactionPolicyCounter() != null\n+        && compactionPolicySwitchInfo.getCompactionPolicyCounter().getCounter() == 0 ||\n+        compactionPolicySwitchInfo.getLastCompactAllTime() + TimeUnit.DAYS.toMillis(\n+            storeConfig.storeCompactionPolicySwitchTimestampDays) <= System.currentTimeMillis();\n+  }\n+\n+  /**\n+   * Update the {@link CompactionPolicySwitchInfo} before the start of {@link CompactAllPolicy}\n+   * Once the compactAllPolicy has been triggered, no matter it's been triggered by timestamp or counter value\n+   * the lastCompactAllTime will be set to current time and the counter value will reset to 0.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   */\n+  private void updateCompactionInfoWhenCompactAll(CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    compactionPolicySwitchInfo.setLastCompactAllTime(System.currentTimeMillis());\n+    compactionPolicySwitchInfo.getCompactionPolicyCounter().setCounter(INIT_COUNTER_VALUE);\n+  }\n+\n+  /**\n+   * @return blobToCompactionPolicySwitchInfoMap which key is storeId and value is {@link CompactionPolicySwitchInfo}\n+   */\n+  Map<String, CompactionPolicySwitchInfo> getBlobToCompactionPolicySwitchInfoMap() {\n+    return this.blobToCompactionPolicySwitchInfoMap;\n+  }\n+\n+  /**\n+   * Back up {@link CompactionPolicySwitchInfo} in Json format for certain {@link BlobStore}\n+   * @param dataDir The directory to store the file.\n+   * @param compactionPolicySwitchInfo the info to determine which {@link CompactionPolicy} to use this round.\n+   */\n+  private void backUpCompactionPolicyInfo(String dataDir, CompactionPolicySwitchInfo compactionPolicySwitchInfo) {\n+    if (dataDir != null && !dataDir.isEmpty()) {\n+      File tempFile = new File(Paths.get(dataDir, COMPACT_POLICY_INFO_PATH).toString());", "originalCommit": "b9046a816f853978ca1e09cf85f9e17f41aab6c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk0MTgzOA==", "url": "https://github.com/linkedin/ambry/pull/1559#discussion_r447941838", "bodyText": "Updated to use temp file and rename after write. I didn't add the file exist check since createNewFile() is able to handle it.", "author": "SophieGuo410", "createdAt": "2020-06-30T19:55:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzkwNDQ1NQ=="}], "type": "inlineReview"}, {"oid": "d823eab417029ca34052d3f19bf7d2aef81acb44", "url": "https://github.com/linkedin/ambry/commit/d823eab417029ca34052d3f19bf7d2aef81acb44", "message": "minor fix", "committedDate": "2020-06-30T19:17:03Z", "type": "commit"}, {"oid": "785decb02a1d5f38a47eadd0dfdb54c9f6e066e9", "url": "https://github.com/linkedin/ambry/commit/785decb02a1d5f38a47eadd0dfdb54c9f6e066e9", "message": "minor fix", "committedDate": "2020-07-01T04:05:58Z", "type": "commit"}]}