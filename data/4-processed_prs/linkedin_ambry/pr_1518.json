{"pr_number": 1518, "pr_title": "Notify routing table updates to replication manager and query the latest leader replica set from Routing table snapshot", "pr_createdAt": "2020-05-12T03:39:47Z", "pr_url": "https://github.com/linkedin/ambry/pull/1518", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3NjA5Mw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r426376093", "bodyText": "I think onRoutingTableChange is called when any of the routing table provider has changed in cluster(Note that each dc has its own routing table provider). In this sense, comments like (of this data center) and (in this data center) are not accurate, can we remove them?", "author": "jsjtzyy", "createdAt": "2020-05-18T05:26:09Z", "path": "ambry-api/src/main/java/com/github/ambry/clustermap/ClusterMapChangeListener.java", "diffHunk": "@@ -26,4 +26,11 @@\n    * @param removedReplicas {@link ReplicaId}(s) that have been removed.\n    */\n   void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaId> removedReplicas);\n+\n+  /**\n+   * Take actions when there is a routing table update. This is triggered whenever there is any change to state of a replicas (of this data center) in the cluster.", "originalCommit": "9cffdd6b68c6f1169a63eb91f5df8842fd159a06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3Njk0Nw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r426376947", "bodyText": "You can make this default, please refer to setInitialLocalPartitions in ClusterParticipant.java. Then you don't need to override it in other classes (where it is no-op)", "author": "jsjtzyy", "createdAt": "2020-05-18T05:29:29Z", "path": "ambry-api/src/main/java/com/github/ambry/clustermap/ClusterMapChangeListener.java", "diffHunk": "@@ -26,4 +26,11 @@\n    * @param removedReplicas {@link ReplicaId}(s) that have been removed.\n    */\n   void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaId> removedReplicas);\n+\n+  /**\n+   * Take actions when there is a routing table update. This is triggered whenever there is any change to state of a replicas (of this data center) in the cluster.\n+   * On this trigger, we can look up the latest states of all the replicas (in this data center) from the routing table snapshot {@link org.apache.helix.spectator.RoutingTableSnapshot}\n+   * with the help of various APIs provided in its class.\n+   */\n+  void onRoutingTableChange();", "originalCommit": "9cffdd6b68c6f1169a63eb91f5df8842fd159a06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3OTg1NQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r426379855", "bodyText": "if onRoutingTableChange has default implementation in the interface class, we are able to remove this.", "author": "jsjtzyy", "createdAt": "2020-05-18T05:41:07Z", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/CloudServiceClusterChangeHandler.java", "diffHunk": "@@ -79,6 +79,13 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n     listeners.forEach(listener -> listener.onReplicaAddedOrRemoved(newReplicas, Collections.emptyList()));\n   }\n \n+  @Override\n+  public void onRoutingTableChange() {", "originalCommit": "9cffdd6b68c6f1169a63eb91f5df8842fd159a06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM4MDM1OQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r426380359", "bodyText": "can be removed", "author": "jsjtzyy", "createdAt": "2020-05-18T05:43:03Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -46,6 +47,7 @@\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantLock;", "originalCommit": "9cffdd6b68c6f1169a63eb91f5df8842fd159a06", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMTU4Mw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427611583", "bodyText": "nit: RoutingTableSnapshot (use CamelCase)", "author": "jsjtzyy", "createdAt": "2020-05-19T21:29:58Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxNzU0OA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427617548", "bodyText": "nit: Try to split these comments into multiple lines.", "author": "jsjtzyy", "createdAt": "2020-05-19T21:42:44Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYyNjgyMQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427626821", "bodyText": "Since we have two read-write locks, we should rename them with more readable(meaningful) names. Following is an example (feel free to use other names)\nrwLock -> replicaMembershipLock\nrwLock1 -> replicaLeaderShipLock", "author": "jsjtzyy", "createdAt": "2020-05-19T22:03:37Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -62,6 +63,7 @@\n   private final StoreConfig storeConfig;\n   private final DataNodeId currentNode;\n   private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  private final ReadWriteLock rwlock1 = new ReentrantReadWriteLock();", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjI3Mw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427736273", "bodyText": "I would suggest renaming (1)currentRemoteLeaderReplicas to previousRemoteLeaderReplicas ;  (2) updatedRemoteLeaderReplicas to currentRemoteLeaderReplicas", "author": "jsjtzyy", "createdAt": "2020-05-20T04:36:25Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0MDc3OQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427740779", "bodyText": "This can be slightly simplified as follows (assuming we new naming in previous comment):\ncurrentRemoteLeaderReplicas = new HashSet<>(partition.getReplicaIdsByState(ReplicaState.LEADER, null));\ncurrentRemoteLeaderReplicas.remove(localLeaderReplica);", "author": "jsjtzyy", "createdAt": "2020-05-20T04:55:33Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)\n+              .stream()\n+              .filter(r -> !r.getDataNodeId().getDatacenterName().equals(dataNodeId.getDatacenterName()))\n+              .collect(Collectors.toList());", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0NjQ3NA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427746474", "bodyText": "No need to generate instance name, we can directly use remoteReplica here. (see toString() in AmbryServerReplica)", "author": "jsjtzyy", "createdAt": "2020-05-20T05:17:01Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)\n+              .stream()\n+              .filter(r -> !r.getDataNodeId().getDatacenterName().equals(dataNodeId.getDatacenterName()))\n+              .collect(Collectors.toList());\n+\n+          //Collect the set of new remote leader replicas\n+          Set<ReplicaId> addedRemoteReplicas = new HashSet<>(updatedRemoteLeaderReplicas);\n+          addedRemoteReplicas.removeAll(currentRemoteLeaderReplicas);\n+\n+          //Collect the set of old remote leader replicas\n+          Set<ReplicaId> removedRemoteReplicas = new HashSet<>(currentRemoteLeaderReplicas);\n+          removedRemoteReplicas.removeAll(updatedRemoteLeaderReplicas);\n+\n+          for (ReplicaId remoteReplica : addedRemoteReplicas) {\n+            // for now, we are just logging the newly found remote leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Adding new remote leader {} for Partition {} to replicate from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc1NzYyMQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427757621", "bodyText": "same here", "author": "jsjtzyy", "createdAt": "2020-05-20T05:55:16Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)\n+              .stream()\n+              .filter(r -> !r.getDataNodeId().getDatacenterName().equals(dataNodeId.getDatacenterName()))\n+              .collect(Collectors.toList());\n+\n+          //Collect the set of new remote leader replicas\n+          Set<ReplicaId> addedRemoteReplicas = new HashSet<>(updatedRemoteLeaderReplicas);\n+          addedRemoteReplicas.removeAll(currentRemoteLeaderReplicas);\n+\n+          //Collect the set of old remote leader replicas\n+          Set<ReplicaId> removedRemoteReplicas = new HashSet<>(currentRemoteLeaderReplicas);\n+          removedRemoteReplicas.removeAll(updatedRemoteLeaderReplicas);\n+\n+          for (ReplicaId remoteReplica : addedRemoteReplicas) {\n+            // for now, we are just logging the newly found remote leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Adding new remote leader {} for Partition {} to replicate from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),\n+                partitionName, remoteReplica.getDataNodeId().getDatacenterName());\n+          }\n+\n+          //remove replicaInfo from existing partitionInfo and replica-threads\n+          List<RemoteReplicaInfo> replicaInfosToRemove = new ArrayList<>();\n+          for (ReplicaId remoteReplica : removedRemoteReplicas) {\n+            // for now, we are just logging the old leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Removing old remote leader {} for Partition {} from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc2MjI2Nw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r427762267", "bodyText": "why we need to check if peerLeaderReplicasByPartition contains partitionName? I feel  it's unnecessary.", "author": "jsjtzyy", "createdAt": "2020-05-20T06:09:37Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -386,33 +451,45 @@ public void onPartitionBecomeStandbyFromBootstrap(String partitionName) {\n     public void onPartitionBecomeLeaderFromStandby(String partitionName) {\n       logger.info(\"Partition state change notification from Standby to Leader received for partition {}\",\n           partitionName);\n-      //Changes for leader based replication - for now, we just log the list of peer leader replicas\n-      // 1. get replica ID of current node from store manager\n-      ReplicaId localReplica = storeManager.getReplica(partitionName);\n+      // Read-write lock to avoid contention between this thread (where partition is added to the map) and thread calling onRoutingTableUpdate() (where partitions are read and updated).\n+      rwlock1.writeLock().lock();\n+      try {\n+        //Changes for leader based replication - for now, we just log the list of peer leader replicas\n+        // 1. get replica ID of current node from store manager\n+        ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+        // 2. Get the peer leader replicas from all data centers for this partition\n+        List<? extends ReplicaId> leaderReplicas =\n+            localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n \n-      // 2. Get the peer leader replicas from all data centers for this partition\n-      List<? extends ReplicaId> leaderReplicas =\n-          localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n-\n-      // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n-      List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n-      for (ReplicaId leaderReplica : leaderReplicas) {\n-        if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n-          peerLeaderReplicas.add(leaderReplica);\n-          logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n-              getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n-              leaderReplica.getDataNodeId().getDatacenterName());\n+        // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+        List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+        for (ReplicaId leaderReplica : leaderReplicas) {\n+          if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+            peerLeaderReplicas.add(leaderReplica);\n+            logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+                getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+                leaderReplica.getDataNodeId().getDatacenterName());\n+          }\n         }\n+        peerLeaderReplicasByPartition.put(partitionName, peerLeaderReplicas);\n+      } finally {\n+        rwlock1.writeLock().unlock();\n       }\n-      peerLeaderReplicasByPartition.put(partitionName, peerLeaderReplicas);\n     }\n \n     @Override\n     public void onPartitionBecomeStandbyFromLeader(String partitionName) {\n       logger.info(\"Partition state change notification from Leader to Standby received for partition {}\",\n           partitionName);\n-      if (peerLeaderReplicasByPartition.containsKey(partitionName)) {\n-        peerLeaderReplicasByPartition.remove((partitionName));\n+      // Read-write lock to avoid contention between this thread (where partition are removed from the map) and thread calling onRoutingTableUpdate() (where partitions are read and updated to map).\n+      rwlock1.writeLock().lock();\n+      try {\n+        if (peerLeaderReplicasByPartition.containsKey(partitionName)) {\n+          peerLeaderReplicasByPartition.remove((partitionName));", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzNTg2Nw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r428135867", "bodyText": "replicaInfosToRemove is never used, is this for future pr?", "author": "jsjtzyy", "createdAt": "2020-05-20T16:11:48Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)\n+              .stream()\n+              .filter(r -> !r.getDataNodeId().getDatacenterName().equals(dataNodeId.getDatacenterName()))\n+              .collect(Collectors.toList());\n+\n+          //Collect the set of new remote leader replicas\n+          Set<ReplicaId> addedRemoteReplicas = new HashSet<>(updatedRemoteLeaderReplicas);\n+          addedRemoteReplicas.removeAll(currentRemoteLeaderReplicas);\n+\n+          //Collect the set of old remote leader replicas\n+          Set<ReplicaId> removedRemoteReplicas = new HashSet<>(currentRemoteLeaderReplicas);\n+          removedRemoteReplicas.removeAll(updatedRemoteLeaderReplicas);\n+\n+          for (ReplicaId remoteReplica : addedRemoteReplicas) {\n+            // for now, we are just logging the newly found remote leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Adding new remote leader {} for Partition {} to replicate from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),\n+                partitionName, remoteReplica.getDataNodeId().getDatacenterName());\n+          }\n+\n+          //remove replicaInfo from existing partitionInfo and replica-threads\n+          List<RemoteReplicaInfo> replicaInfosToRemove = new ArrayList<>();", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzNjk3MQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r428136971", "bodyText": "I know in current PR, peerLeaderReplicasByPartition is only updated but not really used. In future PR, will it be passed into ReplicaThread?", "author": "jsjtzyy", "createdAt": "2020-05-20T16:13:23Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +333,69 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {\n+      // This method is used to update the list of remote leaders used for 'Leader Based replication' model.\n+      // Below are the actions performed in this method:\n+      // 1. For each leader partition in the peerLeaderReplicasByPartition map, we do the following:\n+      // 2. Compare the list of existing remote leaders with the new list obtained from routingtablesnapshot and collect sets of added leaders and removed leaders\n+      // 3. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders\n+      // 4. Update the Replica state (going to be maintained in RemoteReplicaInfo class) of old remote leaders\n+\n+      // Read-write lock here avoids contention between this method and onPartitionBecomeLeaderFromStandby()/onPartitionBecomeStandbyFromLeader() (where leader partitions are added and removed from the map peerLeaderReplicasByPartition).\n+      // Read lock (for threads belonging to different cluster change handlers) is sufficient here because of following reasons:\n+      // 1. We are only updating the existing partitions (not adding or removing) in the 'peerLeaderReplicasByPartition' map. Since, it is a concurrent hash map, PUTs and GETs are clean.\n+      // 2. Updating of Replica state (going to be maintained in RemoteReplicaInfo class) of newly found remote leaders and old leaders will be synchronized in RemoteReplicaInfo class.\n+\n+      rwlock1.readLock().lock();\n+      try {\n+        for (String partitionName : peerLeaderReplicasByPartition.keySet()) {\n+          ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+          PartitionId partition = localLeaderReplica.getPartitionId();\n+          List<ReplicaId> currentRemoteLeaderReplicas = peerLeaderReplicasByPartition.get(partitionName);\n+          List<ReplicaId> updatedRemoteLeaderReplicas = partition.getReplicaIdsByState(ReplicaState.LEADER, null)\n+              .stream()\n+              .filter(r -> !r.getDataNodeId().getDatacenterName().equals(dataNodeId.getDatacenterName()))\n+              .collect(Collectors.toList());\n+\n+          //Collect the set of new remote leader replicas\n+          Set<ReplicaId> addedRemoteReplicas = new HashSet<>(updatedRemoteLeaderReplicas);\n+          addedRemoteReplicas.removeAll(currentRemoteLeaderReplicas);\n+\n+          //Collect the set of old remote leader replicas\n+          Set<ReplicaId> removedRemoteReplicas = new HashSet<>(currentRemoteLeaderReplicas);\n+          removedRemoteReplicas.removeAll(updatedRemoteLeaderReplicas);\n+\n+          for (ReplicaId remoteReplica : addedRemoteReplicas) {\n+            // for now, we are just logging the newly found remote leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Adding new remote leader {} for Partition {} to replicate from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),\n+                partitionName, remoteReplica.getDataNodeId().getDatacenterName());\n+          }\n+\n+          //remove replicaInfo from existing partitionInfo and replica-threads\n+          List<RemoteReplicaInfo> replicaInfosToRemove = new ArrayList<>();\n+          for (ReplicaId remoteReplica : removedRemoteReplicas) {\n+            // for now, we are just logging the old leader replicas\n+            // we will update the Replica state (going to be maintained in RemoteReplicaInfo object) of the relevant remote replicas in later PRs\n+            logger.info(\"Removing old remote leader {} for Partition {} from dc {}\",\n+                getInstanceName(remoteReplica.getDataNodeId().getHostname(), remoteReplica.getDataNodeId().getPort()),\n+                partitionName, remoteReplica.getDataNodeId().getDatacenterName());\n+          }\n+\n+          peerLeaderReplicasByPartition.put(partitionName, updatedRemoteLeaderReplicas);", "originalCommit": "c343a1661f00539cdc79ba691c3c90666e45dc89", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzNjYxOQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429036619", "bodyText": "Any reason to make it protected? Can we change it to private static, something like this:\n  private static final Logger logger = LoggerFactory.getLogger(PartitionLeaderInfo.class);", "author": "jsjtzyy", "createdAt": "2020-05-22T04:28:26Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition;\n+  protected final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  protected final Logger logger = LoggerFactory.getLogger(getClass());", "originalCommit": "4f835ebb3dd5c9916fa2e7b291d3b38c1b47cb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzNjg0OA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429036848", "bodyText": "minor: can remove this and directly initialize it at line 41", "author": "jsjtzyy", "createdAt": "2020-05-22T04:29:42Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition;\n+  protected final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  protected final Logger logger = LoggerFactory.getLogger(getClass());\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    peerLeaderReplicasByPartition = new ConcurrentHashMap<>();", "originalCommit": "4f835ebb3dd5c9916fa2e7b291d3b38c1b47cb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzODA3MA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429038070", "bodyText": "Can you elaborate a little more about purpose of this method? It's never used in current pr, I assume it'll be called in future PR.", "author": "jsjtzyy", "createdAt": "2020-05-22T04:35:58Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition;\n+  protected final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  protected final Logger logger = LoggerFactory.getLogger(getClass());\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+    this.storeManager = storeManager;\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // 2. Get the peer leader replicas from all data centers for this partition\n+    List<? extends ReplicaId> leaderReplicas =\n+        localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+    // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+    List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+    for (ReplicaId leaderReplica : leaderReplicas) {\n+      if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+        peerLeaderReplicas.add(leaderReplica);\n+        logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+            getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+            leaderReplica.getDataNodeId().getDatacenterName());\n+      }\n+    }\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.put(partitionName, new HashSet<>(peerLeaderReplicas));\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Remove a partition from the map of leader partitions. This method is thread safe.\n+   * @param partitionName name of the partition to be removed\n+   */\n+  public void removePartition(String partitionName) {\n+    // Read-write lock avoids contention from threads adding new leaders (addPartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.remove(partitionName);\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Refreshes the list of remote leaders for all leader partitions (by looking at latest leader set in RoutingTableSnapshot). This method is thread safe.\n+   */\n+  public void refreshPeerLeadersForAllPartitions() {\n+\n+    // Read-write lock usage:\n+    // Avoids contention with threads adding new leaders (in addPeerLeadersByPartition()) and removing old leaders (in removePartition()).\n+    // Multiple threads can call this method in parallel as we only update existing partitions (no adding or removing). Since, it is a concurrent hash map, PUTs and GETs will be clean.\n+\n+    rwLock.readLock().lock();\n+    try {\n+      for (Map.Entry<String, Set<ReplicaId>> entry : peerLeaderReplicasByPartition.entrySet()) {\n+        String partitionName = entry.getKey();\n+        ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+        PartitionId partition = localLeaderReplica.getPartitionId();\n+        Set<ReplicaId> previousRemoteLeaderReplicas = entry.getValue();\n+        Set<ReplicaId> currentRemoteLeaderReplicas =\n+            new HashSet<>(partition.getReplicaIdsByState(ReplicaState.LEADER, null));\n+        currentRemoteLeaderReplicas.remove(localLeaderReplica);\n+        if (!previousRemoteLeaderReplicas.equals(currentRemoteLeaderReplicas)) {\n+          peerLeaderReplicasByPartition.put(partitionName, currentRemoteLeaderReplicas);\n+        }\n+      }\n+    } finally {\n+      rwLock.readLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Checks if a partition is a leader\n+   * @param partitionName\n+   * @return true if partition is a leader; else, it returns false\n+   */\n+  public boolean isPartitionPresent(String partitionName) {", "originalCommit": "4f835ebb3dd5c9916fa2e7b291d3b38c1b47cb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0NTU4OQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429045589", "bodyText": "There will be race condition if we use read-lock here.  Following is an example(leader):\nremote DC1           remote DC2\nT1:        R1(R2, R3)             R4(R5, R6)\nT2:       R2 (R1, R3)             R4(R5, R6)           ---> current remote leader replica (R2, R4)\nT3:       R2 (R1, R3)             R5(R4, R6).          ---> current remote leader replica (R2, R5)\nIt's possible (R2, R5) get in first and then be overridden to (R2, R4)", "author": "jsjtzyy", "createdAt": "2020-05-22T05:12:01Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition;\n+  protected final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  protected final Logger logger = LoggerFactory.getLogger(getClass());\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+    this.storeManager = storeManager;\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // 2. Get the peer leader replicas from all data centers for this partition\n+    List<? extends ReplicaId> leaderReplicas =\n+        localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+    // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+    List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+    for (ReplicaId leaderReplica : leaderReplicas) {\n+      if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+        peerLeaderReplicas.add(leaderReplica);\n+        logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+            getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+            leaderReplica.getDataNodeId().getDatacenterName());\n+      }\n+    }\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.put(partitionName, new HashSet<>(peerLeaderReplicas));\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Remove a partition from the map of leader partitions. This method is thread safe.\n+   * @param partitionName name of the partition to be removed\n+   */\n+  public void removePartition(String partitionName) {\n+    // Read-write lock avoids contention from threads adding new leaders (addPartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.remove(partitionName);\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Refreshes the list of remote leaders for all leader partitions (by looking at latest leader set in RoutingTableSnapshot). This method is thread safe.\n+   */\n+  public void refreshPeerLeadersForAllPartitions() {\n+\n+    // Read-write lock usage:\n+    // Avoids contention with threads adding new leaders (in addPeerLeadersByPartition()) and removing old leaders (in removePartition()).\n+    // Multiple threads can call this method in parallel as we only update existing partitions (no adding or removing). Since, it is a concurrent hash map, PUTs and GETs will be clean.\n+\n+    rwLock.readLock().lock();\n+    try {\n+      for (Map.Entry<String, Set<ReplicaId>> entry : peerLeaderReplicasByPartition.entrySet()) {\n+        String partitionName = entry.getKey();\n+        ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+        PartitionId partition = localLeaderReplica.getPartitionId();\n+        Set<ReplicaId> previousRemoteLeaderReplicas = entry.getValue();\n+        Set<ReplicaId> currentRemoteLeaderReplicas =\n+            new HashSet<>(partition.getReplicaIdsByState(ReplicaState.LEADER, null));\n+        currentRemoteLeaderReplicas.remove(localLeaderReplica);\n+        if (!previousRemoteLeaderReplicas.equals(currentRemoteLeaderReplicas)) {\n+          peerLeaderReplicasByPartition.put(partitionName, currentRemoteLeaderReplicas);", "originalCommit": "4f835ebb3dd5c9916fa2e7b291d3b38c1b47cb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0NzY1OA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429047658", "bodyText": "I assume this will be called by replica thread. My question is how frequently it is called?  Do we need to get full snapshot of peerLeaderReplicasByPartition\uff1f", "author": "jsjtzyy", "createdAt": "2020-05-22T05:20:52Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition;\n+  protected final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  protected final Logger logger = LoggerFactory.getLogger(getClass());\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+    this.storeManager = storeManager;\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);", "originalCommit": "4f835ebb3dd5c9916fa2e7b291d3b38c1b47cb9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8f28d49efb53485cdd766ffd26fd2c7113b1c511", "url": "https://github.com/linkedin/ambry/commit/8f28d49efb53485cdd766ffd26fd2c7113b1c511", "message": "Changes to notify replication manager of routing table updates in cluster map. Added new onRoutingTableChange() method in clusterMapChangeListener.", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "2a845420ad80cd2422d0b2a4dfc9d13d88b08b8d", "url": "https://github.com/linkedin/ambry/commit/2a845420ad80cd2422d0b2a4dfc9d13d88b08b8d", "message": "Adding test for checking peerLeadersByPartition map is updated correctly in Replication manager on routing table updates", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "41076009735c9c339bdbfcd7385144626e226669", "url": "https://github.com/linkedin/ambry/commit/41076009735c9c339bdbfcd7385144626e226669", "message": "Make onRoutingTableChange() in ClusterMapChangeListener as default to remove dependency on implementation in CloudServiceClusterChangeHandler and PartitionSelectionHelper", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "e4f36e0270f655f5e6595ea2ec52c44c72b92790", "url": "https://github.com/linkedin/ambry/commit/e4f36e0270f655f5e6595ea2ec52c44c72b92790", "message": "Correct typo in previous commit", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "1b09dcb65442cbb0c3534fb9ff297858be4f517b", "url": "https://github.com/linkedin/ambry/commit/1b09dcb65442cbb0c3534fb9ff297858be4f517b", "message": "Move peerLeaderReplicasByPartition in-mem map inside a new class PartitionLeaderInfo to synchronize access between ReplicationManager and ReplicaThreads", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "url": "https://github.com/linkedin/ambry/commit/46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "message": "Use write lock while refreshing the remote leader replica set in PartitionLeaderInfo", "committedDate": "2020-05-22T20:38:08Z", "type": "commit"}, {"oid": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "url": "https://github.com/linkedin/ambry/commit/46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "message": "Use write lock while refreshing the remote leader replica set in PartitionLeaderInfo", "committedDate": "2020-05-22T20:38:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTQ1MA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429561450", "bodyText": "we should populate partitionLeaderInfo within constructor of ReplicationManager.\nMake sure partitionLeaderInfo.refreshPeerLeadersForAllPartitions()\nis called after\nclusterMap.registerClusterMapListener(new ClusterMapChangeListenerImpl());", "author": "jsjtzyy", "createdAt": "2020-05-23T16:52:57Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationEngine.java", "diffHunk": "@@ -120,7 +120,7 @@ public ReplicationEngine(ReplicationConfig replicationConfig, ClusterMapConfig c\n     this.transformerClassName = transformerClassName;\n     this.storeManager = storeManager;\n     replicaSyncUpManager = clusterParticipant == null ? null : clusterParticipant.getReplicaSyncUpManager();\n-    peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+    partitionLeaderInfo = new PartitionLeaderInfo(storeManager);", "originalCommit": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTYzNg==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429561636", "bodyText": "Please refer to https://docs.google.com/document/d/1L2YuQ-2V9kf3QccYK3VOwCni4HfUM3-SUECeh00fpeU/edit for more details regarding \"Handle cluster changes during startup\"", "author": "jsjtzyy", "createdAt": "2020-05-23T16:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTYyOA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430779628", "bodyText": "Thanks Yingyi. Based our discussion offline, looks like we might not be able to populate leader replicas map in partitionLeaderInfo until the server node participates with helix and the states of the replicas transition to LEADER. Hence, leaving the code as it is and avoiding calling partitionLeaderInfo.refreshPeerLeadersForAllPartitions() in the constructor of replication manager.", "author": "Arun-LinkedIn", "createdAt": "2020-05-27T00:29:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTU0Nw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429561547", "bodyText": "Add following piece of code at the beginning of this method, like what we did in onReplicaAddedOrRemoved:\n      // 1. wait for start() to complete\n      try {\n        startupLatch.await();\n      } catch (InterruptedException e) {\n        logger.warn(\"Waiting for startup is interrupted.\");\n        throw new IllegalStateException(\"Replication manager startup is interrupted while handling routing table change\");\n      }", "author": "jsjtzyy", "createdAt": "2020-05-23T16:54:42Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationManager.java", "diffHunk": "@@ -331,6 +320,18 @@ public void onReplicaAddedOrRemoved(List<ReplicaId> addedReplicas, List<ReplicaI\n         }\n       }\n     }\n+\n+    /**\n+     * {@inheritDoc}\n+     * Note that, this method should be thread-safe because multiple threads (from different cluster change handlers) may\n+     * concurrently call this method.\n+     */\n+    @Override\n+    public void onRoutingTableChange() {", "originalCommit": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTYzOQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430829639", "bodyText": "I just want to double check that there aren't any deadlock scenarios by adding this latch. If onRoutingTableChange is called while startup is happening, we have to make sure that there aren't any other helix callbacks that startup depends on that are waiting to be called from the same helix thread.", "author": "cgtz", "createdAt": "2020-05-27T02:55:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MTU0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5OTgzMw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429599833", "bodyText": "The write lock should be locked ahead of line 67 (before getReplicaIdsByState() is called).  Otherwise there is a race condition to miss remote leadership change.", "author": "jsjtzyy", "createdAt": "2020-05-24T05:12:02Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+  private final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  private static final Logger logger = LoggerFactory.getLogger(PartitionLeaderInfo.class);\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    this.storeManager = storeManager;\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas (this method is only by ReplicationTest for now)\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // 2. Get the peer leader replicas from all data centers for this partition\n+    List<? extends ReplicaId> leaderReplicas =\n+        localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+    // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+    List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+    for (ReplicaId leaderReplica : leaderReplicas) {\n+      if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+        peerLeaderReplicas.add(leaderReplica);\n+        logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+            getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+            leaderReplica.getDataNodeId().getDatacenterName());\n+      }\n+    }\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();", "originalCommit": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5OTkzNw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r429599937", "bodyText": "Just wonder how do we know existingPartition initially has a standby replica on local node?", "author": "jsjtzyy", "createdAt": "2020-05-24T05:13:33Z", "path": "ambry-replication/src/test/java/com/github/ambry/replication/ReplicationTest.java", "diffHunk": "@@ -436,6 +437,67 @@ public void onReplicaAddedOrRemovedCallbackTest() throws Exception {\n     storageManager.shutdown();\n   }\n \n+  /**\n+   * Test cluster map change callback in {@link ReplicationManager} for routing table updates.\n+   * Test setup: When creating partitions, have one replica in LEADER state and rest in STANDBY states on each data center and\n+   * later switch the states of replicas (LEADER to STANDBY and STANDBY to LEADER) on one of the DCs during the test\n+   * Test condition: When replication manager receives onRoutingTableUpdate() indication after the remote replica states were updated,\n+   * map of partition to peer leader replicas stored in replication manager should be updated correctly\n+   * @throws Exception\n+   */\n+  @Test\n+  public void onRoutingTableUpdateCallbackTest() throws Exception {\n+    MockClusterMap clusterMap = new MockClusterMap();\n+    ClusterMapConfig clusterMapConfig = new ClusterMapConfig(verifiableProperties);\n+    MockHelixParticipant.metricRegistry = new MetricRegistry();\n+    MockHelixParticipant mockHelixParticipant = new MockHelixParticipant(clusterMapConfig);\n+    Pair<StorageManager, ReplicationManager> managers =\n+        createStorageManagerAndReplicationManager(clusterMap, clusterMapConfig, mockHelixParticipant);\n+    StorageManager storageManager = managers.getFirst();\n+    MockReplicationManager replicationManager = (MockReplicationManager) managers.getSecond();\n+    MockPartitionId existingPartition =\n+        (MockPartitionId) replicationManager.partitionToPartitionInfo.keySet().iterator().next();\n+    String currentDataCenter =\n+        storageManager.getReplica(existingPartition.toString()).getDataNodeId().getDatacenterName();\n+\n+    //Trigger PartitionStateChangeListener callback to replication manager to notify that a local replica state has changed from STANDBY to LEADER\n+    mockHelixParticipant.onPartitionBecomeLeaderFromStandby(existingPartition.toPathString());", "originalCommit": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "101c17c35420bc6b2062568bbc3374d34ff8f2cd", "url": "https://github.com/linkedin/ambry/commit/101c17c35420bc6b2062568bbc3374d34ff8f2cd", "message": "Add safety latch in onRoutingTableChange() to wait until replication manager starts before processing any routing table updates", "committedDate": "2020-05-27T00:11:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY1ODY0Mw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430658643", "bodyText": "maybe this class and its methods can be package-private (public class -> class)?", "author": "cgtz", "createdAt": "2020-05-26T19:35:39Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {", "originalCommit": "46f82e06fbc3c278f0f30f18ce552a2cb9a5eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgxOTg0Mw==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430819843", "bodyText": "How about making peerLeaderReplicas into a HashSet so that it doesn't have to be copied into a set on line 88?", "author": "cgtz", "createdAt": "2020-05-27T02:16:16Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+  private final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  private static final Logger logger = LoggerFactory.getLogger(PartitionLeaderInfo.class);\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    this.storeManager = storeManager;\n+\n+    // We can't initialize the peerLeaderReplicasByPartition here because we don't know the leader partitions on local node (server) until it has finished participating with Helix.\n+    // peerLeaderReplicasByPartition map will be updated after server participates with Helix and state of replicas transition to LEADER (via onPartitionBecomeLeaderFromStandby())\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas (this method is only by ReplicationTest for now)\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      // 2. Get the peer leader replicas from all data centers for this partition\n+      List<? extends ReplicaId> leaderReplicas =\n+          localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+      // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+      List<ReplicaId> peerLeaderReplicas = new ArrayList<>();", "originalCommit": "101c17c35420bc6b2062568bbc3374d34ff8f2cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNzYyNg==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430827626", "bodyText": "How will this method be used?", "author": "cgtz", "createdAt": "2020-05-27T02:47:33Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+  private final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  private static final Logger logger = LoggerFactory.getLogger(PartitionLeaderInfo.class);\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    this.storeManager = storeManager;\n+\n+    // We can't initialize the peerLeaderReplicasByPartition here because we don't know the leader partitions on local node (server) until it has finished participating with Helix.\n+    // peerLeaderReplicasByPartition map will be updated after server participates with Helix and state of replicas transition to LEADER (via onPartitionBecomeLeaderFromStandby())\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas (this method is only by ReplicationTest for now)\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      // 2. Get the peer leader replicas from all data centers for this partition\n+      List<? extends ReplicaId> leaderReplicas =\n+          localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+      // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+      List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+      for (ReplicaId leaderReplica : leaderReplicas) {\n+        if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+          peerLeaderReplicas.add(leaderReplica);\n+          logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+              getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+              leaderReplica.getDataNodeId().getDatacenterName());\n+        }\n+      }\n+\n+      peerLeaderReplicasByPartition.put(partitionName, new HashSet<>(peerLeaderReplicas));\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Remove a partition from the map of leader partitions. This method is thread safe.\n+   * @param partitionName name of the partition to be removed\n+   */\n+  public void removePartition(String partitionName) {\n+    // Read-write lock avoids contention from threads adding new leaders (addPartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.remove(partitionName);\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Refreshes the list of remote leaders for all leader partitions by querying the latest information from RoutingTableSnapshots of all data centers.\n+   * This method is thread safe.\n+   */\n+  public void refreshPeerLeadersForAllPartitions() {\n+\n+    // Read-write lock usage: Avoids contention between threads doing the following activities:\n+    // 1. Adding new leaders (in addPeerLeadersByPartition())\n+    // 2. Removing old leaders (in removePartition())\n+    // 3. Refreshing remote leader set for existing leaders (current method).\n+    // Explanation for point 3: Multiple threads from different cluster change handlers (we have one cluster change handler for each DC) can trigger onRoutingTableUpdate() in parallel which calls this method to refresh leader partitions.\n+    // We need to make sure that the sequence of gathering remote leaders (from RoutingTableSnapshot of each DC) and updating the map is an atomic operation.\n+\n+    rwLock.writeLock().lock();\n+    try {\n+      for (Map.Entry<String, Set<ReplicaId>> entry : peerLeaderReplicasByPartition.entrySet()) {\n+        String partitionName = entry.getKey();\n+        ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+        PartitionId partition = localLeaderReplica.getPartitionId();\n+        Set<ReplicaId> previousRemoteLeaderReplicas = entry.getValue();\n+        Set<ReplicaId> currentRemoteLeaderReplicas =\n+            new HashSet<>(partition.getReplicaIdsByState(ReplicaState.LEADER, null));\n+        currentRemoteLeaderReplicas.remove(localLeaderReplica);\n+        if (!previousRemoteLeaderReplicas.equals(currentRemoteLeaderReplicas)) {\n+          peerLeaderReplicasByPartition.put(partitionName, currentRemoteLeaderReplicas);\n+        }\n+      }\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Checks if a remote replica is a leader for a partition (Pre-requisite: the partition itself should be a leader locally).\n+   * @param partitionName name of local leader partition\n+   * @param replicaId remote replica to be checked\n+   * @return true if remote replica is a leader for a partition (Pre-requisite: the partition itself should be a leader locally).\n+   */\n+  public boolean isPeerReplicaLeaderForPartition(String partitionName, ReplicaId replicaId) {", "originalCommit": "101c17c35420bc6b2062568bbc3374d34ff8f2cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg0NTY0OQ==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430845649", "bodyText": "We want to use this method in inter-colo ReplicaThreads to filter leader and non-leader (standby, offline, bootstrap) replicas from their lists of remote replicas.", "author": "Arun-LinkedIn", "createdAt": "2020-05-27T04:06:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNzYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyODU3OA==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430828578", "bodyText": "nit: to collapse two map lookups into one, you can change this to:\nSet<ReplicaId> peerLeaders = peerLeaderReplicasByPartition.get(partitionName);\nreturn peerLeaders != null && peerLeaders.contains(replicaId);\nor\nreturn peerLeaders.getOrDefault(partitionName, Collections.emptySet()).contains(replicaId);", "author": "cgtz", "createdAt": "2020-05-27T02:51:23Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * Maintains the list of leader partitions on local node and their corresponding peer leaders in remote data centers\n+ */\n+public class PartitionLeaderInfo {\n+\n+  private final Map<String, Set<ReplicaId>> peerLeaderReplicasByPartition = new ConcurrentHashMap<>();\n+  private final StoreManager storeManager;\n+  private final ReadWriteLock rwLock = new ReentrantReadWriteLock();\n+  private static final Logger logger = LoggerFactory.getLogger(PartitionLeaderInfo.class);\n+\n+  public PartitionLeaderInfo(StoreManager storeManager) {\n+    this.storeManager = storeManager;\n+\n+    // We can't initialize the peerLeaderReplicasByPartition here because we don't know the leader partitions on local node (server) until it has finished participating with Helix.\n+    // peerLeaderReplicasByPartition map will be updated after server participates with Helix and state of replicas transition to LEADER (via onPartitionBecomeLeaderFromStandby())\n+  }\n+\n+  /**\n+   * Get a map of partitions to their sets of peer leader replicas (this method is only by ReplicationTest for now)\n+   * @return an unmodifiable map of peer leader replicas stored by partition {@link PartitionLeaderInfo#peerLeaderReplicasByPartition}\n+   */\n+  public Map<String, Set<ReplicaId>> getPeerLeaderReplicasByPartition() {\n+    return Collections.unmodifiableMap(peerLeaderReplicasByPartition);\n+  }\n+\n+  /**\n+   * Add a leader partition and its set of peer leader replicas. This method is thread safe.\n+   * @param partitionName name of the partition to be added\n+   */\n+  public void addPartition(String partitionName) {\n+\n+    // 1. get local replica from store manager\n+    ReplicaId localReplica = storeManager.getReplica(partitionName);\n+\n+    // Read-write lock avoids contention from threads removing old leader partitions (removePartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      // 2. Get the peer leader replicas from all data centers for this partition\n+      List<? extends ReplicaId> leaderReplicas =\n+          localReplica.getPartitionId().getReplicaIdsByState(ReplicaState.LEADER, null);\n+\n+      // 3. Log the list of leader replicas associated with this partition (will be used later for leadership based replication)\n+      List<ReplicaId> peerLeaderReplicas = new ArrayList<>();\n+      for (ReplicaId leaderReplica : leaderReplicas) {\n+        if (leaderReplica.getDataNodeId() != localReplica.getDataNodeId()) {\n+          peerLeaderReplicas.add(leaderReplica);\n+          logger.info(\"Partition {} on node instance {} is leader in remote dc {}\", partitionName,\n+              getInstanceName(leaderReplica.getDataNodeId().getHostname(), leaderReplica.getDataNodeId().getPort()),\n+              leaderReplica.getDataNodeId().getDatacenterName());\n+        }\n+      }\n+\n+      peerLeaderReplicasByPartition.put(partitionName, new HashSet<>(peerLeaderReplicas));\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Remove a partition from the map of leader partitions. This method is thread safe.\n+   * @param partitionName name of the partition to be removed\n+   */\n+  public void removePartition(String partitionName) {\n+    // Read-write lock avoids contention from threads adding new leaders (addPartition()) and threads updating existing leader partitions (refreshPeerLeadersForAllPartitions())\n+    rwLock.writeLock().lock();\n+    try {\n+      peerLeaderReplicasByPartition.remove(partitionName);\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Refreshes the list of remote leaders for all leader partitions by querying the latest information from RoutingTableSnapshots of all data centers.\n+   * This method is thread safe.\n+   */\n+  public void refreshPeerLeadersForAllPartitions() {\n+\n+    // Read-write lock usage: Avoids contention between threads doing the following activities:\n+    // 1. Adding new leaders (in addPeerLeadersByPartition())\n+    // 2. Removing old leaders (in removePartition())\n+    // 3. Refreshing remote leader set for existing leaders (current method).\n+    // Explanation for point 3: Multiple threads from different cluster change handlers (we have one cluster change handler for each DC) can trigger onRoutingTableUpdate() in parallel which calls this method to refresh leader partitions.\n+    // We need to make sure that the sequence of gathering remote leaders (from RoutingTableSnapshot of each DC) and updating the map is an atomic operation.\n+\n+    rwLock.writeLock().lock();\n+    try {\n+      for (Map.Entry<String, Set<ReplicaId>> entry : peerLeaderReplicasByPartition.entrySet()) {\n+        String partitionName = entry.getKey();\n+        ReplicaId localLeaderReplica = storeManager.getReplica(partitionName);\n+        PartitionId partition = localLeaderReplica.getPartitionId();\n+        Set<ReplicaId> previousRemoteLeaderReplicas = entry.getValue();\n+        Set<ReplicaId> currentRemoteLeaderReplicas =\n+            new HashSet<>(partition.getReplicaIdsByState(ReplicaState.LEADER, null));\n+        currentRemoteLeaderReplicas.remove(localLeaderReplica);\n+        if (!previousRemoteLeaderReplicas.equals(currentRemoteLeaderReplicas)) {\n+          peerLeaderReplicasByPartition.put(partitionName, currentRemoteLeaderReplicas);\n+        }\n+      }\n+    } finally {\n+      rwLock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * Checks if a remote replica is a leader for a partition (Pre-requisite: the partition itself should be a leader locally).\n+   * @param partitionName name of local leader partition\n+   * @param replicaId remote replica to be checked\n+   * @return true if remote replica is a leader for a partition (Pre-requisite: the partition itself should be a leader locally).\n+   */\n+  public boolean isPeerReplicaLeaderForPartition(String partitionName, ReplicaId replicaId) {\n+    rwLock.readLock().lock();\n+    try {\n+      return peerLeaderReplicasByPartition.containsKey(partitionName) && peerLeaderReplicasByPartition.get(", "originalCommit": "101c17c35420bc6b2062568bbc3374d34ff8f2cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "50010713de5d43efedf0727586c2a90d909d6a3f", "url": "https://github.com/linkedin/ambry/commit/50010713de5d43efedf0727586c2a90d909d6a3f", "message": "Making PartitionLeaderInfo to package-private and collapsing two map lookups to one", "committedDate": "2020-05-27T03:56:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg1NjY0Ng==", "url": "https://github.com/linkedin/ambry/pull/1518#discussion_r430856646", "bodyText": "minor: can be removed", "author": "jsjtzyy", "createdAt": "2020-05-27T04:55:55Z", "path": "ambry-replication/src/main/java/com/github/ambry/replication/PartitionLeaderInfo.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+\n+package com.github.ambry.replication;\n+\n+import com.github.ambry.clustermap.PartitionId;\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.server.StoreManager;\n+import java.util.ArrayList;", "originalCommit": "50010713de5d43efedf0727586c2a90d909d6a3f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f8c28c4202794996b0f69f310ce0778beef47fe5", "url": "https://github.com/linkedin/ambry/commit/f8c28c4202794996b0f69f310ce0778beef47fe5", "message": "Minor: removed unused import package", "committedDate": "2020-05-27T05:11:20Z", "type": "commit"}]}