{"pr_number": 1455, "pr_title": "Start supporting undelete in compaction", "pr_createdAt": "2020-04-03T18:45:45Z", "pr_url": "https://github.com/linkedin/ambry/pull/1455", "timeline": [{"oid": "e70902eec88ce420333b4a896a6522b08c21fe74", "url": "https://github.com/linkedin/ambry/commit/e70902eec88ce420333b4a896a6522b08c21fe74", "message": "Fix compactor test", "committedDate": "2020-04-08T03:36:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYyMDAzNQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r406620035", "bodyText": "Can you explain why this logic is moved?", "author": "zzmao", "createdAt": "2020-04-10T06:19:56Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -556,27 +530,15 @@ private boolean copyDataByLogSegment(LogSegment logSegmentToCopy, FileSpan dupli\n   private boolean copyDataByIndexSegment(LogSegment logSegmentToCopy, IndexSegment indexSegmentToCopy,\n       FileSpan duplicateSearchSpan) throws IOException, StoreException {\n     logger.debug(\"Copying data from {}\", indexSegmentToCopy.getFile());\n-    List<IndexEntry> allIndexEntries = new ArrayList<>();\n     // call into diskIOScheduler to make sure we can proceed (assuming it won't be 0).\n     diskIOScheduler.getSlice(INDEX_SEGMENT_READ_JOB_NAME, INDEX_SEGMENT_READ_JOB_NAME, 1);\n-    // get all entries. We get one entry per key\n-    indexSegmentToCopy.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n-        new AtomicLong(0), true);\n-\n-    // save a token for restart (the key gets ignored but is required to be non null for construction)", "originalCommit": "590ee784f0700f82f335a060ece1494f33198e1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg4OTUwNA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r406889504", "bodyText": "It's not removed, it's relocated, to the method getValidIndexEntries.", "author": "justinlin-linkedin", "createdAt": "2020-04-10T18:40:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYyMDAzNQ=="}], "type": "inlineReview"}, {"oid": "069572cfa54e9caf48fcfb808b68354714d93701", "url": "https://github.com/linkedin/ambry/commit/069572cfa54e9caf48fcfb808b68354714d93701", "message": "More comments", "committedDate": "2020-04-10T19:49:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1OTQ2NA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408259464", "bodyText": "java doc please", "author": "jsjtzyy", "createdAt": "2020-04-14T16:10:12Z", "path": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java", "diffHunk": "@@ -110,6 +110,10 @@\n   @Default(\"10*1024*1024\")\n   public final int storeCompactionMinBufferSize;\n \n+  @Config(\"store.compaction.filter\")\n+  @Default(\"IndexSegmentValidEntryFilterWithoutUndelete\")\n+  public final String storeCompactionFilter;", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2NjcwOA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408566708", "bodyText": "it seems filter here is unnecessary, can be removed", "author": "jsjtzyy", "createdAt": "2020-04-15T03:59:28Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -125,6 +127,13 @@ public boolean accept(File dir, String name) {\n     this.sessionId = sessionId;\n     this.incarnationId = incarnationId;\n     this.useDirectIO = Utils.isLinux() && config.storeCompactionEnableDirectIO;\n+    IndexSegmentValidEntryFilter filter;\n+    if (config.storeCompactionFilter.equals(IndexSegmentValidEntryFilterWithoutUndelete.class.getSimpleName())) {\n+      filter = new IndexSegmentValidEntryFilterWithoutUndelete();\n+    } else {\n+      filter = new IndexSegmentValidEntryFilterWithUndelete();\n+    }\n+    validEntryFilter = filter;", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzMjE4OQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409132189", "bodyText": "the validEntryFilter field is a final field, it has to be initialized in the constructor. If we initialize it in the if-else statement, they might be some compile error. This way, the validEntryFilter is sure to be initialized.", "author": "justinlin-linkedin", "createdAt": "2020-04-15T21:03:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2NjcwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE5NDk3MQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409194971", "bodyText": "I actually tried to initialize it in if-else statement, the tests passed and intellij didn't complain. (might worth giving it a shot)", "author": "jsjtzyy", "createdAt": "2020-04-15T23:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2NjcwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgyMDY2OA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409820668", "bodyText": "You are right, I will update.", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:12:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2NjcwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2ODQ4MA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408568480", "bodyText": "nit: without", "author": "jsjtzyy", "createdAt": "2020-04-15T04:07:00Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU4MDAyMw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408580023", "bodyText": "I know this is from previous code, could you clarify a little bit? In current implementation, does this mean TtlUpdate index entry will be dropped,  PUT entry's ttl is set to -1 and TtlUpdate record won't be copied to log segment?", "author": "jsjtzyy", "createdAt": "2020-04-15T04:54:01Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEzNzcwNw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409137707", "bodyText": "These comments are a bit wired, it's not the rules for compaction, it's actually the rules for returning IndexValues from an index segment. Like this Rule PUT + TTL -> PUT, it means if a index segment has PUT and TTL for the same key, indexSegment.getIndexEntriesSince would return PUT.", "author": "justinlin-linkedin", "createdAt": "2020-04-15T21:14:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU4MDAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408590827", "bodyText": "When will this happen?", "author": "jsjtzyy", "createdAt": "2020-04-15T05:34:33Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE0MTk0OQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409141949", "bodyText": "This method would iterate through all the IndexValue in an IndexSegment. For same key, they might be multiple IndexValue, like. PUT + TTL + DELETE, since IndexSegment sorts IndexValues based on their keys, so if we have multiple IndexValues for the same key in the same IndexSegment, then they must follow each other. So here, if we detect the current key equals to the previous one, we don't have to search for the final state for the same key again.", "author": "justinlin-linkedin", "createdAt": "2020-04-15T21:23:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI1OTMyMw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409259323", "bodyText": "Makes sense. Better to add a comment here.", "author": "jsjtzyy", "createdAt": "2020-04-16T03:27:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NjUzOA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411646538", "bodyText": "+1 , comment to explain why don't need to query latest state: because they share same latest state.", "author": "zzmao", "createdAt": "2020-04-20T19:49:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTEwMA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408595100", "bodyText": "what does the retention mean, within retention time?", "author": "jsjtzyy", "createdAt": "2020-04-15T05:48:22Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE0MjE0Mg==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409142142", "bodyText": "Yes, current retention time is 7 days.", "author": "justinlin-linkedin", "createdAt": "2020-04-15T21:23:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTEwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0OTA4NQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411649085", "bodyText": "final -> latest", "author": "zzmao", "createdAt": "2020-04-20T19:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTYyMQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408595621", "bodyText": "Can you explain the intention of currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()?\nAlso looks like if CurrentFinalState.isDelete(), the if/else if can be combined together.", "author": "jsjtzyy", "createdAt": "2020-04-15T05:49:58Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {", "originalCommit": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE0NDEyNA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409144124", "bodyText": "compactionLog.getCompactionDetails().getReferenceTimeMs() is set to be the current time - retention duration when starting compaction. So if the retention duration is 7 days, then the reference time is now - 7d. And this if statement means if the delete's operation is later than now - 7d (e.g. now - 1d), the delete and ttl update should not be compacted.\nI have two if statement under this if (currentFinalState.isDelete()), it's just easier to read in this way.", "author": "justinlin-linkedin", "createdAt": "2020-04-15T21:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTYyMQ=="}], "type": "inlineReview"}, {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa", "url": "https://github.com/linkedin/ambry/commit/f63f846e7ecc866775a4712892c9ae06c1900daa", "message": "Comments", "committedDate": "2020-04-15T21:27:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTIyNTk0OA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409225948", "bodyText": "This can be simplified to return srcValue.isUndelete(). We have precluded other cases.  (The comment doesn't need change. Leave it here is helpful to understand)", "author": "jsjtzyy", "createdAt": "2020-04-16T01:23:23Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+        // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+        // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+        //\n+        // For condition one, we will keep TTL_UPDATE, for condition 2, we will remove it.\n+        if (isOffsetUnderCompaction(srcValue.getOffset())) {\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      if (value == null) {\n+        return false;\n+      }\n+      if (value.getLifeVersion() > srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion higher than source value, then this\n+        // must be a duplicate.\n+        return true;\n+      } else if (value.getLifeVersion() < srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion lower than source value, then this\n+        // is not a duplicate.\n+        return false;\n+      } else {\n+        // When the lifeVersions are the same, the order of the record are **FIX** as\n+        // P/U -> T -> D\n+        if (value.isDelete()) {\n+          return true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          return !srcValue.isDelete();\n+        } else if (value.isUndelete()) {\n+          if (srcValue.isPut()) {\n+            throw new IllegalStateException(\n+                \"An Undelete[\" + value + \"] and a Put[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a UNDELETE without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTIyNjgzMQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409226831", "bodyText": "same here, this can be simplified (as long as comment is here to help us understand the only possible duplicate case is PUT)", "author": "jsjtzyy", "createdAt": "2020-04-16T01:26:23Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+        // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+        // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+        //\n+        // For condition one, we will keep TTL_UPDATE, for condition 2, we will remove it.\n+        if (isOffsetUnderCompaction(srcValue.getOffset())) {\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      if (value == null) {\n+        return false;\n+      }\n+      if (value.getLifeVersion() > srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion higher than source value, then this\n+        // must be a duplicate.\n+        return true;\n+      } else if (value.getLifeVersion() < srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion lower than source value, then this\n+        // is not a duplicate.\n+        return false;\n+      } else {\n+        // When the lifeVersions are the same, the order of the record are **FIX** as\n+        // P/U -> T -> D\n+        if (value.isDelete()) {\n+          return true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          return !srcValue.isDelete();\n+        } else if (value.isUndelete()) {\n+          if (srcValue.isPut()) {\n+            throw new IllegalStateException(\n+                \"An Undelete[\" + value + \"] and a Put[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a UNDELETE without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        } else {\n+          if (srcValue.isUndelete()) {\n+            throw new IllegalStateException(\n+                \"A Put[\" + value + \"] and an Undelete[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a PUT without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI4NTczOA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409285738", "bodyText": "better to change Action to Is Valid", "author": "jsjtzyy", "createdAt": "2020-04-16T05:11:58Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI4NzI0Nw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409287247", "bodyText": "why we don't check retention time here?", "author": "jsjtzyy", "createdAt": "2020-04-16T05:17:07Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgyNTUwNA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409825504", "bodyText": "We don't have to. The current value is delete, and it's only valid when it's the last delete for this blob. eg. P -> D0 -> U1 -> D1. If D0 is the current value, then it's not valid, since D1 is the last delete. eg. P -> D0 -> U1 -> D1 -> U2. If D1 is the current value, then it's not valid, since final state is not a delete.", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI4NzI0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MDQxOQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409290419", "bodyText": "To make the comment clearer. could you change srcValue to PUT?\n(same for it, if I understand correctly, it is also referring to PUT)", "author": "jsjtzyy", "createdAt": "2020-04-16T05:28:02Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgzMjc4NA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409832784", "bodyText": "sure", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MDQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MjMxNQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409292315", "bodyText": "I probably asked this before, here I just want to confirm again: both delete and undelete will carry the TTL from original PUT, right?", "author": "jsjtzyy", "createdAt": "2020-04-16T05:34:26Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgzMzM3Mw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409833373", "bodyText": "They will carry the latest expiration date, which means if there is no TTL_UPDATE after PUT, then they will carry the expiration date of PUT.", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:37:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MjMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NTg3OA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409295878", "bodyText": "if this is a sealed segment, is numberOfEntries(mmap) a fixed value? If yes, it's worth keeping it somewhere without calculating multiple times.", "author": "jsjtzyy", "createdAt": "2020-04-16T05:45:39Z", "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegment.java", "diffHunk": "@@ -1170,5 +1183,34 @@ static Offset getIndexSegmentStartOffset(String filename) {\n     }\n     return new Offset(logSegmentName, Long.parseLong(startOffsetValue));\n   }\n+\n+  /**\n+   * An {@link IndexEntry} {@link Iterator} for a sealed {@link IndexSegment}. This {@link Iterator} should\n+   * only be used in the compaction so that the {@link IndexSegment} should be sealed already.\n+   */\n+  class SealedIndexSegmentEntryIterator implements Iterator<IndexEntry> {\n+    private int currentIdx = 0;\n+    private ByteBuffer mmap = serEntries.duplicate();\n+    private byte[] valueBuf = new byte[valueSize];\n+\n+    @Override\n+    public boolean hasNext() {\n+      return currentIdx < numberOfEntries(mmap);", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgzNDU5NA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409834594", "bodyText": "sure", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:39:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NTg3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NjU0Mg==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409296542", "bodyText": "This will cause NullPointException if it failed to read index. Is this intended?", "author": "jsjtzyy", "createdAt": "2020-04-16T05:47:44Z", "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegment.java", "diffHunk": "@@ -1170,5 +1183,34 @@ static Offset getIndexSegmentStartOffset(String filename) {\n     }\n     return new Offset(logSegmentName, Long.parseLong(startOffsetValue));\n   }\n+\n+  /**\n+   * An {@link IndexEntry} {@link Iterator} for a sealed {@link IndexSegment}. This {@link Iterator} should\n+   * only be used in the compaction so that the {@link IndexSegment} should be sealed already.\n+   */\n+  class SealedIndexSegmentEntryIterator implements Iterator<IndexEntry> {\n+    private int currentIdx = 0;\n+    private ByteBuffer mmap = serEntries.duplicate();\n+    private byte[] valueBuf = new byte[valueSize];\n+\n+    @Override\n+    public boolean hasNext() {\n+      return currentIdx < numberOfEntries(mmap);\n+    }\n+\n+    @Override\n+    public IndexEntry next() {\n+      try {\n+        StoreKey key = getKeyAt(mmap, currentIdx);\n+        mmap.get(valueBuf);\n+        return new IndexEntry(key, new IndexValue(startOffset.getName(), ByteBuffer.wrap(valueBuf), getVersion()));\n+      } catch (Exception e) {\n+        logger.error(\"Failed to read index entry at \" + currentIdx, e);\n+      } finally {\n+        currentIdx++;\n+      }\n+      return null;", "originalCommit": "f63f846e7ecc866775a4712892c9ae06c1900daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTgzNDY2Ng==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409834666", "bodyText": "that's right, will update.", "author": "justinlin-linkedin", "createdAt": "2020-04-16T20:39:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NjU0Mg=="}], "type": "inlineReview"}, {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "url": "https://github.com/linkedin/ambry/commit/6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "message": "Comments", "committedDate": "2020-04-16T21:45:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0MzcxOA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411643718", "bodyText": "minor: index", "author": "zzmao", "createdAt": "2020-04-20T19:44:52Z", "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegmentValidEntryFilter.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import java.util.List;\n+\n+\n+/**\n+ * Interface to return valid entries from a index segment. Those valid entries will be copied to a new segment\n+ * in compaction process.\n+ */\n+public interface IndexSegmentValidEntryFilter {\n+  /**\n+   * Return the valid {@link IndexEntry} from a given {@link IndexSegment}.\n+   * @param indexSegment The {@link IndexSegment} in which to return all valid {@link IndexEntry}.\n+   * @param duplicateSearchSpan The {@link FileSpan} to search for duplication.\n+   * @param checkAlreadyCopied {@code true} if a check for existence in the swap spaces has to be executed (due to\n+   *                                       crash/shutdown), {@code false} otherwise.\n+   * @return the list of valid {@link IndexEntry} sorted by their offset.\n+   */\n+  List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan, boolean checkAlreadyCopied)\n+      throws StoreException;\n+\n+  /**\n+   * Checks if a record already exists in {@code idx}.\n+   * @param idx the {@link PersistentIndex} to search in", "originalCommit": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NDQ2NA==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411644464", "bodyText": "I like it.", "author": "zzmao", "createdAt": "2020-04-20T19:46:09Z", "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegmentValidEntryFilter.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import java.util.List;\n+\n+\n+/**\n+ * Interface to return valid entries from a index segment. Those valid entries will be copied to a new segment\n+ * in compaction process.\n+ */\n+public interface IndexSegmentValidEntryFilter {", "originalCommit": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NjEzMQ==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411646131", "bodyText": "final -> latest   to align with comments.", "author": "zzmao", "createdAt": "2020-04-20T19:48:54Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +996,536 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId, incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Is Valid                                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;", "originalCommit": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0OTI5Nw==", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411649297", "bodyText": "final -> latest", "author": "zzmao", "createdAt": "2020-04-20T19:54:19Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +996,536 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId, incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Is Valid                                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        // If an IndexSegment contains more than one IndexValue for the same StoreKey, then they must follow each other\n+        // since IndexSegment store IndexValues based on StoreKey. If the current key equals to the previous key, then\n+        // we don't have to query the final state again.", "originalCommit": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b22c38bea7080b54e82e38c4d277a8fe76078499", "url": "https://github.com/linkedin/ambry/commit/b22c38bea7080b54e82e38c4d277a8fe76078499", "message": "WIP", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "3be91ecd82e4bc6fe1847e90b1642b568c2864c7", "url": "https://github.com/linkedin/ambry/commit/3be91ecd82e4bc6fe1847e90b1642b568c2864c7", "message": "WIP: start supporting undelete in compaction", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "377623725e10d1fa847c73506e981443cd8f678b", "url": "https://github.com/linkedin/ambry/commit/377623725e10d1fa847c73506e981443cd8f678b", "message": "Fix compactor test", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "ec95e8c2a4b6d56c42c979410da6f4eae5975513", "url": "https://github.com/linkedin/ambry/commit/ec95e8c2a4b6d56c42c979410da6f4eae5975513", "message": "Add new tests", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "f1a0036c183253e7a7f82146c22cd5e4dd3cb055", "url": "https://github.com/linkedin/ambry/commit/f1a0036c183253e7a7f82146c22cd5e4dd3cb055", "message": "Change", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "6fc2a8b3b1f1a5ea16bbcd28868dfdcb0248ad78", "url": "https://github.com/linkedin/ambry/commit/6fc2a8b3b1f1a5ea16bbcd28868dfdcb0248ad78", "message": "More comments", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "1a24fff27b79b52e6cb17d1d3b3ca2d43ea0cfac", "url": "https://github.com/linkedin/ambry/commit/1a24fff27b79b52e6cb17d1d3b3ca2d43ea0cfac", "message": "Use config to turn on new code", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "b896361e4f7eecbbc0cc58652150cf01c4b1966e", "url": "https://github.com/linkedin/ambry/commit/b896361e4f7eecbbc0cc58652150cf01c4b1966e", "message": "Comments", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "bed482ed041cab159fb81d5dbfbb475fcd9ab370", "url": "https://github.com/linkedin/ambry/commit/bed482ed041cab159fb81d5dbfbb475fcd9ab370", "message": "Comments", "committedDate": "2020-04-20T20:07:43Z", "type": "commit"}, {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0", "url": "https://github.com/linkedin/ambry/commit/8819e4cd00112bfa9995a6e6b4145718fb0533f0", "message": "Address comments", "committedDate": "2020-04-20T20:16:54Z", "type": "commit"}, {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0", "url": "https://github.com/linkedin/ambry/commit/8819e4cd00112bfa9995a6e6b4145718fb0533f0", "message": "Address comments", "committedDate": "2020-04-20T20:16:54Z", "type": "forcePushed"}]}