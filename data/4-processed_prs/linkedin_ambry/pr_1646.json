{"pr_number": 1646, "pr_title": "[CLOUD_CONTAINER_DELETION] Core logic for container compaction", "pr_createdAt": "2020-10-05T20:56:28Z", "pr_url": "https://github.com/linkedin/ambry/pull/1646", "timeline": [{"oid": "72e3f583cf2343b2b156220843caf01456f4819f", "url": "https://github.com/linkedin/ambry/commit/72e3f583cf2343b2b156220843caf01456f4819f", "message": "Production code for container compaction.", "committedDate": "2020-10-15T20:24:21Z", "type": "forcePushed"}, {"oid": "3874f5adc6f3ecb7c431bebdf3db626ccbeda43b", "url": "https://github.com/linkedin/ambry/commit/3874f5adc6f3ecb7c431bebdf3db626ccbeda43b", "message": "Refactor compaction code and cleanup interfaces.", "committedDate": "2020-10-16T20:55:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2MDM5MQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r508960391", "bodyText": "Can we start with false and deploy config to turn it on?", "author": "jsjtzyy", "createdAt": "2020-10-21T03:03:26Z", "path": "ambry-api/src/main/java/com/github/ambry/config/CloudConfig.java", "diffHunk": "@@ -198,6 +202,13 @@\n   @Default(\"true\")\n   public final boolean cloudBlobCompactionEnabled;\n \n+  /**\n+   * Whether deprecated container compaction is enabled.\n+   */\n+  @Config(CLOUD_CONTAINER_COMPACTION_ENABLED)\n+  @Default(\"true\")", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3MjMwOA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513072308", "bodyText": "fixed.", "author": "ankagrawal", "createdAt": "2020-10-27T22:32:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2MDM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2MDkxMg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r508960912", "bodyText": "suggest using getIntInRange", "author": "jsjtzyy", "createdAt": "2020-10-21T03:05:19Z", "path": "ambry-api/src/main/java/com/github/ambry/config/CloudConfig.java", "diffHunk": "@@ -360,8 +385,12 @@ public CloudConfig(VerifiableProperties verifiableProperties) {\n     cloudDeletedBlobRetentionDays =\n         verifiableProperties.getInt(CLOUD_DELETED_BLOB_RETENTION_DAYS, DEFAULT_RETENTION_DAYS);\n     cloudBlobCompactionEnabled = verifiableProperties.getBoolean(CLOUD_BLOB_COMPACTION_ENABLED, true);\n+    cloudContainerCompactionEnabled = verifiableProperties.getBoolean(CLOUD_CONTAINER_COMPACTION_ENABLED, true);\n     cloudBlobCompactionIntervalHours = verifiableProperties.getInt(CLOUD_BLOB_COMPACTION_INTERVAL_HOURS, 24);\n+    cloudContainerCompactionIntervalHours = verifiableProperties.getInt(CLOUD_CONTAINER_COMPACTION_INTERVAL_HOURS, 24);\n     cloudBlobCompactionStartupDelaySecs = verifiableProperties.getInt(CLOUD_BLOB_COMPACTION_STARTUP_DELAY_SECS, 600);\n+    cloudContainerCompactionStartupDelaySecs =\n+        verifiableProperties.getInt(CLOUD_CONTAINER_COMPACTION_STARTUP_DELAY_SECS, 600);", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3Mzg2MA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513073860", "bodyText": "fixed.", "author": "ankagrawal", "createdAt": "2020-10-27T22:36:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2MDkxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4NTQ3NQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r508985475", "bodyText": "cloudContainerCompactor.compactAssignedDeprecatedContainers will throw NullPointerException if cloud container compaction is disabled (see line 81)", "author": "jsjtzyy", "createdAt": "2020-10-21T04:41:28Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrReplicationManager.java", "diffHunk": "@@ -123,19 +126,37 @@ public void onPartitionRemoved(PartitionId partitionId) {\n \n     // start background persistent thread\n     // start scheduler thread to persist index in the background\n-    scheduler.scheduleAtFixedRate(persistor, replicationConfig.replicationTokenFlushDelaySeconds,\n-        replicationConfig.replicationTokenFlushIntervalSeconds, TimeUnit.SECONDS);\n+    scheduleTask(persistor, true, replicationConfig.replicationTokenFlushDelaySeconds,\n+        replicationConfig.replicationTokenFlushIntervalSeconds, \"replica token persistor\");\n \n-    if (cloudConfig.cloudBlobCompactionEnabled) {\n-      // Schedule thread to purge dead blobs for this VCR's partitions\n-      // after delay to allow startup to finish.\n-      long delaySec = cloudConfig.cloudBlobCompactionStartupDelaySecs;\n-      long intervalSec = TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours);\n-      scheduler.scheduleAtFixedRate(cloudStorageCompactor, delaySec, intervalSec, TimeUnit.SECONDS);\n-      logger.info(\"Scheduled compaction task to run every {} hours starting in {} seconds.\",\n-          cloudConfig.cloudBlobCompactionIntervalHours, delaySec);\n+    // Schedule thread to purge dead blobs for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(cloudStorageCompactor, cloudConfig.cloudBlobCompactionEnabled,\n+        cloudConfig.cloudBlobCompactionStartupDelaySecs,\n+        TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours), \"cloud blob compaction\");\n+\n+    // Schedule thread to purge blobs belonging to deprecated containers for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(() -> cloudContainerCompactor.compactAssignedDeprecatedContainers(", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5ODMxNw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513098317", "bodyText": "The expression () -> cloudContainerCompactor.compactAssignedDeprecatedContainers(virtualReplicatorCluster.getAssignedPartitionIds()) will not get evaluated until it is actually called inside scheduleTask where it is protected by the isEnabled flag.\nHowever, you are right that it is not a very clean way to do this. So I have fixed it by making sure that cloudContainerCompactor object is not assigned null value.", "author": "ankagrawal", "createdAt": "2020-10-27T23:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4NTQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4NjQ5MA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r508986490", "bodyText": "According to your design, there would be several threads concurrently performing container compaction. Could you point me to the implementation?", "author": "jsjtzyy", "createdAt": "2020-10-21T04:45:12Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrReplicationManager.java", "diffHunk": "@@ -123,19 +126,37 @@ public void onPartitionRemoved(PartitionId partitionId) {\n \n     // start background persistent thread\n     // start scheduler thread to persist index in the background\n-    scheduler.scheduleAtFixedRate(persistor, replicationConfig.replicationTokenFlushDelaySeconds,\n-        replicationConfig.replicationTokenFlushIntervalSeconds, TimeUnit.SECONDS);\n+    scheduleTask(persistor, true, replicationConfig.replicationTokenFlushDelaySeconds,\n+        replicationConfig.replicationTokenFlushIntervalSeconds, \"replica token persistor\");\n \n-    if (cloudConfig.cloudBlobCompactionEnabled) {\n-      // Schedule thread to purge dead blobs for this VCR's partitions\n-      // after delay to allow startup to finish.\n-      long delaySec = cloudConfig.cloudBlobCompactionStartupDelaySecs;\n-      long intervalSec = TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours);\n-      scheduler.scheduleAtFixedRate(cloudStorageCompactor, delaySec, intervalSec, TimeUnit.SECONDS);\n-      logger.info(\"Scheduled compaction task to run every {} hours starting in {} seconds.\",\n-          cloudConfig.cloudBlobCompactionIntervalHours, delaySec);\n+    // Schedule thread to purge dead blobs for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(cloudStorageCompactor, cloudConfig.cloudBlobCompactionEnabled,\n+        cloudConfig.cloudBlobCompactionStartupDelaySecs,\n+        TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours), \"cloud blob compaction\");\n+\n+    // Schedule thread to purge blobs belonging to deprecated containers for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(() -> cloudContainerCompactor.compactAssignedDeprecatedContainers(\n+        virtualReplicatorCluster.getAssignedPartitionIds()), cloudConfig.cloudContainerCompactionEnabled,\n+        cloudConfig.cloudContainerCompactionStartupDelaySecs,\n+        TimeUnit.HOURS.toSeconds(cloudConfig.cloudContainerCompactionIntervalHours), \"cloud container compaction\");\n+  }", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5OTYwNg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513099606", "bodyText": "In this PR its all single threaded. There will be two more optimization needed eventually which will come in future PRs. One is to make this multithreaded and compact multiple container in parallel. And another one is to make the compaction queries more optimal.", "author": "ankagrawal", "createdAt": "2020-10-27T23:54:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4NjQ5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4Njk5Mw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r508986993", "bodyText": "same here, better to use getIntInRange", "author": "jsjtzyy", "createdAt": "2020-10-21T04:47:12Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureCloudConfig.java", "diffHunk": "@@ -135,5 +156,11 @@ public AzureCloudConfig(VerifiableProperties verifiableProperties) {\n     azureBlobContainerStrategy =\n         verifiableProperties.getString(AZURE_BLOB_CONTAINER_STRATEGY, DEFAULT_CONTAINER_STRATEGY);\n     azureNameSchemeVersion = verifiableProperties.getInt(AZURE_NAME_SCHEME_VERSION, DEFAULT_NAME_SCHEME_VERSION);\n+    cosmosContainerDeletionBatchSize =\n+        verifiableProperties.getInt(COSMOS_CONTAINER_DELETION_BATCH_SIZE, DEFAULT_COSMOS_CONTAINER_DELETION_BATCH_SIZE);\n+    containerCompactionAbsPurgeLimit =\n+        verifiableProperties.getInt(CONTAINER_COMPACTION_ABS_PURGE_LIMIT, DEFAULT_CONTAINER_COMPACTION_ABS_PURGE_LIMIT);\n+    containerCompactionCosmosQueryLimit = verifiableProperties.getInt(CONTAINER_COMPACTION_COSMOS_QUERY_LIMIT,\n+        DEFAULT_COSMOS_CONTAINER_DELETION_BATCH_SIZE);", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMTE1Ng==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513101156", "bodyText": "Its not 100% clear what the theoretical upper limit of this value should be. But it definitely shouldn't be less than 1. Made the changes accordingly.", "author": "ankagrawal", "createdAt": "2020-10-27T23:59:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4Njk5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxNDc4NQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509614785", "bodyText": "Seems a little odd that we need a getter for this but not the dead blob compactor, though I trust you had a reason to add it here.", "author": "lightningrob", "createdAt": "2020-10-21T19:36:26Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -145,4 +145,9 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * @throws {@link CloudStorageException} if the operation fails.\n    */\n   void deprecateContainers(Collection<Container> deprecatedContainers) throws CloudStorageException;\n+\n+  /**\n+   * @return {@link CloudContainerCompactor} object that would do container compaction for cloud.\n+   */\n+  CloudContainerCompactor getContainerCompactor();", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3OTA4Mw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513079083", "bodyText": "The main difference between cloud blob compaction and container compactor is that for blob compaction, there is enough common functionality thats not tied to azure implementation. So it made sense to have a CloudBlobCompactor class that could be used for scheduling compaction by VcrReplicationManager. However, most of the logic for container compaction is very specific to azure implementation. Hence cloud compactor didn't need to depend on CloudDestination to get the compaction implementation.", "author": "ankagrawal", "createdAt": "2020-10-27T22:51:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxNDc4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxOTc4Nw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509619787", "bodyText": "Could we change this signature to return Collection instead of List?  Then no need to copy.", "author": "lightningrob", "createdAt": "2020-10-21T19:43:02Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/StaticVcrCluster.java", "diffHunk": "@@ -91,7 +91,13 @@ public void participate() throws Exception {\n \n   @Override\n   public List<? extends PartitionId> getAssignedPartitionIds() {\n-    return assignedPartitionIds;\n+    return new ArrayList<>(assignedPartitionIds);", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MjA3OQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513092079", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-27T23:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxOTc4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyMjQ0Nw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509622447", "bodyText": "Nit: can we call this deprecatedContainerBlobCompactionRate, since it's rate of blob and not container compaction.", "author": "lightningrob", "createdAt": "2020-10-21T19:46:19Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrMetrics.java", "diffHunk": "@@ -33,6 +33,7 @@\n   // Compaction metrics\n   // Rate of blob compaction for VCR instance\n   public final Meter blobCompactionRate;\n+  public final Meter deprecatedContainerCompactionRate;", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MjU3NA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513092574", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-27T23:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyMjQ0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyNDc5Nw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509624797", "bodyText": "Minor: it seems unnecessary to pass isEnabled to this method.  But I'm okay either way.", "author": "lightningrob", "createdAt": "2020-10-21T19:49:53Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrReplicationManager.java", "diffHunk": "@@ -123,19 +126,37 @@ public void onPartitionRemoved(PartitionId partitionId) {\n \n     // start background persistent thread\n     // start scheduler thread to persist index in the background\n-    scheduler.scheduleAtFixedRate(persistor, replicationConfig.replicationTokenFlushDelaySeconds,\n-        replicationConfig.replicationTokenFlushIntervalSeconds, TimeUnit.SECONDS);\n+    scheduleTask(persistor, true, replicationConfig.replicationTokenFlushDelaySeconds,\n+        replicationConfig.replicationTokenFlushIntervalSeconds, \"replica token persistor\");\n \n-    if (cloudConfig.cloudBlobCompactionEnabled) {\n-      // Schedule thread to purge dead blobs for this VCR's partitions\n-      // after delay to allow startup to finish.\n-      long delaySec = cloudConfig.cloudBlobCompactionStartupDelaySecs;\n-      long intervalSec = TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours);\n-      scheduler.scheduleAtFixedRate(cloudStorageCompactor, delaySec, intervalSec, TimeUnit.SECONDS);\n-      logger.info(\"Scheduled compaction task to run every {} hours starting in {} seconds.\",\n-          cloudConfig.cloudBlobCompactionIntervalHours, delaySec);\n+    // Schedule thread to purge dead blobs for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(cloudStorageCompactor, cloudConfig.cloudBlobCompactionEnabled,\n+        cloudConfig.cloudBlobCompactionStartupDelaySecs,\n+        TimeUnit.HOURS.toSeconds(cloudConfig.cloudBlobCompactionIntervalHours), \"cloud blob compaction\");\n+\n+    // Schedule thread to purge blobs belonging to deprecated containers for this VCR's partitions\n+    // after delay to allow startup to finish.\n+    scheduleTask(() -> cloudContainerCompactor.compactAssignedDeprecatedContainers(\n+        virtualReplicatorCluster.getAssignedPartitionIds()), cloudConfig.cloudContainerCompactionEnabled,\n+        cloudConfig.cloudContainerCompactionStartupDelaySecs,\n+        TimeUnit.HOURS.toSeconds(cloudConfig.cloudContainerCompactionIntervalHours), \"cloud container compaction\");\n+  }\n+\n+  /**\n+   * Schedule the specified task if enabled with the specified delay and interval.\n+   * @param task {@link Runnable} task to be scheduled.\n+   * @param isEnabled flag indicating if the task is enabled. If false the task is not scheduled.\n+   * @param delaySec initial delay to allow startup to finish before starting task.\n+   * @param intervalSec period between successive executions.\n+   * @param taskName name of the task being scheduled.\n+   */\n+  private void scheduleTask(Runnable task, boolean isEnabled, long delaySec, long intervalSec, String taskName) {", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwMDU0Ng==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516200546", "bodyText": "I guess its between checking enabled everytime while calling vs checking enabled once in this method. So I opted for the second one.", "author": "ankagrawal", "createdAt": "2020-11-02T19:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyNDc5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyNTA2Mg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509625062", "bodyText": "Minor: missing space", "author": "lightningrob", "createdAt": "2020-10-21T19:50:15Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrReplicationManager.java", "diffHunk": "@@ -223,6 +244,9 @@ public void shutdown() throws ReplicationException {\n     if (cloudStorageCompactor != null) {\n       cloudStorageCompactor.shutdown();\n     }\n+    if(cloudContainerCompactor != null) {", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYzNzM5NQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509637395", "bodyText": "Remove or reword this TODO not to use LI things.", "author": "lightningrob", "createdAt": "2020-10-21T20:01:57Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -489,10 +493,16 @@ private CloudStorageException toCloudStorageException(String message, Exception\n \n   @Override\n   public void deprecateContainers(Collection<Container> deletedContainers) throws CloudStorageException {\n+    //TODO need to set correct partition class for video cluster in call to getAllPartitionIds.", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMTQ4MQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513101481", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:00:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYzNzM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDU3OA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509644578", "bodyText": "Can we keep this method and make it a wrapper around the utility?  Then callers don't need to know the impl details.", "author": "lightningrob", "createdAt": "2020-10-21T20:08:45Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -264,44 +266,6 @@ private int compactPartitionBucketed(String partitionPath, String fieldName, lon\n     }\n   }\n \n-  /**\n-   * Permanently delete the specified blobs in Azure storage.\n-   * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n-   * @throws CloudStorageException if the purge operation fails for any blob.\n-   */\n-  int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageException {", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNTc3MA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516205770", "bodyText": "A list of blobs are purged for container compaction as well as deleted blobs compaction. Thats the reason I separated this out in a util method that purges a list of blobs for callers. I can create a method here as a wrapper around the util. But then ideally we should do the same for container compaction as well.\nLet me know if you think we should do it at both the places.", "author": "ankagrawal", "createdAt": "2020-11-02T19:29:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1Mjk2OQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509652969", "bodyText": "Can we use parameters in the new queries (like LIMIT_PARAM) to be consistent?", "author": "lightningrob", "createdAt": "2020-10-21T20:14:16Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -70,11 +72,12 @@\n   private static final String LIMIT_PARAM = \"@limit\";\n   private static final String EXPIRED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_EXPIRATION_TIME);\n   private static final String DELETED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_DELETION_TIME);\n+  private static final String CONTAINER_BLOBS_QUERY =\n+      \"SELECT TOP %d * FROM c WHERE c.accountId=%d and c.containerId=%d\";", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMDA2NQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513110065", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:30:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1Mjk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1OTQ2MQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509659461", "bodyText": "The code from here down is nearly identical to getDeadBlobs.  Please refactor into a common class that take SqlQuerySpec as argument.", "author": "lightningrob", "createdAt": "2020-10-21T20:18:51Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -425,6 +428,51 @@ CloudBlobMetadata getMetadataOrNull(BlobId blobId) throws DocumentClientExceptio\n     }\n   }\n \n+  /**\n+   * Get the list of blobs in the specified partition that belong to the specified container.\n+   * @param partitionPath the partition to query.\n+   * @param accountId account id of the container.\n+   * @param containerId container id of the container.\n+   * @param queryLimit max number of blobs to return.\n+   * @return a List of {@link CloudBlobMetadata} referencing the blobs belonging to the deprecated containers.\n+   * @throws DocumentClientException in case of any error.\n+   */\n+  List<CloudBlobMetadata> getContainerBlobs(String partitionPath, short accountId, short containerId, int queryLimit)\n+      throws DocumentClientException {\n+    String query = String.format(CONTAINER_BLOBS_QUERY, accountId, containerId);\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    FeedOptions feedOptions = new FeedOptions();", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2MzAzMw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509663033", "bodyText": "Also, since we aren't time bucketing the queries like dead blob compaction does, we could run into the same throttling issues that prompted the use of time bucketing there.", "author": "lightningrob", "createdAt": "2020-10-21T20:22:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1OTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwOTM2NA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516209364", "bodyText": "This is true. I plan to post a final follow up PR that would add two more optimizations\n\nDo the container compaction in multiple threads.\nDo a time based bucketing for deletion.", "author": "ankagrawal", "createdAt": "2020-11-02T19:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1OTQ2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2ODYxNw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509668617", "bodyText": "A lot of this logic is very similar to updateMetadata().  Possible to reuse/combine?", "author": "lightningrob", "createdAt": "2020-10-21T20:28:55Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -559,25 +608,89 @@ public long deprecateContainers(Set<ContainerDeletionEntry> deprecatedContainers\n   }\n \n   /**\n-   * @return a {@link Set} of {@link ContainerDeletionEntry} objects from cosmosdb that are not marked as deleted.\n+   * Fetch a {@link Set} of {@link CosmosContainerDeletionEntry} objects from cosmos db that are not marked as deleted.\n+   * @param maxEntries Max number of entries to fetch on one query.\n+   * @return {@link Set} of {@link CosmosContainerDeletionEntry} objects.\n+   * @throws DocumentClientException in case of any error.\n    */\n-  public Set<ContainerDeletionEntry> getDeprecatedContainers(int maxEntries) {\n+  public Set<CosmosContainerDeletionEntry> getDeprecatedContainers(int maxEntries) throws DocumentClientException {\n     String query = String.format(DEPRECATED_CONTAINERS_QUERY, maxEntries);\n     Timer timer = new Timer();\n-    Iterator<FeedResponse<Document>> iterator =\n-        executeCosmosQuery(cosmosDeletedContainerCollectionLink, null, new SqlQuerySpec(query), new FeedOptions(),\n-            timer).getIterator();\n-    Set<ContainerDeletionEntry> containerDeletionEntries = new HashSet<>();\n-    while (iterator.hasNext()) {\n-      FeedResponse<Document> response = iterator.next();\n-      response.getResults()\n-          .iterator()\n-          .forEachRemaining(\n-              doc -> containerDeletionEntries.add(ContainerDeletionEntry.fromJson(new JSONObject(doc.toJson()))));\n+    Set<CosmosContainerDeletionEntry> containerDeletionEntries = new HashSet<>();\n+    try {\n+      Iterator<FeedResponse<Document>> iterator =\n+          executeCosmosQuery(cosmosDeletedContainerCollectionLink, null, new SqlQuerySpec(query), new FeedOptions(),\n+              timer).getIterator();\n+      while (iterator.hasNext()) {\n+        FeedResponse<Document> response = iterator.next();\n+        response.getResults()\n+            .iterator()\n+            .forEachRemaining(doc -> containerDeletionEntries.add(\n+                CosmosContainerDeletionEntry.fromJson(new JSONObject(doc.toJson()))));\n+      }\n+    } catch (RuntimeException rex) {\n+      if (rex.getCause() instanceof DocumentClientException) {\n+        logger.warn(\"Get deprecated containers query {} got {}\", query,\n+            ((DocumentClientException) rex.getCause()).getStatusCode());\n+        throw (DocumentClientException) rex.getCause();\n+      }\n+      throw rex;\n     }\n     return containerDeletionEntries;\n   }\n \n+  /**\n+   * Update the container deletion entry document in the CosmosDB collection.\n+   * @param containerId the container id for which document is replaced.\n+   * @param accountId the account id for which document is replaced.\n+   * @param updateFields {@link BiConsumer} object to use as callback to update the required fields.\n+   * @return the {@link ResourceResponse} returned by the operation, if successful.\n+   * Returns {@Null} if the field already has the specified value.\n+   * @throws DocumentClientException if the record was not found or if the operation failed.\n+   */\n+  ResourceResponse<Document> updateContainerDeletionEntry(short containerId, short accountId,\n+      BiConsumer<Document, AtomicBoolean> updateFields) throws DocumentClientException {\n+\n+    // Read the existing record\n+    String id = CosmosContainerDeletionEntry.generateContainerDeletionEntryId(accountId, containerId);\n+    String docLink = getContainerDeletionEntryDocumentLink(id);\n+    RequestOptions options = getRequestOptions(id);", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwOTU2Ng==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516209566", "bodyText": "Will add as an action item for follow up PR.", "author": "ankagrawal", "createdAt": "2020-11-02T19:36:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2ODYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY3MDQ0OQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509670449", "bodyText": "Ouch!  Obviously remove this.  We should also change the EI keys since they've been exposed.", "author": "lightningrob", "createdAt": "2020-10-21T20:31:50Z", "path": "ambry-cloud/src/test/resources/azure-test.properties", "diffHunk": "@@ -9,9 +9,8 @@\n # under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n # CONDITIONS OF ANY KIND, either express or implied.\n #\n-azure.storage.connection.string=<storage-account-credentials>\n-cosmos.endpoint=<cosmos-url>\n-cosmos.collection.link=/dbs/ambry-metadata/colls/blob-metadata\n-cosmos.key=<cosmos-key>\n-cosmos.deleted.container.collection.link=/dbs/ambry-metadata-main/colls/deleted-container-test\n-cosmos.direct.https=true\n+azure.storage.connection.string=DefaultEndpointsProtocol=https;AccountName=wus2ambryblobstore1;AccountKey=vqQbP8s6I68IGlyYeqyv1dAwsh3AXXlD+QwgGTVhbU5NEMzE6Tucv9SxS2/6AzB/8VDHjaZmIYV9MWUCWaAqTg==;EndpointSuffix=core.windows.net", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMzA5OQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513113099", "bodyText": "done & done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY3MDQ0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY4MjI4OA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509682288", "bodyText": "Javadoc.", "author": "lightningrob", "createdAt": "2020-10-21T20:46:51Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureCompactionUtil.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.CloudStorageException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+public class AzureCompactionUtil {", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMTY5Mw==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513101693", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:01:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY4MjI4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MjQ1Ng==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509862456", "bodyText": "minor: missing java doc for @param assignedPartitions", "author": "SophieGuo410", "createdAt": "2020-10-22T03:46:19Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -53,38 +66,83 @@\n    * @param azureMetrics {@link AzureMetrics} object.\n    */\n   public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n-      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+      CloudConfig cloudConfig, AzureCloudConfig azureCloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n     this.azureBlobDataAccessor = azureBlobDataAccessor;\n     this.cosmosDataAccessor = cosmosDataAccessor;\n-    this.cloudConfig = cloudConfig;\n     this.vcrMetrics = vcrMetrics;\n     this.azureMetrics = azureMetrics;\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+    this.queryLimit = azureCloudConfig.containerCompactionCosmosQueryLimit;\n+    this.containerDeletionQueryBatchSize = azureCloudConfig.cosmosContainerDeletionBatchSize;\n   }\n \n   /**\n    * Update newly deprecated containers from {@code deprecatedContainers} to CosmosDb since last checkpoint.\n-   * @param deprecatedContainers {@link Collection} of deprecatedd {@link Container}s.\n+   * This method is one of the two entry points in {@link AzureContainerCompactor} along with\n+   * {@link AzureContainerCompactor#compactAssignedDeprecatedContainers(List)}.\n+   * @param deprecatedContainers {@link Collection} of deprecated {@link Container}s.\n+   * @param partitionIds list of partition ids from where the containers have to be removed.\n    * @throws CloudStorageException in case of any error.\n    */\n   public void deprecateContainers(Collection<Container> deprecatedContainers, Collection<String> partitionIds)\n       throws CloudStorageException {\n-    if (deprecatedContainers.isEmpty()) {\n-      logger.info(\"Got empty set to update deprecated containers. Skipping update deprecated containers to cloud.\");\n+    if (deprecatedContainers.isEmpty() || partitionIds.isEmpty()) {\n+      logger.warn(\n+          \"Got either empty container set or empty partition list. Skipping update deprecated containers to cloud.\");\n       return;\n     }\n     long lastUpdatedContainerTimestamp = getLatestContainerDeletionTime();\n     long newLastUpdateContainerTimestamp = requestAgent.doWithRetries(() -> cosmosDataAccessor.deprecateContainers(\n         deprecatedContainers.stream()\n             .filter(container -> container.getDeleteTriggerTime() >= lastUpdatedContainerTimestamp)\n-            .map(container -> ContainerDeletionEntry.fromContainer(container, partitionIds))\n+            .map(container -> CosmosContainerDeletionEntry.fromContainer(container, partitionIds))\n             .collect(Collectors.toSet())), \"updateDeprecatedContainers\", null);\n \n     if (newLastUpdateContainerTimestamp != -1) {\n       saveLatestContainerDeletionTime(newLastUpdateContainerTimestamp);\n     }\n   }\n \n+  /**\n+   * Compact blobs of the deprecated container from cloud. This method is one of the two entry points in the\n+   * {@link AzureContainerCompactor} class along with {@link AzureContainerCompactor#deprecateContainers(Collection, Collection)}.\n+   * Note that this method is not thread safe as it is expected to run in a single thread.\n+   */", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMjA2Ng==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513102066", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:02:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MjQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NTAwNA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509865004", "bodyText": "minor: you can removed them since they are not been used.", "author": "SophieGuo410", "createdAt": "2020-10-22T03:56:33Z", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureTestUtils.java", "diffHunk": "@@ -13,9 +13,12 @@\n  */\n package com.github.ambry.cloud.azure;\n \n+import com.codahale.metrics.Timer;", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMzAwMA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513113000", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:40:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NjIwNg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509866206", "bodyText": "Not used?", "author": "SophieGuo410", "createdAt": "2020-10-22T04:00:52Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosContainerDeletionEntry.java", "diffHunk": "@@ -25,34 +25,36 @@\n /**\n  * Class representing container deletion status in cloud.\n  */\n-public class ContainerDeletionEntry {\n-\n+public class CosmosContainerDeletionEntry {\n   static final String VERSION_KEY = \"version\";\n   static final String CONTAINER_ID_KEY = \"containerId\";\n   static final String ACCOUNT_ID_KEY = \"accountId\";\n-  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n-  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTimestamp\";\n+  static final String DELETED_KEY = \"deleted\";\n   static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+  private static final String ID_KEY = \"id\";", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMjg0MA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513102840", "bodyText": "removed.", "author": "ankagrawal", "createdAt": "2020-10-28T00:05:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NjIwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2ODE1NQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509868155", "bodyText": "Seems the blobCompactedCount never been used?", "author": "SophieGuo410", "createdAt": "2020-10-22T04:08:42Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -53,38 +66,83 @@\n    * @param azureMetrics {@link AzureMetrics} object.\n    */\n   public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n-      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+      CloudConfig cloudConfig, AzureCloudConfig azureCloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n     this.azureBlobDataAccessor = azureBlobDataAccessor;\n     this.cosmosDataAccessor = cosmosDataAccessor;\n-    this.cloudConfig = cloudConfig;\n     this.vcrMetrics = vcrMetrics;\n     this.azureMetrics = azureMetrics;\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+    this.queryLimit = azureCloudConfig.containerCompactionCosmosQueryLimit;\n+    this.containerDeletionQueryBatchSize = azureCloudConfig.cosmosContainerDeletionBatchSize;\n   }\n \n   /**\n    * Update newly deprecated containers from {@code deprecatedContainers} to CosmosDb since last checkpoint.\n-   * @param deprecatedContainers {@link Collection} of deprecatedd {@link Container}s.\n+   * This method is one of the two entry points in {@link AzureContainerCompactor} along with\n+   * {@link AzureContainerCompactor#compactAssignedDeprecatedContainers(List)}.\n+   * @param deprecatedContainers {@link Collection} of deprecated {@link Container}s.\n+   * @param partitionIds list of partition ids from where the containers have to be removed.\n    * @throws CloudStorageException in case of any error.\n    */\n   public void deprecateContainers(Collection<Container> deprecatedContainers, Collection<String> partitionIds)\n       throws CloudStorageException {\n-    if (deprecatedContainers.isEmpty()) {\n-      logger.info(\"Got empty set to update deprecated containers. Skipping update deprecated containers to cloud.\");\n+    if (deprecatedContainers.isEmpty() || partitionIds.isEmpty()) {\n+      logger.warn(\n+          \"Got either empty container set or empty partition list. Skipping update deprecated containers to cloud.\");\n       return;\n     }\n     long lastUpdatedContainerTimestamp = getLatestContainerDeletionTime();\n     long newLastUpdateContainerTimestamp = requestAgent.doWithRetries(() -> cosmosDataAccessor.deprecateContainers(\n         deprecatedContainers.stream()\n             .filter(container -> container.getDeleteTriggerTime() >= lastUpdatedContainerTimestamp)\n-            .map(container -> ContainerDeletionEntry.fromContainer(container, partitionIds))\n+            .map(container -> CosmosContainerDeletionEntry.fromContainer(container, partitionIds))\n             .collect(Collectors.toSet())), \"updateDeprecatedContainers\", null);\n \n     if (newLastUpdateContainerTimestamp != -1) {\n       saveLatestContainerDeletionTime(newLastUpdateContainerTimestamp);\n     }\n   }\n \n+  /**\n+   * Compact blobs of the deprecated container from cloud. This method is one of the two entry points in the\n+   * {@link AzureContainerCompactor} class along with {@link AzureContainerCompactor#deprecateContainers(Collection, Collection)}.\n+   * Note that this method is not thread safe as it is expected to run in a single thread.\n+   */\n+  @Override\n+  public void compactAssignedDeprecatedContainers(List<? extends PartitionId> assignedPartitions) {\n+    try {\n+      SortedSet<CosmosContainerDeletionEntry> containerDeletionEntrySet =\n+          fetchContainerDeletionEntries(assignedPartitions);\n+      while (!containerDeletionEntrySet.isEmpty()) {\n+        CosmosContainerDeletionEntry containerDeletionEntry = containerDeletionEntrySet.first();\n+        containerDeletionEntrySet.remove(containerDeletionEntry);\n+        for (String partitionId : containerDeletionEntry.getDeletePendingPartitions()) {\n+          try {\n+            int blobCompactedCount =", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMjI4OA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513102288", "bodyText": "I will use it in future PR when I plan to add more metrics to the code.", "author": "ankagrawal", "createdAt": "2020-10-28T00:03:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2ODE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDY2OA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r509874668", "bodyText": "minor: GetDeprectedContainers -> GetDeprecatedContainers", "author": "SophieGuo410", "createdAt": "2020-10-22T04:37:15Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -131,4 +193,96 @@ private void saveLatestContainerDeletionTime(long latestContainerDeletionTimesta\n       throw e;\n     }\n   }\n+\n+  /**\n+   * Purge all blobs of the specified container from the specified partition.\n+   * @param containerId container id of the specified container.\n+   * @param accountId account oid of the specified container.\n+   * @param partitionPath partition id from which the blobs have to be deleted.\n+   * @return number of blobs purged.\n+   * @throws CloudStorageException in case of any error.\n+   */\n+  private int compactContainer(short containerId, short accountId, String partitionPath) throws CloudStorageException {\n+    int totalPurged = 0;\n+    while (!isShuttingDown()) {\n+      List<CloudBlobMetadata> blobs = requestAgent.doWithRetries(\n+          () -> cosmosDataAccessor.getContainerBlobs(partitionPath, accountId, containerId, queryLimit),\n+          \"GetDeprecatedContainerBlobs\", partitionPath);\n+\n+      if (blobs.isEmpty()) {\n+        // this means all the blobs of this container have been purged from the partition\n+        updateCompactionProgress(containerId, accountId, partitionPath);\n+        break;\n+      }\n+      if (isShuttingDown()) {\n+        break;\n+      }\n+      totalPurged += requestAgent.doWithRetries(\n+          () -> AzureCompactionUtil.purgeBlobs(blobs, azureBlobDataAccessor, azureMetrics, cosmosDataAccessor),\n+          \"PurgeBlobs\", partitionPath);\n+      vcrMetrics.deprecatedContainerCompactionRate.mark(blobs.size());\n+    }\n+    return totalPurged;\n+  }\n+\n+  /**\n+   * Update the container deletion entry of the specified container to remove the partition from which all blobs of the\n+   * container have been compacted. If there are no more partitions left to compact then mark the container deletion entry as deleted.\n+   * @param containerId container id of the container.\n+   * @param accountId account if of the container.\n+   * @param partitionPath partition id from which all blobs of the container have been deleted.\n+   * @throws CloudStorageException in case of any error.\n+   */\n+  private void updateCompactionProgress(short containerId, short accountId, String partitionPath)\n+      throws CloudStorageException {\n+    // TODO: update the cache and cosmos container deletion entry table to remove the partitionId from deletePendingPartitions list\n+    ResourceResponse<Document> updatedDocument = requestAgent.doWithRetries(\n+        () -> cosmosDataAccessor.updateContainerDeletionEntry(containerId, accountId, (document, fieldsChanged) -> {\n+          Set<String> deletePendingPartitions =\n+              (Set<String>) document.get(CosmosContainerDeletionEntry.DELETE_PENDING_PARTITIONS_KEY);\n+          fieldsChanged.set(deletePendingPartitions.remove(partitionPath));\n+          document.set(CosmosContainerDeletionEntry.DELETE_PENDING_PARTITIONS_KEY, deletePendingPartitions);\n+          if (deletePendingPartitions.isEmpty()) {\n+            document.set(CosmosContainerDeletionEntry.DELETED_KEY, true);\n+            fieldsChanged.set(true);\n+          }\n+        }), \"UpdateContainerDeletionProgress\", partitionPath);\n+  }\n+\n+  /**\n+   * Fetch the {@link CosmosContainerDeletionEntry} from cloud and create a cache with entries that have atleast one partition\n+   * assigned to current node.\n+   */\n+  private SortedSet<CosmosContainerDeletionEntry> fetchContainerDeletionEntries(\n+      List<? extends PartitionId> assignedPartitions) throws CloudStorageException {\n+    Set<CosmosContainerDeletionEntry> containerDeletionEntrySet =\n+        requestAgent.doWithRetries(() -> cosmosDataAccessor.getDeprecatedContainers(containerDeletionQueryBatchSize),\n+            \"GetDeprectedContainers\", null);", "originalCommit": "3f585abb895a1492c72e3414a660f1740d633e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMjQ2Mg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r513102462", "bodyText": "done.", "author": "ankagrawal", "createdAt": "2020-10-28T00:04:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDY2OA=="}], "type": "inlineReview"}, {"oid": "4dcc9f19819a8c7e9f0affe0590fa79054b6a3c3", "url": "https://github.com/linkedin/ambry/commit/4dcc9f19819a8c7e9f0affe0590fa79054b6a3c3", "message": "cleanup", "committedDate": "2020-11-02T20:33:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyNjU4NA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516426584", "bodyText": "I think Cosmos will reject this.  \"%d\" needs to be removed, and you need the \"*\" before FROM.", "author": "lightningrob", "createdAt": "2020-11-03T04:48:58Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -70,10 +70,13 @@\n   private static final String START_TIME_PARAM = \"@startTime\";\n   private static final String END_TIME_PARAM = \"@endTime\";\n   private static final String LIMIT_PARAM = \"@limit\";\n+  private static final String ACCOUNT_ID_PARAM = \"@accountId\";\n+  private static final String CONTAINER_ID_PARAM = \"@containerId\";\n   private static final String EXPIRED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_EXPIRATION_TIME);\n   private static final String DELETED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_DELETION_TIME);\n   private static final String CONTAINER_BLOBS_QUERY =\n-      \"SELECT TOP %d * FROM c WHERE c.accountId=%d and c.containerId=%d\";\n+      \"SELECT TOP %d \" + LIMIT_PARAM + \" FROM c WHERE c.accountId=\" + ACCOUNT_ID_PARAM + \" and c.containerId=\"", "originalCommit": "eccc947808383b9bb41e71a1d50c7490b089f950", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MjMwMg==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r520072302", "bodyText": "fixed.", "author": "ankagrawal", "createdAt": "2020-11-09T19:39:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyNjU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyNjc5MQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r516426791", "bodyText": "Can you change this to LIMIT_PARAM too?  I missed it earlier.", "author": "lightningrob", "createdAt": "2020-11-03T04:49:54Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -70,10 +70,13 @@\n   private static final String START_TIME_PARAM = \"@startTime\";\n   private static final String END_TIME_PARAM = \"@endTime\";\n   private static final String LIMIT_PARAM = \"@limit\";\n+  private static final String ACCOUNT_ID_PARAM = \"@accountId\";\n+  private static final String CONTAINER_ID_PARAM = \"@containerId\";\n   private static final String EXPIRED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_EXPIRATION_TIME);\n   private static final String DELETED_BLOBS_QUERY = constructDeadBlobsQuery(CloudBlobMetadata.FIELD_DELETION_TIME);\n   private static final String CONTAINER_BLOBS_QUERY =\n-      \"SELECT TOP %d * FROM c WHERE c.accountId=%d and c.containerId=%d\";\n+      \"SELECT TOP %d \" + LIMIT_PARAM + \" FROM c WHERE c.accountId=\" + ACCOUNT_ID_PARAM + \" and c.containerId=\"\n+          + CONTAINER_ID_PARAM;\n   private static final String BULK_DELETE_QUERY = \"SELECT c._self FROM c WHERE c.id IN (%s)\";\n   private static final String DEPRECATED_CONTAINERS_QUERY =\n       \"SELECT TOP %d * from c WHERE c.deleted=false order by c.deleteTriggerTimestamp\";", "originalCommit": "eccc947808383b9bb41e71a1d50c7490b089f950", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MjQwNQ==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r520072405", "bodyText": "fixed.", "author": "ankagrawal", "createdAt": "2020-11-09T19:39:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyNjc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDExMzUxNA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r520113514", "bodyText": "What's the difference between LIMIT and MAX_ENTRIES params?", "author": "lightningrob", "createdAt": "2020-11-09T20:54:11Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -72,15 +72,15 @@\n   private static final String LIMIT_PARAM = \"@limit\";\n   private static final String ACCOUNT_ID_PARAM = \"@accountId\";\n   private static final String CONTAINER_ID_PARAM = \"@containerId\";\n+  private static final String MAX_ENTRIES_PARAM = \"@maxEntries\";", "originalCommit": "d67105d3f77d6254018cbc3f098222151e8b7747", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUwNjY3NA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r528506674", "bodyText": "Removed MAX_ENTRIES.", "author": "ankagrawal", "createdAt": "2020-11-23T07:23:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDExMzUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgwOTEzOA==", "url": "https://github.com/linkedin/ambry/pull/1646#discussion_r529809138", "bodyText": "It doesn't look like it.", "author": "lightningrob", "createdAt": "2020-11-24T18:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDExMzUxNA=="}], "type": "inlineReview"}, {"oid": "c847531cd6a15c6b390a814c3d80b02aad4cfd42", "url": "https://github.com/linkedin/ambry/commit/c847531cd6a15c6b390a814c3d80b02aad4cfd42", "message": "Initial implementation of Helix task to sync deleted containers between cloud and helix account service.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "e1d180eca7de006bdf7bd1075ce1f46ec1881499", "url": "https://github.com/linkedin/ambry/commit/e1d180eca7de006bdf7bd1075ce1f46ec1881499", "message": "Fix tests and cleanup.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "8d1c67c772ab1cd3ea48292508fac15b55736c84", "url": "https://github.com/linkedin/ambry/commit/8d1c67c772ab1cd3ea48292508fac15b55736c84", "message": "Fix BlobStoreCompactorTest", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "f541cdae1ce7b2608a7fe0f668b6936796776632", "url": "https://github.com/linkedin/ambry/commit/f541cdae1ce7b2608a7fe0f668b6936796776632", "message": "Add unit tests", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "847b6d58d93a29aa482f8d1a29b828d1ddd79585", "url": "https://github.com/linkedin/ambry/commit/847b6d58d93a29aa482f8d1a29b828d1ddd79585", "message": "Integration test (WIP)", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "8dcd662982ca7d25e3593c0b4b89e04d27d8a20e", "url": "https://github.com/linkedin/ambry/commit/8dcd662982ca7d25e3593c0b4b89e04d27d8a20e", "message": "Save ContainerDeletionEntry for deleted containers in cloud.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "90bdcad3d65f15841efb20355c89ecf22056c349", "url": "https://github.com/linkedin/ambry/commit/90bdcad3d65f15841efb20355c89ecf22056c349", "message": "Fix tests", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "94002e590a8a5575d3016391c9d895cb29b000fb", "url": "https://github.com/linkedin/ambry/commit/94002e590a8a5575d3016391c9d895cb29b000fb", "message": "WIP Implement the container deletion logic", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "27bd18fd1aaece668f503ad083b88df44d37cb88", "url": "https://github.com/linkedin/ambry/commit/27bd18fd1aaece668f503ad083b88df44d37cb88", "message": "Production code for container compaction.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "c3dfdcd9860eca04963f5defb6c440719a98803c", "url": "https://github.com/linkedin/ambry/commit/c3dfdcd9860eca04963f5defb6c440719a98803c", "message": "Rebase and fix tests", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "8c720f1dc878279b01b4ed03a631f31d24daa327", "url": "https://github.com/linkedin/ambry/commit/8c720f1dc878279b01b4ed03a631f31d24daa327", "message": "Add logic to trigger container compaction in cloud.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "b2350429524eed828ec6b0ef1064991fe2911031", "url": "https://github.com/linkedin/ambry/commit/b2350429524eed828ec6b0ef1064991fe2911031", "message": "Refactor compaction code and cleanup interfaces.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "fcb2f63fe362be04c12539d48ef6e9367789ab20", "url": "https://github.com/linkedin/ambry/commit/fcb2f63fe362be04c12539d48ef6e9367789ab20", "message": "Update container on delete", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "9f3b50748ba3ed23f0bd1e67fff594603715845d", "url": "https://github.com/linkedin/ambry/commit/9f3b50748ba3ed23f0bd1e67fff594603715845d", "message": "Add retries to azure operations.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "42924e2a11b25375c28f7b0e4fa2656644bdde2c", "url": "https://github.com/linkedin/ambry/commit/42924e2a11b25375c28f7b0e4fa2656644bdde2c", "message": "Fix integration tests.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "605d93c37d81d58d6ae41b060a82eff06a9a71ae", "url": "https://github.com/linkedin/ambry/commit/605d93c37d81d58d6ae41b060a82eff06a9a71ae", "message": "cleanup", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "5737cd6b338784feb86de7ab930dcc14c4f04a12", "url": "https://github.com/linkedin/ambry/commit/5737cd6b338784feb86de7ab930dcc14c4f04a12", "message": "Add cleanup after tests.", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "ac9b750a9be3f395c63699edfca03174cedd0107", "url": "https://github.com/linkedin/ambry/commit/ac9b750a9be3f395c63699edfca03174cedd0107", "message": "Address review comments", "committedDate": "2020-11-22T18:53:11Z", "type": "commit"}, {"oid": "bbcda12d1dae8538164b50095b1d294286886333", "url": "https://github.com/linkedin/ambry/commit/bbcda12d1dae8538164b50095b1d294286886333", "message": "Address review comments", "committedDate": "2020-11-22T18:53:12Z", "type": "commit"}, {"oid": "0d6df8c4dc37362f2dfe912631c80eade88be59c", "url": "https://github.com/linkedin/ambry/commit/0d6df8c4dc37362f2dfe912631c80eade88be59c", "message": "cleanup", "committedDate": "2020-11-22T18:53:12Z", "type": "commit"}, {"oid": "7f6c44c85632698194ba9031def8194c46711c97", "url": "https://github.com/linkedin/ambry/commit/7f6c44c85632698194ba9031def8194c46711c97", "message": "Address review comments", "committedDate": "2020-11-22T18:53:12Z", "type": "commit"}, {"oid": "7f6c44c85632698194ba9031def8194c46711c97", "url": "https://github.com/linkedin/ambry/commit/7f6c44c85632698194ba9031def8194c46711c97", "message": "Address review comments", "committedDate": "2020-11-22T18:53:12Z", "type": "forcePushed"}, {"oid": "f23c798c37ad7c327637d68734731c0198e2003a", "url": "https://github.com/linkedin/ambry/commit/f23c798c37ad7c327637d68734731c0198e2003a", "message": "Add integration tests for cloud container compaction.", "committedDate": "2020-11-23T07:20:16Z", "type": "commit"}, {"oid": "9fbd36ae8f1afd3042cd614509ef1a2586edbcc1", "url": "https://github.com/linkedin/ambry/commit/9fbd36ae8f1afd3042cd614509ef1a2586edbcc1", "message": "Address review comments.", "committedDate": "2020-11-24T19:01:12Z", "type": "commit"}]}