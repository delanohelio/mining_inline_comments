{"pr_number": 1391, "pr_title": "Remove getMessageInfo from MessageStoreHardDelete interface", "pr_createdAt": "2020-02-19T18:35:10Z", "pr_url": "https://github.com/linkedin/ambry/pull/1391", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQyOTMxNw==", "url": "https://github.com/linkedin/ambry/pull/1391#discussion_r383429317", "bodyText": "isFlagSet -> isTTLUpdate", "author": "jsjtzyy", "createdAt": "2020-02-24T18:13:36Z", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1055,57 +1055,20 @@ BlobReadOptions getBlobReadInfo(StoreKey id, EnumSet<StoreGetOptions> getOptions\n   private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key,\n       ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n     BlobReadOptions readOptions;\n-    try {\n-      IndexValue putValue =\n-          findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n-              indexSegments);\n-      if (value.getOriginalMessageOffset() != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET\n-          && value.getOriginalMessageOffset() != value.getOffset().getOffset()) {\n-        // PUT record in the same log segment.\n-        String logSegmentName = value.getOffset().getName();\n-        // The delete entry in the index might not contain the information about the size of the original blob. So we\n-        // use the Message format to read and provide the information. The range in log that we provide starts at the\n-        // original message offset and ends at the delete message's start offset (the original message surely cannot go\n-        // beyond the start offset of the delete message).\n-        MessageInfo deletedBlobInfo =\n-            hardDelete.getMessageInfo(log.getSegment(logSegmentName), value.getOriginalMessageOffset(), factory);\n-        if (putValue != null && putValue.getOffset().getName().equals(value.getOffset().getName())) {\n-          if (putValue.getOffset().getOffset() != value.getOriginalMessageOffset()) {\n-            logger.error(\n-                \"Offset in PUT index entry {} is different from original message offset in delete entry {} for key {}\",\n-                putValue.getOffset().getOffset(), value.getOriginalMessageOffset(), key);\n-            metrics.putEntryDeletedInfoMismatchCount.inc();\n-          }\n-          if (putValue.getSize() != deletedBlobInfo.getSize()) {\n-            logger.error(\"Size in PUT index entry {} is different from that in the PUT record {} for ID {}\",\n-                putValue.getSize(), deletedBlobInfo.getSize(), key);\n-            metrics.putEntryDeletedInfoMismatchCount.inc();\n-          }\n-        }\n-        Offset offset = new Offset(logSegmentName, value.getOriginalMessageOffset());\n-        // use the expiration time from the original value because it may have been updated\n-        readOptions = new BlobReadOptions(log, offset,\n-            new MessageInfo(deletedBlobInfo.getStoreKey(), deletedBlobInfo.getSize(), true,\n-                value.isFlagSet(IndexValue.Flags.Ttl_Update_Index), value.getExpiresAtMs(),\n-                deletedBlobInfo.getAccountId(), deletedBlobInfo.getContainerId(),\n-                deletedBlobInfo.getOperationTimeMs()));\n-      } else if (putValue != null) {\n-        // PUT record in a different log segment.\n-        // use the expiration time from the original value because it may have been updated\n-        readOptions = new BlobReadOptions(log, putValue.getOffset(),\n-            new MessageInfo(key, putValue.getSize(), true, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index),\n-                value.getExpiresAtMs(), putValue.getAccountId(), putValue.getContainerId(),\n-                putValue.getOperationTimeInMs()));\n-      } else {\n-        // PUT record no longer available.\n-        throw new StoreException(\"Did not find PUT index entry for key [\" + key\n-            + \"] and the the original offset in value of the DELETE entry was [\" + value.getOriginalMessageOffset()\n-            + \"]\", StoreErrorCodes.ID_Deleted);\n-      }\n-    } catch (IOException e) {\n-      StoreErrorCodes errorCode = StoreException.resolveErrorCode(e);\n-      throw new StoreException(errorCode.toString() + \" when reading delete blob info from the log \" + dataDir, e,\n-          errorCode);\n+    IndexValue putValue =\n+        findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n+            indexSegments);\n+    if (putValue != null) {\n+      // use the expiration time from the original value because it may have been updated\n+      readOptions = new BlobReadOptions(log, putValue.getOffset(),\n+          new MessageInfo(key, putValue.getSize(), true, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index),", "originalCommit": "bfffc3b67916d597093580b47026c278f9bce760", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTA3Ng==", "url": "https://github.com/linkedin/ambry/pull/1391#discussion_r383431076", "bodyText": "also, this metric can be deleted from StoreMetrics", "author": "jsjtzyy", "createdAt": "2020-02-24T18:17:19Z", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1055,57 +1055,20 @@ BlobReadOptions getBlobReadInfo(StoreKey id, EnumSet<StoreGetOptions> getOptions\n   private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key,\n       ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n     BlobReadOptions readOptions;\n-    try {\n-      IndexValue putValue =\n-          findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n-              indexSegments);\n-      if (value.getOriginalMessageOffset() != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET\n-          && value.getOriginalMessageOffset() != value.getOffset().getOffset()) {\n-        // PUT record in the same log segment.\n-        String logSegmentName = value.getOffset().getName();\n-        // The delete entry in the index might not contain the information about the size of the original blob. So we\n-        // use the Message format to read and provide the information. The range in log that we provide starts at the\n-        // original message offset and ends at the delete message's start offset (the original message surely cannot go\n-        // beyond the start offset of the delete message).\n-        MessageInfo deletedBlobInfo =\n-            hardDelete.getMessageInfo(log.getSegment(logSegmentName), value.getOriginalMessageOffset(), factory);\n-        if (putValue != null && putValue.getOffset().getName().equals(value.getOffset().getName())) {\n-          if (putValue.getOffset().getOffset() != value.getOriginalMessageOffset()) {\n-            logger.error(\n-                \"Offset in PUT index entry {} is different from original message offset in delete entry {} for key {}\",\n-                putValue.getOffset().getOffset(), value.getOriginalMessageOffset(), key);\n-            metrics.putEntryDeletedInfoMismatchCount.inc();", "originalCommit": "bfffc3b67916d597093580b47026c278f9bce760", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTc4MQ==", "url": "https://github.com/linkedin/ambry/pull/1391#discussion_r383431781", "bodyText": "I wonder why the error code is ID_Deleted rather than ID_Not_Found?", "author": "jsjtzyy", "createdAt": "2020-02-24T18:18:51Z", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1055,57 +1055,20 @@ BlobReadOptions getBlobReadInfo(StoreKey id, EnumSet<StoreGetOptions> getOptions\n   private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key,\n       ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n     BlobReadOptions readOptions;\n-    try {\n-      IndexValue putValue =\n-          findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n-              indexSegments);\n-      if (value.getOriginalMessageOffset() != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET\n-          && value.getOriginalMessageOffset() != value.getOffset().getOffset()) {\n-        // PUT record in the same log segment.\n-        String logSegmentName = value.getOffset().getName();\n-        // The delete entry in the index might not contain the information about the size of the original blob. So we\n-        // use the Message format to read and provide the information. The range in log that we provide starts at the\n-        // original message offset and ends at the delete message's start offset (the original message surely cannot go\n-        // beyond the start offset of the delete message).\n-        MessageInfo deletedBlobInfo =\n-            hardDelete.getMessageInfo(log.getSegment(logSegmentName), value.getOriginalMessageOffset(), factory);\n-        if (putValue != null && putValue.getOffset().getName().equals(value.getOffset().getName())) {\n-          if (putValue.getOffset().getOffset() != value.getOriginalMessageOffset()) {\n-            logger.error(\n-                \"Offset in PUT index entry {} is different from original message offset in delete entry {} for key {}\",\n-                putValue.getOffset().getOffset(), value.getOriginalMessageOffset(), key);\n-            metrics.putEntryDeletedInfoMismatchCount.inc();\n-          }\n-          if (putValue.getSize() != deletedBlobInfo.getSize()) {\n-            logger.error(\"Size in PUT index entry {} is different from that in the PUT record {} for ID {}\",\n-                putValue.getSize(), deletedBlobInfo.getSize(), key);\n-            metrics.putEntryDeletedInfoMismatchCount.inc();\n-          }\n-        }\n-        Offset offset = new Offset(logSegmentName, value.getOriginalMessageOffset());\n-        // use the expiration time from the original value because it may have been updated\n-        readOptions = new BlobReadOptions(log, offset,\n-            new MessageInfo(deletedBlobInfo.getStoreKey(), deletedBlobInfo.getSize(), true,\n-                value.isFlagSet(IndexValue.Flags.Ttl_Update_Index), value.getExpiresAtMs(),\n-                deletedBlobInfo.getAccountId(), deletedBlobInfo.getContainerId(),\n-                deletedBlobInfo.getOperationTimeMs()));\n-      } else if (putValue != null) {\n-        // PUT record in a different log segment.\n-        // use the expiration time from the original value because it may have been updated\n-        readOptions = new BlobReadOptions(log, putValue.getOffset(),\n-            new MessageInfo(key, putValue.getSize(), true, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index),\n-                value.getExpiresAtMs(), putValue.getAccountId(), putValue.getContainerId(),\n-                putValue.getOperationTimeInMs()));\n-      } else {\n-        // PUT record no longer available.\n-        throw new StoreException(\"Did not find PUT index entry for key [\" + key\n-            + \"] and the the original offset in value of the DELETE entry was [\" + value.getOriginalMessageOffset()\n-            + \"]\", StoreErrorCodes.ID_Deleted);\n-      }\n-    } catch (IOException e) {\n-      StoreErrorCodes errorCode = StoreException.resolveErrorCode(e);\n-      throw new StoreException(errorCode.toString() + \" when reading delete blob info from the log \" + dataDir, e,\n-          errorCode);\n+    IndexValue putValue =\n+        findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n+            indexSegments);\n+    if (putValue != null) {\n+      // use the expiration time from the original value because it may have been updated\n+      readOptions = new BlobReadOptions(log, putValue.getOffset(),\n+          new MessageInfo(key, putValue.getSize(), true, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index),\n+              value.getExpiresAtMs(), putValue.getAccountId(), putValue.getContainerId(),\n+              putValue.getOperationTimeInMs()));\n+    } else {\n+      // PUT record no longer available.\n+      throw new StoreException(\"Did not find PUT index entry for key [\" + key\n+          + \"] and the the original offset in value of the DELETE entry was [\" + value.getOriginalMessageOffset() + \"]\",\n+          StoreErrorCodes.ID_Deleted);", "originalCommit": "bfffc3b67916d597093580b47026c278f9bce760", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzUzNjk3NA==", "url": "https://github.com/linkedin/ambry/pull/1391#discussion_r383536974", "bodyText": "I think it can be further detailed. It means the original Put Record is already compacted, or this node only replicates the Delete record from it's peer, which are different error code and ambry frontend would react differently. But this is what we have before so I will just keep as it's.", "author": "justinlin-linkedin", "createdAt": "2020-02-24T21:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTc4MQ=="}], "type": "inlineReview"}, {"oid": "11ef2d46fccfdfc40b6f27961be36cbb96b6d925", "url": "https://github.com/linkedin/ambry/commit/11ef2d46fccfdfc40b6f27961be36cbb96b6d925", "message": "Remove getMessageInfo from MessageStoreHardDelete interface", "committedDate": "2020-02-24T21:48:54Z", "type": "commit"}, {"oid": "5df0bd6e4f4d6d58b6620a9e9c646aa09a569aa9", "url": "https://github.com/linkedin/ambry/commit/5df0bd6e4f4d6d58b6620a9e9c646aa09a569aa9", "message": "Yingyi comemnts", "committedDate": "2020-02-24T21:53:35Z", "type": "commit"}, {"oid": "5df0bd6e4f4d6d58b6620a9e9c646aa09a569aa9", "url": "https://github.com/linkedin/ambry/commit/5df0bd6e4f4d6d58b6620a9e9c646aa09a569aa9", "message": "Yingyi comemnts", "committedDate": "2020-02-24T21:53:35Z", "type": "forcePushed"}]}