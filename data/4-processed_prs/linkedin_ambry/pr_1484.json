{"pr_number": 1484, "pr_title": "Use conservative findEntriesSince size limit when update and put are both in the journal", "pr_createdAt": "2020-04-22T16:30:39Z", "pr_url": "https://github.com/linkedin/ambry/pull/1484", "timeline": [{"oid": "3136b78331d4108fa2af717258d10fd9d2862cdb", "url": "https://github.com/linkedin/ambry/commit/3136b78331d4108fa2af717258d10fd9d2862cdb", "message": "Use conservative findEntriesSince size limit when update and put are both in the journal\n\nIn the case where both a put and a TTL update show up when finding\nentries in the journal, the most recent index entry found will always be\nthe ttl update entry. This means that the size of the TTL update entry\nwill be checked against the size limit of the findEntriesSince query.\nHowever, a replicator will often have to fetch the original blob\nanyways if they don't already have the put entry, which means that\nthey could do a much larger batch get than expected, leading to\nexcessive memory consumption on the batch-get server side.\n\nTo addresss this issue, this commit adds logic to check if there is a\nput that the caller of findEntriesSince has likely not encountered yet,\nand, if so, counts its size towards the limit. This behavior is not used\nfor deletes, since there is no need to fetch a deleted blob's data.", "committedDate": "2020-04-22T17:20:11Z", "type": "commit"}, {"oid": "3136b78331d4108fa2af717258d10fd9d2862cdb", "url": "https://github.com/linkedin/ambry/commit/3136b78331d4108fa2af717258d10fd9d2862cdb", "message": "Use conservative findEntriesSince size limit when update and put are both in the journal\n\nIn the case where both a put and a TTL update show up when finding\nentries in the journal, the most recent index entry found will always be\nthe ttl update entry. This means that the size of the TTL update entry\nwill be checked against the size limit of the findEntriesSince query.\nHowever, a replicator will often have to fetch the original blob\nanyways if they don't already have the put entry, which means that\nthey could do a much larger batch get than expected, leading to\nexcessive memory consumption on the batch-get server side.\n\nTo addresss this issue, this commit adds logic to check if there is a\nput that the caller of findEntriesSince has likely not encountered yet,\nand, if so, counts its size towards the limit. This behavior is not used\nfor deletes, since there is no need to fetch a deleted blob's data.", "committedDate": "2020-04-22T17:20:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIyMTc1Nw==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r413221757", "bodyText": "nit: Now we are no this, I think we can do a de-dup before going through the for loop.", "author": "justinlin-linkedin", "createdAt": "2020-04-22T18:32:21Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -1162,14 +1167,12 @@ FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws St\n                   + entries.size());\n           Offset offsetEnd = offsetToStart;\n           long currentTotalSizeOfEntries = 0;\n+          Offset endOffsetOfSnapshot = getCurrentEndOffset(indexSegments);\n           for (JournalEntry entry : entries) {", "originalCommit": "3136b78331d4108fa2af717258d10fd9d2862cdb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQyMTAxNA==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r413421014", "bodyText": "addressed this with caching of messageinfos", "author": "cgtz", "createdAt": "2020-04-23T00:22:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIyMTc1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI2MDE3Ng==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r413260176", "bodyText": "nit: ttlUpdate or undelete", "author": "jsjtzyy", "createdAt": "2020-04-22T19:29:41Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -1574,6 +1575,41 @@ private StoreFindToken findEntriesFromSegmentStartOffset(Offset initialSegmentSt\n     }\n   }\n \n+  /**\n+   * Helper method for getting a {@link MessageInfo} and an estimate of fetch size corresponding to the provided\n+   * {@link JournalEntry}. This size may include the size of the put record if the put record is at or past the offset\n+   * in the journal entry and the latest value in the index is a delete or undelete. In this case, we can make a safe", "originalCommit": "3136b78331d4108fa2af717258d10fd9d2862cdb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI2ODcwOQ==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r413268709", "bodyText": "removed?", "author": "jsjtzyy", "createdAt": "2020-04-22T19:39:43Z", "path": "ambry-store/src/test/java/com/github/ambry/store/IndexTest.java", "diffHunk": "@@ -2455,6 +2457,7 @@ private void doFindEntriesSinceTest(StoreFindToken startToken, long maxTotalSize\n       StoreFindToken expectedEndToken) throws StoreException {\n     FindInfo findInfo = state.index.findEntriesSince(startToken, maxTotalSizeOfEntries);\n     StoreFindToken token = (StoreFindToken) findInfo.getFindToken();\n+    System.out.println(\"GOT token: \" + token);", "originalCommit": "3136b78331d4108fa2af717258d10fd9d2862cdb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9c7b95d04484b2d69c297d680187c5125528ef9a", "url": "https://github.com/linkedin/ambry/commit/9c7b95d04484b2d69c297d680187c5125528ef9a", "message": "Cache MessageInfos to limit redundant index searches, address other comments", "committedDate": "2020-04-23T00:18:41Z", "type": "commit"}, {"oid": "70c607d61397471da497c9f8122189abb1d4114a", "url": "https://github.com/linkedin/ambry/commit/70c607d61397471da497c9f8122189abb1d4114a", "message": "Remove unused import", "committedDate": "2020-04-23T17:23:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAwNTA1Ng==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r414005056", "bodyText": "Logic looks good. I just need some clarification on PUT. If current entry is a PUT, how can we avoid its size being added to currentTotalSizeOfEntries twice ?", "author": "jsjtzyy", "createdAt": "2020-04-23T17:55:23Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -1574,6 +1573,52 @@ private StoreFindToken findEntriesFromSegmentStartOffset(Offset initialSegmentSt\n     }\n   }\n \n+  /**\n+   * Helper method for getting a {@link MessageInfo} and an estimate of fetch size corresponding to the provided\n+   * {@link JournalEntry} and adding it to the list of messages. This size added to currentTotalSizeOfEntries may\n+   * include the size of the put record if the put record is at or past the offset in the journal entry and the latest\n+   * value in the index is a ttlUpdate or undelete. In this case, we can make a safe guess that the original blob also\n+   * has to be fetched by a replicator anyways and should be counted towards the size limit.\n+   * @param entry the {@link JournalEntry} to look up in the index.\n+   * @param endOffsetOfSearchRange the end of the range to search in the index.\n+   * @param indexSegments the index snapshot to search.\n+   * @param messageInfoCache a cache to store previous find results for a key. Queries from an earlier offset will\n+   *                         return the same latest value, so there is no need to search the index a second time for\n+   *                         keys with multiple journal entries.\n+   * @param messageEntries the list of {@link MessageInfo} being constructed. This list will be added to.\n+   * @param currentTotalSizeOfEntries a counter for the size in bytes of the entries returned in the query.\n+   * @throws StoreException on index search errors.\n+   */\n+  private void addMessageInfoForJournalEntry(JournalEntry entry, Offset endOffsetOfSearchRange,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments, Map<StoreKey, MessageInfo> messageInfoCache,\n+      List<MessageInfo> messageEntries, AtomicLong currentTotalSizeOfEntries) throws StoreException {\n+    MessageInfo messageInfo = messageInfoCache.get(entry.getKey());\n+    if (messageInfo == null) {\n+      // we only need to do an index lookup once per key since the following method will honor any index\n+      // values for the key past entry.getOffset()\n+      List<IndexValue> valuesInRange =\n+          findAllIndexValuesForKey(entry.getKey(), new FileSpan(entry.getOffset(), endOffsetOfSearchRange),\n+              EnumSet.allOf(IndexEntryType.class), indexSegments);\n+\n+      IndexValue latestValue = valuesInRange.get(0);\n+      if (valuesInRange.size() > 1 && (latestValue.isTtlUpdate() || latestValue.isUndelete())) {\n+        IndexValue earliestValue = valuesInRange.get(valuesInRange.size() - 1);\n+        if (earliestValue.isPut() && earliestValue.getOffset().compareTo(entry.getOffset()) >= 0) {\n+          currentTotalSizeOfEntries.addAndGet(earliestValue.getSize());\n+        }\n+      }\n+      messageInfo =\n+          new MessageInfo(entry.getKey(), latestValue.getSize(), latestValue.isDelete(), latestValue.isTtlUpdate(),\n+              latestValue.isUndelete(), latestValue.getExpiresAtMs(), null, latestValue.getAccountId(),\n+              latestValue.getContainerId(), latestValue.getOperationTimeInMs(), latestValue.getLifeVersion());\n+      messageInfoCache.put(entry.getKey(), messageInfo);\n+    }\n+    // We may add duplicate MessageInfos to the list here since the ordering of the list is used to calculate\n+    // bytes read for the token. The duplicates will be cleaned up by eliminateDuplicates.\n+    currentTotalSizeOfEntries.addAndGet(messageInfo.getSize());", "originalCommit": "70c607d61397471da497c9f8122189abb1d4114a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE1MTMyMg==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r414151322", "bodyText": "Sorry, I missed this question this morning. If the most recent entry is a put, the check on line 1604 will be false, so the size will only be accounted for once. The only case where extra size will be added if there are more than one entry found past entry.getOffset() and the latest entry is a TTL update or undelete and the earliest value at or past the journal entry is a put (e.g. not something else that doesn't indicate a need for blob fetching like a delete).", "author": "cgtz", "createdAt": "2020-04-23T21:57:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAwNTA1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4NzA1OQ==", "url": "https://github.com/linkedin/ambry/pull/1484#discussion_r414187059", "bodyText": "got it", "author": "jsjtzyy", "createdAt": "2020-04-23T23:20:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDAwNTA1Ng=="}], "type": "inlineReview"}]}