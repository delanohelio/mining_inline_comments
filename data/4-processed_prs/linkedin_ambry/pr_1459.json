{"pr_number": 1459, "pr_title": "Add some metrics for cloud replication feed", "pr_createdAt": "2020-04-07T20:48:23Z", "pr_url": "https://github.com/linkedin/ambry/pull/1459", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5OTY2OA==", "url": "https://github.com/linkedin/ambry/pull/1459#discussion_r405799668", "bodyText": "I probably misunderstand this but I am thinking if there is a case where cacheHit is true before entering the while(true) loop.  The first time code enters in while loop, index  == fetchedEntries.size() and jumps to else branch. It does fetch new changes and index is reset to 0. Then it goes back to beginning of while(true) and azureMetrics.changeFeedCacheHitRate.mark(); will be called. I just want to confirm this is a possible case and updating changeFeedCacheHitRate is what we expect.", "author": "jsjtzyy", "createdAt": "2020-04-08T20:38:16Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -169,49 +170,64 @@ public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccesso\n   @Override\n   public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,\n       String partitionPath) throws DocumentClientException {\n-    List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n-    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n-    int index = cosmosChangeFeedFindToken.getIndex();\n-    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(cosmosChangeFeedFindToken.getCacheSessionId());\n-    if (changeFeedCacheEntry == null || !isCacheValid(partitionPath, cosmosChangeFeedFindToken, changeFeedCacheEntry)) {\n-      // the cache may not be valid. So we cannot use session id\n-      changeFeedCacheEntry = getNextChangeFeed(partitionPath, cosmosChangeFeedFindToken.getStartContinuationToken());\n-      // invalidate the previous token's cache\n-      changeFeedCache.remove(cosmosChangeFeedFindToken.getCacheSessionId());\n-      index = 0;\n-    }\n+    Timer.Context operationTimer = azureMetrics.replicationFeedQueryTime.time();\n+    try {\n+      List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n+      CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n+      int index = cosmosChangeFeedFindToken.getIndex();\n+      ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(cosmosChangeFeedFindToken.getCacheSessionId());\n+      boolean cacheHit = true;\n+      if (changeFeedCacheEntry == null || !isCacheValid(partitionPath, cosmosChangeFeedFindToken,\n+          changeFeedCacheEntry)) {\n+        // the cache may not be valid. So we cannot use session id\n+        azureMetrics.changeFeedCacheMissRate.mark();\n+        cacheHit = false;\n+        changeFeedCacheEntry = getNextChangeFeed(partitionPath, cosmosChangeFeedFindToken.getStartContinuationToken());\n+        // invalidate the previous token's cache\n+        changeFeedCache.remove(cosmosChangeFeedFindToken.getCacheSessionId());\n+        index = 0;\n+      }\n \n-    long resultSize = 0;\n+      long resultSize = 0;\n \n-    List<CloudBlobMetadata> fetchedEntries = changeFeedCacheEntry.getFetchedEntries();\n-    while (true) {\n-      if (index < fetchedEntries.size()) {\n-        if (resultSize + fetchedEntries.get(index).getSize() < maxTotalSizeOfEntries || resultSize == 0) {\n-          nextEntries.add(fetchedEntries.get(index));\n-          resultSize = resultSize + fetchedEntries.get(index).getSize();\n-          index++;\n+      List<CloudBlobMetadata> fetchedEntries = changeFeedCacheEntry.getFetchedEntries();\n+      while (true) {\n+        if (index < fetchedEntries.size()) {\n+          if (cacheHit) {\n+            azureMetrics.changeFeedCacheHitRate.mark();\n+            cacheHit = false;\n+          }\n+          if (resultSize + fetchedEntries.get(index).getSize() < maxTotalSizeOfEntries || resultSize == 0) {\n+            nextEntries.add(fetchedEntries.get(index));\n+            resultSize = resultSize + fetchedEntries.get(index).getSize();\n+            index++;\n+          } else {\n+            break;\n+          }\n         } else {\n-          break;\n-        }\n-      } else {\n-        // we can reuse the session id in this case, because we know that the cache ran out of new items.\n-        changeFeedCacheEntry = getNextChangeFeed(partitionPath, changeFeedCacheEntry.getEndContinuationToken(),\n-            changeFeedCacheEntry.getCacheSessionId());\n-        fetchedEntries = changeFeedCacheEntry.getFetchedEntries();\n-        if (fetchedEntries.isEmpty()) {\n-          // this means that there are no new changes\n-          break;\n+          // we can reuse the session id in this case, because we know that the cache ran out of new items.\n+          changeFeedCacheEntry = getNextChangeFeed(partitionPath, changeFeedCacheEntry.getEndContinuationToken(),\n+              changeFeedCacheEntry.getCacheSessionId());\n+          fetchedEntries = changeFeedCacheEntry.getFetchedEntries();\n+          if (fetchedEntries.isEmpty()) {\n+            // this means that there are no new changes\n+            break;\n+          } else {\n+            azureMetrics.changeFeedCacheRefreshRate.mark();\n+          }\n+          index = 0;", "originalCommit": "f8badb8f8c13165b2bd15ab1e45a7846ceba3b23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg0MTc0MQ==", "url": "https://github.com/linkedin/ambry/pull/1459#discussion_r405841741", "bodyText": "First time code enters in while loop, and if index == fetchedEntries.size(), then we refresh the cache as you mentioned. In this case we don't want to say cacheHit just yet, because we haven't yet hit the cache. We will refresh the cache and mark it so.\nNow once the refresh is successful, there can be either no results (fetchedEntries.isEmpty()), in which case we will come out of loop without marking cache hit (as expected because it isn't technically a cache hit). If there are entries returned  in cache refresh, then loop's next iteration will run, which will mark the cache hit.\nIn short unless we are able to consume something from the cache, we don't mark a cache hit. Hope this makes sense.", "author": "ankagrawal", "createdAt": "2020-04-08T22:04:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5OTY2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2MzI3OQ==", "url": "https://github.com/linkedin/ambry/pull/1459#discussion_r405863279", "bodyText": "make sense", "author": "jsjtzyy", "createdAt": "2020-04-08T23:02:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc5OTY2OA=="}], "type": "inlineReview"}, {"oid": "58770f91a547d5708ddad00850f744a041401fb3", "url": "https://github.com/linkedin/ambry/commit/58770f91a547d5708ddad00850f744a041401fb3", "message": "Implementation of metrics for cloud replication feed", "committedDate": "2020-04-08T22:42:36Z", "type": "commit"}, {"oid": "1c5c86d9e6875265d9a58a5b1f47864658982014", "url": "https://github.com/linkedin/ambry/commit/1c5c86d9e6875265d9a58a5b1f47864658982014", "message": "Add tests for metrics.", "committedDate": "2020-04-08T22:42:36Z", "type": "commit"}, {"oid": "1c5c86d9e6875265d9a58a5b1f47864658982014", "url": "https://github.com/linkedin/ambry/commit/1c5c86d9e6875265d9a58a5b1f47864658982014", "message": "Add tests for metrics.", "committedDate": "2020-04-08T22:42:36Z", "type": "forcePushed"}]}