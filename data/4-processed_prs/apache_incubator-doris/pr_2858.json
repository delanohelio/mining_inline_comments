{"pr_number": 2858, "pr_title": "fix core when using grouping sets in large data", "pr_createdAt": "2020-02-07T10:30:44Z", "pr_url": "https://github.com/apache/incubator-doris/pull/2858", "timeline": [{"oid": "eaa048ba7bf5c6880fd69f8470e58b69051a1754", "url": "https://github.com/apache/incubator-doris/commit/eaa048ba7bf5c6880fd69f8470e58b69051a1754", "message": "fix core when using grouping sets in large data", "committedDate": "2020-02-07T11:18:18Z", "type": "commit"}, {"oid": "eaa048ba7bf5c6880fd69f8470e58b69051a1754", "url": "https://github.com/apache/incubator-doris/commit/eaa048ba7bf5c6880fd69f8470e58b69051a1754", "message": "fix core when using grouping sets in large data", "committedDate": "2020-02-07T11:18:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM1ODU2OQ==", "url": "https://github.com/apache/incubator-doris/pull/2858#discussion_r376358569", "bodyText": "is it a typo? should we use row_batch's capacity to calculate one tuple's memory size?", "author": "lingbin", "createdAt": "2020-02-07T12:08:03Z", "path": "be/src/exec/repeat_node.cpp", "diffHunk": "@@ -100,10 +93,18 @@ Status RepeatNode::get_repeated_batch(\n                 continue;\n             }\n \n-            char* new_tuple = reinterpret_cast<char*>(dst_tuples[j]);\n-            new_tuple += (*dst_it)->byte_size();\n-            dst_tuples[j] = reinterpret_cast<Tuple*>(new_tuple);\n-\n+            if (dst_tuples[j] == nullptr) {\n+                int size = row_batch->capacity() * (*dst_it)->byte_size();", "originalCommit": "eaa048ba7bf5c6880fd69f8470e58b69051a1754", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM5NDM5MQ==", "url": "https://github.com/apache/incubator-doris/pull/2858#discussion_r376394391", "bodyText": "the memory of dst_tuples[j] is all the memory of jth tuple in the row batch", "author": "yangzhg", "createdAt": "2020-02-07T13:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM1ODU2OQ=="}], "type": "inlineReview"}]}