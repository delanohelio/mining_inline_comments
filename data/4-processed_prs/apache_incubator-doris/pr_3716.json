{"pr_number": 3716, "pr_title": "[Spark load][Fe 5/6] Fe submit spark etl job", "pr_createdAt": "2020-05-28T16:55:45Z", "pr_url": "https://github.com/apache/incubator-doris/pull/3716", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxNDc4MA==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432214780", "bodyText": "prefer return byte[]", "author": "imay", "createdAt": "2020-05-29T01:52:25Z", "path": "fe/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -139,4 +142,326 @@ public static String printBroker(String brokerName, TNetworkAddress address) {\n         return Lists.newArrayList(columns);\n     }\n \n+    public static String readBrokerFile(String path, BrokerDesc brokerDesc) throws UserException {", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxNTAyOA==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432215028", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static void writeBrokerFile(byte[] data, String destFilePath, BrokerDesc brokerDesc) throws UserException {\n          \n          \n            \n                public static void writeFile(byte[] data, String destFilePath, BrokerDesc brokerDesc) throws UserException {", "author": "imay", "createdAt": "2020-05-29T01:53:28Z", "path": "fe/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -139,4 +142,326 @@ public static String printBroker(String brokerName, TNetworkAddress address) {\n         return Lists.newArrayList(columns);\n     }\n \n+    public static String readBrokerFile(String path, BrokerDesc brokerDesc) throws UserException {\n+        TNetworkAddress address = getAddress(brokerDesc);\n+        TPaloBrokerService.Client client = borrowClient(address);\n+        boolean failed = true;\n+        TBrokerFD fd = null;\n+        try {\n+            // get file size\n+            TBrokerListPathRequest request = new TBrokerListPathRequest(\n+                    TBrokerVersion.VERSION_ONE, path, false, brokerDesc.getProperties());\n+            TBrokerListResponse tBrokerListResponse = null;\n+            try {\n+                tBrokerListResponse = client.listPath(request);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tBrokerListResponse = client.listPath(request);\n+            }\n+            if (tBrokerListResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker list path failed. path=\" + path + \", broker=\" + address\n+                                                + \",msg=\" + tBrokerListResponse.getOpStatus().getMessage());\n+            }\n+            List<TBrokerFileStatus> fileStatuses = tBrokerListResponse.getFiles();\n+            if (fileStatuses.size() != 1) {\n+                throw new UserException(\"Broker files num error. path=\" + path + \", broker=\" + address\n+                                                + \", files num: \" + fileStatuses.size());\n+            }\n+\n+            Preconditions.checkState(!fileStatuses.get(0).isIsDir());\n+            long fileSize = fileStatuses.get(0).getSize();\n+\n+            // open reader\n+            String clientId = FrontendOptions.getLocalHostAddress() + \":\" + Config.rpc_port;\n+            TBrokerOpenReaderRequest tOpenReaderRequest = new TBrokerOpenReaderRequest(\n+                    TBrokerVersion.VERSION_ONE, path, 0, clientId, brokerDesc.getProperties());\n+            TBrokerOpenReaderResponse tOpenReaderResponse = null;\n+            try {\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            }\n+            if (tOpenReaderResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker open reader failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tOpenReaderResponse.getOpStatus().getMessage());\n+            }\n+            fd = tOpenReaderResponse.getFd();\n+\n+            // read\n+            TBrokerPReadRequest tPReadRequest = new TBrokerPReadRequest(\n+                    TBrokerVersion.VERSION_ONE, fd, 0, fileSize);\n+            TBrokerReadResponse tReadResponse = null;\n+            try {\n+                tReadResponse = client.pread(tPReadRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tReadResponse = client.pread(tPReadRequest);\n+            }\n+            if (tReadResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker read failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tReadResponse.getOpStatus().getMessage());\n+            }\n+            failed = false;\n+            byte[] data = tReadResponse.getData();\n+            return new String(data, \"UTF-8\");\n+        } catch (TException | UnsupportedEncodingException e) {\n+            String failMsg = \"Broker read file exception. path=\" + path + \", broker=\" + address;\n+            LOG.warn(failMsg, e);\n+            throw new UserException(failMsg);\n+        } finally {\n+            // close reader\n+            if (fd != null) {\n+                failed = true;\n+                TBrokerCloseReaderRequest tCloseReaderRequest = new TBrokerCloseReaderRequest(\n+                        TBrokerVersion.VERSION_ONE, fd);\n+                TBrokerOperationStatus tOperationStatus = null;\n+                try {\n+                    tOperationStatus = client.closeReader(tCloseReaderRequest);\n+                } catch (TException e) {\n+                    reopenClient(client);\n+                    try {\n+                        tOperationStatus = client.closeReader(tCloseReaderRequest);\n+                    } catch (TException ex) {\n+                        LOG.warn(\"Broker close reader failed. path={}, address={}\", path, address, ex);\n+                    }\n+                }\n+                if (tOperationStatus == null || tOperationStatus.getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                    LOG.warn(\"Broker close reader failed. path={}, address={}, error={}\", path, address,\n+                             tOperationStatus.getMessage());\n+                } else {\n+                    failed = false;\n+                }\n+            }\n+\n+            // return client\n+            returnClient(client, address, failed);\n+        }\n+    }\n+\n+    public static void writeBrokerFile(byte[] data, String destFilePath, BrokerDesc brokerDesc) throws UserException {", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxNTE3OA==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432215178", "bodyText": "should add comment for public function to help other to use it.", "author": "imay", "createdAt": "2020-05-29T01:54:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxNTAyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxNTA3Nw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432215077", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static void writeBrokerFile(String srcFilePath, String destFilePath,\n          \n          \n            \n                public static void writeFile(String srcFilePath, String destFilePath,", "author": "imay", "createdAt": "2020-05-29T01:53:39Z", "path": "fe/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -139,4 +142,326 @@ public static String printBroker(String brokerName, TNetworkAddress address) {\n         return Lists.newArrayList(columns);\n     }\n \n+    public static String readBrokerFile(String path, BrokerDesc brokerDesc) throws UserException {\n+        TNetworkAddress address = getAddress(brokerDesc);\n+        TPaloBrokerService.Client client = borrowClient(address);\n+        boolean failed = true;\n+        TBrokerFD fd = null;\n+        try {\n+            // get file size\n+            TBrokerListPathRequest request = new TBrokerListPathRequest(\n+                    TBrokerVersion.VERSION_ONE, path, false, brokerDesc.getProperties());\n+            TBrokerListResponse tBrokerListResponse = null;\n+            try {\n+                tBrokerListResponse = client.listPath(request);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tBrokerListResponse = client.listPath(request);\n+            }\n+            if (tBrokerListResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker list path failed. path=\" + path + \", broker=\" + address\n+                                                + \",msg=\" + tBrokerListResponse.getOpStatus().getMessage());\n+            }\n+            List<TBrokerFileStatus> fileStatuses = tBrokerListResponse.getFiles();\n+            if (fileStatuses.size() != 1) {\n+                throw new UserException(\"Broker files num error. path=\" + path + \", broker=\" + address\n+                                                + \", files num: \" + fileStatuses.size());\n+            }\n+\n+            Preconditions.checkState(!fileStatuses.get(0).isIsDir());\n+            long fileSize = fileStatuses.get(0).getSize();\n+\n+            // open reader\n+            String clientId = FrontendOptions.getLocalHostAddress() + \":\" + Config.rpc_port;\n+            TBrokerOpenReaderRequest tOpenReaderRequest = new TBrokerOpenReaderRequest(\n+                    TBrokerVersion.VERSION_ONE, path, 0, clientId, brokerDesc.getProperties());\n+            TBrokerOpenReaderResponse tOpenReaderResponse = null;\n+            try {\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            }\n+            if (tOpenReaderResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker open reader failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tOpenReaderResponse.getOpStatus().getMessage());\n+            }\n+            fd = tOpenReaderResponse.getFd();\n+\n+            // read\n+            TBrokerPReadRequest tPReadRequest = new TBrokerPReadRequest(\n+                    TBrokerVersion.VERSION_ONE, fd, 0, fileSize);\n+            TBrokerReadResponse tReadResponse = null;\n+            try {\n+                tReadResponse = client.pread(tPReadRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tReadResponse = client.pread(tPReadRequest);\n+            }\n+            if (tReadResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker read failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tReadResponse.getOpStatus().getMessage());\n+            }\n+            failed = false;\n+            byte[] data = tReadResponse.getData();\n+            return new String(data, \"UTF-8\");\n+        } catch (TException | UnsupportedEncodingException e) {\n+            String failMsg = \"Broker read file exception. path=\" + path + \", broker=\" + address;\n+            LOG.warn(failMsg, e);\n+            throw new UserException(failMsg);\n+        } finally {\n+            // close reader\n+            if (fd != null) {\n+                failed = true;\n+                TBrokerCloseReaderRequest tCloseReaderRequest = new TBrokerCloseReaderRequest(\n+                        TBrokerVersion.VERSION_ONE, fd);\n+                TBrokerOperationStatus tOperationStatus = null;\n+                try {\n+                    tOperationStatus = client.closeReader(tCloseReaderRequest);\n+                } catch (TException e) {\n+                    reopenClient(client);\n+                    try {\n+                        tOperationStatus = client.closeReader(tCloseReaderRequest);\n+                    } catch (TException ex) {\n+                        LOG.warn(\"Broker close reader failed. path={}, address={}\", path, address, ex);\n+                    }\n+                }\n+                if (tOperationStatus == null || tOperationStatus.getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                    LOG.warn(\"Broker close reader failed. path={}, address={}, error={}\", path, address,\n+                             tOperationStatus.getMessage());\n+                } else {\n+                    failed = false;\n+                }\n+            }\n+\n+            // return client\n+            returnClient(client, address, failed);\n+        }\n+    }\n+\n+    public static void writeBrokerFile(byte[] data, String destFilePath, BrokerDesc brokerDesc) throws UserException {\n+        BrokerWriter writer = new BrokerWriter(destFilePath, brokerDesc);\n+        try {\n+            writer.open();\n+            ByteBuffer byteBuffer = ByteBuffer.wrap(data);\n+            writer.write(byteBuffer, data.length);\n+        } finally {\n+            writer.close();\n+        }\n+    }\n+\n+    public static void writeBrokerFile(String srcFilePath, String destFilePath,", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDA5Ng==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432800096", "bodyText": "There is keysType is MaterializedIndexMeta. You can get it directly.", "author": "morningman", "createdAt": "2020-05-30T02:00:02Z", "path": "fe/src/main/java/org/apache/doris/catalog/OlapTable.java", "diffHunk": "@@ -530,6 +530,30 @@ public KeysType getKeysType() {\n         return keysType;\n     }\n \n+    public KeysType getKeysTypeByIndexId(long indexId) {", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDIxMw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432800213", "bodyText": "I'am not sure this is ok, cause there is no guarantee that the F and S object can also be serialized by GSON", "author": "morningman", "createdAt": "2020-05-30T02:01:13Z", "path": "fe/src/main/java/org/apache/doris/common/Pair.java", "diffHunk": "@@ -25,7 +27,9 @@\n public class Pair<F, S> {\n     public static PairComparator<Pair<?, Comparable>> PAIR_VALUE_COMPARATOR = new PairComparator<>();\n \n+    @SerializedName(value = \"first\")", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1MDk0Mw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r433350943", "bodyText": "Users guarantee this when use Pair class\uff1flike Map and List.", "author": "wyb", "createdAt": "2020-06-01T16:33:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NDQ1MQ==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r437874451", "bodyText": "I checked, this is not work", "author": "morningman", "createdAt": "2020-06-10T05:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4ODk5NQ==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r441988995", "bodyText": "I add a comment.\nWhen using Pair for persistence, users need to guarantee that F and S can be serialized through Gson", "author": "wyb", "createdAt": "2020-06-18T06:10:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMDMxNA==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432800314", "bodyText": "add unit to name.", "author": "morningman", "createdAt": "2020-05-30T02:02:49Z", "path": "fe/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -17,52 +17,59 @@\n \n package org.apache.doris.common.util;\n \n-import com.google.common.collect.Lists;\n import org.apache.doris.analysis.BrokerDesc;\n import org.apache.doris.catalog.Catalog;\n import org.apache.doris.catalog.FsBroker;\n import org.apache.doris.common.AnalysisException;\n import org.apache.doris.common.ClientPool;\n+import org.apache.doris.common.Config;\n import org.apache.doris.common.UserException;\n import org.apache.doris.service.FrontendOptions;\n+import org.apache.doris.thrift.TBrokerCloseReaderRequest;\n+import org.apache.doris.thrift.TBrokerCloseWriterRequest;\n+import org.apache.doris.thrift.TBrokerDeletePathRequest;\n+import org.apache.doris.thrift.TBrokerFD;\n import org.apache.doris.thrift.TBrokerFileStatus;\n import org.apache.doris.thrift.TBrokerListPathRequest;\n import org.apache.doris.thrift.TBrokerListResponse;\n+import org.apache.doris.thrift.TBrokerOpenMode;\n+import org.apache.doris.thrift.TBrokerOpenReaderRequest;\n+import org.apache.doris.thrift.TBrokerOpenReaderResponse;\n+import org.apache.doris.thrift.TBrokerOpenWriterRequest;\n+import org.apache.doris.thrift.TBrokerOpenWriterResponse;\n+import org.apache.doris.thrift.TBrokerOperationStatus;\n import org.apache.doris.thrift.TBrokerOperationStatusCode;\n+import org.apache.doris.thrift.TBrokerPReadRequest;\n+import org.apache.doris.thrift.TBrokerPWriteRequest;\n+import org.apache.doris.thrift.TBrokerReadResponse;\n import org.apache.doris.thrift.TBrokerVersion;\n import org.apache.doris.thrift.TNetworkAddress;\n import org.apache.doris.thrift.TPaloBrokerService;\n \n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.thrift.TException;\n \n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n import java.util.Collections;\n import java.util.List;\n \n public class BrokerUtil {\n     private static final Logger LOG = LogManager.getLogger(BrokerUtil.class);\n \n+    private static int READ_BUFFER_SIZE = 1024 * 1024;", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMTQ1Mw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432801453", "bodyText": "How about get it from SparkEtlJob.class.getXXX()?", "author": "morningman", "createdAt": "2020-05-30T02:19:19Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -0,0 +1,170 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.thrift.TEtlState;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+import org.apache.spark.launcher.SparkAppHandle.Listener;\n+import org.apache.spark.launcher.SparkAppHandle.State;\n+import org.apache.spark.launcher.SparkLauncher;\n+\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.Map;\n+\n+/**\n+ * SparkEtlJobHandler is responsible for\n+ * 1. submit spark etl job\n+ * 2. get spark etl job status\n+ * 3. kill spark etl job\n+ * 4. get spark etl file paths\n+ * 5. delete etl output path\n+ */\n+public class SparkEtlJobHandler {\n+    private static final Logger LOG = LogManager.getLogger(SparkEtlJobHandler.class);\n+\n+    private static final String APP_RESOURCE_NAME = \"palo-fe.jar\";\n+    private static final String CONFIG_FILE_NAME = \"jobconfig.json\";\n+    private static final String APP_RESOURCE_LOCAL_PATH = PaloFe.DORIS_HOME_DIR + \"/lib/\" + APP_RESOURCE_NAME;\n+    private static final String JOB_CONFIG_DIR = \"configs\";\n+    private static final String MAIN_CLASS = \"org.apache.doris.load.loadv2.etl.SparkEtlJob\";", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1MTkyMw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r433351923", "bodyText": "ok, I comment and will replace with it when SparkEtlJob class is merged.", "author": "wyb", "createdAt": "2020-06-01T16:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMTQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMTU5NQ==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r432801595", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            throw new LoadException(errMsg + \"wait too much time for getting appid. spark app state: \"\n          \n          \n            \n                            throw new LoadException(errMsg + \" wait too much time for getting appi d. spark app state: \"", "author": "morningman", "createdAt": "2020-05-30T02:21:16Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -0,0 +1,170 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.thrift.TEtlState;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+import org.apache.spark.launcher.SparkAppHandle.Listener;\n+import org.apache.spark.launcher.SparkAppHandle.State;\n+import org.apache.spark.launcher.SparkLauncher;\n+\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.Map;\n+\n+/**\n+ * SparkEtlJobHandler is responsible for\n+ * 1. submit spark etl job\n+ * 2. get spark etl job status\n+ * 3. kill spark etl job\n+ * 4. get spark etl file paths\n+ * 5. delete etl output path\n+ */\n+public class SparkEtlJobHandler {\n+    private static final Logger LOG = LogManager.getLogger(SparkEtlJobHandler.class);\n+\n+    private static final String APP_RESOURCE_NAME = \"palo-fe.jar\";\n+    private static final String CONFIG_FILE_NAME = \"jobconfig.json\";\n+    private static final String APP_RESOURCE_LOCAL_PATH = PaloFe.DORIS_HOME_DIR + \"/lib/\" + APP_RESOURCE_NAME;\n+    private static final String JOB_CONFIG_DIR = \"configs\";\n+    private static final String MAIN_CLASS = \"org.apache.doris.load.loadv2.etl.SparkEtlJob\";\n+    private static final String ETL_JOB_NAME = \"doris__%s\";\n+    // 5min\n+    private static final int GET_APPID_MAX_RETRY_TIMES = 300;\n+    private static final int GET_APPID_SLEEP_MS = 1000;\n+\n+    class SparkAppListener implements Listener {\n+        @Override\n+        public void stateChanged(SparkAppHandle sparkAppHandle) {}\n+\n+        @Override\n+        public void infoChanged(SparkAppHandle sparkAppHandle) {}\n+    }\n+\n+    public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,\n+                             BrokerDesc brokerDesc, SparkPendingTaskAttachment attachment) throws LoadException {\n+        // delete outputPath\n+        deleteEtlOutputPath(etlJobConfig.outputPath, brokerDesc);\n+\n+        // upload app resource and jobconfig to hdfs\n+        String configsHdfsDir = etlJobConfig.outputPath + \"/\" + JOB_CONFIG_DIR + \"/\";\n+        String appResourceHdfsPath = configsHdfsDir + APP_RESOURCE_NAME;\n+        String jobConfigHdfsPath = configsHdfsDir + CONFIG_FILE_NAME;\n+        try {\n+            BrokerUtil.writeBrokerFile(APP_RESOURCE_LOCAL_PATH, appResourceHdfsPath, brokerDesc);\n+            byte[] configData = etlJobConfig.configToJson().getBytes(\"UTF-8\");\n+            BrokerUtil.writeBrokerFile(configData, jobConfigHdfsPath, brokerDesc);\n+        } catch (UserException | UnsupportedEncodingException e) {\n+            throw new LoadException(e.getMessage());\n+        }\n+\n+        SparkLauncher launcher = new SparkLauncher();\n+        // master      |  deployMode\n+        // ------------|-------------\n+        // yarn        |  cluster\n+        // spark://xx  |  client\n+        launcher.setMaster(resource.getMaster())\n+                .setDeployMode(resource.getDeployMode().name().toLowerCase())\n+                .setAppResource(appResourceHdfsPath)\n+                .setMainClass(MAIN_CLASS)\n+                .setAppName(String.format(ETL_JOB_NAME, loadLabel))\n+                .addAppArgs(jobConfigHdfsPath);\n+        // spark configs\n+        for (Map.Entry<String, String> entry : resource.getSparkConfigs().entrySet()) {\n+            launcher.setConf(entry.getKey(), entry.getValue());\n+        }\n+\n+        // start app\n+        SparkAppHandle handle = null;\n+        State state = null;\n+        String appId = null;\n+        int retry = 0;\n+        String errMsg = \"start spark app failed. error: \";\n+        try {\n+            handle = launcher.startApplication(new SparkAppListener());\n+        } catch (IOException e) {\n+            LOG.warn(errMsg, e);\n+            throw new LoadException(errMsg + e.getMessage());\n+        }\n+\n+        while (retry++ < GET_APPID_MAX_RETRY_TIMES) {\n+            appId = handle.getAppId();\n+            if (appId != null) {\n+                break;\n+            }\n+\n+            // check state and retry\n+            state = handle.getState();\n+            if (fromSparkState(state) == TEtlState.CANCELLED) {\n+                throw new LoadException(errMsg + \"spark app state: \" + state.toString());\n+            }\n+            if (retry >= GET_APPID_MAX_RETRY_TIMES) {\n+                throw new LoadException(errMsg + \"wait too much time for getting appid. spark app state: \"", "originalCommit": "90de0a81e22aa950ed0c219c8d501ac18a79beaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1MjY5Nw==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r433352697", "bodyText": "errMsg already have a space at the end.", "author": "wyb", "createdAt": "2020-06-01T16:37:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMTU5NQ=="}], "type": "inlineReview"}, {"oid": "6d0ef4e0893ef257820d37c338f951b7540941da", "url": "https://github.com/apache/incubator-doris/commit/6d0ef4e0893ef257820d37c338f951b7540941da", "message": "Fe submit spark etl job", "committedDate": "2020-06-13T11:46:28Z", "type": "forcePushed"}, {"oid": "20df6f27abcef44628c0dca27466820f9fa10544", "url": "https://github.com/apache/incubator-doris/commit/20df6f27abcef44628c0dca27466820f9fa10544", "message": "Fe submit spark etl job", "committedDate": "2020-06-13T12:36:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU1NjIxNA==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r441556214", "bodyText": "broker's pread() method does not guarantee to read the specified length of data currently.\nBut #3881 is trying to solve this problem. Just for remind.", "author": "morningman", "createdAt": "2020-06-17T13:44:39Z", "path": "fe/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -139,4 +148,352 @@ public static String printBroker(String brokerName, TNetworkAddress address) {\n         return Lists.newArrayList(columns);\n     }\n \n+    /**\n+     * Read binary data from path with broker\n+     * @param path\n+     * @param brokerDesc\n+     * @return byte[]\n+     * @throws UserException if broker op failed or not only one file\n+     */\n+    public static byte[] readFile(String path, BrokerDesc brokerDesc) throws UserException {\n+        TNetworkAddress address = getAddress(brokerDesc);\n+        TPaloBrokerService.Client client = borrowClient(address);\n+        boolean failed = true;\n+        TBrokerFD fd = null;\n+        try {\n+            // get file size\n+            TBrokerListPathRequest request = new TBrokerListPathRequest(\n+                    TBrokerVersion.VERSION_ONE, path, false, brokerDesc.getProperties());\n+            TBrokerListResponse tBrokerListResponse = null;\n+            try {\n+                tBrokerListResponse = client.listPath(request);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tBrokerListResponse = client.listPath(request);\n+            }\n+            if (tBrokerListResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker list path failed. path=\" + path + \", broker=\" + address\n+                                                + \",msg=\" + tBrokerListResponse.getOpStatus().getMessage());\n+            }\n+            List<TBrokerFileStatus> fileStatuses = tBrokerListResponse.getFiles();\n+            if (fileStatuses.size() != 1) {\n+                throw new UserException(\"Broker files num error. path=\" + path + \", broker=\" + address\n+                                                + \", files num: \" + fileStatuses.size());\n+            }\n+\n+            Preconditions.checkState(!fileStatuses.get(0).isIsDir());\n+            long fileSize = fileStatuses.get(0).getSize();\n+\n+            // open reader\n+            String clientId = FrontendOptions.getLocalHostAddress() + \":\" + Config.rpc_port;\n+            TBrokerOpenReaderRequest tOpenReaderRequest = new TBrokerOpenReaderRequest(\n+                    TBrokerVersion.VERSION_ONE, path, 0, clientId, brokerDesc.getProperties());\n+            TBrokerOpenReaderResponse tOpenReaderResponse = null;\n+            try {\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tOpenReaderResponse = client.openReader(tOpenReaderRequest);\n+            }\n+            if (tOpenReaderResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker open reader failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tOpenReaderResponse.getOpStatus().getMessage());\n+            }\n+            fd = tOpenReaderResponse.getFd();\n+\n+            // read\n+            TBrokerPReadRequest tPReadRequest = new TBrokerPReadRequest(\n+                    TBrokerVersion.VERSION_ONE, fd, 0, fileSize);\n+            TBrokerReadResponse tReadResponse = null;\n+            try {\n+                tReadResponse = client.pread(tPReadRequest);\n+            } catch (TException e) {\n+                reopenClient(client);\n+                tReadResponse = client.pread(tPReadRequest);\n+            }\n+            if (tReadResponse.getOpStatus().getStatusCode() != TBrokerOperationStatusCode.OK) {\n+                throw new UserException(\"Broker read failed. path=\" + path + \", broker=\" + address\n+                                                + \", msg=\" + tReadResponse.getOpStatus().getMessage());\n+            }\n+            failed = false;\n+            return tReadResponse.getData();", "originalCommit": "dc7a607af6a17103fc078501f4f579386310cd6e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4ODA3NQ==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r441988075", "bodyText": "ok", "author": "wyb", "createdAt": "2020-06-18T06:07:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU1NjIxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU2NjY1Ng==", "url": "https://github.com/apache/incubator-doris/pull/3716#discussion_r441566656", "bodyText": "Print a log for tracing the state changing", "author": "morningman", "createdAt": "2020-06-17T13:58:32Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -127,6 +140,76 @@ private void setResourceInfo() throws DdlException {\n         brokerDesc = new BrokerDesc(sparkResource.getBroker(), brokerProperties);\n     }\n \n+    @Override\n+    public void beginTxn()\n+            throws LabelAlreadyUsedException, BeginTransactionException, AnalysisException, DuplicatedRequestException {\n+       transactionId = Catalog.getCurrentGlobalTransactionMgr()\n+                .beginTransaction(dbId, Lists.newArrayList(fileGroupAggInfo.getAllTableIds()), label, null,\n+                                  new TxnCoordinator(TxnSourceType.FE, FrontendOptions.getLocalHostAddress()),\n+                                  LoadJobSourceType.FRONTEND, id, timeoutSecond);\n+    }\n+\n+    @Override\n+    protected void unprotectedExecuteJob() throws LoadException {\n+        // create pending task\n+        LoadTask task = new SparkLoadPendingTask(this, fileGroupAggInfo.getAggKeyToFileGroups(),\n+                                                 sparkResource, brokerDesc);\n+        task.init();\n+        idToTasks.put(task.getSignature(), task);\n+        Catalog.getCurrentCatalog().getLoadTaskScheduler().submit(task);\n+    }\n+\n+    @Override\n+    public void onTaskFinished(TaskAttachment attachment) {\n+        if (attachment instanceof SparkPendingTaskAttachment) {\n+            onPendingTaskFinished((SparkPendingTaskAttachment) attachment);\n+        }\n+    }\n+\n+    private void onPendingTaskFinished(SparkPendingTaskAttachment attachment) {\n+        writeLock();\n+        try {\n+            // check if job has been cancelled\n+            if (isTxnDone()) {\n+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)\n+                                 .add(\"state\", state)\n+                                 .add(\"error_msg\", \"this task will be ignored when job is: \" + state)\n+                                 .build());\n+                return;\n+            }\n+\n+            if (finishedTaskIds.contains(attachment.getTaskId())) {\n+                LOG.warn(new LogBuilder(LogKey.LOAD_JOB, id)\n+                                 .add(\"task_id\", attachment.getTaskId())\n+                                 .add(\"error_msg\", \"this is a duplicated callback of pending task \"\n+                                         + \"when broker already has loading task\")\n+                                 .build());\n+                return;\n+            }\n+\n+            // add task id into finishedTaskIds\n+            finishedTaskIds.add(attachment.getTaskId());\n+\n+            sparkAppHandle = attachment.getHandle();\n+            appId = attachment.getAppId();\n+            etlOutputPath = attachment.getOutputPath();\n+\n+            executeEtl();\n+            // log etl state\n+            unprotectedLogUpdateStateInfo();\n+        } finally {\n+            writeUnlock();\n+        }\n+    }\n+\n+    /**\n+     * update etl start time and state in spark load job\n+     */\n+    private void executeEtl() {\n+        etlStartTimestamp = System.currentTimeMillis();\n+        state = JobState.ETL;", "originalCommit": "dc7a607af6a17103fc078501f4f579386310cd6e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "efcc79658314d0382006a5e5b20511be5d26e0a3", "url": "https://github.com/apache/incubator-doris/commit/efcc79658314d0382006a5e5b20511be5d26e0a3", "message": "Fe submit spark etl job", "committedDate": "2020-06-19T02:43:35Z", "type": "commit"}, {"oid": "efcc79658314d0382006a5e5b20511be5d26e0a3", "url": "https://github.com/apache/incubator-doris/commit/efcc79658314d0382006a5e5b20511be5d26e0a3", "message": "Fe submit spark etl job", "committedDate": "2020-06-19T02:43:35Z", "type": "forcePushed"}]}