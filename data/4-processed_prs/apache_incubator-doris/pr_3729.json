{"pr_number": 3729, "pr_title": "(#3728) Using SparkDpp to complete some calculation in Spark Load", "pr_createdAt": "2020-05-29T12:32:43Z", "pr_url": "https://github.com/apache/incubator-doris/pull/3729", "timeline": [{"oid": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "url": "https://github.com/apache/incubator-doris/commit/0758b09dc3d0dbd7754a7a1935b1103d86f35396", "message": "(#3728) Using SparkDpp to complete some calculation in Spark Load", "committedDate": "2020-05-29T12:30:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NTk3OQ==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435245979", "bodyText": "Decimal?", "author": "wyb", "createdAt": "2020-06-04T13:19:04Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -0,0 +1,175 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.doris.common.UserException;\n+import org.joda.time.DateTime;\n+\n+import java.io.Serializable;\n+import java.util.Date;\n+\n+// Parser to validate value for different type\n+public abstract class ColumnParser implements Serializable {\n+    public static ColumnParser create(String columnType) throws UserException {\n+        if (columnType.equalsIgnoreCase(\"TINYINT\")) {\n+            return new TinyIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"SMALLINT\")) {\n+            return new SmallIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"INT\")) {\n+            return new IntParser();\n+        } else if (columnType.equalsIgnoreCase(\"BIGINT\")) {\n+            return new BigIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"FLOAT\")) {\n+            return new FloatParser();\n+        } else if (columnType.equalsIgnoreCase(\"DOUBLE\")) {\n+            return new DoubleParser();\n+        } else if (columnType.equalsIgnoreCase(\"BOOLEAN\")) {\n+            return new BooleanParser();\n+        } else if (columnType.equalsIgnoreCase(\"DATE\")) {\n+            return new DateParser();\n+        } else if (columnType.equalsIgnoreCase(\"DATETIME\")) {\n+            return new DatetimeParser();\n+        } else if (columnType.equalsIgnoreCase(\"VARCHAR\")\n+                || columnType.equalsIgnoreCase(\"CHAR\")\n+                || columnType.equalsIgnoreCase(\"BITMAP\")\n+                || columnType.equalsIgnoreCase(\"HLL\")) {\n+            return new StringParser();\n+        } else {\n+            throw new UserException(\"unsupported type:\" + columnType);", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc2OTM3OA==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435769378", "bodyText": "maybe later support after first merged?", "author": "wangbo", "createdAt": "2020-06-05T08:29:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NTk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NjMzOQ==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435246339", "bodyText": "char and varchar type need to check byte length", "author": "wyb", "createdAt": "2020-06-04T13:19:36Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -0,0 +1,175 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.doris.common.UserException;\n+import org.joda.time.DateTime;\n+\n+import java.io.Serializable;\n+import java.util.Date;\n+\n+// Parser to validate value for different type\n+public abstract class ColumnParser implements Serializable {\n+    public static ColumnParser create(String columnType) throws UserException {\n+        if (columnType.equalsIgnoreCase(\"TINYINT\")) {\n+            return new TinyIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"SMALLINT\")) {\n+            return new SmallIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"INT\")) {\n+            return new IntParser();\n+        } else if (columnType.equalsIgnoreCase(\"BIGINT\")) {\n+            return new BigIntParser();\n+        } else if (columnType.equalsIgnoreCase(\"FLOAT\")) {\n+            return new FloatParser();\n+        } else if (columnType.equalsIgnoreCase(\"DOUBLE\")) {\n+            return new DoubleParser();\n+        } else if (columnType.equalsIgnoreCase(\"BOOLEAN\")) {\n+            return new BooleanParser();\n+        } else if (columnType.equalsIgnoreCase(\"DATE\")) {\n+            return new DateParser();\n+        } else if (columnType.equalsIgnoreCase(\"DATETIME\")) {\n+            return new DatetimeParser();\n+        } else if (columnType.equalsIgnoreCase(\"VARCHAR\")\n+                || columnType.equalsIgnoreCase(\"CHAR\")\n+                || columnType.equalsIgnoreCase(\"BITMAP\")\n+                || columnType.equalsIgnoreCase(\"HLL\")) {\n+            return new StringParser();\n+        } else {\n+            throw new UserException(\"unsupported type:\" + columnType);\n+        }\n+    }\n+\n+    public abstract boolean parse(String value);\n+}\n+\n+class TinyIntParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Short parsed = Short.parseShort(value);\n+            if (parsed > 127 || parsed < -128) {\n+                return false;\n+            }\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class SmallIntParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Short.parseShort(value);\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class IntParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Integer.parseInt(value);\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class BigIntParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Integer.parseInt(value);\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class FloatParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Float.parseFloat(value);\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class DoubleParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Double.parseDouble(value);\n+        } catch (NumberFormatException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class BooleanParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        if (value.equalsIgnoreCase(\"true\")\n+                || value.equalsIgnoreCase(\"false\")) {\n+            return true;\n+        }\n+        return false;\n+    }\n+}\n+\n+class DateParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            Date.parse(value);\n+        } catch (IllegalArgumentException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class DatetimeParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            DateTime.parse(value);\n+        } catch (IllegalArgumentException e) {\n+            return false;\n+        }\n+        return true;\n+    }\n+}\n+\n+class StringParser extends ColumnParser {\n+    @Override\n+    public boolean parse(String value) {\n+        return true;", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc2OTYyMA==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435769620", "bodyText": "\ud83d\udc4c", "author": "wangbo", "createdAt": "2020-06-05T08:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NjMzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NzE3OQ==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435247179", "bodyText": "Don't use short value for getting tinyint type hash value, this will cause the tinyint data partitioned into wrong bucket.", "author": "wyb", "createdAt": "2020-06-04T13:20:47Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/DppUtils.java", "diffHunk": "@@ -0,0 +1,252 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.Row;\n+\n+import com.google.common.collect.Lists;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.zip.CRC32;\n+\n+public class DppUtils {\n+    public static final String BUCKET_ID = \"__bucketId__\";\n+    public static Class getClassFromDataType(DataType dataType) {\n+        if (dataType == null) {\n+            return null;\n+        }\n+        if (dataType.equals(DataTypes.BooleanType)) {\n+            return Boolean.class;\n+        } else if (dataType.equals(DataTypes.ShortType)) {\n+            return Short.class;\n+        } else if (dataType.equals(DataTypes.IntegerType)) {\n+            return Integer.class;\n+        } else if (dataType.equals(DataTypes.LongType)) {\n+            return Long.class;\n+        } else if (dataType.equals(DataTypes.FloatType)) {\n+            return Float.class;\n+        } else if (dataType.equals(DataTypes.DoubleType)) {\n+            return Double.class;\n+        } else if (dataType.equals(DataTypes.DateType)) {\n+            return Date.class;\n+        } else if (dataType.equals(DataTypes.StringType)) {\n+            return String.class;\n+        } else if (dataType instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType)dataType;\n+            return BigDecimal.valueOf(decimalType.precision(), decimalType.scale()).getClass();\n+        } else if (dataType.equals(DataTypes.TimestampType)) {\n+            return Long.class;\n+        }\n+        return null;\n+    }\n+\n+    public static Class getClassFromColumn(EtlJobConfig.EtlColumn column) throws UserException {\n+        switch (column.columnType) {\n+            case \"BOOLEAN\":\n+                return Boolean.class;\n+            case \"TINYINT\":\n+            case \"SMALLINT\":\n+                return Short.class;\n+            case \"INT\":\n+                return Integer.class;\n+            case \"DATETIME\":\n+            case \"BIGINT\":\n+                return Long.class;\n+            case \"LARGEINT\":\n+                throw new UserException(\"LARGEINT is not supported now\");\n+            case \"FLOAT\":\n+                return Float.class;\n+            case \"DOUBLE\":\n+                return Double.class;\n+            case \"DATE\":\n+                return Date.class;\n+            case \"HLL\":\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+            case \"BITMAP\":\n+            case \"OBJECT\":\n+                return String.class;\n+            case \"DECIMALV2\":\n+                return BigDecimal.valueOf(column.precision, column.scale).getClass();\n+            default:\n+                return String.class;\n+        }\n+    }\n+\n+    public static DataType getDataTypeFromColumn(EtlJobConfig.EtlColumn column, boolean regardDistinctColumnAsBinary) {\n+        DataType dataType = DataTypes.StringType;\n+        switch (column.columnType) {\n+            case \"BOOLEAN\":\n+                dataType = DataTypes.StringType;\n+                break;\n+            case \"TINYINT\":\n+            case \"SMALLINT\":\n+                dataType = DataTypes.ShortType;\n+                break;\n+            case \"INT\":\n+                dataType = DataTypes.IntegerType;\n+                break;\n+            case \"DATETIME\":\n+                dataType = DataTypes.TimestampType;\n+                break;\n+            case \"BIGINT\":\n+                dataType = DataTypes.LongType;\n+                break;\n+            case \"LARGEINT\":\n+                dataType = DataTypes.StringType;\n+                break;\n+            case \"FLOAT\":\n+                dataType = DataTypes.FloatType;\n+                break;\n+            case \"DOUBLE\":\n+                dataType = DataTypes.DoubleType;\n+                break;\n+            case \"DATE\":\n+                dataType = DataTypes.DateType;\n+                break;\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+            case \"OBJECT\":\n+                dataType = DataTypes.StringType;\n+                break;\n+            case \"HLL\":\n+            case \"BITMAP\":\n+                dataType = regardDistinctColumnAsBinary ? DataTypes.BinaryType : DataTypes.StringType;\n+                break;\n+            case \"DECIMALV2\":\n+                dataType = DecimalType.apply(column.precision, column.scale);\n+                break;\n+            default:\n+                throw new RuntimeException(\"Reason: invalid column type:\" + column);\n+        }\n+        return dataType;\n+    }\n+\n+    public static ByteBuffer getHashValue(Object o, DataType type) {\n+        ByteBuffer buffer = ByteBuffer.allocate(8);\n+        buffer.order(ByteOrder.LITTLE_ENDIAN);\n+        if (o == null) {\n+            buffer.putInt(0);\n+            return buffer;\n+        }\n+        if (type.equals(DataTypes.ShortType)) {\n+            buffer.putShort((Short)o);", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc3MzQwMg==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435773402", "bodyText": "\ud83d\udc4c", "author": "wangbo", "createdAt": "2020-06-05T08:37:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NzE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0Nzc2OA==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435247768", "bodyText": "Remove the semicolon", "author": "wyb", "createdAt": "2020-06-04T13:21:36Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -0,0 +1,851 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import com.google.common.base.Strings;\n+import com.google.gson.Gson;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.function.ForeachPartitionFunction;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.functions;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.TaskContext;\n+\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+\n+import java.io.IOException;\n+import java.math.BigInteger;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import scala.collection.JavaConverters;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+// This class is a Spark-based data preprocessing program,\n+// which will make use of the distributed compute framework of spark to\n+// do ETL job/sort/preaggregate jobs in spark job\n+// to boost the process of large amount of data load.\n+// the process steps are as following:\n+// 1. load data\n+//     1.1 load data from path/hive table\n+//     1.2 do the etl process\n+// 2. repartition data by using doris data model(partition and bucket)\n+// 3. process aggregation if needed\n+// 4. write data to parquet file\n+public final class SparkDpp implements java.io.Serializable {\n+    private static final Logger LOG = LogManager.getLogger(SparkDpp.class);\n+\n+    private static final String NULL_FLAG = \"\\\\N\";\n+    private static final String DPP_RESULT_FILE = \"dpp_result.json\";\n+    private static final String BITMAP_TYPE = \"bitmap\";\n+    private SparkSession spark = null;\n+    private EtlJobConfig etlJobConfig = null;\n+    private LongAccumulator abnormalRowAcc = null;\n+    private LongAccumulator unselectedRowAcc = null;\n+    private LongAccumulator scannedRowsAcc = null;\n+    private LongAccumulator fileNumberAcc = null;\n+    private LongAccumulator fileSizeAcc = null;\n+    private Map<String, Integer> bucketKeyMap = new HashMap<>();\n+    // accumulator to collect invalid rows\n+    private StringAccumulator invalidRows = new StringAccumulator();\n+\n+    public SparkDpp(SparkSession spark, EtlJobConfig etlJobConfig) {\n+        this.spark = spark;\n+        this.etlJobConfig = etlJobConfig;\n+    }\n+\n+    public void init() {\n+        abnormalRowAcc = spark.sparkContext().longAccumulator(\"abnormalRowAcc\");\n+        unselectedRowAcc = spark.sparkContext().longAccumulator(\"unselectedRowAcc\");\n+        scannedRowsAcc = spark.sparkContext().longAccumulator(\"scannedRowsAcc\");\n+        fileNumberAcc = spark.sparkContext().longAccumulator(\"fileNumberAcc\");\n+        fileSizeAcc = spark.sparkContext().longAccumulator(\"fileSizeAcc\");\n+        spark.sparkContext().register(invalidRows, \"InvalidRowsAccumulator\");\n+    }\n+\n+    private Dataset<Row> processRDDAggAndRepartition(Dataset<Row> dataframe, EtlJobConfig.EtlIndex currentIndexMeta) throws UserException {\n+        final boolean isDuplicateTable = !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"AGGREGATE\")\n+                && !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"UNIQUE\");\n+\n+        // 1 make metadata for map/reduce\n+        int keyLen = 0;\n+        for (EtlJobConfig.EtlColumn etlColumn : currentIndexMeta.columns) {\n+            keyLen = etlColumn.isKey ? keyLen + 1 : keyLen;\n+        }\n+\n+        SparkRDDAggregator[] sparkRDDAggregators = new SparkRDDAggregator[currentIndexMeta.columns.size() - keyLen];\n+\n+        for (int i = 0 ; i < currentIndexMeta.columns.size(); i++) {\n+            if (!currentIndexMeta.columns.get(i).isKey && !isDuplicateTable) {\n+                sparkRDDAggregators[i - keyLen] = SparkRDDAggregator.buildAggregator(currentIndexMeta.columns.get(i));\n+            }\n+        }\n+\n+        PairFunction<Row, List<Object>, Object[]> encodePairFunction = isDuplicateTable ?\n+                // add 1 to include bucketId\n+                new EncodeDuplicateTableFunction(keyLen + 1, currentIndexMeta.columns.size() - keyLen)\n+                : new EncodeAggregateTableFunction(sparkRDDAggregators, keyLen + 1);\n+\n+        // 2 convert dataframe to rdd\n+        // TODO(wb) use rdd to avoid bitamp/hll serialize\n+        JavaPairRDD<List<Object>, Object[]> currentRollupRDD = dataframe.toJavaRDD().mapToPair(encodePairFunction);\n+\n+        // 3 do aggregate\n+        // TODO(wb) set the reduce concurrency by statistic instead of hard code 200\n+        int aggregateConcurrency = 200;\n+        JavaPairRDD<List<Object>, Object[]> reduceResultRDD = isDuplicateTable ? currentRollupRDD\n+                : currentRollupRDD.reduceByKey(new AggregateReduceFunction(sparkRDDAggregators), aggregateConcurrency);\n+\n+        // 4 repartition\n+        JavaRDD<Row> finalRDD = reduceResultRDD\n+                .repartitionAndSortWithinPartitions(new BucketPartitioner(bucketKeyMap), new BucketComparator())\n+                .map(record -> {\n+                    List<Object> keys = record._1;\n+                    Object[] values = record._2;\n+                    int size = keys.size() + values.length;\n+                    Object[] result = new Object[size];\n+\n+                    for (int i = 0; i < keys.size(); i++) {\n+                        result[i] = keys.get(i);\n+                    }\n+\n+                    for (int i = keys.size(); i < size; i++) {\n+                        int valueIdx = i - keys.size();\n+                        result[i] = isDuplicateTable ? values[valueIdx] : sparkRDDAggregators[valueIdx].finalize(values[valueIdx]);\n+                    }\n+\n+                    return RowFactory.create(result);\n+                });\n+\n+        // 4 convert to dataframe\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(currentIndexMeta.columns, true, true);\n+        dataframe = spark.createDataFrame(finalRDD, tableSchemaWithBucketId);\n+        return dataframe;\n+\n+    }\n+\n+    // write data to parquet file by using writing the parquet scheme of spark.\n+    private void writePartitionedAndSortedDataframeToParquet(Dataset<Row> dataframe,\n+                                                             String pathPattern,\n+                                                             long tableId,\n+                                                             EtlJobConfig.EtlIndex indexMeta) throws UserException {\n+        StructType outputSchema = dataframe.schema();\n+        StructType dstSchema = DataTypes.createStructType(\n+                Arrays.asList(outputSchema.fields()).stream()\n+                        .filter(field -> !field.name().equalsIgnoreCase(DppUtils.BUCKET_ID))\n+                        .collect(Collectors.toList()));\n+        ExpressionEncoder encoder = RowEncoder.apply(dstSchema);\n+        dataframe.foreachPartition(new ForeachPartitionFunction<Row>() {\n+            @Override\n+            public void call(Iterator<Row> t) throws Exception {\n+                // write the data to dst file\n+                Configuration conf = new Configuration();\n+                FileSystem fs = FileSystem.get(URI.create(etlJobConfig.outputPath), conf);\n+                String lastBucketKey = null;\n+                ParquetWriter<InternalRow> parquetWriter = null;\n+                TaskContext taskContext = TaskContext.get();\n+                long taskAttemptId = taskContext.taskAttemptId();\n+                String dstPath = \"\";\n+                String tmpPath = \"\";\n+\n+                while (t.hasNext()) {\n+                    Row row = t.next();\n+                    if (row.length() <= 1) {\n+                        LOG.warn(\"invalid row:\" + row);\n+                        continue;\n+                    }\n+\n+\n+                    String curBucketKey = row.getString(0);\n+                    List<Object> columnObjects = new ArrayList<>();\n+                    for (int i = 1; i < row.length(); ++i) {\n+                        Object columnValue = row.get(i);\n+                        columnObjects.add(columnValue);\n+                    }\n+                    System.out.println();\n+                    Row rowWithoutBucketKey = RowFactory.create(columnObjects.toArray());\n+                    // if the bucket key is new, it will belong to a new tablet\n+                    if (lastBucketKey == null || !curBucketKey.equals(lastBucketKey)) {\n+                        if (parquetWriter != null) {\n+                            parquetWriter.close();\n+                            // rename tmpPath to path\n+                            try {\n+                                fs.rename(new Path(tmpPath), new Path(dstPath));\n+                            } catch (IOException ioe) {\n+                                LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                                throw ioe;\n+                            }\n+                        }\n+                        // flush current writer and create a new writer\n+                        String[] bucketKey = curBucketKey.split(\"_\");\n+                        if (bucketKey.length != 2) {\n+                            LOG.warn(\"invalid bucket key:\" + curBucketKey);\n+                            continue;\n+                        }\n+                        int partitionId = Integer.parseInt(bucketKey[0]);\n+                        int bucketId = Integer.parseInt(bucketKey[1]);\n+                        dstPath = String.format(pathPattern, tableId, partitionId, indexMeta.indexId,\n+                                bucketId, indexMeta.schemaHash);\n+                        tmpPath = dstPath + \".\" + taskAttemptId;\n+                        conf.setBoolean(\"spark.sql.parquet.writeLegacyFormat\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int64AsTimestampMillis\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int96AsTimestamp\", true);\n+                        conf.setBoolean(\"spark.sql.parquet.binaryAsString\", false);\n+                        conf.set(\"spark.sql.parquet.outputTimestampType\", \"INT96\");\n+                        ParquetWriteSupport.setSchema(dstSchema, conf);\n+                        ParquetWriteSupport parquetWriteSupport = new ParquetWriteSupport();\n+                        parquetWriter = new ParquetWriter<InternalRow>(new Path(tmpPath), parquetWriteSupport,\n+                                CompressionCodecName.SNAPPY, 256 * 1024 * 1024, 16 * 1024, 1024 * 1024,\n+                                true, false,\n+                                ParquetProperties.WriterVersion.PARQUET_1_0, conf);\n+                        if (parquetWriter != null) {\n+                            LOG.info(\"[HdfsOperate]>> initialize writer succeed! path:\" + tmpPath);\n+                        }\n+                        lastBucketKey = curBucketKey;\n+                    }\n+                    InternalRow internalRow = encoder.toRow(rowWithoutBucketKey);\n+                    parquetWriter.write(internalRow);\n+                }\n+                if (parquetWriter != null) {\n+                    parquetWriter.close();\n+                    try {\n+                        fs.rename(new Path(tmpPath), new Path(dstPath));\n+                    } catch (IOException ioe) {\n+                        LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                        throw ioe;\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    private void processRollupTree(RollupTreeNode rootNode,\n+                                   Dataset<Row> rootDataframe,\n+                                   long tableId, EtlJobConfig.EtlTable tableMeta,\n+                                   EtlJobConfig.EtlIndex baseIndex) throws UserException {\n+        Queue<RollupTreeNode> nodeQueue = new LinkedList<>();\n+        nodeQueue.offer(rootNode);\n+        int currentLevel = 0;\n+        // level travel the tree\n+        Map<Long, Dataset<Row>> parentDataframeMap = new HashMap<>();\n+        parentDataframeMap.put(baseIndex.indexId, rootDataframe);\n+        Map<Long, Dataset<Row>> childrenDataframeMap = new HashMap<>();\n+        String pathPattern = etlJobConfig.outputPath + \"/\" + etlJobConfig.outputFilePattern;\n+        while (!nodeQueue.isEmpty()) {\n+            RollupTreeNode curNode = nodeQueue.poll();\n+            LOG.info(\"start to process index:\" + curNode.indexId);\n+            if (curNode.children != null) {\n+                for (RollupTreeNode child : curNode.children) {\n+                    nodeQueue.offer(child);\n+                }\n+            }\n+            Dataset<Row> curDataFrame = null;\n+            // column select for rollup\n+            if (curNode.level != currentLevel) {\n+                for (Dataset<Row> dataframe : parentDataframeMap.values()) {\n+                    dataframe.unpersist();\n+                }\n+                currentLevel = curNode.level;\n+                parentDataframeMap.clear();\n+                parentDataframeMap = childrenDataframeMap;\n+                childrenDataframeMap = new HashMap<>();\n+            }\n+\n+            long parentIndexId = baseIndex.indexId;\n+            if (curNode.parent != null) {\n+                parentIndexId = curNode.parent.indexId;\n+            }\n+\n+            Dataset<Row> parentDataframe = parentDataframeMap.get(parentIndexId);\n+            List<Column> columns = new ArrayList<>();\n+            List<Column> keyColumns = new ArrayList<>();\n+            Column bucketIdColumn = new Column(DppUtils.BUCKET_ID);\n+            keyColumns.add(bucketIdColumn);\n+            columns.add(bucketIdColumn);\n+            for (String keyName : curNode.keyColumnNames) {\n+                columns.add(new Column(keyName));\n+                keyColumns.add(new Column(keyName));\n+            }\n+            for (String valueName : curNode.valueColumnNames) {\n+                columns.add(new Column(valueName));\n+            }\n+            Seq<Column> columnSeq = JavaConverters.asScalaIteratorConverter(columns.iterator()).asScala().toSeq();\n+            curDataFrame = parentDataframe.select(columnSeq);\n+            // aggregate and repartition\n+            curDataFrame = processRDDAggAndRepartition(curDataFrame, curNode.indexMeta);\n+\n+            childrenDataframeMap.put(curNode.indexId, curDataFrame);\n+\n+            if (curNode.children != null && curNode.children.size() > 1) {\n+                // if the children number larger than 1, persist the dataframe for performance\n+                curDataFrame.persist();\n+            }\n+            writePartitionedAndSortedDataframeToParquet(curDataFrame, pathPattern, tableId, curNode.indexMeta);\n+        }\n+    }\n+\n+    // repartition dataframe by partitionid_bucketid\n+    // so data in the same bucket will be consecutive.\n+    private Dataset<Row> repartitionDataframeByBucketId(SparkSession spark, Dataset<Row> dataframe,\n+                                                        EtlJobConfig.EtlPartitionInfo partitionInfo,\n+                                                        List<Integer> partitionKeyIndex,\n+                                                        List<Class> partitionKeySchema,\n+                                                        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys,\n+                                                        List<String> keyColumnNames,\n+                                                        List<String> valueColumnNames,\n+                                                        StructType dstTableSchema,\n+                                                        EtlJobConfig.EtlIndex baseIndex,\n+                                                        List<Long> validPartitionIds) throws UserException {\n+        List<String> distributeColumns = partitionInfo.distributionColumnRefs;\n+        Partitioner partitioner = new DorisRangePartitioner(partitionInfo, partitionKeyIndex, partitionRangeKeys);\n+        Set<Integer> validPartitionIndex = new HashSet<>();\n+        if (validPartitionIds == null) {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                validPartitionIndex.add(i);\n+            }\n+        } else {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                if (validPartitionIds.contains(partitionInfo.partitions.get(i).partitionId)) {\n+                    validPartitionIndex.add(i);\n+                }\n+            }\n+        }\n+        // use PairFlatMapFunction instead of PairMapFunction because the there will be\n+        // 0 or 1 output row for 1 input row\n+        JavaPairRDD<String, DppColumns> pairRDD = dataframe.javaRDD().flatMapToPair(\n+                new PairFlatMapFunction<Row, String, DppColumns>() {\n+                    @Override\n+                    public Iterator<Tuple2<String, DppColumns>> call(Row row) {\n+                        List<Object> columns = new ArrayList<>();\n+                        List<Object> keyColumns = new ArrayList<>();\n+                        for (String columnName : keyColumnNames) {\n+                            Object columnObject = row.get(row.fieldIndex(columnName));\n+                            columns.add(columnObject);\n+                            keyColumns.add(columnObject);\n+                        }\n+\n+                        for (String columnName : valueColumnNames) {\n+                            columns.add(row.get(row.fieldIndex(columnName)));\n+                        }\n+                        DppColumns dppColumns = new DppColumns(columns);\n+                        DppColumns key = new DppColumns(keyColumns);\n+                        List<Tuple2<String, DppColumns>> result = new ArrayList<>();\n+                        int pid = partitioner.getPartition(key);\n+                        if (!validPartitionIndex.contains(pid)) {\n+                            LOG.warn(\"invalid partition for row:\" + row + \", pid:\" + pid);\n+                            abnormalRowAcc.add(1);\n+                            LOG.info(\"abnormalRowAcc:\" + abnormalRowAcc);\n+                            if (abnormalRowAcc.value() < 5) {\n+                                LOG.info(\"add row to invalidRows:\" + row.toString());\n+                                invalidRows.add(row.toString());\n+                                LOG.info(\"invalid rows contents:\" + invalidRows.value());\n+                            }\n+                        } else {\n+                            long hashValue = DppUtils.getHashValue(row, distributeColumns, dstTableSchema);\n+                            int bucketId = (int) ((hashValue & 0xffffffff) % partitionInfo.partitions.get(pid).bucketNum);\n+                            long partitionId = partitionInfo.partitions.get(pid).partitionId;\n+                            // bucketKey is partitionId_bucketId\n+                            String bucketKey = partitionId + \"_\" + bucketId;\n+                            Tuple2<String, DppColumns> newTuple = new Tuple2<String, DppColumns>(bucketKey, dppColumns);\n+                            result.add(newTuple);\n+                        }\n+                        return result.iterator();\n+                    }\n+                });\n+\n+        JavaRDD<Row> resultRdd = pairRDD.map(record -> {\n+                    String bucketKey = record._1;\n+                    List<Object> row = new ArrayList<>();\n+                    // bucketKey as the first key\n+                    row.add(bucketKey);\n+                    row.addAll(record._2.columns);\n+                    return RowFactory.create(row.toArray());\n+                }\n+        );\n+\n+        // TODO(wb): using rdd instead of dataframe from here\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(baseIndex.columns, true, false);\n+        dataframe = spark.createDataFrame(resultRdd, tableSchemaWithBucketId);\n+        // use bucket number as the parallel number\n+        int reduceNum = 0;\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            for (int i = 0; i < partition.bucketNum; i++) {\n+                bucketKeyMap.put(partition.partitionId + \"_\" + i, reduceNum);\n+                reduceNum++;\n+            }\n+        }\n+\n+        // print to system.out for easy to find log info\n+        System.out.println(\"print bucket key map:\" + bucketKeyMap.toString());\n+\n+        return dataframe;\n+    }\n+\n+    // do the etl process\n+    private Dataset<Row> convertSrcDataframeToDstDataframe(EtlJobConfig.EtlIndex baseIndex,\n+                                                           Dataset<Row> srcDataframe,\n+                                                           StructType dstTableSchema,\n+                                                           EtlJobConfig.EtlFileGroup fileGroup) throws UserException {\n+        Dataset<Row> dataframe = srcDataframe;\n+        StructType srcSchema = dataframe.schema();\n+        Set<String> srcColumnNames = new HashSet<>();\n+        for (StructField field : srcSchema.fields()) {\n+            srcColumnNames.add(field.name());\n+        }\n+        Map<String, EtlJobConfig.EtlColumnMapping> columnMappings = fileGroup.columnMappings;\n+        // 1. process simple columns\n+        Set<String> mappingColumns = null;\n+        if (columnMappings != null) {\n+            mappingColumns = columnMappings.keySet();\n+        }\n+        List<String> dstColumnNames = new ArrayList<>();\n+        for (StructField dstField : dstTableSchema.fields()) {\n+            dstColumnNames.add(dstField.name());\n+            EtlJobConfig.EtlColumn column = baseIndex.getColumn(dstField.name());\n+            if (!srcColumnNames.contains(dstField.name())) {\n+                if (mappingColumns != null && mappingColumns.contains(dstField.name())) {\n+                    // mapping columns will be processed in next step\n+                    continue;\n+                }\n+                if (column.defaultValue != null) {\n+                    if (column.defaultValue.equals(NULL_FLAG)) {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                    } else {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(column.defaultValue));\n+                    }\n+                } else if (column.isAllowNull) {\n+                    dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                } else {\n+                    throw new UserException(\"Reason: no data for column:\" + dstField.name());\n+                }\n+            }\n+            if (column.columnType.equalsIgnoreCase(\"DATE\")) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(\"date\"));\n+            } else if (column.columnType.equalsIgnoreCase(\"BOOLEAN\")) {\n+                dataframe = dataframe.withColumn(dstField.name(),\n+                        functions.when(dataframe.col(dstField.name()).equalTo(\"true\"), \"1\")\n+                                .otherwise(\"0\"));\n+            } else if (!column.columnType.equalsIgnoreCase(BITMAP_TYPE) && !dstField.dataType().equals(DataTypes.StringType)) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(dstField.dataType()));\n+            }\n+            if (fileGroup.isNegative && !column.isKey) {\n+                // negative load\n+                // value will be convert te -1 * value\n+                dataframe = dataframe.withColumn(dstField.name(), functions.expr(\"-1 *\" + dstField.name()));\n+            }\n+        }\n+        // 2. process the mapping columns\n+        for (String mappingColumn : mappingColumns) {\n+            String mappingDescription = columnMappings.get(mappingColumn).toDescription();\n+            if (mappingDescription.toLowerCase().contains(\"hll_hash\")) {\n+                continue;\n+            }\n+            // here should cast data type to dst column type\n+            dataframe = dataframe.withColumn(mappingColumn,\n+                    functions.expr(mappingDescription).cast(dstTableSchema.apply(mappingColumn).dataType()));\n+        }\n+        // projection and reorder the columns\n+        dataframe.createOrReplaceTempView(\"src_table\");\n+        StringBuilder selectSqlBuilder = new StringBuilder();\n+        selectSqlBuilder.append(\"select \");\n+        for (String name : dstColumnNames) {\n+            selectSqlBuilder.append(name + \",\");\n+        }\n+        selectSqlBuilder.deleteCharAt(selectSqlBuilder.length() - 1);\n+        selectSqlBuilder.append(\" from src_table\");\n+        String selectSql = selectSqlBuilder.toString();\n+        dataframe = spark.sql(selectSql);\n+        return dataframe;\n+    }\n+\n+    private Dataset<Row> loadDataFromPath(SparkSession spark,\n+                                          EtlJobConfig.EtlFileGroup fileGroup,\n+                                          String fileUrl,\n+                                          EtlJobConfig.EtlIndex baseIndex,\n+                                          List<EtlJobConfig.EtlColumn> columns) throws UserException {\n+        List<String> columnValueFromPath = DppUtils.parseColumnsFromPath(fileUrl, fileGroup.columnsFromPath);\n+        List<String> dataSrcColumns = fileGroup.fileFieldNames;\n+        if (dataSrcColumns == null) {\n+            // if there is no source columns info\n+            // use base index columns as source columns\n+            dataSrcColumns = new ArrayList<>();\n+            for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+                dataSrcColumns.add(column.columnName);\n+            }\n+        }\n+        List<String> dstTableNames = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            dstTableNames.add(column.columnName);\n+        }\n+        List<String> srcColumnsWithColumnsFromPath = new ArrayList<>();\n+        srcColumnsWithColumnsFromPath.addAll(dataSrcColumns);\n+        if (fileGroup.columnsFromPath != null) {\n+            srcColumnsWithColumnsFromPath.addAll(fileGroup.columnsFromPath);\n+        }\n+        StructType srcSchema = createScrSchema(srcColumnsWithColumnsFromPath);\n+        JavaRDD<String> sourceDataRdd = spark.read().textFile(fileUrl).toJavaRDD();\n+        int columnSize = dataSrcColumns.size();\n+        List<ColumnParser> parsers = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            parsers.add(ColumnParser.create(column.columnType));\n+        }\n+        // now we first support csv file\n+        // TODO: support parquet file and orc file\n+        JavaRDD<Row> rowRDD = sourceDataRdd.flatMap(\n+                record -> {\n+                    scannedRowsAcc.add(1);\n+                    String[] attributes = record.split(fileGroup.columnSeparator);\n+                    List<Row> result = new ArrayList<>();\n+                    boolean validRow = true;\n+                    if (attributes.length != columnSize) {\n+                        LOG.warn(\"invalid src schema, data columns:\"\n+                                + attributes.length + \", file group columns:\"\n+                                + columnSize + \", row:\" + record);\n+                        validRow = false;\n+                    } else {\n+                        for (int i = 0; i < attributes.length; ++i) {\n+                            if (attributes[i].equals(NULL_FLAG)) {\n+                                if (baseIndex.columns.get(i).isAllowNull) {\n+                                    attributes[i] = null;\n+                                } else {\n+                                    LOG.warn(\"colunm:\" + i + \" can not be null. row:\" + record);\n+                                    validRow = false;\n+                                    break;\n+                                }\n+                            }\n+                            boolean isStrictMode = (boolean) etlJobConfig.properties.strictMode;\n+                            if (isStrictMode) {\n+                                StructField field = srcSchema.apply(i);\n+                                if (dstTableNames.contains(field.name())) {\n+                                    String type = columns.get(i).columnType;\n+                                    if (type.equalsIgnoreCase(\"CHAR\")\n+                                            || type.equalsIgnoreCase(\"VARCHAR\")) {\n+                                        continue;\n+                                    }\n+                                    ColumnParser parser = parsers.get(i);\n+                                    boolean valid = parser.parse(attributes[i]);\n+                                    if (!valid) {\n+                                        validRow = false;\n+                                        LOG.warn(\"invalid row:\" + record\n+                                                + \", attribute \" + i + \": \" + attributes[i] + \" parsed failed\");\n+                                        break;\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                    if (validRow) {\n+                        Row row = null;\n+                        if (fileGroup.columnsFromPath == null) {\n+                            row = RowFactory.create(attributes);\n+                        } else {\n+                            // process columns from path\n+                            // append columns from path to the tail\n+                            List<String> columnAttributes = new ArrayList<>();\n+                            columnAttributes.addAll(Arrays.asList(attributes));\n+                            columnAttributes.addAll(columnValueFromPath);\n+                            row = RowFactory.create(columnAttributes.toArray());\n+                        }\n+                        result.add(row);\n+                    } else {\n+                        abnormalRowAcc.add(1);\n+                        // at most add 5 rows to invalidRows\n+                        if (abnormalRowAcc.value() <= 5) {\n+                            invalidRows.add(record);\n+                        }\n+                    }\n+                    return result.iterator();\n+                }\n+        );\n+\n+        Dataset<Row> dataframe = spark.createDataFrame(rowRDD, srcSchema);\n+        return dataframe;\n+    }\n+\n+    private StructType createScrSchema(List<String> srcColumns) {\n+        List<StructField> fields = new ArrayList<>();\n+        for (String srcColumn : srcColumns) {\n+            // user StringType to load source data\n+            StructField field = DataTypes.createStructField(srcColumn, DataTypes.StringType, true);\n+            fields.add(field);\n+        }\n+        StructType srcSchema = DataTypes.createStructType(fields);\n+        return srcSchema;\n+    }\n+\n+    // partition keys will be parsed into double from json\n+    // so need to convert it to partition columns' type\n+    private Object convertPartitionKey(Object srcValue, Class dstClass) throws UserException {\n+        if (dstClass.equals(Float.class) || dstClass.equals(Double.class)) {\n+            return null;\n+        }\n+        if (srcValue instanceof Double) {\n+            if (dstClass.equals(Short.class)) {\n+                return ((Double) srcValue).shortValue();\n+            } else if (dstClass.equals(Integer.class)) {\n+                return ((Double) srcValue).intValue();\n+            } else if (dstClass.equals(Long.class)) {\n+                return ((Double) srcValue).longValue();\n+            } else if (dstClass.equals(BigInteger.class)) {\n+                return new BigInteger(((Double) srcValue).toString());\n+            } else {\n+                // dst type is string\n+                return srcValue.toString();\n+            }\n+        } else {\n+            LOG.warn(\"unsupport partition key:\" + srcValue);\n+            throw new UserException(\"unsupport partition key:\" + srcValue);\n+        }\n+    }\n+\n+    private List<DorisRangePartitioner.PartitionRangeKey> createPartitionRangeKeys(\n+            EtlJobConfig.EtlPartitionInfo partitionInfo, List<Class> partitionKeySchema) throws UserException {\n+        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys = new ArrayList<>();\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            DorisRangePartitioner.PartitionRangeKey partitionRangeKey = new DorisRangePartitioner.PartitionRangeKey();\n+            List<Object> startKeyColumns = new ArrayList<>();\n+            for (int i = 0; i < partition.startKeys.size(); i++) {\n+                Object value = partition.startKeys.get(i);\n+                startKeyColumns.add(convertPartitionKey(value, partitionKeySchema.get(i)));\n+            }\n+            partitionRangeKey.startKeys = new DppColumns(startKeyColumns);\n+            ;", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc3MjAzOQ==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435772039", "bodyText": "\ud83d\udc4c", "author": "wangbo", "createdAt": "2020-06-05T08:34:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0Nzc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0ODA3Nw==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435248077", "bodyText": "isMaxPartition = true;\nxuyang already fixed, maybe the code is not up to date", "author": "wyb", "createdAt": "2020-06-04T13:22:04Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -0,0 +1,851 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import com.google.common.base.Strings;\n+import com.google.gson.Gson;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.function.ForeachPartitionFunction;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.functions;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.TaskContext;\n+\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+\n+import java.io.IOException;\n+import java.math.BigInteger;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import scala.collection.JavaConverters;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+// This class is a Spark-based data preprocessing program,\n+// which will make use of the distributed compute framework of spark to\n+// do ETL job/sort/preaggregate jobs in spark job\n+// to boost the process of large amount of data load.\n+// the process steps are as following:\n+// 1. load data\n+//     1.1 load data from path/hive table\n+//     1.2 do the etl process\n+// 2. repartition data by using doris data model(partition and bucket)\n+// 3. process aggregation if needed\n+// 4. write data to parquet file\n+public final class SparkDpp implements java.io.Serializable {\n+    private static final Logger LOG = LogManager.getLogger(SparkDpp.class);\n+\n+    private static final String NULL_FLAG = \"\\\\N\";\n+    private static final String DPP_RESULT_FILE = \"dpp_result.json\";\n+    private static final String BITMAP_TYPE = \"bitmap\";\n+    private SparkSession spark = null;\n+    private EtlJobConfig etlJobConfig = null;\n+    private LongAccumulator abnormalRowAcc = null;\n+    private LongAccumulator unselectedRowAcc = null;\n+    private LongAccumulator scannedRowsAcc = null;\n+    private LongAccumulator fileNumberAcc = null;\n+    private LongAccumulator fileSizeAcc = null;\n+    private Map<String, Integer> bucketKeyMap = new HashMap<>();\n+    // accumulator to collect invalid rows\n+    private StringAccumulator invalidRows = new StringAccumulator();\n+\n+    public SparkDpp(SparkSession spark, EtlJobConfig etlJobConfig) {\n+        this.spark = spark;\n+        this.etlJobConfig = etlJobConfig;\n+    }\n+\n+    public void init() {\n+        abnormalRowAcc = spark.sparkContext().longAccumulator(\"abnormalRowAcc\");\n+        unselectedRowAcc = spark.sparkContext().longAccumulator(\"unselectedRowAcc\");\n+        scannedRowsAcc = spark.sparkContext().longAccumulator(\"scannedRowsAcc\");\n+        fileNumberAcc = spark.sparkContext().longAccumulator(\"fileNumberAcc\");\n+        fileSizeAcc = spark.sparkContext().longAccumulator(\"fileSizeAcc\");\n+        spark.sparkContext().register(invalidRows, \"InvalidRowsAccumulator\");\n+    }\n+\n+    private Dataset<Row> processRDDAggAndRepartition(Dataset<Row> dataframe, EtlJobConfig.EtlIndex currentIndexMeta) throws UserException {\n+        final boolean isDuplicateTable = !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"AGGREGATE\")\n+                && !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"UNIQUE\");\n+\n+        // 1 make metadata for map/reduce\n+        int keyLen = 0;\n+        for (EtlJobConfig.EtlColumn etlColumn : currentIndexMeta.columns) {\n+            keyLen = etlColumn.isKey ? keyLen + 1 : keyLen;\n+        }\n+\n+        SparkRDDAggregator[] sparkRDDAggregators = new SparkRDDAggregator[currentIndexMeta.columns.size() - keyLen];\n+\n+        for (int i = 0 ; i < currentIndexMeta.columns.size(); i++) {\n+            if (!currentIndexMeta.columns.get(i).isKey && !isDuplicateTable) {\n+                sparkRDDAggregators[i - keyLen] = SparkRDDAggregator.buildAggregator(currentIndexMeta.columns.get(i));\n+            }\n+        }\n+\n+        PairFunction<Row, List<Object>, Object[]> encodePairFunction = isDuplicateTable ?\n+                // add 1 to include bucketId\n+                new EncodeDuplicateTableFunction(keyLen + 1, currentIndexMeta.columns.size() - keyLen)\n+                : new EncodeAggregateTableFunction(sparkRDDAggregators, keyLen + 1);\n+\n+        // 2 convert dataframe to rdd\n+        // TODO(wb) use rdd to avoid bitamp/hll serialize\n+        JavaPairRDD<List<Object>, Object[]> currentRollupRDD = dataframe.toJavaRDD().mapToPair(encodePairFunction);\n+\n+        // 3 do aggregate\n+        // TODO(wb) set the reduce concurrency by statistic instead of hard code 200\n+        int aggregateConcurrency = 200;\n+        JavaPairRDD<List<Object>, Object[]> reduceResultRDD = isDuplicateTable ? currentRollupRDD\n+                : currentRollupRDD.reduceByKey(new AggregateReduceFunction(sparkRDDAggregators), aggregateConcurrency);\n+\n+        // 4 repartition\n+        JavaRDD<Row> finalRDD = reduceResultRDD\n+                .repartitionAndSortWithinPartitions(new BucketPartitioner(bucketKeyMap), new BucketComparator())\n+                .map(record -> {\n+                    List<Object> keys = record._1;\n+                    Object[] values = record._2;\n+                    int size = keys.size() + values.length;\n+                    Object[] result = new Object[size];\n+\n+                    for (int i = 0; i < keys.size(); i++) {\n+                        result[i] = keys.get(i);\n+                    }\n+\n+                    for (int i = keys.size(); i < size; i++) {\n+                        int valueIdx = i - keys.size();\n+                        result[i] = isDuplicateTable ? values[valueIdx] : sparkRDDAggregators[valueIdx].finalize(values[valueIdx]);\n+                    }\n+\n+                    return RowFactory.create(result);\n+                });\n+\n+        // 4 convert to dataframe\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(currentIndexMeta.columns, true, true);\n+        dataframe = spark.createDataFrame(finalRDD, tableSchemaWithBucketId);\n+        return dataframe;\n+\n+    }\n+\n+    // write data to parquet file by using writing the parquet scheme of spark.\n+    private void writePartitionedAndSortedDataframeToParquet(Dataset<Row> dataframe,\n+                                                             String pathPattern,\n+                                                             long tableId,\n+                                                             EtlJobConfig.EtlIndex indexMeta) throws UserException {\n+        StructType outputSchema = dataframe.schema();\n+        StructType dstSchema = DataTypes.createStructType(\n+                Arrays.asList(outputSchema.fields()).stream()\n+                        .filter(field -> !field.name().equalsIgnoreCase(DppUtils.BUCKET_ID))\n+                        .collect(Collectors.toList()));\n+        ExpressionEncoder encoder = RowEncoder.apply(dstSchema);\n+        dataframe.foreachPartition(new ForeachPartitionFunction<Row>() {\n+            @Override\n+            public void call(Iterator<Row> t) throws Exception {\n+                // write the data to dst file\n+                Configuration conf = new Configuration();\n+                FileSystem fs = FileSystem.get(URI.create(etlJobConfig.outputPath), conf);\n+                String lastBucketKey = null;\n+                ParquetWriter<InternalRow> parquetWriter = null;\n+                TaskContext taskContext = TaskContext.get();\n+                long taskAttemptId = taskContext.taskAttemptId();\n+                String dstPath = \"\";\n+                String tmpPath = \"\";\n+\n+                while (t.hasNext()) {\n+                    Row row = t.next();\n+                    if (row.length() <= 1) {\n+                        LOG.warn(\"invalid row:\" + row);\n+                        continue;\n+                    }\n+\n+\n+                    String curBucketKey = row.getString(0);\n+                    List<Object> columnObjects = new ArrayList<>();\n+                    for (int i = 1; i < row.length(); ++i) {\n+                        Object columnValue = row.get(i);\n+                        columnObjects.add(columnValue);\n+                    }\n+                    System.out.println();\n+                    Row rowWithoutBucketKey = RowFactory.create(columnObjects.toArray());\n+                    // if the bucket key is new, it will belong to a new tablet\n+                    if (lastBucketKey == null || !curBucketKey.equals(lastBucketKey)) {\n+                        if (parquetWriter != null) {\n+                            parquetWriter.close();\n+                            // rename tmpPath to path\n+                            try {\n+                                fs.rename(new Path(tmpPath), new Path(dstPath));\n+                            } catch (IOException ioe) {\n+                                LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                                throw ioe;\n+                            }\n+                        }\n+                        // flush current writer and create a new writer\n+                        String[] bucketKey = curBucketKey.split(\"_\");\n+                        if (bucketKey.length != 2) {\n+                            LOG.warn(\"invalid bucket key:\" + curBucketKey);\n+                            continue;\n+                        }\n+                        int partitionId = Integer.parseInt(bucketKey[0]);\n+                        int bucketId = Integer.parseInt(bucketKey[1]);\n+                        dstPath = String.format(pathPattern, tableId, partitionId, indexMeta.indexId,\n+                                bucketId, indexMeta.schemaHash);\n+                        tmpPath = dstPath + \".\" + taskAttemptId;\n+                        conf.setBoolean(\"spark.sql.parquet.writeLegacyFormat\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int64AsTimestampMillis\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int96AsTimestamp\", true);\n+                        conf.setBoolean(\"spark.sql.parquet.binaryAsString\", false);\n+                        conf.set(\"spark.sql.parquet.outputTimestampType\", \"INT96\");\n+                        ParquetWriteSupport.setSchema(dstSchema, conf);\n+                        ParquetWriteSupport parquetWriteSupport = new ParquetWriteSupport();\n+                        parquetWriter = new ParquetWriter<InternalRow>(new Path(tmpPath), parquetWriteSupport,\n+                                CompressionCodecName.SNAPPY, 256 * 1024 * 1024, 16 * 1024, 1024 * 1024,\n+                                true, false,\n+                                ParquetProperties.WriterVersion.PARQUET_1_0, conf);\n+                        if (parquetWriter != null) {\n+                            LOG.info(\"[HdfsOperate]>> initialize writer succeed! path:\" + tmpPath);\n+                        }\n+                        lastBucketKey = curBucketKey;\n+                    }\n+                    InternalRow internalRow = encoder.toRow(rowWithoutBucketKey);\n+                    parquetWriter.write(internalRow);\n+                }\n+                if (parquetWriter != null) {\n+                    parquetWriter.close();\n+                    try {\n+                        fs.rename(new Path(tmpPath), new Path(dstPath));\n+                    } catch (IOException ioe) {\n+                        LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                        throw ioe;\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    private void processRollupTree(RollupTreeNode rootNode,\n+                                   Dataset<Row> rootDataframe,\n+                                   long tableId, EtlJobConfig.EtlTable tableMeta,\n+                                   EtlJobConfig.EtlIndex baseIndex) throws UserException {\n+        Queue<RollupTreeNode> nodeQueue = new LinkedList<>();\n+        nodeQueue.offer(rootNode);\n+        int currentLevel = 0;\n+        // level travel the tree\n+        Map<Long, Dataset<Row>> parentDataframeMap = new HashMap<>();\n+        parentDataframeMap.put(baseIndex.indexId, rootDataframe);\n+        Map<Long, Dataset<Row>> childrenDataframeMap = new HashMap<>();\n+        String pathPattern = etlJobConfig.outputPath + \"/\" + etlJobConfig.outputFilePattern;\n+        while (!nodeQueue.isEmpty()) {\n+            RollupTreeNode curNode = nodeQueue.poll();\n+            LOG.info(\"start to process index:\" + curNode.indexId);\n+            if (curNode.children != null) {\n+                for (RollupTreeNode child : curNode.children) {\n+                    nodeQueue.offer(child);\n+                }\n+            }\n+            Dataset<Row> curDataFrame = null;\n+            // column select for rollup\n+            if (curNode.level != currentLevel) {\n+                for (Dataset<Row> dataframe : parentDataframeMap.values()) {\n+                    dataframe.unpersist();\n+                }\n+                currentLevel = curNode.level;\n+                parentDataframeMap.clear();\n+                parentDataframeMap = childrenDataframeMap;\n+                childrenDataframeMap = new HashMap<>();\n+            }\n+\n+            long parentIndexId = baseIndex.indexId;\n+            if (curNode.parent != null) {\n+                parentIndexId = curNode.parent.indexId;\n+            }\n+\n+            Dataset<Row> parentDataframe = parentDataframeMap.get(parentIndexId);\n+            List<Column> columns = new ArrayList<>();\n+            List<Column> keyColumns = new ArrayList<>();\n+            Column bucketIdColumn = new Column(DppUtils.BUCKET_ID);\n+            keyColumns.add(bucketIdColumn);\n+            columns.add(bucketIdColumn);\n+            for (String keyName : curNode.keyColumnNames) {\n+                columns.add(new Column(keyName));\n+                keyColumns.add(new Column(keyName));\n+            }\n+            for (String valueName : curNode.valueColumnNames) {\n+                columns.add(new Column(valueName));\n+            }\n+            Seq<Column> columnSeq = JavaConverters.asScalaIteratorConverter(columns.iterator()).asScala().toSeq();\n+            curDataFrame = parentDataframe.select(columnSeq);\n+            // aggregate and repartition\n+            curDataFrame = processRDDAggAndRepartition(curDataFrame, curNode.indexMeta);\n+\n+            childrenDataframeMap.put(curNode.indexId, curDataFrame);\n+\n+            if (curNode.children != null && curNode.children.size() > 1) {\n+                // if the children number larger than 1, persist the dataframe for performance\n+                curDataFrame.persist();\n+            }\n+            writePartitionedAndSortedDataframeToParquet(curDataFrame, pathPattern, tableId, curNode.indexMeta);\n+        }\n+    }\n+\n+    // repartition dataframe by partitionid_bucketid\n+    // so data in the same bucket will be consecutive.\n+    private Dataset<Row> repartitionDataframeByBucketId(SparkSession spark, Dataset<Row> dataframe,\n+                                                        EtlJobConfig.EtlPartitionInfo partitionInfo,\n+                                                        List<Integer> partitionKeyIndex,\n+                                                        List<Class> partitionKeySchema,\n+                                                        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys,\n+                                                        List<String> keyColumnNames,\n+                                                        List<String> valueColumnNames,\n+                                                        StructType dstTableSchema,\n+                                                        EtlJobConfig.EtlIndex baseIndex,\n+                                                        List<Long> validPartitionIds) throws UserException {\n+        List<String> distributeColumns = partitionInfo.distributionColumnRefs;\n+        Partitioner partitioner = new DorisRangePartitioner(partitionInfo, partitionKeyIndex, partitionRangeKeys);\n+        Set<Integer> validPartitionIndex = new HashSet<>();\n+        if (validPartitionIds == null) {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                validPartitionIndex.add(i);\n+            }\n+        } else {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                if (validPartitionIds.contains(partitionInfo.partitions.get(i).partitionId)) {\n+                    validPartitionIndex.add(i);\n+                }\n+            }\n+        }\n+        // use PairFlatMapFunction instead of PairMapFunction because the there will be\n+        // 0 or 1 output row for 1 input row\n+        JavaPairRDD<String, DppColumns> pairRDD = dataframe.javaRDD().flatMapToPair(\n+                new PairFlatMapFunction<Row, String, DppColumns>() {\n+                    @Override\n+                    public Iterator<Tuple2<String, DppColumns>> call(Row row) {\n+                        List<Object> columns = new ArrayList<>();\n+                        List<Object> keyColumns = new ArrayList<>();\n+                        for (String columnName : keyColumnNames) {\n+                            Object columnObject = row.get(row.fieldIndex(columnName));\n+                            columns.add(columnObject);\n+                            keyColumns.add(columnObject);\n+                        }\n+\n+                        for (String columnName : valueColumnNames) {\n+                            columns.add(row.get(row.fieldIndex(columnName)));\n+                        }\n+                        DppColumns dppColumns = new DppColumns(columns);\n+                        DppColumns key = new DppColumns(keyColumns);\n+                        List<Tuple2<String, DppColumns>> result = new ArrayList<>();\n+                        int pid = partitioner.getPartition(key);\n+                        if (!validPartitionIndex.contains(pid)) {\n+                            LOG.warn(\"invalid partition for row:\" + row + \", pid:\" + pid);\n+                            abnormalRowAcc.add(1);\n+                            LOG.info(\"abnormalRowAcc:\" + abnormalRowAcc);\n+                            if (abnormalRowAcc.value() < 5) {\n+                                LOG.info(\"add row to invalidRows:\" + row.toString());\n+                                invalidRows.add(row.toString());\n+                                LOG.info(\"invalid rows contents:\" + invalidRows.value());\n+                            }\n+                        } else {\n+                            long hashValue = DppUtils.getHashValue(row, distributeColumns, dstTableSchema);\n+                            int bucketId = (int) ((hashValue & 0xffffffff) % partitionInfo.partitions.get(pid).bucketNum);\n+                            long partitionId = partitionInfo.partitions.get(pid).partitionId;\n+                            // bucketKey is partitionId_bucketId\n+                            String bucketKey = partitionId + \"_\" + bucketId;\n+                            Tuple2<String, DppColumns> newTuple = new Tuple2<String, DppColumns>(bucketKey, dppColumns);\n+                            result.add(newTuple);\n+                        }\n+                        return result.iterator();\n+                    }\n+                });\n+\n+        JavaRDD<Row> resultRdd = pairRDD.map(record -> {\n+                    String bucketKey = record._1;\n+                    List<Object> row = new ArrayList<>();\n+                    // bucketKey as the first key\n+                    row.add(bucketKey);\n+                    row.addAll(record._2.columns);\n+                    return RowFactory.create(row.toArray());\n+                }\n+        );\n+\n+        // TODO(wb): using rdd instead of dataframe from here\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(baseIndex.columns, true, false);\n+        dataframe = spark.createDataFrame(resultRdd, tableSchemaWithBucketId);\n+        // use bucket number as the parallel number\n+        int reduceNum = 0;\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            for (int i = 0; i < partition.bucketNum; i++) {\n+                bucketKeyMap.put(partition.partitionId + \"_\" + i, reduceNum);\n+                reduceNum++;\n+            }\n+        }\n+\n+        // print to system.out for easy to find log info\n+        System.out.println(\"print bucket key map:\" + bucketKeyMap.toString());\n+\n+        return dataframe;\n+    }\n+\n+    // do the etl process\n+    private Dataset<Row> convertSrcDataframeToDstDataframe(EtlJobConfig.EtlIndex baseIndex,\n+                                                           Dataset<Row> srcDataframe,\n+                                                           StructType dstTableSchema,\n+                                                           EtlJobConfig.EtlFileGroup fileGroup) throws UserException {\n+        Dataset<Row> dataframe = srcDataframe;\n+        StructType srcSchema = dataframe.schema();\n+        Set<String> srcColumnNames = new HashSet<>();\n+        for (StructField field : srcSchema.fields()) {\n+            srcColumnNames.add(field.name());\n+        }\n+        Map<String, EtlJobConfig.EtlColumnMapping> columnMappings = fileGroup.columnMappings;\n+        // 1. process simple columns\n+        Set<String> mappingColumns = null;\n+        if (columnMappings != null) {\n+            mappingColumns = columnMappings.keySet();\n+        }\n+        List<String> dstColumnNames = new ArrayList<>();\n+        for (StructField dstField : dstTableSchema.fields()) {\n+            dstColumnNames.add(dstField.name());\n+            EtlJobConfig.EtlColumn column = baseIndex.getColumn(dstField.name());\n+            if (!srcColumnNames.contains(dstField.name())) {\n+                if (mappingColumns != null && mappingColumns.contains(dstField.name())) {\n+                    // mapping columns will be processed in next step\n+                    continue;\n+                }\n+                if (column.defaultValue != null) {\n+                    if (column.defaultValue.equals(NULL_FLAG)) {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                    } else {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(column.defaultValue));\n+                    }\n+                } else if (column.isAllowNull) {\n+                    dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                } else {\n+                    throw new UserException(\"Reason: no data for column:\" + dstField.name());\n+                }\n+            }\n+            if (column.columnType.equalsIgnoreCase(\"DATE\")) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(\"date\"));\n+            } else if (column.columnType.equalsIgnoreCase(\"BOOLEAN\")) {\n+                dataframe = dataframe.withColumn(dstField.name(),\n+                        functions.when(dataframe.col(dstField.name()).equalTo(\"true\"), \"1\")\n+                                .otherwise(\"0\"));\n+            } else if (!column.columnType.equalsIgnoreCase(BITMAP_TYPE) && !dstField.dataType().equals(DataTypes.StringType)) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(dstField.dataType()));\n+            }\n+            if (fileGroup.isNegative && !column.isKey) {\n+                // negative load\n+                // value will be convert te -1 * value\n+                dataframe = dataframe.withColumn(dstField.name(), functions.expr(\"-1 *\" + dstField.name()));\n+            }\n+        }\n+        // 2. process the mapping columns\n+        for (String mappingColumn : mappingColumns) {\n+            String mappingDescription = columnMappings.get(mappingColumn).toDescription();\n+            if (mappingDescription.toLowerCase().contains(\"hll_hash\")) {\n+                continue;\n+            }\n+            // here should cast data type to dst column type\n+            dataframe = dataframe.withColumn(mappingColumn,\n+                    functions.expr(mappingDescription).cast(dstTableSchema.apply(mappingColumn).dataType()));\n+        }\n+        // projection and reorder the columns\n+        dataframe.createOrReplaceTempView(\"src_table\");\n+        StringBuilder selectSqlBuilder = new StringBuilder();\n+        selectSqlBuilder.append(\"select \");\n+        for (String name : dstColumnNames) {\n+            selectSqlBuilder.append(name + \",\");\n+        }\n+        selectSqlBuilder.deleteCharAt(selectSqlBuilder.length() - 1);\n+        selectSqlBuilder.append(\" from src_table\");\n+        String selectSql = selectSqlBuilder.toString();\n+        dataframe = spark.sql(selectSql);\n+        return dataframe;\n+    }\n+\n+    private Dataset<Row> loadDataFromPath(SparkSession spark,\n+                                          EtlJobConfig.EtlFileGroup fileGroup,\n+                                          String fileUrl,\n+                                          EtlJobConfig.EtlIndex baseIndex,\n+                                          List<EtlJobConfig.EtlColumn> columns) throws UserException {\n+        List<String> columnValueFromPath = DppUtils.parseColumnsFromPath(fileUrl, fileGroup.columnsFromPath);\n+        List<String> dataSrcColumns = fileGroup.fileFieldNames;\n+        if (dataSrcColumns == null) {\n+            // if there is no source columns info\n+            // use base index columns as source columns\n+            dataSrcColumns = new ArrayList<>();\n+            for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+                dataSrcColumns.add(column.columnName);\n+            }\n+        }\n+        List<String> dstTableNames = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            dstTableNames.add(column.columnName);\n+        }\n+        List<String> srcColumnsWithColumnsFromPath = new ArrayList<>();\n+        srcColumnsWithColumnsFromPath.addAll(dataSrcColumns);\n+        if (fileGroup.columnsFromPath != null) {\n+            srcColumnsWithColumnsFromPath.addAll(fileGroup.columnsFromPath);\n+        }\n+        StructType srcSchema = createScrSchema(srcColumnsWithColumnsFromPath);\n+        JavaRDD<String> sourceDataRdd = spark.read().textFile(fileUrl).toJavaRDD();\n+        int columnSize = dataSrcColumns.size();\n+        List<ColumnParser> parsers = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            parsers.add(ColumnParser.create(column.columnType));\n+        }\n+        // now we first support csv file\n+        // TODO: support parquet file and orc file\n+        JavaRDD<Row> rowRDD = sourceDataRdd.flatMap(\n+                record -> {\n+                    scannedRowsAcc.add(1);\n+                    String[] attributes = record.split(fileGroup.columnSeparator);\n+                    List<Row> result = new ArrayList<>();\n+                    boolean validRow = true;\n+                    if (attributes.length != columnSize) {\n+                        LOG.warn(\"invalid src schema, data columns:\"\n+                                + attributes.length + \", file group columns:\"\n+                                + columnSize + \", row:\" + record);\n+                        validRow = false;\n+                    } else {\n+                        for (int i = 0; i < attributes.length; ++i) {\n+                            if (attributes[i].equals(NULL_FLAG)) {\n+                                if (baseIndex.columns.get(i).isAllowNull) {\n+                                    attributes[i] = null;\n+                                } else {\n+                                    LOG.warn(\"colunm:\" + i + \" can not be null. row:\" + record);\n+                                    validRow = false;\n+                                    break;\n+                                }\n+                            }\n+                            boolean isStrictMode = (boolean) etlJobConfig.properties.strictMode;\n+                            if (isStrictMode) {\n+                                StructField field = srcSchema.apply(i);\n+                                if (dstTableNames.contains(field.name())) {\n+                                    String type = columns.get(i).columnType;\n+                                    if (type.equalsIgnoreCase(\"CHAR\")\n+                                            || type.equalsIgnoreCase(\"VARCHAR\")) {\n+                                        continue;\n+                                    }\n+                                    ColumnParser parser = parsers.get(i);\n+                                    boolean valid = parser.parse(attributes[i]);\n+                                    if (!valid) {\n+                                        validRow = false;\n+                                        LOG.warn(\"invalid row:\" + record\n+                                                + \", attribute \" + i + \": \" + attributes[i] + \" parsed failed\");\n+                                        break;\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                    if (validRow) {\n+                        Row row = null;\n+                        if (fileGroup.columnsFromPath == null) {\n+                            row = RowFactory.create(attributes);\n+                        } else {\n+                            // process columns from path\n+                            // append columns from path to the tail\n+                            List<String> columnAttributes = new ArrayList<>();\n+                            columnAttributes.addAll(Arrays.asList(attributes));\n+                            columnAttributes.addAll(columnValueFromPath);\n+                            row = RowFactory.create(columnAttributes.toArray());\n+                        }\n+                        result.add(row);\n+                    } else {\n+                        abnormalRowAcc.add(1);\n+                        // at most add 5 rows to invalidRows\n+                        if (abnormalRowAcc.value() <= 5) {\n+                            invalidRows.add(record);\n+                        }\n+                    }\n+                    return result.iterator();\n+                }\n+        );\n+\n+        Dataset<Row> dataframe = spark.createDataFrame(rowRDD, srcSchema);\n+        return dataframe;\n+    }\n+\n+    private StructType createScrSchema(List<String> srcColumns) {\n+        List<StructField> fields = new ArrayList<>();\n+        for (String srcColumn : srcColumns) {\n+            // user StringType to load source data\n+            StructField field = DataTypes.createStructField(srcColumn, DataTypes.StringType, true);\n+            fields.add(field);\n+        }\n+        StructType srcSchema = DataTypes.createStructType(fields);\n+        return srcSchema;\n+    }\n+\n+    // partition keys will be parsed into double from json\n+    // so need to convert it to partition columns' type\n+    private Object convertPartitionKey(Object srcValue, Class dstClass) throws UserException {\n+        if (dstClass.equals(Float.class) || dstClass.equals(Double.class)) {\n+            return null;\n+        }\n+        if (srcValue instanceof Double) {\n+            if (dstClass.equals(Short.class)) {\n+                return ((Double) srcValue).shortValue();\n+            } else if (dstClass.equals(Integer.class)) {\n+                return ((Double) srcValue).intValue();\n+            } else if (dstClass.equals(Long.class)) {\n+                return ((Double) srcValue).longValue();\n+            } else if (dstClass.equals(BigInteger.class)) {\n+                return new BigInteger(((Double) srcValue).toString());\n+            } else {\n+                // dst type is string\n+                return srcValue.toString();\n+            }\n+        } else {\n+            LOG.warn(\"unsupport partition key:\" + srcValue);\n+            throw new UserException(\"unsupport partition key:\" + srcValue);\n+        }\n+    }\n+\n+    private List<DorisRangePartitioner.PartitionRangeKey> createPartitionRangeKeys(\n+            EtlJobConfig.EtlPartitionInfo partitionInfo, List<Class> partitionKeySchema) throws UserException {\n+        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys = new ArrayList<>();\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            DorisRangePartitioner.PartitionRangeKey partitionRangeKey = new DorisRangePartitioner.PartitionRangeKey();\n+            List<Object> startKeyColumns = new ArrayList<>();\n+            for (int i = 0; i < partition.startKeys.size(); i++) {\n+                Object value = partition.startKeys.get(i);\n+                startKeyColumns.add(convertPartitionKey(value, partitionKeySchema.get(i)));\n+            }\n+            partitionRangeKey.startKeys = new DppColumns(startKeyColumns);\n+            ;\n+            if (!partition.isMaxPartition) {\n+                partitionRangeKey.isMaxPartition = false;\n+                List<Object> endKeyColumns = new ArrayList<>();\n+                for (int i = 0; i < partition.endKeys.size(); i++) {\n+                    Object value = partition.endKeys.get(i);\n+                    endKeyColumns.add(convertPartitionKey(value, partitionKeySchema.get(i)));\n+                }\n+                partitionRangeKey.endKeys = new DppColumns(endKeyColumns);\n+            } else {\n+                partitionRangeKey.isMaxPartition = false;", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc3MzUxNg==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435773516", "bodyText": "\ud83d\udc4c", "author": "wangbo", "createdAt": "2020-06-05T08:37:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0ODA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0ODQxMA==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435248410", "bodyText": "Remove the temporary code", "author": "wyb", "createdAt": "2020-06-04T13:22:33Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -0,0 +1,851 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import com.google.common.base.Strings;\n+import com.google.gson.Gson;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.function.ForeachPartitionFunction;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.functions;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.TaskContext;\n+\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+\n+import java.io.IOException;\n+import java.math.BigInteger;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import scala.collection.JavaConverters;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+// This class is a Spark-based data preprocessing program,\n+// which will make use of the distributed compute framework of spark to\n+// do ETL job/sort/preaggregate jobs in spark job\n+// to boost the process of large amount of data load.\n+// the process steps are as following:\n+// 1. load data\n+//     1.1 load data from path/hive table\n+//     1.2 do the etl process\n+// 2. repartition data by using doris data model(partition and bucket)\n+// 3. process aggregation if needed\n+// 4. write data to parquet file\n+public final class SparkDpp implements java.io.Serializable {\n+    private static final Logger LOG = LogManager.getLogger(SparkDpp.class);\n+\n+    private static final String NULL_FLAG = \"\\\\N\";\n+    private static final String DPP_RESULT_FILE = \"dpp_result.json\";\n+    private static final String BITMAP_TYPE = \"bitmap\";\n+    private SparkSession spark = null;\n+    private EtlJobConfig etlJobConfig = null;\n+    private LongAccumulator abnormalRowAcc = null;\n+    private LongAccumulator unselectedRowAcc = null;\n+    private LongAccumulator scannedRowsAcc = null;\n+    private LongAccumulator fileNumberAcc = null;\n+    private LongAccumulator fileSizeAcc = null;\n+    private Map<String, Integer> bucketKeyMap = new HashMap<>();\n+    // accumulator to collect invalid rows\n+    private StringAccumulator invalidRows = new StringAccumulator();\n+\n+    public SparkDpp(SparkSession spark, EtlJobConfig etlJobConfig) {\n+        this.spark = spark;\n+        this.etlJobConfig = etlJobConfig;\n+    }\n+\n+    public void init() {\n+        abnormalRowAcc = spark.sparkContext().longAccumulator(\"abnormalRowAcc\");\n+        unselectedRowAcc = spark.sparkContext().longAccumulator(\"unselectedRowAcc\");\n+        scannedRowsAcc = spark.sparkContext().longAccumulator(\"scannedRowsAcc\");\n+        fileNumberAcc = spark.sparkContext().longAccumulator(\"fileNumberAcc\");\n+        fileSizeAcc = spark.sparkContext().longAccumulator(\"fileSizeAcc\");\n+        spark.sparkContext().register(invalidRows, \"InvalidRowsAccumulator\");\n+    }\n+\n+    private Dataset<Row> processRDDAggAndRepartition(Dataset<Row> dataframe, EtlJobConfig.EtlIndex currentIndexMeta) throws UserException {\n+        final boolean isDuplicateTable = !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"AGGREGATE\")\n+                && !StringUtils.equalsIgnoreCase(currentIndexMeta.indexType, \"UNIQUE\");\n+\n+        // 1 make metadata for map/reduce\n+        int keyLen = 0;\n+        for (EtlJobConfig.EtlColumn etlColumn : currentIndexMeta.columns) {\n+            keyLen = etlColumn.isKey ? keyLen + 1 : keyLen;\n+        }\n+\n+        SparkRDDAggregator[] sparkRDDAggregators = new SparkRDDAggregator[currentIndexMeta.columns.size() - keyLen];\n+\n+        for (int i = 0 ; i < currentIndexMeta.columns.size(); i++) {\n+            if (!currentIndexMeta.columns.get(i).isKey && !isDuplicateTable) {\n+                sparkRDDAggregators[i - keyLen] = SparkRDDAggregator.buildAggregator(currentIndexMeta.columns.get(i));\n+            }\n+        }\n+\n+        PairFunction<Row, List<Object>, Object[]> encodePairFunction = isDuplicateTable ?\n+                // add 1 to include bucketId\n+                new EncodeDuplicateTableFunction(keyLen + 1, currentIndexMeta.columns.size() - keyLen)\n+                : new EncodeAggregateTableFunction(sparkRDDAggregators, keyLen + 1);\n+\n+        // 2 convert dataframe to rdd\n+        // TODO(wb) use rdd to avoid bitamp/hll serialize\n+        JavaPairRDD<List<Object>, Object[]> currentRollupRDD = dataframe.toJavaRDD().mapToPair(encodePairFunction);\n+\n+        // 3 do aggregate\n+        // TODO(wb) set the reduce concurrency by statistic instead of hard code 200\n+        int aggregateConcurrency = 200;\n+        JavaPairRDD<List<Object>, Object[]> reduceResultRDD = isDuplicateTable ? currentRollupRDD\n+                : currentRollupRDD.reduceByKey(new AggregateReduceFunction(sparkRDDAggregators), aggregateConcurrency);\n+\n+        // 4 repartition\n+        JavaRDD<Row> finalRDD = reduceResultRDD\n+                .repartitionAndSortWithinPartitions(new BucketPartitioner(bucketKeyMap), new BucketComparator())\n+                .map(record -> {\n+                    List<Object> keys = record._1;\n+                    Object[] values = record._2;\n+                    int size = keys.size() + values.length;\n+                    Object[] result = new Object[size];\n+\n+                    for (int i = 0; i < keys.size(); i++) {\n+                        result[i] = keys.get(i);\n+                    }\n+\n+                    for (int i = keys.size(); i < size; i++) {\n+                        int valueIdx = i - keys.size();\n+                        result[i] = isDuplicateTable ? values[valueIdx] : sparkRDDAggregators[valueIdx].finalize(values[valueIdx]);\n+                    }\n+\n+                    return RowFactory.create(result);\n+                });\n+\n+        // 4 convert to dataframe\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(currentIndexMeta.columns, true, true);\n+        dataframe = spark.createDataFrame(finalRDD, tableSchemaWithBucketId);\n+        return dataframe;\n+\n+    }\n+\n+    // write data to parquet file by using writing the parquet scheme of spark.\n+    private void writePartitionedAndSortedDataframeToParquet(Dataset<Row> dataframe,\n+                                                             String pathPattern,\n+                                                             long tableId,\n+                                                             EtlJobConfig.EtlIndex indexMeta) throws UserException {\n+        StructType outputSchema = dataframe.schema();\n+        StructType dstSchema = DataTypes.createStructType(\n+                Arrays.asList(outputSchema.fields()).stream()\n+                        .filter(field -> !field.name().equalsIgnoreCase(DppUtils.BUCKET_ID))\n+                        .collect(Collectors.toList()));\n+        ExpressionEncoder encoder = RowEncoder.apply(dstSchema);\n+        dataframe.foreachPartition(new ForeachPartitionFunction<Row>() {\n+            @Override\n+            public void call(Iterator<Row> t) throws Exception {\n+                // write the data to dst file\n+                Configuration conf = new Configuration();\n+                FileSystem fs = FileSystem.get(URI.create(etlJobConfig.outputPath), conf);\n+                String lastBucketKey = null;\n+                ParquetWriter<InternalRow> parquetWriter = null;\n+                TaskContext taskContext = TaskContext.get();\n+                long taskAttemptId = taskContext.taskAttemptId();\n+                String dstPath = \"\";\n+                String tmpPath = \"\";\n+\n+                while (t.hasNext()) {\n+                    Row row = t.next();\n+                    if (row.length() <= 1) {\n+                        LOG.warn(\"invalid row:\" + row);\n+                        continue;\n+                    }\n+\n+\n+                    String curBucketKey = row.getString(0);\n+                    List<Object> columnObjects = new ArrayList<>();\n+                    for (int i = 1; i < row.length(); ++i) {\n+                        Object columnValue = row.get(i);\n+                        columnObjects.add(columnValue);\n+                    }\n+                    System.out.println();\n+                    Row rowWithoutBucketKey = RowFactory.create(columnObjects.toArray());\n+                    // if the bucket key is new, it will belong to a new tablet\n+                    if (lastBucketKey == null || !curBucketKey.equals(lastBucketKey)) {\n+                        if (parquetWriter != null) {\n+                            parquetWriter.close();\n+                            // rename tmpPath to path\n+                            try {\n+                                fs.rename(new Path(tmpPath), new Path(dstPath));\n+                            } catch (IOException ioe) {\n+                                LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                                throw ioe;\n+                            }\n+                        }\n+                        // flush current writer and create a new writer\n+                        String[] bucketKey = curBucketKey.split(\"_\");\n+                        if (bucketKey.length != 2) {\n+                            LOG.warn(\"invalid bucket key:\" + curBucketKey);\n+                            continue;\n+                        }\n+                        int partitionId = Integer.parseInt(bucketKey[0]);\n+                        int bucketId = Integer.parseInt(bucketKey[1]);\n+                        dstPath = String.format(pathPattern, tableId, partitionId, indexMeta.indexId,\n+                                bucketId, indexMeta.schemaHash);\n+                        tmpPath = dstPath + \".\" + taskAttemptId;\n+                        conf.setBoolean(\"spark.sql.parquet.writeLegacyFormat\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int64AsTimestampMillis\", false);\n+                        conf.setBoolean(\"spark.sql.parquet.int96AsTimestamp\", true);\n+                        conf.setBoolean(\"spark.sql.parquet.binaryAsString\", false);\n+                        conf.set(\"spark.sql.parquet.outputTimestampType\", \"INT96\");\n+                        ParquetWriteSupport.setSchema(dstSchema, conf);\n+                        ParquetWriteSupport parquetWriteSupport = new ParquetWriteSupport();\n+                        parquetWriter = new ParquetWriter<InternalRow>(new Path(tmpPath), parquetWriteSupport,\n+                                CompressionCodecName.SNAPPY, 256 * 1024 * 1024, 16 * 1024, 1024 * 1024,\n+                                true, false,\n+                                ParquetProperties.WriterVersion.PARQUET_1_0, conf);\n+                        if (parquetWriter != null) {\n+                            LOG.info(\"[HdfsOperate]>> initialize writer succeed! path:\" + tmpPath);\n+                        }\n+                        lastBucketKey = curBucketKey;\n+                    }\n+                    InternalRow internalRow = encoder.toRow(rowWithoutBucketKey);\n+                    parquetWriter.write(internalRow);\n+                }\n+                if (parquetWriter != null) {\n+                    parquetWriter.close();\n+                    try {\n+                        fs.rename(new Path(tmpPath), new Path(dstPath));\n+                    } catch (IOException ioe) {\n+                        LOG.warn(\"rename from tmpPath\" + tmpPath + \" to dstPath:\" + dstPath + \" failed. exception:\" + ioe);\n+                        throw ioe;\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    private void processRollupTree(RollupTreeNode rootNode,\n+                                   Dataset<Row> rootDataframe,\n+                                   long tableId, EtlJobConfig.EtlTable tableMeta,\n+                                   EtlJobConfig.EtlIndex baseIndex) throws UserException {\n+        Queue<RollupTreeNode> nodeQueue = new LinkedList<>();\n+        nodeQueue.offer(rootNode);\n+        int currentLevel = 0;\n+        // level travel the tree\n+        Map<Long, Dataset<Row>> parentDataframeMap = new HashMap<>();\n+        parentDataframeMap.put(baseIndex.indexId, rootDataframe);\n+        Map<Long, Dataset<Row>> childrenDataframeMap = new HashMap<>();\n+        String pathPattern = etlJobConfig.outputPath + \"/\" + etlJobConfig.outputFilePattern;\n+        while (!nodeQueue.isEmpty()) {\n+            RollupTreeNode curNode = nodeQueue.poll();\n+            LOG.info(\"start to process index:\" + curNode.indexId);\n+            if (curNode.children != null) {\n+                for (RollupTreeNode child : curNode.children) {\n+                    nodeQueue.offer(child);\n+                }\n+            }\n+            Dataset<Row> curDataFrame = null;\n+            // column select for rollup\n+            if (curNode.level != currentLevel) {\n+                for (Dataset<Row> dataframe : parentDataframeMap.values()) {\n+                    dataframe.unpersist();\n+                }\n+                currentLevel = curNode.level;\n+                parentDataframeMap.clear();\n+                parentDataframeMap = childrenDataframeMap;\n+                childrenDataframeMap = new HashMap<>();\n+            }\n+\n+            long parentIndexId = baseIndex.indexId;\n+            if (curNode.parent != null) {\n+                parentIndexId = curNode.parent.indexId;\n+            }\n+\n+            Dataset<Row> parentDataframe = parentDataframeMap.get(parentIndexId);\n+            List<Column> columns = new ArrayList<>();\n+            List<Column> keyColumns = new ArrayList<>();\n+            Column bucketIdColumn = new Column(DppUtils.BUCKET_ID);\n+            keyColumns.add(bucketIdColumn);\n+            columns.add(bucketIdColumn);\n+            for (String keyName : curNode.keyColumnNames) {\n+                columns.add(new Column(keyName));\n+                keyColumns.add(new Column(keyName));\n+            }\n+            for (String valueName : curNode.valueColumnNames) {\n+                columns.add(new Column(valueName));\n+            }\n+            Seq<Column> columnSeq = JavaConverters.asScalaIteratorConverter(columns.iterator()).asScala().toSeq();\n+            curDataFrame = parentDataframe.select(columnSeq);\n+            // aggregate and repartition\n+            curDataFrame = processRDDAggAndRepartition(curDataFrame, curNode.indexMeta);\n+\n+            childrenDataframeMap.put(curNode.indexId, curDataFrame);\n+\n+            if (curNode.children != null && curNode.children.size() > 1) {\n+                // if the children number larger than 1, persist the dataframe for performance\n+                curDataFrame.persist();\n+            }\n+            writePartitionedAndSortedDataframeToParquet(curDataFrame, pathPattern, tableId, curNode.indexMeta);\n+        }\n+    }\n+\n+    // repartition dataframe by partitionid_bucketid\n+    // so data in the same bucket will be consecutive.\n+    private Dataset<Row> repartitionDataframeByBucketId(SparkSession spark, Dataset<Row> dataframe,\n+                                                        EtlJobConfig.EtlPartitionInfo partitionInfo,\n+                                                        List<Integer> partitionKeyIndex,\n+                                                        List<Class> partitionKeySchema,\n+                                                        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys,\n+                                                        List<String> keyColumnNames,\n+                                                        List<String> valueColumnNames,\n+                                                        StructType dstTableSchema,\n+                                                        EtlJobConfig.EtlIndex baseIndex,\n+                                                        List<Long> validPartitionIds) throws UserException {\n+        List<String> distributeColumns = partitionInfo.distributionColumnRefs;\n+        Partitioner partitioner = new DorisRangePartitioner(partitionInfo, partitionKeyIndex, partitionRangeKeys);\n+        Set<Integer> validPartitionIndex = new HashSet<>();\n+        if (validPartitionIds == null) {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                validPartitionIndex.add(i);\n+            }\n+        } else {\n+            for (int i = 0; i < partitionInfo.partitions.size(); ++i) {\n+                if (validPartitionIds.contains(partitionInfo.partitions.get(i).partitionId)) {\n+                    validPartitionIndex.add(i);\n+                }\n+            }\n+        }\n+        // use PairFlatMapFunction instead of PairMapFunction because the there will be\n+        // 0 or 1 output row for 1 input row\n+        JavaPairRDD<String, DppColumns> pairRDD = dataframe.javaRDD().flatMapToPair(\n+                new PairFlatMapFunction<Row, String, DppColumns>() {\n+                    @Override\n+                    public Iterator<Tuple2<String, DppColumns>> call(Row row) {\n+                        List<Object> columns = new ArrayList<>();\n+                        List<Object> keyColumns = new ArrayList<>();\n+                        for (String columnName : keyColumnNames) {\n+                            Object columnObject = row.get(row.fieldIndex(columnName));\n+                            columns.add(columnObject);\n+                            keyColumns.add(columnObject);\n+                        }\n+\n+                        for (String columnName : valueColumnNames) {\n+                            columns.add(row.get(row.fieldIndex(columnName)));\n+                        }\n+                        DppColumns dppColumns = new DppColumns(columns);\n+                        DppColumns key = new DppColumns(keyColumns);\n+                        List<Tuple2<String, DppColumns>> result = new ArrayList<>();\n+                        int pid = partitioner.getPartition(key);\n+                        if (!validPartitionIndex.contains(pid)) {\n+                            LOG.warn(\"invalid partition for row:\" + row + \", pid:\" + pid);\n+                            abnormalRowAcc.add(1);\n+                            LOG.info(\"abnormalRowAcc:\" + abnormalRowAcc);\n+                            if (abnormalRowAcc.value() < 5) {\n+                                LOG.info(\"add row to invalidRows:\" + row.toString());\n+                                invalidRows.add(row.toString());\n+                                LOG.info(\"invalid rows contents:\" + invalidRows.value());\n+                            }\n+                        } else {\n+                            long hashValue = DppUtils.getHashValue(row, distributeColumns, dstTableSchema);\n+                            int bucketId = (int) ((hashValue & 0xffffffff) % partitionInfo.partitions.get(pid).bucketNum);\n+                            long partitionId = partitionInfo.partitions.get(pid).partitionId;\n+                            // bucketKey is partitionId_bucketId\n+                            String bucketKey = partitionId + \"_\" + bucketId;\n+                            Tuple2<String, DppColumns> newTuple = new Tuple2<String, DppColumns>(bucketKey, dppColumns);\n+                            result.add(newTuple);\n+                        }\n+                        return result.iterator();\n+                    }\n+                });\n+\n+        JavaRDD<Row> resultRdd = pairRDD.map(record -> {\n+                    String bucketKey = record._1;\n+                    List<Object> row = new ArrayList<>();\n+                    // bucketKey as the first key\n+                    row.add(bucketKey);\n+                    row.addAll(record._2.columns);\n+                    return RowFactory.create(row.toArray());\n+                }\n+        );\n+\n+        // TODO(wb): using rdd instead of dataframe from here\n+        StructType tableSchemaWithBucketId = DppUtils.createDstTableSchema(baseIndex.columns, true, false);\n+        dataframe = spark.createDataFrame(resultRdd, tableSchemaWithBucketId);\n+        // use bucket number as the parallel number\n+        int reduceNum = 0;\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            for (int i = 0; i < partition.bucketNum; i++) {\n+                bucketKeyMap.put(partition.partitionId + \"_\" + i, reduceNum);\n+                reduceNum++;\n+            }\n+        }\n+\n+        // print to system.out for easy to find log info\n+        System.out.println(\"print bucket key map:\" + bucketKeyMap.toString());\n+\n+        return dataframe;\n+    }\n+\n+    // do the etl process\n+    private Dataset<Row> convertSrcDataframeToDstDataframe(EtlJobConfig.EtlIndex baseIndex,\n+                                                           Dataset<Row> srcDataframe,\n+                                                           StructType dstTableSchema,\n+                                                           EtlJobConfig.EtlFileGroup fileGroup) throws UserException {\n+        Dataset<Row> dataframe = srcDataframe;\n+        StructType srcSchema = dataframe.schema();\n+        Set<String> srcColumnNames = new HashSet<>();\n+        for (StructField field : srcSchema.fields()) {\n+            srcColumnNames.add(field.name());\n+        }\n+        Map<String, EtlJobConfig.EtlColumnMapping> columnMappings = fileGroup.columnMappings;\n+        // 1. process simple columns\n+        Set<String> mappingColumns = null;\n+        if (columnMappings != null) {\n+            mappingColumns = columnMappings.keySet();\n+        }\n+        List<String> dstColumnNames = new ArrayList<>();\n+        for (StructField dstField : dstTableSchema.fields()) {\n+            dstColumnNames.add(dstField.name());\n+            EtlJobConfig.EtlColumn column = baseIndex.getColumn(dstField.name());\n+            if (!srcColumnNames.contains(dstField.name())) {\n+                if (mappingColumns != null && mappingColumns.contains(dstField.name())) {\n+                    // mapping columns will be processed in next step\n+                    continue;\n+                }\n+                if (column.defaultValue != null) {\n+                    if (column.defaultValue.equals(NULL_FLAG)) {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                    } else {\n+                        dataframe = dataframe.withColumn(dstField.name(), functions.lit(column.defaultValue));\n+                    }\n+                } else if (column.isAllowNull) {\n+                    dataframe = dataframe.withColumn(dstField.name(), functions.lit(null));\n+                } else {\n+                    throw new UserException(\"Reason: no data for column:\" + dstField.name());\n+                }\n+            }\n+            if (column.columnType.equalsIgnoreCase(\"DATE\")) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(\"date\"));\n+            } else if (column.columnType.equalsIgnoreCase(\"BOOLEAN\")) {\n+                dataframe = dataframe.withColumn(dstField.name(),\n+                        functions.when(dataframe.col(dstField.name()).equalTo(\"true\"), \"1\")\n+                                .otherwise(\"0\"));\n+            } else if (!column.columnType.equalsIgnoreCase(BITMAP_TYPE) && !dstField.dataType().equals(DataTypes.StringType)) {\n+                dataframe = dataframe.withColumn(dstField.name(), dataframe.col(dstField.name()).cast(dstField.dataType()));\n+            }\n+            if (fileGroup.isNegative && !column.isKey) {\n+                // negative load\n+                // value will be convert te -1 * value\n+                dataframe = dataframe.withColumn(dstField.name(), functions.expr(\"-1 *\" + dstField.name()));\n+            }\n+        }\n+        // 2. process the mapping columns\n+        for (String mappingColumn : mappingColumns) {\n+            String mappingDescription = columnMappings.get(mappingColumn).toDescription();\n+            if (mappingDescription.toLowerCase().contains(\"hll_hash\")) {\n+                continue;\n+            }\n+            // here should cast data type to dst column type\n+            dataframe = dataframe.withColumn(mappingColumn,\n+                    functions.expr(mappingDescription).cast(dstTableSchema.apply(mappingColumn).dataType()));\n+        }\n+        // projection and reorder the columns\n+        dataframe.createOrReplaceTempView(\"src_table\");\n+        StringBuilder selectSqlBuilder = new StringBuilder();\n+        selectSqlBuilder.append(\"select \");\n+        for (String name : dstColumnNames) {\n+            selectSqlBuilder.append(name + \",\");\n+        }\n+        selectSqlBuilder.deleteCharAt(selectSqlBuilder.length() - 1);\n+        selectSqlBuilder.append(\" from src_table\");\n+        String selectSql = selectSqlBuilder.toString();\n+        dataframe = spark.sql(selectSql);\n+        return dataframe;\n+    }\n+\n+    private Dataset<Row> loadDataFromPath(SparkSession spark,\n+                                          EtlJobConfig.EtlFileGroup fileGroup,\n+                                          String fileUrl,\n+                                          EtlJobConfig.EtlIndex baseIndex,\n+                                          List<EtlJobConfig.EtlColumn> columns) throws UserException {\n+        List<String> columnValueFromPath = DppUtils.parseColumnsFromPath(fileUrl, fileGroup.columnsFromPath);\n+        List<String> dataSrcColumns = fileGroup.fileFieldNames;\n+        if (dataSrcColumns == null) {\n+            // if there is no source columns info\n+            // use base index columns as source columns\n+            dataSrcColumns = new ArrayList<>();\n+            for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+                dataSrcColumns.add(column.columnName);\n+            }\n+        }\n+        List<String> dstTableNames = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            dstTableNames.add(column.columnName);\n+        }\n+        List<String> srcColumnsWithColumnsFromPath = new ArrayList<>();\n+        srcColumnsWithColumnsFromPath.addAll(dataSrcColumns);\n+        if (fileGroup.columnsFromPath != null) {\n+            srcColumnsWithColumnsFromPath.addAll(fileGroup.columnsFromPath);\n+        }\n+        StructType srcSchema = createScrSchema(srcColumnsWithColumnsFromPath);\n+        JavaRDD<String> sourceDataRdd = spark.read().textFile(fileUrl).toJavaRDD();\n+        int columnSize = dataSrcColumns.size();\n+        List<ColumnParser> parsers = new ArrayList<>();\n+        for (EtlJobConfig.EtlColumn column : baseIndex.columns) {\n+            parsers.add(ColumnParser.create(column.columnType));\n+        }\n+        // now we first support csv file\n+        // TODO: support parquet file and orc file\n+        JavaRDD<Row> rowRDD = sourceDataRdd.flatMap(\n+                record -> {\n+                    scannedRowsAcc.add(1);\n+                    String[] attributes = record.split(fileGroup.columnSeparator);\n+                    List<Row> result = new ArrayList<>();\n+                    boolean validRow = true;\n+                    if (attributes.length != columnSize) {\n+                        LOG.warn(\"invalid src schema, data columns:\"\n+                                + attributes.length + \", file group columns:\"\n+                                + columnSize + \", row:\" + record);\n+                        validRow = false;\n+                    } else {\n+                        for (int i = 0; i < attributes.length; ++i) {\n+                            if (attributes[i].equals(NULL_FLAG)) {\n+                                if (baseIndex.columns.get(i).isAllowNull) {\n+                                    attributes[i] = null;\n+                                } else {\n+                                    LOG.warn(\"colunm:\" + i + \" can not be null. row:\" + record);\n+                                    validRow = false;\n+                                    break;\n+                                }\n+                            }\n+                            boolean isStrictMode = (boolean) etlJobConfig.properties.strictMode;\n+                            if (isStrictMode) {\n+                                StructField field = srcSchema.apply(i);\n+                                if (dstTableNames.contains(field.name())) {\n+                                    String type = columns.get(i).columnType;\n+                                    if (type.equalsIgnoreCase(\"CHAR\")\n+                                            || type.equalsIgnoreCase(\"VARCHAR\")) {\n+                                        continue;\n+                                    }\n+                                    ColumnParser parser = parsers.get(i);\n+                                    boolean valid = parser.parse(attributes[i]);\n+                                    if (!valid) {\n+                                        validRow = false;\n+                                        LOG.warn(\"invalid row:\" + record\n+                                                + \", attribute \" + i + \": \" + attributes[i] + \" parsed failed\");\n+                                        break;\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                    if (validRow) {\n+                        Row row = null;\n+                        if (fileGroup.columnsFromPath == null) {\n+                            row = RowFactory.create(attributes);\n+                        } else {\n+                            // process columns from path\n+                            // append columns from path to the tail\n+                            List<String> columnAttributes = new ArrayList<>();\n+                            columnAttributes.addAll(Arrays.asList(attributes));\n+                            columnAttributes.addAll(columnValueFromPath);\n+                            row = RowFactory.create(columnAttributes.toArray());\n+                        }\n+                        result.add(row);\n+                    } else {\n+                        abnormalRowAcc.add(1);\n+                        // at most add 5 rows to invalidRows\n+                        if (abnormalRowAcc.value() <= 5) {\n+                            invalidRows.add(record);\n+                        }\n+                    }\n+                    return result.iterator();\n+                }\n+        );\n+\n+        Dataset<Row> dataframe = spark.createDataFrame(rowRDD, srcSchema);\n+        return dataframe;\n+    }\n+\n+    private StructType createScrSchema(List<String> srcColumns) {\n+        List<StructField> fields = new ArrayList<>();\n+        for (String srcColumn : srcColumns) {\n+            // user StringType to load source data\n+            StructField field = DataTypes.createStructField(srcColumn, DataTypes.StringType, true);\n+            fields.add(field);\n+        }\n+        StructType srcSchema = DataTypes.createStructType(fields);\n+        return srcSchema;\n+    }\n+\n+    // partition keys will be parsed into double from json\n+    // so need to convert it to partition columns' type\n+    private Object convertPartitionKey(Object srcValue, Class dstClass) throws UserException {\n+        if (dstClass.equals(Float.class) || dstClass.equals(Double.class)) {\n+            return null;\n+        }\n+        if (srcValue instanceof Double) {\n+            if (dstClass.equals(Short.class)) {\n+                return ((Double) srcValue).shortValue();\n+            } else if (dstClass.equals(Integer.class)) {\n+                return ((Double) srcValue).intValue();\n+            } else if (dstClass.equals(Long.class)) {\n+                return ((Double) srcValue).longValue();\n+            } else if (dstClass.equals(BigInteger.class)) {\n+                return new BigInteger(((Double) srcValue).toString());\n+            } else {\n+                // dst type is string\n+                return srcValue.toString();\n+            }\n+        } else {\n+            LOG.warn(\"unsupport partition key:\" + srcValue);\n+            throw new UserException(\"unsupport partition key:\" + srcValue);\n+        }\n+    }\n+\n+    private List<DorisRangePartitioner.PartitionRangeKey> createPartitionRangeKeys(\n+            EtlJobConfig.EtlPartitionInfo partitionInfo, List<Class> partitionKeySchema) throws UserException {\n+        List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys = new ArrayList<>();\n+        for (EtlJobConfig.EtlPartition partition : partitionInfo.partitions) {\n+            DorisRangePartitioner.PartitionRangeKey partitionRangeKey = new DorisRangePartitioner.PartitionRangeKey();\n+            List<Object> startKeyColumns = new ArrayList<>();\n+            for (int i = 0; i < partition.startKeys.size(); i++) {\n+                Object value = partition.startKeys.get(i);\n+                startKeyColumns.add(convertPartitionKey(value, partitionKeySchema.get(i)));\n+            }\n+            partitionRangeKey.startKeys = new DppColumns(startKeyColumns);\n+            ;\n+            if (!partition.isMaxPartition) {\n+                partitionRangeKey.isMaxPartition = false;\n+                List<Object> endKeyColumns = new ArrayList<>();\n+                for (int i = 0; i < partition.endKeys.size(); i++) {\n+                    Object value = partition.endKeys.get(i);\n+                    endKeyColumns.add(convertPartitionKey(value, partitionKeySchema.get(i)));\n+                }\n+                partitionRangeKey.endKeys = new DppColumns(endKeyColumns);\n+            } else {\n+                partitionRangeKey.isMaxPartition = false;\n+            }\n+            partitionRangeKeys.add(partitionRangeKey);\n+        }\n+        return partitionRangeKeys;\n+    }\n+\n+    private Dataset<Row> loadDataFromFilePaths(SparkSession spark,\n+                                               EtlJobConfig.EtlIndex baseIndex,\n+                                               List<String> filePaths,\n+                                               EtlJobConfig.EtlFileGroup fileGroup,\n+                                               StructType dstTableSchema)\n+            throws UserException, IOException, URISyntaxException {\n+        Dataset<Row> fileGroupDataframe = null;\n+        for (String filePath : filePaths) {\n+            fileNumberAcc.add(1);\n+            try {\n+                Configuration conf = new Configuration();\n+                URI uri = new URI(filePath);\n+                FileSystem fs = FileSystem.get(uri, conf);\n+                FileStatus fileStatus = fs.getFileStatus(new Path(filePath));\n+                fileSizeAcc.add(fileStatus.getLen());\n+            } catch (Exception e) {\n+                LOG.warn(\"parse path failed:\" + filePath);\n+                throw e;\n+            }\n+            if (fileGroup.columnSeparator == null) {\n+                LOG.warn(\"invalid null column separator!\");\n+                throw new UserException(\"Reason: invalid null column separator!\");\n+            }\n+            Dataset<Row> dataframe = null;\n+\n+            dataframe = loadDataFromPath(spark, fileGroup, filePath, baseIndex, baseIndex.columns);\n+            dataframe = convertSrcDataframeToDstDataframe(baseIndex, dataframe, dstTableSchema, fileGroup);\n+            if (fileGroupDataframe == null) {\n+                fileGroupDataframe = dataframe;\n+            } else {\n+                fileGroupDataframe.union(dataframe);\n+            }\n+        }\n+        return fileGroupDataframe;\n+    }\n+\n+    private Dataset<Row> loadDataFromHiveTable(SparkSession spark,\n+                                               String hiveTableName,\n+                                               EtlJobConfig.EtlIndex baseIndex,\n+                                               EtlJobConfig.EtlFileGroup fileGroup,\n+                                               StructType dstTableSchema) throws UserException {\n+        Dataset<Row> dataframe = spark.sql(\"select * from \" + hiveTableName);\n+        dataframe = convertSrcDataframeToDstDataframe(baseIndex, dataframe, dstTableSchema, fileGroup);\n+        return dataframe;\n+    }\n+\n+    private DppResult process() throws Exception {\n+        DppResult dppResult = new DppResult();\n+        try {\n+            for (Map.Entry<Long, EtlJobConfig.EtlTable> entry : etlJobConfig.tables.entrySet()) {\n+                Long tableId = entry.getKey();\n+                EtlJobConfig.EtlTable etlTable = entry.getValue();\n+\n+                // get the base index meta\n+                EtlJobConfig.EtlIndex baseIndex = null;\n+                for (EtlJobConfig.EtlIndex indexMeta : etlTable.indexes) {\n+                    if (indexMeta.isBaseIndex) {\n+                        baseIndex = indexMeta;\n+                        break;\n+                    }\n+                }\n+\n+                // get key column names and value column names seperately\n+                List<String> keyColumnNames = new ArrayList<>();\n+                List<String> valueColumnNames = new ArrayList<>();\n+                for (EtlJobConfig.EtlColumn etlColumn : baseIndex.columns) {\n+                    if (etlColumn.isKey) {\n+                        keyColumnNames.add(etlColumn.columnName);\n+                    } else {\n+                        valueColumnNames.add(etlColumn.columnName);\n+                    }\n+                }\n+\n+                EtlJobConfig.EtlPartitionInfo partitionInfo = etlTable.partitionInfo;\n+                List<Integer> partitionKeyIndex = new ArrayList<Integer>();\n+                List<Class> partitionKeySchema = new ArrayList<>();\n+                for (String key : partitionInfo.partitionColumnRefs) {\n+                    for (int i = 0; i < baseIndex.columns.size(); ++i) {\n+                        EtlJobConfig.EtlColumn column = baseIndex.columns.get(i);\n+                        if (column.columnName.equals(key)) {\n+                            partitionKeyIndex.add(i);\n+                            partitionKeySchema.add(DppUtils.getClassFromColumn(column));\n+                            break;\n+                        }\n+                    }\n+                }\n+                List<DorisRangePartitioner.PartitionRangeKey> partitionRangeKeys = createPartitionRangeKeys(partitionInfo, partitionKeySchema);\n+                StructType dstTableSchema = DppUtils.createDstTableSchema(baseIndex.columns, false, false);\n+                RollupTreeBuilder rollupTreeParser = new MinimumCoverageRollupTreeBuilder();\n+                RollupTreeNode rootNode = rollupTreeParser.build(etlTable);\n+                LOG.info(\"Start to process rollup tree:\" + rootNode);\n+\n+                Dataset<Row> tableDataframe = null;\n+                for (EtlJobConfig.EtlFileGroup fileGroup : etlTable.fileGroups) {\n+                    List<String> filePaths = fileGroup.filePaths;\n+                    Dataset<Row> fileGroupDataframe = null;\n+                    if (Strings.isNullOrEmpty(fileGroup.hiveTableName)) {\n+                        fileGroupDataframe = loadDataFromFilePaths(spark, baseIndex, filePaths, fileGroup, dstTableSchema);\n+                    } else {\n+                        String taskId = etlJobConfig.outputPath.substring(etlJobConfig.outputPath.lastIndexOf(\"/\") + 1);\n+                        String dorisIntermediateHiveTable = String.format(EtlJobConfig.DORIS_INTERMEDIATE_HIVE_TABLE_NAME,\n+                                tableId, taskId);\n+                        dorisIntermediateHiveTable = \"kylin2x_test.doris_intermediate_hive_table_31105_66130\"; // 3kw", "originalCommit": "0758b09dc3d0dbd7754a7a1935b1103d86f35396", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc3MzYxNA==", "url": "https://github.com/apache/incubator-doris/pull/3729#discussion_r435773614", "bodyText": "\ud83d\udc4c", "author": "wangbo", "createdAt": "2020-06-05T08:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0ODQxMA=="}], "type": "inlineReview"}, {"oid": "bc18a9416973621402435c8df61e1bf68af10c4d", "url": "https://github.com/apache/incubator-doris/commit/bc18a9416973621402435c8df61e1bf68af10c4d", "message": "1 support date/datetime as partition column\n2 some minor fix\n\n1 add ut for sparkdpp util\n2 some minor fix", "committedDate": "2020-06-17T09:46:03Z", "type": "commit"}, {"oid": "bc18a9416973621402435c8df61e1bf68af10c4d", "url": "https://github.com/apache/incubator-doris/commit/bc18a9416973621402435c8df61e1bf68af10c4d", "message": "1 support date/datetime as partition column\n2 some minor fix\n\n1 add ut for sparkdpp util\n2 some minor fix", "committedDate": "2020-06-17T09:46:03Z", "type": "forcePushed"}, {"oid": "5561f4f729e60ef6dc1a442ba4a6e8ebc9f1d31f", "url": "https://github.com/apache/incubator-doris/commit/5561f4f729e60ef6dc1a442ba4a6e8ebc9f1d31f", "message": "Merge branch 'upstream_master' into spark_dpp", "committedDate": "2020-06-22T07:02:35Z", "type": "commit"}, {"oid": "1f3718cdb8f6be4208a34643d7b6ffcd5e840cdc", "url": "https://github.com/apache/incubator-doris/commit/1f3718cdb8f6be4208a34643d7b6ffcd5e840cdc", "message": "rebase from master", "committedDate": "2020-06-22T07:09:27Z", "type": "commit"}, {"oid": "600b166989272730c3e15d51c65539dee1888da9", "url": "https://github.com/apache/incubator-doris/commit/600b166989272730c3e15d51c65539dee1888da9", "message": "fix ut error", "committedDate": "2020-06-22T09:36:48Z", "type": "commit"}]}