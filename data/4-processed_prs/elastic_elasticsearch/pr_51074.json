{"pr_number": 51074, "pr_title": "Add CoolDown Period to S3 Repository", "pr_createdAt": "2020-01-15T22:01:28Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/51074", "timeline": [{"oid": "de5a65ca7725bd983d6b389205f71a0ecb139b1f", "url": "https://github.com/elastic/elasticsearch/commit/de5a65ca7725bd983d6b389205f71a0ecb139b1f", "message": "bck", "committedDate": "2020-01-15T13:49:41Z", "type": "commit"}, {"oid": "c14ac7ce798a2d3f60ed180ba7323aa2b538abf7", "url": "https://github.com/elastic/elasticsearch/commit/c14ac7ce798a2d3f60ed180ba7323aa2b538abf7", "message": "Merge remote-tracking branch 'elastic/master' into s3-cooldown", "committedDate": "2020-01-15T16:32:01Z", "type": "commit"}, {"oid": "5c25c52802e704a64aca0558229f243721ab1d3f", "url": "https://github.com/elastic/elasticsearch/commit/5c25c52802e704a64aca0558229f243721ab1d3f", "message": "Add Cooldown Period to S3 Repository\n\nWIP, still missing tests but would like to confirm we agree on the approach taken here first.", "committedDate": "2020-01-15T21:58:24Z", "type": "commit"}, {"oid": "f4a8e098bd9952c8949ac017e72d657f4b5f7ad2", "url": "https://github.com/elastic/elasticsearch/commit/f4a8e098bd9952c8949ac017e72d657f4b5f7ad2", "message": "cleanup", "committedDate": "2020-01-15T22:00:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzM4Nzc3OA==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367387778", "bodyText": "@ywelsch I went with doing it this way instead of just keeping track of a timestamp and then failing a new snapshot if it's started to close to the last timestamp. I'm afraid having random failures from concurrent snapshot exceptions when no running snapshot is visible to APIs could mess with Cloud orchestration (not necessarily breaking it but causing an unreasonable amount of _status requests).", "author": "original-brownbear", "createdAt": "2020-01-16T12:20:16Z", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java", "diffHunk": "@@ -186,6 +220,58 @@\n                 storageClass);\n     }\n \n+    /**\n+     * Holds a reference to delayed repository operation {@link Scheduler.Cancellable} so it can be cancelled should the repository be\n+     * closed concurrently.\n+     */\n+    private final AtomicReference<Scheduler.Cancellable> finalizationFuture = new AtomicReference<>();\n+\n+    @Override\n+    public void finalizeSnapshot(SnapshotId snapshotId, ShardGenerations shardGenerations, long startTime, String failure, int totalShards,\n+                                 List<SnapshotShardFailure> shardFailures, long repositoryStateId, boolean includeGlobalState,\n+                                 MetaData clusterMetaData, Map<String, Object> userMetadata, boolean writeShardGens,\n+                                 ActionListener<SnapshotInfo> listener) {\n+        if (writeShardGens == false) {\n+            listener = delayedListener(listener);\n+        }\n+        super.finalizeSnapshot(snapshotId, shardGenerations, startTime, failure, totalShards, shardFailures, repositoryStateId,\n+            includeGlobalState, clusterMetaData, userMetadata, writeShardGens, listener);\n+    }\n+\n+    @Override\n+    public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, boolean writeShardGens, ActionListener<Void> listener) {\n+        if (writeShardGens == false) {\n+            listener = delayedListener(listener);\n+        }\n+        super.deleteSnapshot(snapshotId, repositoryStateId, writeShardGens, listener);\n+    }\n+\n+    /**\n+     * Wraps given listener such that it is executed with a delay of {@link #coolDown} on the snapshot thread-pool after being invoked.\n+     * See {@link #COOLDOWN_PERIOD} for details.\n+     */\n+    private <T> ActionListener<T> delayedListener(ActionListener<T> listener) {\n+        final ActionListener<T> wrappedListener = ActionListener.runBefore(listener, () -> {\n+            final Scheduler.Cancellable cancellable = finalizationFuture.getAndSet(null);\n+            assert cancellable != null;\n+        });\n+        return new ActionListener<>() {\n+            @Override\n+            public void onResponse(T response) {\n+                final Scheduler.Cancellable existing = finalizationFuture.getAndSet(", "originalCommit": "f4a8e098bd9952c8949ac017e72d657f4b5f7ad2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQzNDM5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367434396", "bodyText": "I had to think a bit about this, and consulted @DaveCTurner as well. We both agree that this is the right path forward (simpler to explain to users and simpler for existing orchestration tools).\nIn short, this artificially extends the duration of the snapshot, i.e., taking or deleting a snapshot takes 3 minutes longer. Can we add a log message that details why we are doing this (and that we are in a repo with legacy snapshots)? Let's also document this somewhere (with the setting). This gives users the choice e.g. to move to a different repo.", "author": "ywelsch", "createdAt": "2020-01-16T14:05:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzM4Nzc3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzUzNDA0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367534047", "bodyText": "Let's also document this somewhere (with the setting)\n\nShould we really document this? It seems to me that if you're on AWS S3 not having the cool down is a risk in 100% of cases. If we document it, those that this functionality is intended to protect might opt to turn it off to \"speed things up\"?\nMaybe just document the waiting but not the setting?", "author": "original-brownbear", "createdAt": "2020-01-16T16:52:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzM4Nzc3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU2ODU1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367568551", "bodyText": "I think we must document how users can safely speed it up (i.e. by moving to a new repo or deleting all their legacy snapshots). I'm ok with not documenting the setting itself - we already have form for leaving dangerous settings undocumented (see MergePolicyConfig for instance). Let's add this reasoning to its Javadoc along with explicit instructions not to adjust it and instead to move to a new repo or delete all the legacy snapshots, to deal with the inevitable user who comes across it in the source code.\nCan we also mention {@link Version#V_7_6_0} in the Javadoc so we get a reminder to remove this in v9?", "author": "DaveCTurner", "createdAt": "2020-01-16T18:03:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzM4Nzc3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3MTA5MA==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367671090", "bodyText": "Alright, added docs to the setting, a link to 7.6, an explanatory log message and a test in f9047d7 :)", "author": "original-brownbear", "createdAt": "2020-01-16T21:56:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzM4Nzc3OA=="}], "type": "inlineReview"}, {"oid": "d0ffb44bc4ba1c54745913bcd61367c13573178f", "url": "https://github.com/elastic/elasticsearch/commit/d0ffb44bc4ba1c54745913bcd61367c13573178f", "message": "Merge remote-tracking branch 'elastic/master' into s3-cooldown", "committedDate": "2020-01-16T20:04:53Z", "type": "commit"}, {"oid": "f9047d7738f432dea04cd21efb1128d1686fb1be", "url": "https://github.com/elastic/elasticsearch/commit/f9047d7738f432dea04cd21efb1128d1686fb1be", "message": "test + docs", "committedDate": "2020-01-16T21:53:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3Mjg4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367672883", "bodyText": "This is admittedly quite the hacky test and it takes 2x 5s of hard waits to verify behaviour.\nWe could create a cleaner test by adding some BwC test infrastructure to the S3 plugin tests but I'm not sure it's worth the complexity. Also, running a real rest test to verify the timing here makes the test even more prone to run into random CI slowness and fail in the last step that verifies no waiting is happening when moving to a repo without any old version snapshot => this seemed like the least bad option to me.\nWe are using a O(5s) hard timeout in some other repo IO-timeout tests and so far that hasn't failed us due to CI running into a longer pause so I'm hopeful this will be stable.", "author": "original-brownbear", "createdAt": "2020-01-16T22:00:58Z", "path": "plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreRepositoryTests.java", "diffHunk": "@@ -92,6 +111,41 @@ protected Settings nodeSettings(int nodeOrdinal) {\n             .build();\n     }\n \n+    public void testEnforcedCooldownPeriod() throws IOException {", "originalCommit": "f9047d7738f432dea04cd21efb1128d1686fb1be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3ODE2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367978165", "bodyText": "Can we have a SnapshotResiliencyTests with a mock repo that is eventually consistent on actions for X seconds, and then becomes consistent, and then use that one to verify all is going well? Would be a stronger test than this, which just verifies that some sleep is somewhere in place.", "author": "ywelsch", "createdAt": "2020-01-17T14:59:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3Mjg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5NjcxMw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367996713", "bodyText": "Not trivial but doable =>  On it :)", "author": "original-brownbear", "createdAt": "2020-01-17T15:34:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3Mjg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAwMjk0OA==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r368002948", "bodyText": "Argh never mind ... then we'd have to move the cool down logic to BlobStoreRepository. We can't use the mock repository together with the S3 plugin. We also don't really have any mock infrastructure left for S3 so either we test this on the BlobStoreRepository or this requires some infrastructure that combines the mock S3 REST api infra + the snapshot resiliency test infrastructure.\nThink this is worth it, given that this is a stop-gap solution?", "author": "original-brownbear", "createdAt": "2020-01-17T15:46:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3Mjg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQwODYwNw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r368408607", "bodyText": "bummer :/", "author": "ywelsch", "createdAt": "2020-01-20T08:02:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3Mjg4Mw=="}], "type": "inlineReview"}, {"oid": "4f5816701cd953fa16bcec12f14e5834037c6c98", "url": "https://github.com/elastic/elasticsearch/commit/4f5816701cd953fa16bcec12f14e5834037c6c98", "message": "fix test", "committedDate": "2020-01-17T05:43:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3MzgyMw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367973823", "bodyText": "what if we are rejected from the snapshot threadpool? Let's force it onto the threadpool (use AbstractRunnable),  and notify listener as welll on AbstractRunnable.onFaillure", "author": "ywelsch", "createdAt": "2020-01-17T14:51:07Z", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java", "diffHunk": "@@ -186,6 +225,69 @@\n                 storageClass);\n     }\n \n+    /**\n+     * Holds a reference to delayed repository operation {@link Scheduler.Cancellable} so it can be cancelled should the repository be\n+     * closed concurrently.\n+     */\n+    private final AtomicReference<Scheduler.Cancellable> finalizationFuture = new AtomicReference<>();\n+\n+    @Override\n+    public void finalizeSnapshot(SnapshotId snapshotId, ShardGenerations shardGenerations, long startTime, String failure, int totalShards,\n+                                 List<SnapshotShardFailure> shardFailures, long repositoryStateId, boolean includeGlobalState,\n+                                 MetaData clusterMetaData, Map<String, Object> userMetadata, boolean writeShardGens,\n+                                 ActionListener<SnapshotInfo> listener) {\n+        if (writeShardGens == false) {\n+            listener = delayedListener(listener);\n+        }\n+        super.finalizeSnapshot(snapshotId, shardGenerations, startTime, failure, totalShards, shardFailures, repositoryStateId,\n+            includeGlobalState, clusterMetaData, userMetadata, writeShardGens, listener);\n+    }\n+\n+    @Override\n+    public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, boolean writeShardGens, ActionListener<Void> listener) {\n+        if (writeShardGens == false) {\n+            listener = delayedListener(listener);\n+        }\n+        super.deleteSnapshot(snapshotId, repositoryStateId, writeShardGens, listener);\n+    }\n+\n+    /**\n+     * Wraps given listener such that it is executed with a delay of {@link #coolDown} on the snapshot thread-pool after being invoked.\n+     * See {@link #COOLDOWN_PERIOD} for details.\n+     */\n+    private <T> ActionListener<T> delayedListener(ActionListener<T> listener) {\n+        final ActionListener<T> wrappedListener = ActionListener.runBefore(listener, () -> {\n+            final Scheduler.Cancellable cancellable = finalizationFuture.getAndSet(null);\n+            assert cancellable != null;\n+        });\n+        return new ActionListener<>() {\n+            @Override\n+            public void onResponse(T response) {\n+                logCooldownInfo();\n+                final Scheduler.Cancellable existing = finalizationFuture.getAndSet(\n+                    threadPool.schedule(() -> wrappedListener.onResponse(response), coolDown, ThreadPool.Names.SNAPSHOT));", "originalCommit": "4f5816701cd953fa16bcec12f14e5834037c6c98", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk4MTc2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367981769", "bodyText": "what if we are rejected from the snapshot threadpool?\n\nI think that's impossible unless the snapshot pool is shutting down (in which case it's kinda irrelevant what we do anyway I guess). No other action will go onto the snapshot pool until this listener is resolved (because the snapshot or delete in progress in the CS will prevent anything else from running + we specifically made it so that no steps in the snapshot operations runs on the SNAPSHOT pool before we checked the CS to avoid any deadlocks here).", "author": "original-brownbear", "createdAt": "2020-01-17T15:05:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3MzgyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAxNjY0OA==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r368016648", "bodyText": "I wouldn't bet my life on that (think e.g. about master failover). Let's make this safe.", "author": "ywelsch", "createdAt": "2020-01-17T16:13:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3MzgyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3NTMxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367975315", "bodyText": "Do we need to log this at warn level?", "author": "ywelsch", "createdAt": "2020-01-17T14:53:52Z", "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java", "diffHunk": "@@ -210,4 +312,14 @@ protected BlobStore getBlobStore() {\n     protected ByteSizeValue chunkSize() {\n         return chunkSize;\n     }\n+\n+    @Override\n+    protected void doClose() {\n+        final Scheduler.Cancellable cancellable = finalizationFuture.getAndSet(null);\n+        if (cancellable != null) {\n+            logger.warn(\"Repository closed during cooldown period\");", "originalCommit": "4f5816701cd953fa16bcec12f14e5834037c6c98", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk4NzI2MA==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367987260", "bodyText": "I figured this should be somewhat visible, it's not great if this happens because the next master will not start waiting again. This may be something we want to add but I figured it might not be worth the extra complication because a master failover won't be instant (since the current master must have worked fine to set safe and pending generation equal before getting to the wait) so that \"wait' might be good enough?\n... retracted see below", "author": "original-brownbear", "createdAt": "2020-01-17T15:16:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3NTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE2OTUwMw==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r368169503", "bodyText": "Actually the situation here is better than I described above ... since if we're running into this we're always re-running the last step of the delete or snapshot operation on the next master (and will fail there because the repository generation has already moved) which will trigger another wait period. So this isn't a bad spot at all :) => moving this to DEBUG.", "author": "original-brownbear", "createdAt": "2020-01-17T22:54:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3NTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3ODY3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367978675", "bodyText": "I wonder if there are situations where ThreadPool.schedule will return before the time value specified. This would then fail here.", "author": "ywelsch", "createdAt": "2020-01-17T14:59:55Z", "path": "plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreRepositoryTests.java", "diffHunk": "@@ -92,6 +111,41 @@ protected Settings nodeSettings(int nodeOrdinal) {\n             .build();\n     }\n \n+    public void testEnforcedCooldownPeriod() throws IOException {\n+        final String repoName = createRepository(randomName(), Settings.builder().put(repositorySettings())\n+            .put(S3Repository.COOLDOWN_PERIOD.getKey(), TEST_COOLDOWN_PERIOD).build());\n+\n+        final SnapshotId fakeOldSnapshot = client().admin().cluster().prepareCreateSnapshot(repoName, \"snapshot-old\")\n+            .setWaitForCompletion(true).setIndices().get().getSnapshotInfo().snapshotId();\n+        final RepositoriesService repositoriesService = internalCluster().getCurrentMasterNodeInstance(RepositoriesService.class);\n+        final BlobStoreRepository repository = (BlobStoreRepository) repositoriesService.repository(repoName);\n+        final RepositoryData repositoryData =\n+            PlainActionFuture.get(f -> repository.threadPool().generic().execute(() -> repository.getRepositoryData(f)));\n+        final RepositoryData modifiedRepositoryData = repositoryData.withVersions(Collections.singletonMap(fakeOldSnapshot,\n+            SnapshotsService.SHARD_GEN_IN_REPO_DATA_VERSION.minimumCompatibilityVersion()));\n+        final BytesReference serialized =\n+            BytesReference.bytes(modifiedRepositoryData.snapshotsToXContent(XContentFactory.jsonBuilder(), false));\n+        PlainActionFuture.get(f -> repository.threadPool().generic().execute(ActionRunnable.run(f, () -> {\n+            try (InputStream stream = serialized.streamInput()) {\n+                repository.blobStore().blobContainer(repository.basePath()).writeBlobAtomic(\n+                    BlobStoreRepository.INDEX_FILE_PREFIX + modifiedRepositoryData.getGenId(), stream, serialized.length(), true);\n+            }\n+        })));\n+\n+        final String newSnapshotName = \"snapshot-new\";\n+        final long beforeThrottledSnapshot = repository.threadPool().relativeTimeInNanos();\n+        client().admin().cluster().prepareCreateSnapshot(repoName, newSnapshotName).setWaitForCompletion(true).setIndices().get();\n+        assertThat(repository.threadPool().relativeTimeInNanos() - beforeThrottledSnapshot, greaterThan(TEST_COOLDOWN_PERIOD.getNanos()));\n+\n+        final long beforeThrottledDelete = repository.threadPool().relativeTimeInNanos();\n+        client().admin().cluster().prepareDeleteSnapshot(repoName, newSnapshotName).get();\n+        assertThat(repository.threadPool().relativeTimeInNanos() - beforeThrottledDelete, greaterThan(TEST_COOLDOWN_PERIOD.getNanos()));\n+\n+        final long beforeFastDelete = repository.threadPool().relativeTimeInNanos();\n+        client().admin().cluster().prepareDeleteSnapshot(repoName, fakeOldSnapshot.getName()).get();\n+        assertThat(repository.threadPool().relativeTimeInNanos() - beforeFastDelete, lessThan(TEST_COOLDOWN_PERIOD.getNanos()));", "originalCommit": "4f5816701cd953fa16bcec12f14e5834037c6c98", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5NjIzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/51074#discussion_r367996235", "bodyText": "I wonder if there are situations where ThreadPool.schedule will return before the time value specified.\n\nI figured that's impossible since I turned off the timestamp cache? I think otherwise the underlying primitives in ThreadPoolExecutor are accurate (at least on Linux).\n\nThis would then fail here.\n\nAnd rightfully so?\n=> that said :) ... let me see about the suggested test via the resiliency tests", "author": "original-brownbear", "createdAt": "2020-01-17T15:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk3ODY3NQ=="}], "type": "inlineReview"}, {"oid": "f9ad026e9105c8f6464e6771184ca219b174ef36", "url": "https://github.com/elastic/elasticsearch/commit/f9ad026e9105c8f6464e6771184ca219b174ef36", "message": "Merge remote-tracking branch 'elastic/master' into s3-cooldown", "committedDate": "2020-01-17T15:21:39Z", "type": "commit"}, {"oid": "9f1c00d56d2af7fd72338fee56d48870f81f42f3", "url": "https://github.com/elastic/elasticsearch/commit/9f1c00d56d2af7fd72338fee56d48870f81f42f3", "message": "CR comments", "committedDate": "2020-01-17T22:58:50Z", "type": "commit"}, {"oid": "88ffa99cdb51188c29a96f513f2b455039f2d977", "url": "https://github.com/elastic/elasticsearch/commit/88ffa99cdb51188c29a96f513f2b455039f2d977", "message": "Merge remote-tracking branch 'elastic/master' into s3-cooldown", "committedDate": "2020-01-19T13:45:54Z", "type": "commit"}]}