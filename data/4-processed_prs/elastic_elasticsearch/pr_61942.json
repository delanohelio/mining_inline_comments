{"pr_number": 61942, "pr_title": "[DOCS] Document shard sizing guide", "pr_createdAt": "2020-09-03T17:26:58Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/61942", "timeline": [{"oid": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "url": "https://github.com/elastic/elasticsearch/commit/7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "message": "[DOCS] Document shard sizing guide\n\nRevises the current 'How to avoid oversharding' docs to incorporate\ninformation from our [shard sizing blog post][0].\n\nChanges:\n\n* Streamlines introduction\n* Adds \"Things to remember\" section to describe how shards work\n* Adds \"Guidelines\" section based on blog tips\n* Creates a \"Fix an oversharded cluster\" section\n\n[0]: https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster", "committedDate": "2020-09-03T21:41:11Z", "type": "commit"}, {"oid": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "url": "https://github.com/elastic/elasticsearch/commit/7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "message": "[DOCS] Document shard sizing guide\n\nRevises the current 'How to avoid oversharding' docs to incorporate\ninformation from our [shard sizing blog post][0].\n\nChanges:\n\n* Streamlines introduction\n* Adds \"Things to remember\" section to describe how shards work\n* Adds \"Guidelines\" section based on blog tips\n* Creates a \"Fix an oversharded cluster\" section\n\n[0]: https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster", "committedDate": "2020-09-03T21:41:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjA0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r483276047", "bodyText": "Note for reviewers:\nI'm not sure how much we want to emphasize this now that we've reduced heap usage:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.7/release-highlights.html#_significant_reduction_of_heap_usage_of_segments", "author": "jrodewig", "createdAt": "2020-09-03T21:58:33Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,308 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.", "originalCommit": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4MjE4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488782189", "bodyText": "IMO this section still applies. We fail less easily under many small shards than we used to, but we still like large shards better.", "author": "jpountz", "createdAt": "2020-09-15T15:59:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjMxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r483276315", "bodyText": "Note for reviewers:\nNot sure if this guideline still hold now that heap usage is reduced:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.7/release-highlights.html#_significant_reduction_of_heap_usage_of_segments\nAny updated guidance is appreciated!", "author": "jrodewig", "createdAt": "2020-09-03T21:59:15Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,308 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+Data streams let you store time series data across multiple, time-based backing\n+indices. You can use <<index-lifecycle-management,{ilm} ({ilm-init})>>\n+lifecycle policies to automatically manage these backing indices.\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase the rollover threshold in your {ilm-init} policy.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the cat shards API to check the number of\n+shards per node.", "originalCommit": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1d2dc0a5b2b3932c49dc7c28c401e0c1cb8b380c", "url": "https://github.com/elastic/elasticsearch/commit/1d2dc0a5b2b3932c49dc7c28c401e0c1cb8b380c", "message": "Add more xrefs", "committedDate": "2020-09-04T16:32:59Z", "type": "commit"}, {"oid": "71aefc7ac70499026754294f752dddf680c2452c", "url": "https://github.com/elastic/elasticsearch/commit/71aefc7ac70499026754294f752dddf680c2452c", "message": "DS xref + ILM image", "committedDate": "2020-09-04T18:56:26Z", "type": "commit"}, {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "url": "https://github.com/elastic/elasticsearch/commit/5243b9492e79506bff6b1d9d4b879810ff754ef8", "message": "Merge branch 'master' into docs__sharding-strategy", "committedDate": "2020-09-15T15:42:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc3NjI0MA==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488776240", "bodyText": "I wonder if we should say stability rather than performance, since this is more commonly the problem.", "author": "jpountz", "createdAt": "2020-09-15T15:50:53Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMjQxNg==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488832416", "bodyText": "Thanks for the feedback. Updated with ce5f86c.", "author": "jrodewig", "createdAt": "2020-09-15T17:19:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc3NjI0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4Mzk2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488783961", "bodyText": "I wonder if we want to simplify this messaging with the work we're doing on data tiers. cc @dakrone", "author": "jpountz", "createdAt": "2020-09-15T16:01:55Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4NDI4Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488984287", "bodyText": "Simplified this a bit with 1f856d2. Would still like feedback from @dakrone  or @debadair. I'll also add a xref when the data tier docs are live.", "author": "jrodewig", "createdAt": "2020-09-15T21:30:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4Mzk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NDY5OA==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488784698", "bodyText": "isn't 'lifecycle' redundant here?", "author": "jpountz", "createdAt": "2020-09-15T16:03:00Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMjc1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488832757", "bodyText": "Removed with ce5f86c.", "author": "jrodewig", "createdAt": "2020-09-15T17:20:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NDY5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NzU1MA==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488787550", "bodyText": "I wouldn't insist on \"primary\" shards since primaries and replicas perform the same amount of work, except in the update case, which is not typical for hot-warm architectures. I'd say something like \"if a single node contains many shards for an index with a high indexing volume\" instead.", "author": "jpountz", "createdAt": "2020-09-15T16:07:04Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMzc2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488833763", "bodyText": "Updated the wording with 6a2277b.", "author": "jrodewig", "createdAt": "2020-09-15T17:21:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NzU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc5MDE0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488790149", "bodyText": "I suspect we'd want a much lower value in this example, since we're talking about the maximum number of shards of a single index on a node. E.g. 5 or 10.", "author": "jpountz", "createdAt": "2020-09-15T16:10:53Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzNDA0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488834041", "bodyText": "Good catch. Updated with ce5f86c", "author": "jrodewig", "createdAt": "2020-09-15T17:21:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc5MDE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgyNjI0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488826243", "bodyText": "This is a slower, more resource-intensive version of shrinking, so I wonder if we should mention it at all.", "author": "jpountz", "createdAt": "2020-09-15T17:08:48Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400\n+  }\n+}\n+--------------------------------------------------\n+// TEST[setup:my_index]\n+\n+\n+[discrete]\n+[[fix-an-oversharded-cluster]]\n+=== Fix an oversharded cluster\n+\n+If your cluster is experiencing performance issues due to oversharded indices,\n+you can use one or more of the following methods to fix them.\n+\n+[discrete]\n+[[reindex-indices-from-shorter-periods-into-longer-periods]]\n+==== Create time-based indices that cover longer periods\n+\n+For time series data, you can create indices that cover longer time intervals.\n+For example, instead of daily indices, you can create indices on a monthly or\n+yearly basis.\n+\n+If you're using {ilm-init}, you can do this by increasing the `max_age`\n+threshold for the <<ilm-rollover,rollover action>>.\n+\n+[discrete]\n+[[delete-empty-indices]]\n+==== Delete empty or unneeded indices\n+\n+If you're using {ilm-init} and roll over indices based on a `max_age` threshold,\n+you can inadvertently create indices with no documents. These empty indices\n+provide no benefit but still consume resources.\n+\n+You can find these empty indices using the <<cat-count,cat count API>>.\n+\n+[source,console]\n+----\n+GET /_cat/count/my-index-000001?v\n+----\n+// TEST[setup:my_index]\n+\n+Once you have a list of empty indices, you can delete them using the\n+<<indices-delete-index,delete index API>>. You can also delete any other\n+unneeded indices.\n+\n+[source,console]\n+----\n+DELETE /my-index-*\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[force-merge-during-off-peak-hours]]\n+==== Force merge during off-peak hours\n+\n+If you no longer write to an index, you can use the <<indices-forcemerge,force\n+merge API>> to <<index-modules-merge,merge>> smaller segments into larger ones.\n+This can reduce shard overhead and improve search speeds. However, force merges\n+are resource-intensive. If possible, run the force merge during off-peak hours.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_forcemerge\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[shrink-existing-index-to-fewer-shards]]\n+==== Shrink an existing index to fewer shards\n+\n+If you no longer write to an index, you can use the\n+<<indices-shrink-index,shrink index API>> to reduce its shard count.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_shrink/my-shrunken-index-000001\n+----\n+// TEST[s/^/PUT my-index-000001\\n{\"settings\":{\"index.number_of_shards\":2,\"blocks.write\":true}}\\n/]\n+\n+{ilm-init} also has a <<ilm-shrink-action,shrink action>> for indices in the\n+warm phase.\n+\n+[discrete]\n+[[reindex-an-existing-index-to-fewer-shards]]\n+==== Reindex to an index with fewer shards\n+\n+You can use the <<docs-reindex,reindex API>> to reindex data from an oversharded\n+index to an index with fewer shards. After the reindex, delete the oversharded\n+index.\n+\n+[source,console]\n+----\n+POST /_reindex\n+{\n+  \"source\": {\n+    \"index\": \"my-oversharded-index-000001\"\n+  },\n+  \"dest\": {\n+    \"index\": \"my-new-index-000001\"\n+  }\n+}\n+----\n+// TEST[s/^/PUT my-oversharded-index-000001\\n/]", "originalCommit": "5243b9492e79506bff6b1d9d4b879810ff754ef8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzNDUxNg==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488834516", "bodyText": "+1. Removed with ce5f86c.", "author": "jrodewig", "createdAt": "2020-09-15T17:21:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgyNjI0Mw=="}], "type": "inlineReview"}, {"oid": "ce5f86c4a9f483461c39672bf03a2e26385a8036", "url": "https://github.com/elastic/elasticsearch/commit/ce5f86c4a9f483461c39672bf03a2e26385a8036", "message": "Address feedback", "committedDate": "2020-09-15T17:18:38Z", "type": "commit"}, {"oid": "6a2277bdb70dd6f7a165fe24206e143efa667e85", "url": "https://github.com/elastic/elasticsearch/commit/6a2277bdb70dd6f7a165fe24206e143efa667e85", "message": "reword", "committedDate": "2020-09-15T17:20:50Z", "type": "commit"}, {"oid": "1f856d20c1f8be44d3ce8ca4bd2586a371b6e07c", "url": "https://github.com/elastic/elasticsearch/commit/1f856d20c1f8be44d3ce8ca4bd2586a371b6e07c", "message": "Updates for data tiers", "committedDate": "2020-09-15T21:27:47Z", "type": "commit"}, {"oid": "b0009b0efb7028a922d35020291369a46dc3c09c", "url": "https://github.com/elastic/elasticsearch/commit/b0009b0efb7028a922d35020291369a46dc3c09c", "message": "Reword", "committedDate": "2020-09-15T21:33:54Z", "type": "commit"}, {"oid": "fa05573d16b91bbd5e1199e043c971c572ae94b1", "url": "https://github.com/elastic/elasticsearch/commit/fa05573d16b91bbd5e1199e043c971c572ae94b1", "message": "reword for consistency", "committedDate": "2020-09-16T18:40:52Z", "type": "commit"}, {"oid": "981f02e40962621727713a34c20fcc2c3eff075f", "url": "https://github.com/elastic/elasticsearch/commit/981f02e40962621727713a34c20fcc2c3eff075f", "message": "reword", "committedDate": "2020-09-16T18:42:03Z", "type": "commit"}, {"oid": "195b185680bf1795fb6935028180038154b0e5bf", "url": "https://github.com/elastic/elasticsearch/commit/195b185680bf1795fb6935028180038154b0e5bf", "message": "reword", "committedDate": "2020-09-16T19:06:47Z", "type": "commit"}, {"oid": "1dd0fc7f2e8f89c06effecf403acb3b9f559693a", "url": "https://github.com/elastic/elasticsearch/commit/1dd0fc7f2e8f89c06effecf403acb3b9f559693a", "message": "More wording tweaks", "committedDate": "2020-09-17T00:56:59Z", "type": "commit"}, {"oid": "a7bf9495ffd4b663acdceecf05e23dc47861b624", "url": "https://github.com/elastic/elasticsearch/commit/a7bf9495ffd4b663acdceecf05e23dc47861b624", "message": "reword", "committedDate": "2020-09-17T01:04:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MDc1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490580752", "bodyText": "\"Things to remember\" seems a bit vague. Maybe \"Sizing considerations\" and then use \"things\" in the intro sentence?", "author": "debadair", "createdAt": "2020-09-17T21:47:29Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember", "originalCommit": "a7bf9495ffd4b663acdceecf05e23dc47861b624", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MjA0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490582045", "bodyText": "These feel more like best practices than guidelines.", "author": "debadair", "createdAt": "2020-09-17T21:50:21Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines", "originalCommit": "a7bf9495ffd4b663acdceecf05e23dc47861b624", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MzA0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490583042", "bodyText": "I'd lead with the imperative, \"Use data streams...\"", "author": "debadair", "createdAt": "2020-09-17T21:52:44Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleted documents aren't immediately removed from {es}'s file system.\n+Instead, {es} marks the document as deleted on each related shard. The marked\n+document will continue to use resources until it's removed during a periodic\n+<<index-modules-merge,segment merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}", "originalCommit": "a7bf9495ffd4b663acdceecf05e23dc47861b624", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NDQ5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490584497", "bodyText": "Seems like this should recommend using criteria other than the time period.", "author": "debadair", "createdAt": "2020-09-17T21:56:07Z", "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleted documents aren't immediately removed from {es}'s file system.\n+Instead, {es} marks the document as deleted on each related shard. The marked\n+document will continue to use resources until it's removed during a periodic\n+<<index-modules-merge,segment merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the node's shards across the data tier's\n+remaining nodes. Shards larger than 50GB can be harder to move across a network\n+and may tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap memory\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap memory>>. For example, a node with 30GB of heap memory should\n+have at most 600 shards. The further below this limit you can keep your nodes,\n+the better. If you find your nodes exceeding more than 20 shards per GB,\n+consider adding another node. You can use the <<cat-shards,cat shards API>> to\n+check the number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If too many shards are allocated to a specific node, the node can become a\n+hotspot. For example, if a single node contains too many shards for an index\n+with a high indexing volume, the node is likely to have issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 5\n+  }\n+}\n+--------------------------------------------------\n+// TEST[setup:my_index]\n+\n+\n+[discrete]\n+[[fix-an-oversharded-cluster]]\n+=== Fix an oversharded cluster\n+\n+If your cluster is experiencing stability issues due to oversharded indices,\n+you can use one or more of the following methods to fix them.\n+\n+[discrete]\n+[[reindex-indices-from-shorter-periods-into-longer-periods]]\n+==== Create time-based indices that cover longer periods\n+\n+For time series data, you can create indices that cover longer time intervals.", "originalCommit": "a7bf9495ffd4b663acdceecf05e23dc47861b624", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "944f599f71f1b54de909fa92213a12a5be47dfe5", "url": "https://github.com/elastic/elasticsearch/commit/944f599f71f1b54de909fa92213a12a5be47dfe5", "message": "Review feedback", "committedDate": "2020-09-18T13:36:25Z", "type": "commit"}, {"oid": "50a5e78238a9e67b65e096ee401e9b6e2cf5ec1b", "url": "https://github.com/elastic/elasticsearch/commit/50a5e78238a9e67b65e096ee401e9b6e2cf5ec1b", "message": "Reword", "committedDate": "2020-09-18T13:37:27Z", "type": "commit"}, {"oid": "a301c44d642f93a6a7fd318ccf345a44e2e5da5a", "url": "https://github.com/elastic/elasticsearch/commit/a301c44d642f93a6a7fd318ccf345a44e2e5da5a", "message": "word", "committedDate": "2020-09-18T13:37:59Z", "type": "commit"}]}