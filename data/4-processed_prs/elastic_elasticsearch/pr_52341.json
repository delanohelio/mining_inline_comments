{"pr_number": 52341, "pr_title": "Add Caching for RepositoryData in BlobStoreRepository", "pr_createdAt": "2020-02-13T21:15:46Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/52341", "timeline": [{"oid": "f6833a8c237206ca6a00192792e4f86ae84b394a", "url": "https://github.com/elastic/elasticsearch/commit/f6833a8c237206ca6a00192792e4f86ae84b394a", "message": "Add Caching for RepositoryData in BlobStoreRepository\n\nWIP", "committedDate": "2020-02-13T21:14:40Z", "type": "commit"}, {"oid": "3f198b1b73aa3242f7278ae2ba9079e3a03cf1a9", "url": "https://github.com/elastic/elasticsearch/commit/3f198b1b73aa3242f7278ae2ba9079e3a03cf1a9", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-14T07:42:13Z", "type": "commit"}, {"oid": "246d5ee447f0ed59c548bf3757527ffd39ceb2d9", "url": "https://github.com/elastic/elasticsearch/commit/246d5ee447f0ed59c548bf3757527ffd39ceb2d9", "message": "docs + safer", "committedDate": "2020-02-14T08:10:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwMjYwNw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r379302607", "bodyText": "Just fixing a bug here that prevented node restarts in tests for nodes that used secure settings. If we always return the same SecureString for getString then if a node ever closes that SecureString it couldn't get the setting again.", "author": "original-brownbear", "createdAt": "2020-02-14T08:29:22Z", "path": "test/framework/src/main/java/org/elasticsearch/common/settings/MockSecureSettings.java", "diffHunk": "@@ -36,7 +36,7 @@\n  */\n public class MockSecureSettings implements SecureSettings {\n \n-    private Map<String, SecureString> secureStrings = new HashMap<>();\n+    private Map<String, String> secureStrings = new HashMap<>();", "originalCommit": "246d5ee447f0ed59c548bf3757527ffd39ceb2d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNDAyMw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r379304023", "bodyText": "This is admittedly pretty dirty now test-wise. Obviously, it also illustrates that this change slightly weakens our ability to detect concurrent repository writes from multiple clusters.\nIMO, this is a fair trade-off though and one we've already been making over and over in other spots (anytime we optimised away a loading of RepositoryData we weakened the ability to detect concurrent load obviously).", "author": "original-brownbear", "createdAt": "2020-02-14T08:33:10Z", "path": "server/src/test/java/org/elasticsearch/snapshots/CorruptedBlobStoreRepositoryIT.java", "diffHunk": "@@ -356,4 +367,9 @@ private void assertRepositoryBlocked(Client client, String repo, String existing\n         assertThat(repositoryException4.getMessage(),\n             containsString(\"Could not read repository data because the contents of the repository do not match its expected state.\"));\n     }\n+\n+    private void fullRestart() throws Exception {", "originalCommit": "246d5ee447f0ed59c548bf3757527ffd39ceb2d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b5d135470c1f6b1b31b0f1e94a1ded21bf9baff1", "url": "https://github.com/elastic/elasticsearch/commit/b5d135470c1f6b1b31b0f1e94a1ded21bf9baff1", "message": "shorter", "committedDate": "2020-02-14T08:39:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMzMTkzMA==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r379331930", "bodyText": "This is a little dirty and we talked about it in other PRs .. the generation here is -1 due to the weird way the repository data is loaded initially and we have to eventually set it. I'll clean that up in a follow up, for now just setting it here where it matters (it doesn't matter in the above code when serializing filteredRepositoryData to the repo) seemed the driest.", "author": "original-brownbear", "createdAt": "2020-02-14T09:37:34Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1359,6 +1398,7 @@ public void onFailure(String source, Exception e) {\n \n                     @Override\n                     public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+                        cacheRepositoryData(filteredRepositoryData.withGenId(newGen));", "originalCommit": "b5d135470c1f6b1b31b0f1e94a1ded21bf9baff1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTM2MjY1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381362656", "bodyText": "Maybe add a TODO then?", "author": "tlrx", "createdAt": "2020-02-19T15:51:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMzMTkzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ2OTQ3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381469477", "bodyText": "Not sure what to even put in it though. In the end, this is simply the way RepositoryData works for now. We have the same kind of code for the cluster state version as well I guess. I don't have a good idea for a better abstract yet :(", "author": "original-brownbear", "createdAt": "2020-02-19T18:41:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMzMTkzMA=="}], "type": "inlineReview"}, {"oid": "7abac783bfb90a85efd792a634da62b70abc2f72", "url": "https://github.com/elastic/elasticsearch/commit/7abac783bfb90a85efd792a634da62b70abc2f72", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-17T14:52:22Z", "type": "commit"}, {"oid": "dff276e33aac592ee1bf014a7f31be827cbbec25", "url": "https://github.com/elastic/elasticsearch/commit/dff276e33aac592ee1bf014a7f31be827cbbec25", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-19T09:38:01Z", "type": "commit"}, {"oid": "7a2ecf6866fa048467c3c9a1e8e8d459fbb8fdf0", "url": "https://github.com/elastic/elasticsearch/commit/7a2ecf6866fa048467c3c9a1e8e8d459fbb8fdf0", "message": "Size Limit Cache and Intern Strings", "committedDate": "2020-02-19T11:07:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIyNjczOQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381226739", "bodyText": "Moving to interning here and for the snapshot id actually makes the on heap RepositoryData about the same size as the serialized one (unfortunately for the time being we serialize it as json uncompressed) => I think the safety check for 500kb I added is valid.", "author": "original-brownbear", "createdAt": "2020-02-19T11:11:53Z", "path": "server/src/main/java/org/elasticsearch/repositories/IndexId.java", "diffHunk": "@@ -42,16 +42,14 @@\n     private final int hashCode;\n \n     public IndexId(final String name, final String id) {\n-        this.name = name;\n-        this.id = id;\n+        this.name = name.intern();", "originalCommit": "7a2ecf6866fa048467c3c9a1e8e8d459fbb8fdf0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI3NzUxNA==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381277514", "bodyText": "I would prefer to avoid JVM String interning. If we can deduplicate the Strings ourselves, that's fine, but adding more and more stuff to be internalized feels dangerous (another source of OOM).", "author": "ywelsch", "createdAt": "2020-02-19T13:06:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIyNjczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIzMDYwNg==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381230606", "bodyText": "For context:\non heap 1k shards in a snapshot means about 25kb of shard generations which themselves make up ~50% of the size of RepositoryData on heap with the interning changes I added.\nThe number of snapshots doesn't matter much by comparison. So in the Cloud case of 100 snapshots, this would allow for caching snapshots of up to ~ 15k shards which should work fine for most users I'm assuming.", "author": "original-brownbear", "createdAt": "2020-02-19T11:20:16Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1116,6 +1135,44 @@ public void getRepositoryData(ActionListener<RepositoryData> listener) {\n         }\n     }\n \n+    /**\n+     * Puts the given {@link RepositoryData} into the cache if it is of a newer generation and only if the repository is not using\n+     * {@link #bestEffortConsistency}. When using {@link #bestEffortConsistency} the repository is using listing to find the latest\n+     * {@code index-N} blob and there are no hard guarantees that a given repository generation won't be reused since an external\n+     * modification can lead to moving from a higher {@code N} to a lower {@code N} value which mean we can't safely assume that a given\n+     * generation will always contain the same {@link RepositoryData}.\n+     *\n+     * @param updated RepositoryData to cache if newer than the cache contents\n+     */\n+    private void cacheRepositoryData(RepositoryData updated) {\n+        if (bestEffortConsistency == false) {\n+            try {\n+                final int len =\n+                    BytesReference.bytes(updated.snapshotsToXContent(XContentFactory.jsonBuilder(), true)).length();\n+                if (len > ByteSizeUnit.KB.toBytes(500)) {", "originalCommit": "7a2ecf6866fa048467c3c9a1e8e8d459fbb8fdf0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "039c0d05323ae4e769c7e89c61feb049051c1d08", "url": "https://github.com/elastic/elasticsearch/commit/039c0d05323ae4e769c7e89c61feb049051c1d08", "message": "intern all the way", "committedDate": "2020-02-19T11:22:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI3OTExNw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381279117", "bodyText": "maybe we can just cache the serialized (and compressed) bytes instead of the object. Decompressing should still be fast.", "author": "ywelsch", "createdAt": "2020-02-19T13:10:04Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -513,10 +514,13 @@ public void deleteSnapshot(SnapshotId snapshotId, long repositoryStateId, Versio\n     private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, BlobMetaData> rootBlobs) {\n         final long generation = latestGeneration(rootBlobs.keySet());\n         final long genToLoad;\n+        final RepositoryData cached;", "originalCommit": "039c0d05323ae4e769c7e89c61feb049051c1d08", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ3NDgxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381474819", "bodyText": "Yea I experimented a little with this and it's really sweet so I moved to that now. The compression ration is massive (more than a factor of 10x for 100 snapshots with 1k shards each) and I can hardly imagine a repo that could break 500kb for the cached size (100 snapshots of 1k shards is only ~20kb and adding additional snapshots is cheap as well since each new snapshot effectively just adds that snapshots uuid + name if it doesn't contain new shards).", "author": "original-brownbear", "createdAt": "2020-02-19T18:51:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI3OTExNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI4MTQxMw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381281413", "bodyText": "If the node can't serialize the RepositoryData, it would die here. Perhaps just catch and log (while having an assert as well for our tests)", "author": "ywelsch", "createdAt": "2020-02-19T13:14:35Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1116,6 +1135,44 @@ public void getRepositoryData(ActionListener<RepositoryData> listener) {\n         }\n     }\n \n+    /**\n+     * Puts the given {@link RepositoryData} into the cache if it is of a newer generation and only if the repository is not using\n+     * {@link #bestEffortConsistency}. When using {@link #bestEffortConsistency} the repository is using listing to find the latest\n+     * {@code index-N} blob and there are no hard guarantees that a given repository generation won't be reused since an external\n+     * modification can lead to moving from a higher {@code N} to a lower {@code N} value which mean we can't safely assume that a given\n+     * generation will always contain the same {@link RepositoryData}.\n+     *\n+     * @param updated RepositoryData to cache if newer than the cache contents\n+     */\n+    private void cacheRepositoryData(RepositoryData updated) {\n+        if (bestEffortConsistency == false) {\n+            try {\n+                final int len =\n+                    BytesReference.bytes(updated.snapshotsToXContent(XContentFactory.jsonBuilder(), true)).length();\n+                if (len > ByteSizeUnit.KB.toBytes(500)) {\n+                    logger.debug(\"Not caching repository data of size [{}] for repository [{}] because it is larger than 500KB in\" +\n+                        \" serialized size\", len, metadata.name());\n+                    if (len > ByteSizeUnit.MB.toBytes(5)) {\n+                        logger.warn(\"Your repository metadata blob for repository [{}] is larger than 5MB. Consider moving to a fresh\" +\n+                            \" repository for new snapshots or deleting unneeded snapshots from your repository to ensure stable\" +\n+                            \" repository behavior going forward.\", metadata.name());\n+                    }\n+                    // Set empty repository data to not waste heap for an outdated cached value\n+                    latestKnownRepositoryData.set(RepositoryData.EMPTY);\n+                    return;\n+                }\n+            } catch (IOException e) {\n+                throw new AssertionError(\"Impossible, no IO happens here\", e);", "originalCommit": "039c0d05323ae4e769c7e89c61feb049051c1d08", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ3NTQxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381475419", "bodyText": "Sure thing, though I figured this is actually impossible because the same code must have serialized that repository data to even make it visible here (since we only cache what we previously wrote to the repo).", "author": "original-brownbear", "createdAt": "2020-02-19T18:52:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTI4MTQxMw=="}], "type": "inlineReview"}, {"oid": "9c060f9895214b003cb2313a2afc0b0fd8d13ce3", "url": "https://github.com/elastic/elasticsearch/commit/9c060f9895214b003cb2313a2afc0b0fd8d13ce3", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-19T15:19:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTM2MTkyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381361925", "bodyText": "Maybe make RepositoryData implements Accountable? Should we craft an estimating function instead of serializing the whole RepositoryData back again? If we were serializing back to XContent we could pass a FilterOutputStream to the XContentBuilder so that it just count bytes and not build everything in heap.\nOr if we follow Yannick's suggestion on caching the serialized and compressed bytes then maybe we should keep track of the length of the original blob.", "author": "tlrx", "createdAt": "2020-02-19T15:50:42Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1116,6 +1135,44 @@ public void getRepositoryData(ActionListener<RepositoryData> listener) {\n         }\n     }\n \n+    /**\n+     * Puts the given {@link RepositoryData} into the cache if it is of a newer generation and only if the repository is not using\n+     * {@link #bestEffortConsistency}. When using {@link #bestEffortConsistency} the repository is using listing to find the latest\n+     * {@code index-N} blob and there are no hard guarantees that a given repository generation won't be reused since an external\n+     * modification can lead to moving from a higher {@code N} to a lower {@code N} value which mean we can't safely assume that a given\n+     * generation will always contain the same {@link RepositoryData}.\n+     *\n+     * @param updated RepositoryData to cache if newer than the cache contents\n+     */\n+    private void cacheRepositoryData(RepositoryData updated) {\n+        if (bestEffortConsistency == false) {\n+            try {\n+                final int len =\n+                    BytesReference.bytes(updated.snapshotsToXContent(XContentFactory.jsonBuilder(), true)).length();", "originalCommit": "039c0d05323ae4e769c7e89c61feb049051c1d08", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ3NTU2OA==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381475568", "bodyText": "Jup, went with Yannicks approach now :)", "author": "original-brownbear", "createdAt": "2020-02-19T18:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTM2MTkyNQ=="}], "type": "inlineReview"}, {"oid": "44ba2707de568f2354a6c5461b10019e5405ee45", "url": "https://github.com/elastic/elasticsearch/commit/44ba2707de568f2354a6c5461b10019e5405ee45", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-19T16:55:23Z", "type": "commit"}, {"oid": "520716e419104af476d2b37fe3e849afc9378942", "url": "https://github.com/elastic/elasticsearch/commit/520716e419104af476d2b37fe3e849afc9378942", "message": "Cache compressed serialized", "committedDate": "2020-02-19T18:36:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4Nzg1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381887857", "bodyText": "I think I would prefer an explicit undocumented setting to disable the cache. This might also turn out to be useful if we see any issues with this new functionality", "author": "ywelsch", "createdAt": "2020-02-20T09:45:28Z", "path": "plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreRepositoryTests.java", "diffHunk": "@@ -132,6 +132,8 @@ public void testEnforcedCooldownPeriod() throws IOException {\n             }\n         })));\n \n+        // Master failover to clear RepositoryData cache", "originalCommit": "520716e419104af476d2b37fe3e849afc9378942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyNTM5OA==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381925398", "bodyText": "Yea that's much nicer indeed :) added that setting now and used it in tests.", "author": "original-brownbear", "createdAt": "2020-02-20T10:54:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4Nzg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4ODgxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381888815", "bodyText": "Should cached.v2() always be non-null if cached != null?", "author": "ywelsch", "createdAt": "2020-02-20T09:47:04Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -529,6 +536,9 @@ private RepositoryData safeRepositoryData(long repositoryStateId, Map<String, Bl\n             throw new RepositoryException(metadata.name(), \"concurrent modification of the index-N file, expected current generation [\" +\n                 repositoryStateId + \"], actual current generation [\" + genToLoad + \"]\");\n         }\n+        if (cached != null && cached.v1() == genToLoad && cached.v2() != null) {", "originalCommit": "520716e419104af476d2b37fe3e849afc9378942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyNTQ1OA==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381925458", "bodyText": "Jup now it is.", "author": "original-brownbear", "createdAt": "2020-02-20T10:54:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4ODgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4OTAxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381889019", "bodyText": "perhaps just initialize to null?", "author": "ywelsch", "createdAt": "2020-02-20T09:47:29Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1057,6 +1067,10 @@ public void endVerification(String seed) {\n     // and concurrent modifications.\n     private final AtomicLong latestKnownRepoGen = new AtomicLong(RepositoryData.UNKNOWN_REPO_GEN);\n \n+    // Best effort cache of the latest known repository data and its generation, cached serialized as compressed json\n+    private final AtomicReference<Tuple<Long, BytesReference>> latestKnownRepositoryData =\n+        new AtomicReference<>(new Tuple<>(RepositoryData.EMPTY_REPO_GEN, null));", "originalCommit": "520716e419104af476d2b37fe3e849afc9378942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyNTQ5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381925499", "bodyText": "++", "author": "original-brownbear", "createdAt": "2020-02-20T10:54:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg4OTAxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg5MTE3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381891177", "bodyText": "just set to null?", "author": "ywelsch", "createdAt": "2020-02-20T09:51:17Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1116,6 +1139,62 @@ public void getRepositoryData(ActionListener<RepositoryData> listener) {\n         }\n     }\n \n+    /**\n+     * Puts the given {@link RepositoryData} into the cache if it is of a newer generation and only if the repository is not using\n+     * {@link #bestEffortConsistency}. When using {@link #bestEffortConsistency} the repository is using listing to find the latest\n+     * {@code index-N} blob and there are no hard guarantees that a given repository generation won't be reused since an external\n+     * modification can lead to moving from a higher {@code N} to a lower {@code N} value which mean we can't safely assume that a given\n+     * generation will always contain the same {@link RepositoryData}.\n+     *\n+     * @param updated RepositoryData to cache if newer than the cache contents\n+     */\n+    private void cacheRepositoryData(RepositoryData updated) {\n+        if (bestEffortConsistency == false) {\n+            final BytesReference serialized;\n+            BytesStreamOutput out = new BytesStreamOutput();\n+            try {\n+                try (StreamOutput tmp = CompressorFactory.COMPRESSOR.streamOutput(out);\n+                     XContentBuilder builder = XContentFactory.jsonBuilder(tmp)) {\n+                    updated.snapshotsToXContent(builder, true);\n+                }\n+                serialized = out.bytes();\n+                final int len = serialized.length();\n+                if (len > ByteSizeUnit.KB.toBytes(500)) {\n+                    logger.debug(\"Not caching repository data of size [{}] for repository [{}] because it is larger than 500KB in\" +\n+                        \" serialized size\", len, metadata.name());\n+                    if (len > ByteSizeUnit.MB.toBytes(5)) {\n+                        logger.warn(\"Your repository metadata blob for repository [{}] is larger than 5MB. Consider moving to a fresh\" +\n+                            \" repository for new snapshots or deleting unneeded snapshots from your repository to ensure stable\" +\n+                            \" repository behavior going forward.\", metadata.name());\n+                    }\n+                    // Set empty repository data to not waste heap for an outdated cached value\n+                    latestKnownRepositoryData.set(new Tuple<>(RepositoryData.EMPTY_REPO_GEN, null));", "originalCommit": "520716e419104af476d2b37fe3e849afc9378942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyNTYyMw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381925623", "bodyText": "++", "author": "original-brownbear", "createdAt": "2020-02-20T10:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg5MTE3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg5MjkzMw==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381892933", "bodyText": "I think this optimization is unnecessary, and complicating the checks on null (see my suggestions above)", "author": "ywelsch", "createdAt": "2020-02-20T09:54:25Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1116,6 +1139,62 @@ public void getRepositoryData(ActionListener<RepositoryData> listener) {\n         }\n     }\n \n+    /**\n+     * Puts the given {@link RepositoryData} into the cache if it is of a newer generation and only if the repository is not using\n+     * {@link #bestEffortConsistency}. When using {@link #bestEffortConsistency} the repository is using listing to find the latest\n+     * {@code index-N} blob and there are no hard guarantees that a given repository generation won't be reused since an external\n+     * modification can lead to moving from a higher {@code N} to a lower {@code N} value which mean we can't safely assume that a given\n+     * generation will always contain the same {@link RepositoryData}.\n+     *\n+     * @param updated RepositoryData to cache if newer than the cache contents\n+     */\n+    private void cacheRepositoryData(RepositoryData updated) {\n+        if (bestEffortConsistency == false) {\n+            final BytesReference serialized;\n+            BytesStreamOutput out = new BytesStreamOutput();\n+            try {\n+                try (StreamOutput tmp = CompressorFactory.COMPRESSOR.streamOutput(out);\n+                     XContentBuilder builder = XContentFactory.jsonBuilder(tmp)) {\n+                    updated.snapshotsToXContent(builder, true);\n+                }\n+                serialized = out.bytes();\n+                final int len = serialized.length();\n+                if (len > ByteSizeUnit.KB.toBytes(500)) {\n+                    logger.debug(\"Not caching repository data of size [{}] for repository [{}] because it is larger than 500KB in\" +\n+                        \" serialized size\", len, metadata.name());\n+                    if (len > ByteSizeUnit.MB.toBytes(5)) {\n+                        logger.warn(\"Your repository metadata blob for repository [{}] is larger than 5MB. Consider moving to a fresh\" +\n+                            \" repository for new snapshots or deleting unneeded snapshots from your repository to ensure stable\" +\n+                            \" repository behavior going forward.\", metadata.name());\n+                    }\n+                    // Set empty repository data to not waste heap for an outdated cached value\n+                    latestKnownRepositoryData.set(new Tuple<>(RepositoryData.EMPTY_REPO_GEN, null));\n+                    return;\n+                }\n+            } catch (IOException e) {\n+                assert false : new AssertionError(\"Impossible, no IO happens here\", e);\n+                logger.warn(\"Failed to serialize repository data\", e);\n+                return;\n+            }\n+            latestKnownRepositoryData.updateAndGet(known -> {\n+                if (known.v1() > updated.getGenId()) {\n+                    return known;\n+                }\n+                return new Tuple<>(updated.getGenId(), serialized);\n+            });\n+        }\n+    }\n+\n+    private RepositoryData repositoryDataFromCachedEntry(Tuple<Long, BytesReference> cacheEntry) throws IOException {\n+        if (cacheEntry.v1() == RepositoryData.EMPTY_REPO_GEN) {", "originalCommit": "520716e419104af476d2b37fe3e849afc9378942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTkyNTY1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/52341#discussion_r381925656", "bodyText": "++", "author": "original-brownbear", "createdAt": "2020-02-20T10:54:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg5MjkzMw=="}], "type": "inlineReview"}, {"oid": "a02f12943ac5ffb0414fe7f73daf73bf6dacf2e3", "url": "https://github.com/elastic/elasticsearch/commit/a02f12943ac5ffb0414fe7f73daf73bf6dacf2e3", "message": "Merge remote-tracking branch 'elastic/master' into cache-latest-repository-data", "committedDate": "2020-02-20T10:31:23Z", "type": "commit"}, {"oid": "533b4f89a8201fa851620824c1d0a3e5f447724a", "url": "https://github.com/elastic/elasticsearch/commit/533b4f89a8201fa851620824c1d0a3e5f447724a", "message": "CR: setting to disable cache and null out cache", "committedDate": "2020-02-20T10:53:30Z", "type": "commit"}]}