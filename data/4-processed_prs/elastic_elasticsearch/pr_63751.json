{"pr_number": 63751, "pr_title": "SQL: Import Tableau connector", "pr_createdAt": "2020-10-15T14:58:04Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/63751", "timeline": [{"oid": "be7c59010b94a03712bdd740d84a8d5e863fe332", "url": "https://github.com/elastic/elasticsearch/commit/be7c59010b94a03712bdd740d84a8d5e863fe332", "message": "Import Tableau connector\n\nInitial Tableau connector import.", "committedDate": "2020-10-15T14:44:22Z", "type": "commit"}, {"oid": "869bbb8d62199f92780c2bcb46ae93f386c6bdb9", "url": "https://github.com/elastic/elasticsearch/commit/869bbb8d62199f92780c2bcb46ae93f386c6bdb9", "message": "Comment fixes\n\nRemove (c) notices.", "committedDate": "2020-10-15T14:51:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MjY5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r508352693", "bodyText": "A slash is missing after C:\\Users.", "author": "astefan", "createdAt": "2020-10-20T09:33:10Z", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,20 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors", "originalCommit": "869bbb8d62199f92780c2bcb46ae93f386c6bdb9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3NDQyOA==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r508574428", "bodyText": "Fixed.", "author": "bpintea", "createdAt": "2020-10-20T14:48:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MjY5Mw=="}], "type": "inlineReview"}, {"oid": "a6b08c4455f9e07abe3e45be6c0203293189b848", "url": "https://github.com/elastic/elasticsearch/commit/a6b08c4455f9e07abe3e45be6c0203293189b848", "message": "Automate the TDVT test runs\n\nAdd script to automate the execution of test runs.\nIt still requires Tableau installed and licensed and the JDBC driver\npre-installed.", "committedDate": "2020-10-20T10:48:32Z", "type": "commit"}, {"oid": "a32f43480efde8fa5bf2a733d638634fce1a67a8", "url": "https://github.com/elastic/elasticsearch/commit/a32f43480efde8fa5bf2a733d638634fce1a67a8", "message": "Add steps for loading test data with Logstash\n\nAdd the instructions and config file for loading the test data with\nLogstash.\n\nAlso document running the tests automatically with the `tdvt_run.py`\ntool.\n\nAddress review comment.", "committedDate": "2020-10-20T14:45:49Z", "type": "commit"}, {"oid": "33a1dd180ac0481e08cd9875221a9ba5c13d70f6", "url": "https://github.com/elastic/elasticsearch/commit/33a1dd180ac0481e08cd9875221a9ba5c13d70f6", "message": "Merge branch 'master' into import_tableau_connector", "committedDate": "2020-10-20T19:49:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAyODgwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509028809", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors\n          \n          \n            \n                - Windows: C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors", "author": "bpintea", "createdAt": "2020-10-21T06:50:00Z", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,20 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors", "originalCommit": "33a1dd180ac0481e08cd9875221a9ba5c13d70f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c320a49e8f3889941d0e90b9e45cd2188301ebc6", "url": "https://github.com/elastic/elasticsearch/commit/c320a49e8f3889941d0e90b9e45cd2188301ebc6", "message": "Address review comments", "committedDate": "2020-10-21T06:50:27Z", "type": "commit"}, {"oid": "0154788b41719a3b81173453dd7a082fc7b331a2", "url": "https://github.com/elastic/elasticsearch/commit/0154788b41719a3b81173453dd7a082fc7b331a2", "message": "Update logstash.confn file\n\nFix logstash.conf version to add.", "committedDate": "2020-10-21T07:00:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3OTQwNg==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509079406", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Relaunch Logstash and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n          \n          \n            \n            3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "author": "bpintea", "createdAt": "2020-10-21T08:16:46Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,118 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "originalCommit": "0154788b41719a3b81173453dd7a082fc7b331a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f", "url": "https://github.com/elastic/elasticsearch/commit/3762362fadaafe876b7bae804fa1303fe3ae419f", "message": "Address review comments", "committedDate": "2020-10-21T08:18:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5NzkyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509097925", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n          \n          \n            \n            1. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.", "author": "bpintea", "createdAt": "2020-10-21T08:43:10Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODIxNA==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098214", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n          \n          \n            \n            2. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).", "author": "bpintea", "createdAt": "2020-10-21T08:43:35Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODM4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098383", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n          \n          \n            \n            3. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.", "author": "bpintea", "createdAt": "2020-10-21T08:43:49Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODUwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098505", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n          \n          \n            \n            4. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "author": "bpintea", "createdAt": "2020-10-21T08:44:00Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODgwOA==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098808", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n          \n          \n            \n            1. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.", "author": "bpintea", "createdAt": "2020-10-21T08:44:25Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODk1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098952", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n          \n          \n            \n            2. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").", "author": "bpintea", "createdAt": "2020-10-21T08:44:38Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTE0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099146", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n          \n          \n            \n            3. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:", "author": "bpintea", "createdAt": "2020-10-21T08:44:53Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTMwMw==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099303", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Same as in the automated testing.\n          \n          \n            \n            1. Same as in the automated testing.", "author": "bpintea", "createdAt": "2020-10-21T08:45:05Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTQ2NA==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099464", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n          \n          \n            \n            2. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.", "author": "bpintea", "createdAt": "2020-10-21T08:45:17Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTYyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099629", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n          \n          \n            \n            3. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.", "author": "bpintea", "createdAt": "2020-10-21T08:45:30Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTg2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099865", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Generate the tests by invoking TDVT as follows:\n          \n          \n            \n            4. Generate the tests by invoking TDVT as follows:", "author": "bpintea", "createdAt": "2020-10-21T08:45:49Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+3. Generate the tests by invoking TDVT as follows:", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEwMDA1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509100053", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            4. Run the tests:\n          \n          \n            \n            5. Run the tests:", "author": "bpintea", "createdAt": "2020-10-21T08:46:06Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+3. Generate the tests by invoking TDVT as follows:\n+\n+\t```\n+\t$TDVT action --add_ds elastic\n+\t```\n+\n+\tWhen asked for a password, use the same value as in step \"2.\" above when connecting to Elasticsearch.\n+\tFor the \"logical query config\" use `simple_lower`.\n+\n+\tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".\n+\n+4. Run the tests:", "originalCommit": "3762362fadaafe876b7bae804fa1303fe3ae419f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c13cab68a0c5d9ff4d304702aa4b5abf0d36ddf6", "url": "https://github.com/elastic/elasticsearch/commit/c13cab68a0c5d9ff4d304702aa4b5abf0d36ddf6", "message": "Consistent numbering in the docs\r\n\r\nSteps numbering brought inline with chapter numbering.", "committedDate": "2020-10-21T08:47:32Z", "type": "commit"}, {"oid": "ad44abdf6626157192acc24da227ef3020f91eeb", "url": "https://github.com/elastic/elasticsearch/commit/ad44abdf6626157192acc24da227ef3020f91eeb", "message": "Address review comments", "committedDate": "2020-10-21T08:48:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExODYxMg==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509118612", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".\n          \n          \n            \n            \tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"2.\".", "author": "bpintea", "createdAt": "2020-10-21T09:12:50Z", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+1. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+2. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+3. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+4. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+1. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+2. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+3. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+1. Same as in the automated testing.\n+\n+2. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+3. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+4. Generate the tests by invoking TDVT as follows:\n+\n+\t```\n+\t$TDVT action --add_ds elastic\n+\t```\n+\n+\tWhen asked for a password, use the same value as in step \"2.\" above when connecting to Elasticsearch.\n+\tFor the \"logical query config\" use `simple_lower`.\n+\n+\tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".", "originalCommit": "ad44abdf6626157192acc24da227ef3020f91eeb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "09eeb0a9420f0aff108cb0af34463ef3f396757c", "url": "https://github.com/elastic/elasticsearch/commit/09eeb0a9420f0aff108cb0af34463ef3f396757c", "message": "Address review comments\r\n\r\nUpdate numbering references within text.", "committedDate": "2020-10-21T09:13:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE3ODY4OA==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509178688", "bodyText": "Should we mention something about appropriate versions based on the es node/cluster version?", "author": "matriv", "createdAt": "2020-10-21T10:48:55Z", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors\n+    -  macOS: /Users/[user]/Documents/My Tableau Repository/Connectors\n+2. Elasticsearch JDBC Driver Installation:\n+ - Go to the [Driver Download](https://www.elastic.co/downloads/jdbc-client) page.\n+ - Download the Elasticsearch JDBC Driver _.jar_ file and move it into the following directory:", "originalCommit": "09eeb0a9420f0aff108cb0af34463ef3f396757c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE4Nzc3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509187779", "bodyText": "That's a good point. I would however follow up on this:\n\nonce the connector releasing is hooked into the RM (and we'll then easily be able to pair the connector with the driver, version-wise);\nwith a doc PR stating our drivers' compatibility policy that we can refer to (from here, among other places).", "author": "bpintea", "createdAt": "2020-10-21T11:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE3ODY4OA=="}], "type": "inlineReview"}, {"oid": "b142e1e49c034fb9ff27fb385ce5e67bac8dc6cb", "url": "https://github.com/elastic/elasticsearch/commit/b142e1e49c034fb9ff27fb385ce5e67bac8dc6cb", "message": "Fix taco default dir definition\n\nRemove hard-coded user with current login.", "committedDate": "2020-10-21T14:51:37Z", "type": "commit"}, {"oid": "1f51559132f3a5e580e1036f2041d6d7e25d7d28", "url": "https://github.com/elastic/elasticsearch/commit/1f51559132f3a5e580e1036f2041d6d7e25d7d28", "message": "Merge branch 'import_tableau_connector' of github.com:bpintea/elasticsearch into import_tableau_connector", "committedDate": "2020-10-21T14:52:54Z", "type": "commit"}, {"oid": "6f8d92944c2ce8b39fc0b0e7ed80a28af0ffc0ef", "url": "https://github.com/elastic/elasticsearch/commit/6f8d92944c2ce8b39fc0b0e7ed80a28af0ffc0ef", "message": "Merge branch 'master' into import_tableau_connector", "committedDate": "2020-10-21T15:10:06Z", "type": "commit"}, {"oid": "7af38977a2476d1c096d262e2b06194a2b063559", "url": "https://github.com/elastic/elasticsearch/commit/7af38977a2476d1c096d262e2b06194a2b063559", "message": "Add Tableau's SDK MIT license\n\nThe x-pack/plugin/sql/connectors/tableau/connector/ files are modified\ncopies of ones provided by Tableau under:\n https://github.com/tableau/connector-plugin-sdk/tree/master/samples/plugins/postgres_jdbc\nThe x-pack/plugin/sql/connectors/tableau/tdvt/tds/ files are modified\nfiles generated by the Tableau Desktop application.", "committedDate": "2020-10-30T15:56:15Z", "type": "commit"}, {"oid": "bed121bb882ca7b0bfba1711d0670350dac4fb3f", "url": "https://github.com/elastic/elasticsearch/commit/bed121bb882ca7b0bfba1711d0670350dac4fb3f", "message": "Merge branch 'master' into import_tableau_connector", "committedDate": "2020-10-30T15:59:25Z", "type": "commit"}]}