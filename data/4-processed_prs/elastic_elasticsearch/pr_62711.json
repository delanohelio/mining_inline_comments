{"pr_number": 62711, "pr_title": "Ensure MockRepository is Unblocked on Node Close", "pr_createdAt": "2020-09-21T15:28:40Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62711", "timeline": [{"oid": "a01b184af15f4983fe5caded8b029a0c293a272b", "url": "https://github.com/elastic/elasticsearch/commit/a01b184af15f4983fe5caded8b029a0c293a272b", "message": "Ensure MockRepository is Unblocked on Node Close\n\n`RepositoriesService#doClose` was never called which lead to\nmock repositories not unblocking until the `ThreadPool` interrupts\nall threads. Thus stopping a node that is blocked on a mock repository operation wastes `10s`\nin each test that does it (which is quite a few as it turns out).", "committedDate": "2020-09-21T15:21:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjE1MzI3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62711#discussion_r492153277", "bodyText": "We didn't throw here before but then again we only got here when all the threads were already interrupted -> I figured throwing here keeps things nice and deterministic.", "author": "original-brownbear", "createdAt": "2020-09-21T15:29:27Z", "path": "test/framework/src/main/java/org/elasticsearch/snapshots/mockstore/MockRepository.java", "diffHunk": "@@ -322,9 +322,13 @@ private void maybeIOExceptionOrBlock(String blobName) throws IOException {\n                 }\n             }\n \n-            private void blockExecutionAndMaybeWait(final String blobName) {\n+            private void blockExecutionAndMaybeWait(final String blobName) throws IOException {\n                 logger.info(\"[{}] blocking I/O operation for file [{}] at path [{}]\", metadata.name(), blobName, path());\n-                if (blockExecution() && waitAfterUnblock > 0) {\n+                final boolean wasBlocked = blockExecution();\n+                if (wasBlocked && lifecycle.stoppedOrClosed()) {\n+                    throw new IOException(\"already closed\");", "originalCommit": "a01b184af15f4983fe5caded8b029a0c293a272b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "93d3b74129251fb9a5c1b44c334600d8fcde8e41", "url": "https://github.com/elastic/elasticsearch/commit/93d3b74129251fb9a5c1b44c334600d8fcde8e41", "message": "Merge remote-tracking branch 'elastic/master' into faster-mock-repo-close", "committedDate": "2020-09-21T16:09:00Z", "type": "commit"}, {"oid": "79578acec2faddd007aaf87f7356330dbdadb389", "url": "https://github.com/elastic/elasticsearch/commit/79578acec2faddd007aaf87f7356330dbdadb389", "message": "fix", "committedDate": "2020-09-21T17:18:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjIyOTM1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62711#discussion_r492229352", "bodyText": "We need to actually wait here for the cluster change to be fully registerd, otherwise we just randomly pick a node that hasn't yet seen the cleanup in progress in the CS and fail on the leaked running cleanup.\nObviously, there's a bit of a risk with this change in general and it might lead to more failures that need a check like this added now because they implicitly relied on the 10s wait when closing a blocked node but IMO it's worth it given the almost 10s per affected test (and it's quite a few) savings.", "author": "original-brownbear", "createdAt": "2020-09-21T17:29:58Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/repositories/blobstore/BlobStoreRepositoryCleanupIT.java", "diffHunk": "@@ -42,9 +42,12 @@\n     public void testMasterFailoverDuringCleanup() throws Exception {\n         startBlockedCleanup(\"test-repo\");\n \n+        final int nodeCount = internalCluster().numDataAndMasterNodes();\n         logger.info(\"-->  stopping master node\");\n         internalCluster().stopCurrentMasterNode();\n \n+        ensureStableCluster(nodeCount - 1);", "originalCommit": "79578acec2faddd007aaf87f7356330dbdadb389", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUzMDI4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62711#discussion_r492530282", "bodyText": "We can maybe run this PR on CI multiple times before merging, just to catch the most failing tests if any. But I agree with you, it's better to not have test relying on the implicit 10s.", "author": "tlrx", "createdAt": "2020-09-22T07:37:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjIyOTM1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU0MDI1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62711#discussion_r492540250", "bodyText": "We can maybe run this PR on CI multiple times before merging, just to catch the most failing tests if any\n\nRan it all night on my local CI :D only shook out an endless series of #62713 for now :) I'm more worried about some low-frequency timing issues (from request retries) but now that we're aware of it, it should be easy to track those down if they actually occur :)", "author": "original-brownbear", "createdAt": "2020-09-22T07:56:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjIyOTM1Mg=="}], "type": "inlineReview"}]}