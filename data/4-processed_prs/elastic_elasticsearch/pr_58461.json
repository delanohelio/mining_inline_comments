{"pr_number": 58461, "pr_title": "Executes incremental reduce in the search thread pool", "pr_createdAt": "2020-06-23T22:51:53Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/58461", "timeline": [{"oid": "6a4ec0ff642e59df3cb1fc92eda4e8899939f317", "url": "https://github.com/elastic/elasticsearch/commit/6a4ec0ff642e59df3cb1fc92eda4e8899939f317", "message": "Executes incremental reduce in the search thread pool\n\nThis change forks the execution of partial\nreduces in the coordinating node to the search thread pool.\nIt also ensures that partial reduces are executed sequentially\nand asynchronously in order to limit the memory and cpu that a\nsingle search request can use but also to avoid blocking a\nnetwork thread.\nIf a partial reduce fails with an exception, the search\nrequest is cancelled and the reporting of the error is\ndelayed to the start of the fetch phase (when the final\nreduce is performed). This ensures that we cleanup the\nin-flight search requests before returning an error to\nthe user.\n\nCloses #53411\nRelates #51857", "committedDate": "2020-06-23T22:50:29Z", "type": "commit"}, {"oid": "baa10e1deebbf6dd9252bb9bfeaf69b01fd9d782", "url": "https://github.com/elastic/elasticsearch/commit/baa10e1deebbf6dd9252bb9bfeaf69b01fd9d782", "message": "take into  account the last reduced result when computing the buffer size", "committedDate": "2020-06-23T23:17:37Z", "type": "commit"}, {"oid": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "url": "https://github.com/elastic/elasticsearch/commit/ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "message": "use a consistent ordering on final reduce", "committedDate": "2020-06-23T23:40:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNTY5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445405696", "bodyText": "Was this class only extracted from SearchPhaseController or what changes have been made to it? Maybe for easier reviewing it could have stayed where it was, at least initially? Would that be possible?", "author": "javanna", "createdAt": "2020-06-25T08:49:59Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {", "originalCommit": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIzOTM0NA==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r449239344", "bodyText": "We could but the change is substantial so I don't think it would help. This PR is a complete rewrite of the QueryPhaseResultConsumer.", "author": "jimczi", "createdAt": "2020-07-02T20:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNTY5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4MzgwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453783809", "bodyText": "Yeah. This looks so much bigger than before and I don't think it would have made review a ton easier to keep it where it was.", "author": "nik9000", "createdAt": "2020-07-13T16:41:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNTY5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNjkzNw==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445406937", "bodyText": "Maybe add some prefix to the message? I am always afraid that the exc message may be null or not so understandable, with a prefix we would know for sure where the cancel comes from.", "author": "javanna", "createdAt": "2020-06-25T08:51:57Z", "path": "server/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java", "diffHunk": "@@ -634,6 +641,14 @@ public void run() {\n         }\n     }\n \n+    private void cancelTask(SearchTask task, Exception exc) {\n+        CancelTasksRequest req = new CancelTasksRequest()\n+            .setTaskId(new TaskId(client.getLocalNodeId(), task.getId()))\n+            .setReason(exc.getMessage());", "originalCommit": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNzU1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445407557", "bodyText": "why do we need to do this compared to before?", "author": "javanna", "createdAt": "2020-06-25T08:53:00Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -138,6 +140,28 @@ public void execute(SearchContext context) {\n                 fieldsVisitor = new CustomFieldsVisitor(storedToRequestedFields.keySet(), loadSource);\n             }\n         }\n+        int[] docIds = new int[context.docIdsToLoadSize()];\n+        int[] sortedDocIds = new int[context.docIdsToLoadSize()];\n+        int[] indices = new int[context.docIdsToLoadSize()];\n+        Sorter sorter = new IntroSorter() {\n+            @Override\n+            protected void swap(int i, int j) {\n+                int left = docIds[i];\n+                docIds[i] = docIds[j];\n+                docIds[j] = left;\n+            }\n+\n+            @Override\n+            protected void setPivot(int i) {\n+\n+            }\n+\n+            @Override\n+            protected int comparePivot(int j) {\n+                return 0;\n+            }\n+        };", "originalCommit": "ee22e3ff6c8d5dd22cace2a4915f9dce9a9c5c01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ1NDI0NA==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r445454244", "bodyText": "That's an unrelated change sorry :(. I removed it from this pr.", "author": "jimczi", "createdAt": "2020-06-25T10:14:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQwNzU1Nw=="}], "type": "inlineReview"}, {"oid": "cbd97762d04acf411eb8831b9774a8ebba701a55", "url": "https://github.com/elastic/elasticsearch/commit/cbd97762d04acf411eb8831b9774a8ebba701a55", "message": "remove unrelated change", "committedDate": "2020-06-25T10:14:09Z", "type": "commit"}, {"oid": "74850ee738ba34d43302af424a6900c4cdb92858", "url": "https://github.com/elastic/elasticsearch/commit/74850ee738ba34d43302af424a6900c4cdb92858", "message": "Merge branch 'master' into partial_reduce_thread_pool", "committedDate": "2020-07-02T20:00:06Z", "type": "commit"}, {"oid": "b381f3b420ee6f99eae54545f68cdcb57ee62fc8", "url": "https://github.com/elastic/elasticsearch/commit/b381f3b420ee6f99eae54545f68cdcb57ee62fc8", "message": "fix test compliation", "committedDate": "2020-07-02T20:38:19Z", "type": "commit"}, {"oid": "c24f38eb3a97cacdafe572901ddb7a8c7f5937eb", "url": "https://github.com/elastic/elasticsearch/commit/c24f38eb3a97cacdafe572901ddb7a8c7f5937eb", "message": "Merge branch 'master' into partial_reduce_thread_pool", "committedDate": "2020-07-03T12:58:31Z", "type": "commit"}, {"oid": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "url": "https://github.com/elastic/elasticsearch/commit/8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "message": "fix another test", "committedDate": "2020-07-03T13:14:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MDc1NA==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453770754", "bodyText": "s/shouldExecuteImmediatly/executeNextImmediately/?", "author": "nik9000", "createdAt": "2020-07-13T16:21:01Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;\n+    private volatile TopDocs reducedTopDocs;\n+    private volatile Serialized<InternalAggregations> reducedAggs;\n+\n+    private final List<QuerySearchResult> buffer = new ArrayList<>();\n+    private final ArrayDeque<MergeTask> queue = new ArrayDeque<>();\n+    private final AtomicReference<MergeTask> runningTask = new AtomicReference<>();\n+    private final AtomicReference<Exception> fatalFailure = new AtomicReference<>();\n+\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.bufferSize = bufferSize;\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.topDocsStats = new TopDocsStats(trackTotalHitsUpTo);\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    public int getNumReducePhases() {\n+        return numReducePhases;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        consumeInternal(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    private void consumeInternal(QuerySearchResult result, Runnable next) {\n+        boolean shouldExecuteImmediatly = true;", "originalCommit": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwOTg3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460909872", "bodyText": "I pushed 1fa4880", "author": "jimczi", "createdAt": "2020-07-27T13:58:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MDc1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MTU2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453771562", "bodyText": "I think it is worth a comment about why you can't use reducedAggs != null. I believe the reason for that is that you want to set this to true before the reduced aggs are ready.", "author": "nik9000", "createdAt": "2020-07-13T16:22:22Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;", "originalCommit": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwOTY3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460909672", "bodyText": "++, I pushed 1fa4880", "author": "jimczi", "createdAt": "2020-07-27T13:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3MTU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3NjcxMg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r453776712", "bodyText": "Do you see the task being consumed twice? It is probably worth a comment explaining why.", "author": "nik9000", "createdAt": "2020-07-13T16:30:10Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final int bufferSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final TopDocsStats topDocsStats;\n+    private volatile int numReducePhases;\n+    private List<SearchShard> processedShards = new ArrayList<>();\n+    private boolean hasPartialReduce;\n+    private volatile TopDocs reducedTopDocs;\n+    private volatile Serialized<InternalAggregations> reducedAggs;\n+\n+    private final List<QuerySearchResult> buffer = new ArrayList<>();\n+    private final ArrayDeque<MergeTask> queue = new ArrayDeque<>();\n+    private final AtomicReference<MergeTask> runningTask = new AtomicReference<>();\n+    private final AtomicReference<Exception> fatalFailure = new AtomicReference<>();\n+\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.bufferSize = bufferSize;\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.topDocsStats = new TopDocsStats(trackTotalHitsUpTo);\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    public int getNumReducePhases() {\n+        return numReducePhases;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        consumeInternal(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    private void consumeInternal(QuerySearchResult result, Runnable next) {\n+        boolean shouldExecuteImmediatly = true;\n+        synchronized (this) {\n+            if (hasFailure() || result.isNull()) {\n+                SearchShardTarget target = result.getSearchShardTarget();\n+                processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n+                result.consumeAll();\n+            } else {\n+                int size = buffer.size() + (hasPartialReduce ? 1 : 0);\n+                if (size == bufferSize) {\n+                    hasPartialReduce = true;\n+                    shouldExecuteImmediatly = false;\n+                    MergeTask task = new MergeTask(buffer, next);\n+                    buffer.clear();\n+                    tryExecute(task);\n+                }\n+                buffer.add(result);\n+            }\n+        }\n+        if (shouldExecuteImmediatly) {\n+            next.run();\n+        }\n+    }\n+\n+    @Override\n+    SearchPhaseController.ReducedQueryPhase reduce() throws Exception {\n+        if (hasPendingMerges()) {\n+            throw new AssertionError(\"partial reduce in-flight\");\n+        } else if (hasFailure()) {\n+            throw fatalFailure.get();\n+        }\n+\n+        logger.trace(\"aggs final reduction [{}] max [{}]\", aggsCurrentBufferSize, aggsMaxBufferSize);\n+        Collections.sort(buffer, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n+        SearchPhaseController.ReducedQueryPhase reducePhase = controller.reducedQueryPhase(results.asList(), consumeAggs(buffer),\n+            consumeTopDocs(buffer), topDocsStats, numReducePhases, false, aggReduceContextBuilder, performFinalReduce);\n+        progressListener.notifyFinalReduce(SearchProgressListener.buildSearchShards(results.asList()),\n+            reducePhase.totalHits, reducePhase.aggregations, reducePhase.numReducePhases);\n+        return reducePhase;\n+    }\n+\n+\n+    private void partialReduce(List<QuerySearchResult> toConsume) {\n+        final List<TopDocs> topDocsToConsume;\n+        final List<InternalAggregations> aggsToConsume;\n+        synchronized (this) {\n+            if (hasFailure()) {\n+                return;\n+            }\n+            topDocsToConsume = consumeTopDocs(toConsume);\n+            aggsToConsume = consumeAggs(toConsume);\n+\n+        }\n+        final TopDocs newTopDocs;\n+        if (hasTopDocs) {\n+            newTopDocs = mergeTopDocs(topDocsToConsume,\n+                // we have to merge here in the same way we collect on a shard\n+                topNSize, 0);\n+        } else {\n+            newTopDocs = null;\n+        }\n+\n+        final Serialized<InternalAggregations> newAggs;\n+        if (hasAggs) {\n+            InternalAggregations result = InternalAggregations.topLevelReduce(aggsToConsume,\n+                aggReduceContextBuilder.forPartialReduction());\n+            newAggs = DelayableWriteable.referencing(result).asSerialized(InternalAggregations::readFrom, namedWriteableRegistry);\n+            long previousBufferSize = aggsCurrentBufferSize;\n+            aggsCurrentBufferSize = newAggs.ramBytesUsed();\n+            aggsMaxBufferSize = Math.max(aggsCurrentBufferSize, aggsMaxBufferSize);\n+            logger.trace(\"aggs partial reduction [{}->{}] max [{}]\",\n+                previousBufferSize, aggsCurrentBufferSize, aggsMaxBufferSize);\n+        } else {\n+            newAggs = null;\n+        }\n+        synchronized (this) {\n+            if (hasFailure())  {\n+                return;\n+            }\n+            reducedTopDocs = newTopDocs;\n+            reducedAggs = newAggs;\n+            ++ numReducePhases;\n+            for (QuerySearchResult result : toConsume) {\n+                SearchShardTarget target = result.getSearchShardTarget();\n+                processedShards.add(new SearchShard(target.getClusterAlias(), target.getShardId()));\n+            }\n+            progressListener.onPartialReduce(processedShards, topDocsStats.getTotalHits(), reducedAggs, numReducePhases);\n+        }\n+    }\n+\n+    private List<InternalAggregations> consumeAggs(List<QuerySearchResult> results) {\n+        if (hasAggs) {\n+            List<InternalAggregations> aggsList = new ArrayList<>();\n+            if (reducedAggs != null) {\n+                aggsList.add(reducedAggs.expand());\n+                reducedAggs = null;\n+            }\n+            for (QuerySearchResult result : results) {\n+                aggsList.add(result.consumeAggs().expand());\n+            }\n+            return aggsList;\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    private List<TopDocs> consumeTopDocs(List<QuerySearchResult> results) {\n+        if (hasTopDocs) {\n+            List<TopDocs> topDocsList = new ArrayList<>();\n+            if (reducedTopDocs != null)  {\n+                topDocsList.add(reducedTopDocs);\n+            }\n+            for (QuerySearchResult result : results) {\n+                TopDocsAndMaxScore topDocs = result.consumeTopDocs();\n+                topDocsStats.add(topDocs, result.searchTimedOut(), result.terminatedEarly());\n+                setShardIndex(topDocs.topDocs, result.getShardIndex());\n+                topDocsList.add(topDocs.topDocs);\n+            }\n+            return topDocsList;\n+        } else {\n+            return null;\n+        }\n+    }\n+\n+    private synchronized void onFatalFailure(Exception exc) {\n+        onPartialMergeFailure.accept(exc);\n+        fatalFailure.compareAndSet(null, exc);\n+        MergeTask task = runningTask.get();\n+        if (task != null) {\n+            task.cancel();\n+        }\n+        runningTask.compareAndSet(task, null);\n+        queue.stream().forEach(MergeTask::cancel);\n+        queue.clear();\n+        reducedTopDocs = null;\n+        reducedAggs = null;\n+    }\n+\n+    private boolean hasFailure() {\n+        return fatalFailure.get() != null;\n+    }\n+\n+    synchronized boolean hasPendingMerges() {\n+        return queue.isEmpty() == false || runningTask.get() != null;\n+    }\n+\n+    synchronized void tryExecute(MergeTask task) {\n+        queue.add(task);\n+        tryExecuteNext();\n+    }\n+\n+    private void tryExecuteNext() {\n+        final MergeTask task;\n+        synchronized (this) {\n+            if (queue.isEmpty()\n+                    || runningTask.get() != null\n+                    || hasFailure()) {\n+                return;\n+            }\n+            runningTask.compareAndSet(null, queue.peek());\n+            task = queue.poll();\n+        }\n+        executor.execute(new AbstractRunnable() {\n+            @Override\n+            protected void doRun() throws Exception {\n+                List<QuerySearchResult> toConsume = task.consumeBuffer();", "originalCommit": "8cb330b4130d3d86da19d7a8ad24fcfdf47e0163", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkwOTU2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460909566", "bodyText": "If the task is cancelled, we nullify the buffer in the task. That's why we need this check, I added a comment in 1fa4880", "author": "jimczi", "createdAt": "2020-07-27T13:58:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc3NjcxMg=="}], "type": "inlineReview"}, {"oid": "4b5d157540de29c8b601315038017cf4be0fa6ff", "url": "https://github.com/elastic/elasticsearch/commit/4b5d157540de29c8b601315038017cf4be0fa6ff", "message": "Merge branch 'master' into partial_reduce_thread_pool", "committedDate": "2020-07-21T11:56:22Z", "type": "commit"}, {"oid": "1fa48801f433588a8a0de2e64851c1e145cf0640", "url": "https://github.com/elastic/elasticsearch/commit/1fa48801f433588a8a0de2e64851c1e145cf0640", "message": "address review + cleanup", "committedDate": "2020-07-27T13:55:50Z", "type": "commit"}, {"oid": "a2331f4bc6e5bae50ae32103f329a2de7fa7e2bd", "url": "https://github.com/elastic/elasticsearch/commit/a2331f4bc6e5bae50ae32103f329a2de7fa7e2bd", "message": "Merge branch 'master' into partial_reduce_thread_pool", "committedDate": "2020-07-27T14:05:27Z", "type": "commit"}, {"oid": "9f5d075c154ab9be7f5b90189272acd839f54256", "url": "https://github.com/elastic/elasticsearch/commit/9f5d075c154ab9be7f5b90189272acd839f54256", "message": "restore consistent ordering on reduce", "committedDate": "2020-07-27T14:29:28Z", "type": "commit"}, {"oid": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "url": "https://github.com/elastic/elasticsearch/commit/c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "message": "handle null buffer", "committedDate": "2020-07-27T14:59:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2MTE2MA==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460961160", "bodyText": "Did you keep the same class here because folks are used to setting the logger for SearchPhaseController? If so, its worth a comment. If no, I'd switch this to QueryPhaseResultConsumer.", "author": "nik9000", "createdAt": "2020-07-27T15:08:10Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);", "originalCommit": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTAyNjY3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r461026672", "bodyText": "I switched to QueryPhaseResultConsumer, thanks for noticing", "author": "jimczi", "createdAt": "2020-07-27T16:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2MTE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2NDIzNg==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r460964236", "bodyText": "I think it is more clear to write this is DelayableWriteable.Serialized<InternalAggregations>.", "author": "nik9000", "createdAt": "2020-07-27T15:12:31Z", "path": "server/src/main/java/org/elasticsearch/action/search/QueryPhaseResultConsumer.java", "diffHunk": "@@ -0,0 +1,411 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.search;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.search.TopDocs;\n+import org.elasticsearch.action.search.SearchPhaseController.TopDocsStats;\n+import org.elasticsearch.common.io.stream.DelayableWriteable;\n+import org.elasticsearch.common.io.stream.DelayableWriteable.Serialized;\n+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n+import org.elasticsearch.common.lucene.search.TopDocsAndMaxScore;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.search.SearchPhaseResult;\n+import org.elasticsearch.search.SearchShardTarget;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContextBuilder;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.query.QuerySearchResult;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.SearchPhaseController.mergeTopDocs;\n+import static org.elasticsearch.action.search.SearchPhaseController.setShardIndex;\n+\n+/**\n+ * A {@link ArraySearchPhaseResults} implementation that incrementally reduces aggregation results\n+ * as shard results are consumed.\n+ * This implementation can be configured to batch up a certain amount of results and reduce\n+ * them asynchronously in the provided {@link Executor} iff the buffer is exhausted.\n+ */\n+class QueryPhaseResultConsumer extends ArraySearchPhaseResults<SearchPhaseResult> {\n+    private static final Logger logger = LogManager.getLogger(SearchPhaseController.class);\n+\n+    private final Executor executor;\n+    private final SearchPhaseController controller;\n+    private final SearchProgressListener progressListener;\n+    private final ReduceContextBuilder aggReduceContextBuilder;\n+    private final NamedWriteableRegistry namedWriteableRegistry;\n+\n+    private final int topNSize;\n+    private final boolean hasTopDocs;\n+    private final boolean hasAggs;\n+    private final boolean performFinalReduce;\n+\n+    private final PendingMerges pendingMerges;\n+    private final Consumer<Exception> onPartialMergeFailure;\n+\n+    private volatile long aggsMaxBufferSize;\n+    private volatile long aggsCurrentBufferSize;\n+\n+    /**\n+     * Creates a {@link QueryPhaseResultConsumer} that incrementally reduces aggregation results\n+     * as shard results are consumed.\n+     */\n+    QueryPhaseResultConsumer(Executor executor,\n+                             SearchPhaseController controller,\n+                             SearchProgressListener progressListener,\n+                             ReduceContextBuilder aggReduceContextBuilder,\n+                             NamedWriteableRegistry namedWriteableRegistry,\n+                             int expectedResultSize,\n+                             int bufferSize,\n+                             boolean hasTopDocs,\n+                             boolean hasAggs,\n+                             int trackTotalHitsUpTo,\n+                             int topNSize,\n+                             boolean performFinalReduce,\n+                             Consumer<Exception> onPartialMergeFailure) {\n+        super(expectedResultSize);\n+        this.executor = executor;\n+        this.controller = controller;\n+        this.progressListener = progressListener;\n+        this.aggReduceContextBuilder = aggReduceContextBuilder;\n+        this.namedWriteableRegistry = namedWriteableRegistry;\n+        this.topNSize = topNSize;\n+        this.pendingMerges = new PendingMerges(bufferSize, trackTotalHitsUpTo);\n+        this.hasTopDocs = hasTopDocs;\n+        this.hasAggs = hasAggs;\n+        this.performFinalReduce = performFinalReduce;\n+        this.onPartialMergeFailure = onPartialMergeFailure;\n+    }\n+\n+    @Override\n+    void consumeResult(SearchPhaseResult result, Runnable next) {\n+        super.consumeResult(result, () -> {});\n+        QuerySearchResult querySearchResult = result.queryResult();\n+        pendingMerges.consume(querySearchResult, next);\n+        progressListener.notifyQueryResult(querySearchResult.getShardIndex());\n+    }\n+\n+    @Override\n+    SearchPhaseController.ReducedQueryPhase reduce() throws Exception {\n+        if (pendingMerges.hasPendingMerges()) {\n+            throw new AssertionError(\"partial reduce in-flight\");\n+        } else if (pendingMerges.hasFailure()) {\n+            throw pendingMerges.getFailure();\n+        }\n+\n+        logger.trace(\"aggs final reduction [{}] max [{}]\", aggsCurrentBufferSize, aggsMaxBufferSize);\n+        // ensure consistent ordering\n+        pendingMerges.sortBuffer();\n+        final TopDocsStats topDocsStats = pendingMerges.consumeTopDocsStats();\n+        final List<TopDocs> topDocsList = pendingMerges.consumeTopDocs();\n+        final List<InternalAggregations> aggsList = pendingMerges.consumeAggs();\n+        SearchPhaseController.ReducedQueryPhase reducePhase = controller.reducedQueryPhase(results.asList(), aggsList,\n+            topDocsList, topDocsStats, pendingMerges.numReducePhases, false, aggReduceContextBuilder, performFinalReduce);\n+        progressListener.notifyFinalReduce(SearchProgressListener.buildSearchShards(results.asList()),\n+            reducePhase.totalHits, reducePhase.aggregations, reducePhase.numReducePhases);\n+        return reducePhase;\n+    }\n+\n+    private MergeResult partialReduce(MergeTask task,\n+                                      TopDocsStats topDocsStats,\n+                                      MergeResult lastMerge,\n+                                      int numReducePhases) {\n+        final QuerySearchResult[] toConsume = task.consumeBuffer();\n+        if (toConsume == null) {\n+            // the task is cancelled\n+            return null;\n+        }\n+        // ensure consistent ordering\n+        Arrays.sort(toConsume, Comparator.comparingInt(QuerySearchResult::getShardIndex));\n+\n+        for (QuerySearchResult result : toConsume) {\n+            topDocsStats.add(result.topDocs(), result.searchTimedOut(), result.terminatedEarly());\n+        }\n+\n+        final TopDocs newTopDocs;\n+        if (hasTopDocs) {\n+            List<TopDocs> topDocsList = new ArrayList<>();\n+            if (lastMerge != null) {\n+                topDocsList.add(lastMerge.reducedTopDocs);\n+            }\n+            for (QuerySearchResult result : toConsume) {\n+                TopDocsAndMaxScore topDocs = result.consumeTopDocs();\n+                setShardIndex(topDocs.topDocs, result.getShardIndex());\n+                topDocsList.add(topDocs.topDocs);\n+            }\n+            newTopDocs = mergeTopDocs(topDocsList,\n+                // we have to merge here in the same way we collect on a shard\n+                topNSize, 0);\n+        } else {\n+            newTopDocs = null;\n+        }\n+\n+        final Serialized<InternalAggregations> newAggs;", "originalCommit": "c15a1e6d0915f4c7cc98ea45f43f9fd054620c79", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTAyNjg2NA==", "url": "https://github.com/elastic/elasticsearch/pull/58461#discussion_r461026864", "bodyText": "I pushed a6c4ff4", "author": "jimczi", "createdAt": "2020-07-27T16:47:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk2NDIzNg=="}], "type": "inlineReview"}, {"oid": "065adca4ace02d0d313d583c09e8c874dcc91a5e", "url": "https://github.com/elastic/elasticsearch/commit/065adca4ace02d0d313d583c09e8c874dcc91a5e", "message": "handle empty results in progress listener", "committedDate": "2020-07-27T16:44:13Z", "type": "commit"}, {"oid": "a6c4ff4545a5423f6f53f92e07fb35e3e2ff189a", "url": "https://github.com/elastic/elasticsearch/commit/a6c4ff4545a5423f6f53f92e07fb35e3e2ff189a", "message": "address review", "committedDate": "2020-07-27T16:46:24Z", "type": "commit"}, {"oid": "139a8bf1678f1e9b91739a307f73a17ee8d1ae16", "url": "https://github.com/elastic/elasticsearch/commit/139a8bf1678f1e9b91739a307f73a17ee8d1ae16", "message": "add more logging in tests", "committedDate": "2020-07-27T17:38:55Z", "type": "commit"}, {"oid": "f06c1f09f455acd5598e8d64edc75871aced59f5", "url": "https://github.com/elastic/elasticsearch/commit/f06c1f09f455acd5598e8d64edc75871aced59f5", "message": "Merge branch 'master' into partial_reduce_thread_pool", "committedDate": "2020-07-27T17:57:53Z", "type": "commit"}, {"oid": "16d68df1fb37e16c315e33d68eed97ba748ac035", "url": "https://github.com/elastic/elasticsearch/commit/16d68df1fb37e16c315e33d68eed97ba748ac035", "message": "fix race condition for tests", "committedDate": "2020-07-27T18:37:39Z", "type": "commit"}, {"oid": "e337ee7bf75badef1b2c06f296976adc0258f5d3", "url": "https://github.com/elastic/elasticsearch/commit/e337ee7bf75badef1b2c06f296976adc0258f5d3", "message": "remove spurious log", "committedDate": "2020-07-28T08:01:26Z", "type": "commit"}]}