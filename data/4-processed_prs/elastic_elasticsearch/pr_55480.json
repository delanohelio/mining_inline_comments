{"pr_number": 55480, "pr_title": "[DOCS] Add 'how to' doc about avoiding oversharding", "pr_createdAt": "2020-04-20T17:43:39Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55480", "timeline": [{"oid": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "url": "https://github.com/elastic/elasticsearch/commit/1cb38b5a540fb7a888c80e2707105863b3bf49c5", "message": "Add avoid oversharding how to doc", "committedDate": "2020-04-20T16:52:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NzkzNg==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411647936", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [[avoiding-oversharding]]\n          \n          \n            \n            [[avoid-oversharding]]", "author": "jrodewig", "createdAt": "2020-04-20T19:51:58Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MDA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411650066", "bodyText": "Can you rewrap lines throughout to keep to a 80 character line length?", "author": "jrodewig", "createdAt": "2020-04-20T19:55:36Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MDgxMg==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411650812", "bodyText": "The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried.\n\nI think this would work better as an unordered list.", "author": "jrodewig", "createdAt": "2020-04-20T19:56:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MDA2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MTQyNA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411651424", "bodyText": "Where possible, we try to add explicit anchors. It ensure that links to the content are less likely to break.\nWe also try to use [discrete] over [float] as that's preferred in Asciidoctor. You'll still see lots of historical [float] attributes tho.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            === Why oversharding is inefficient\n          \n          \n            \n            [discrete]\n          \n          \n            \n            [[oversharding-inefficient]]\n          \n          \n            \n            === Why oversharding is inefficient", "author": "jrodewig", "createdAt": "2020-04-20T19:57:58Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MTkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411651913", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n          \n          \n            \n            Each segment has metadata that needs to be kept in heap memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory.", "author": "jrodewig", "createdAt": "2020-04-20T19:58:45Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. ", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1Mjc1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411652752", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            === How to reduce shard counts and increase shard size\n          \n          \n            \n            [discrete]\n          \n          \n            \n            [[reduce-shard-counts-increase-shard-size]]\n          \n          \n            \n            === How to reduce shard counts and increase shard size", "author": "jrodewig", "createdAt": "2020-04-20T20:00:08Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MzQxOA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411653418", "bodyText": "I don't think the numbering is needed or helps here. These steps could be applied in any order. It will also be easier to add/remove tips without numbers.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Try these six methods to reduce oversharding.\n          \n          \n            \n            Try these methods to reduce oversharding.", "author": "jrodewig", "createdAt": "2020-04-20T20:01:16Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1Mzk0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411653949", "bodyText": "I don't think [discrete] or [float] is needed for H4s.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 1. Reduce the number of shards for new indices\n          \n          \n            \n            [[reduce-shards-for-new-indices]]\n          \n          \n            \n            ==== Reduce the number of shards for new indices", "author": "jrodewig", "createdAt": "2020-04-20T20:02:15Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NDI2NA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411654264", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 2. Create larger shards by increasing the rollover thresholds\n          \n          \n            \n            [[create-larger-shards-by-increasing-rollover-thresholds]]\n          \n          \n            \n            ==== Create larger shards by increasing rollover thresholds", "author": "jrodewig", "createdAt": "2020-04-20T20:02:51Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NDYwNw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411654607", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 3. Create larger shards by using index patterns spanning longer time periods\n          \n          \n            \n            [[create-larger-shards-with-index-patterns]]\n          \n          \n            \n            ==== Create larger shards by using index patterns spanning longer time periods", "author": "jrodewig", "createdAt": "2020-04-20T20:03:25Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NDgzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411654835", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 4. Shrink an existing index to fewer shards\n          \n          \n            \n            [[shrink-existing-index-to-fewer-shards]]\n          \n          \n            \n            ==== Shrink an existing index to fewer shards", "author": "jrodewig", "createdAt": "2020-04-20T20:03:43Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NTA2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411655061", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 5. Reindex an existing index to fewer shards\n          \n          \n            \n            [[reindex-an-existing-index-to-fewer-shards]]\n          \n          \n            \n            ==== Reindex an existing index to fewer shards", "author": "jrodewig", "createdAt": "2020-04-20T20:04:03Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NTMwMg==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411655302", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [float]\n          \n          \n            \n            ==== 6. Reindex indices from shorter periods into longer periods\n          \n          \n            \n            [[reindex-indices-from-shorter-periods-into-longer-periods]]\n          \n          \n            \n            ==== Reindex indices from shorter periods into longer periods", "author": "jrodewig", "createdAt": "2020-04-20T20:04:26Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.\n+\n+[float]\n+==== 6. Reindex indices from shorter periods into longer periods", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NTY3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411655673", "bodyText": "Curator is outdated. I'd just remove this reference.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            link:https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html[Curator] can also link:https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ex_reindex.html#_filter_selected_indices_2[help with reindexing time-based indices].", "author": "jrodewig", "createdAt": "2020-04-20T20:05:05Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.\n+\n+[float]\n+==== 6. Reindex indices from shorter periods into longer periods\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex multiple small indices covering shorter time periods into a larger index covering a longer time period. For example, daily indices from October with naming patterns such as `foo-2019.10.11` could be combined into a monthly `foo-2019.10` index, like this:\n+\n+[source,console]\n+--------------------------------------------------\n+POST _reindex\n+{\n+  \"source\": {\n+    \"index\": \"foo-2019.10.*\"\n+  },\n+  \"dest\": {\n+    \"index\": \"foo-2019.10\"\n+  }\n+}\n+--------------------------------------------------\n+\n+link:https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html[Curator] can also link:https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ex_reindex.html#_filter_selected_indices_2[help with reindexing time-based indices].", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NjA5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411656099", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The <<docs-reindex,Reindex API>> can be used to reindex multiple small indices covering shorter time periods into a larger index covering a longer time period. For example, daily indices from October with naming patterns such as `foo-2019.10.11` could be combined into a monthly `foo-2019.10` index, like this:\n          \n          \n            \n            You can use the <<docs-reindex,reindex API>> to reindex multiple small indices covering shorter time periods into a larger index covering a longer time period. For example, daily indices from October with naming patterns such as `foo-2019.10.11` could be combined into a monthly `foo-2019.10` index, like this:", "author": "jrodewig", "createdAt": "2020-04-20T20:05:43Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.\n+\n+[float]\n+==== 6. Reindex indices from shorter periods into longer periods\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex multiple small indices covering shorter time periods into a larger index covering a longer time period. For example, daily indices from October with naming patterns such as `foo-2019.10.11` could be combined into a monthly `foo-2019.10` index, like this:", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1NjUxMw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411656513", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.\n          \n          \n            \n            You can use the <<docs-reindex,reindex API>> to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the oversharded index can be deleted.", "author": "jrodewig", "createdAt": "2020-04-20T20:06:23Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1Nzc2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411657766", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n          \n          \n            \n            <<index-lifecycle-management,{ilm}>> also has a <<ilm-shrink-action,shrink action>> available for indices in the warm phase.", "author": "jrodewig", "createdAt": "2020-04-20T20:08:26Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1ODA3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411658073", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n          \n          \n            \n            You can use the <<indices-shrink-index,shrink index API>> to shrink an existing index down to a fewer shards.", "author": "jrodewig", "createdAt": "2020-04-20T20:08:52Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1OTAyMw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411659023", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n          \n          \n            \n            If creating indices using {ls}, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the {es} output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. {beats} also lets you change the date math expression defined in the `index` property of the {es} output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].", "author": "jrodewig", "createdAt": "2020-04-20T20:10:27Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1OTY1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411659653", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n          \n          \n            \n            Creating indices covering longer time periods reduces index and shard counts while increasing index sizes. For example, instead of daily indices, you can create monthly, or even yearly indices.", "author": "jrodewig", "createdAt": "2020-04-20T20:11:35Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2MDYzNA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411660634", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n          \n          \n            \n            Take special note of any empty indices. These may be managed by an {ilm-init} policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more {beats} stop sending data. If the {ilm-init}-managed indices for those {beats{ are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the <<cat-count,cat count API>>.", "author": "jrodewig", "createdAt": "2020-04-20T20:13:16Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2MTg5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411661895", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n          \n          \n            \n            You can roll over indices using the <<indices-rollover-index,rollover index API>> or by specifying the <<ilm-rollover-action,rollover action>> in an {ilm-init} policy. If using an {ilm-init} policy, increase the rollover condition thresholds (`max_age`, `max_docs`, `max_size`)  to allow the indices to grow to a larger size before being rolled over, which creates larger shards.", "author": "jrodewig", "createdAt": "2020-04-20T20:15:38Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2MzQ0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411663442", "bodyText": "I don't think we need to call out the default values here.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n          \n          \n            \n            You can specify the `index.number_of_shards` setting  for new indices created with the <<indices-create-index,create index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,{ilm} ({ilm-init})>>.", "author": "jrodewig", "createdAt": "2020-04-20T20:18:14Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2Mzg3MA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411663870", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n          \n          \n            \n            You can override the `index.number_of_shards`  when rolling over an index using the <<rollover-index-api-example,rollover index API>>.", "author": "jrodewig", "createdAt": "2020-04-20T20:18:54Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2NDA4NA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411664084", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            POST _reindex\n          \n          \n            \n            POST /_reindex", "author": "jrodewig", "createdAt": "2020-04-20T20:19:17Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n+\n+[float]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in (heap) memory. These include lists of fields, the number of documents, and terms dictionaries. As a shard grows in size, the size of its segments generally grow because smaller segments are <<index-modules-merge,merged>> into fewer, larger segments. This typically reduces the amount of heap required by a shard\u2019s segment metadata for a given data volume. At a bare minimum shards should be at least larger than 1GB to make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB, a cluster full of 1GB shards will likely still perform poorly. This is because having many small shards can also have a negative impact on search and indexing operations. Each query or indexing operation is executed in a single thread per shard of indices being queried or indexed to. The node receiving a request from a client becomes responsible for distributing that request to the appropriate shards as well as reducing the results from those individual shards into a single response. Even assuming that a cluster has sufficient <<modules-threadpool,search threadpool threads>> available to immediately process the requested action against all shards required by the request, the overhead associated with making network requests to the nodes holding those shards and with having to merge the results of results from many small shards can lead to increased latency. This in turn can lead to exhaustion of the threadpool and, as a result, decreased throughput.\n+\n+[float]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these six methods to reduce oversharding.\n+\n+[float]\n+==== 1. Reduce the number of shards for new indices\n+\n+The `index.number_of_shards` setting can be specified for new indices created with the <<indices-create-index,Create Index API>> or as part of <<indices-templates,index templates>> for indices automatically created by <<index-lifecycle-management,Index Lifecycle Management>>. For Elasticsearch versions below 7.0, the default value for this setting was `5`. For versions 7.0 and higher, the default value for this setting is `1`.\n+\n+The <<rollover-index-api-example,rollover request>> body can also override that same `index.number_of_shards` setting.\n+\n+[float]\n+==== 2. Create larger shards by increasing the rollover thresholds\n+\n+If indices are rolled over by either directly using the Rollover API or by specifying the Index Lifecycle Management {ref}/_actions.html#ilm-rollover-action[Rollover action], then the rollover condition thresholds (`max_age`, `max_docs`, `max_size`) can be increased to allow the indices to grow to a larger size before being rolled over, thereby generating larger shards.\n+\n+Take special note of any indices that are empty. These may be managed by an ILM policy that is rolling over the indices because the `max_age` threshold is met. In this case, you may need to adjust the policy to make use of the `max_docs` or `max_size` properties to prevent the creation of these empty indices. One example where this may happen is if one or more Beats stop sending data. If the ILM-managed indices for those Beats are configured to rollover daily, then new, empty indices will be generated each day. Empty indices can be identified using the following {ref}/cat-count.html[cat count API]: `GET /_cat/count?v`.\n+\n+\n+[float]\n+==== 3. Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods will also reduce the index, and thereby shard, counts and increase index sizes. For example, instead of daily indices, create monthly, or even yearly indices.\n+\n+If creating indices using Logstash, the {logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index] property of the Elasticsearch output can be modified to a <<date-math-index-names,date math expression>> covering a longer time period. For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}`` to create monthly, rather than daily, indices. Beats also allow changing the date math expression defined in the `index` property of the Elasticsearch output, such as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+\n+[float]\n+==== 4. Shrink an existing index to fewer shards\n+\n+The {ref}/indices-shrink-index.html[Shrink index API] can shrink an existing index down to a fewer shards.\n+\n+{ref}/index-lifecycle-management.html[Index Lifecycle Management] also has a shrink action available as part of its warm phase.\n+\n+\n+[float]\n+==== 5. Reindex an existing index to fewer shards\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex from an existing index to a new index with fewer shards. After the data has been reindexed, the initial oversharded index can be deleted.\n+\n+[float]\n+==== 6. Reindex indices from shorter periods into longer periods\n+\n+The <<docs-reindex,Reindex API>> can be used to reindex multiple small indices covering shorter time periods into a larger index covering a longer time period. For example, daily indices from October with naming patterns such as `foo-2019.10.11` could be combined into a monthly `foo-2019.10` index, like this:\n+\n+[source,console]\n+--------------------------------------------------\n+POST _reindex", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY2NTk5NA==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411665994", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.\n          \n          \n            \n            Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. {kib}'s {kibana-ref}/elasticsearch-metrics.html[{es} monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.", "author": "jrodewig", "createdAt": "2020-04-20T20:22:40Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,73 @@\n+[[avoiding-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the same amount of data will lead to a more effective use of system resources (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d. The number of shards where this inflection point occurs depends on a variety of factors, including available hardware, indexing load, data volume, the types of queries executed against the cluster, the rate of these queries being issued, and the volume of data being queried. Testing against production data with production queries on production hardware is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB are commonly used, and this may be a useful starting point from which to experiment. Kibana\u2019s {kibana-ref}/elasticsearch-metrics.html[Elasticsearch monitoring] provides a useful view of historical cluster performance when evaluating the impact of different shard sizes.", "originalCommit": "1cb38b5a540fb7a888c80e2707105863b3bf49c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1ac75fb5698ec26d2a6eb9d0507b90262b369a75", "url": "https://github.com/elastic/elasticsearch/commit/1ac75fb5698ec26d2a6eb9d0507b90262b369a75", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:34:34Z", "type": "commit"}, {"oid": "6ed3ed705992fd60dfeb6411eed7d50096eea108", "url": "https://github.com/elastic/elasticsearch/commit/6ed3ed705992fd60dfeb6411eed7d50096eea108", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:36:16Z", "type": "commit"}, {"oid": "7081d8fb9125f1a7d3b3fda03eaa1aa6a575c108", "url": "https://github.com/elastic/elasticsearch/commit/7081d8fb9125f1a7d3b3fda03eaa1aa6a575c108", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:36:43Z", "type": "commit"}, {"oid": "b678d2dea5af6b9ecabd4fdbb525fe9a5fab09b0", "url": "https://github.com/elastic/elasticsearch/commit/b678d2dea5af6b9ecabd4fdbb525fe9a5fab09b0", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:37:36Z", "type": "commit"}, {"oid": "59e4aed07e08d63c9bf94fdcaec4348aeab8a150", "url": "https://github.com/elastic/elasticsearch/commit/59e4aed07e08d63c9bf94fdcaec4348aeab8a150", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:38:06Z", "type": "commit"}, {"oid": "c5312d48366a5eb003b9ae7b1e58b00b511563ac", "url": "https://github.com/elastic/elasticsearch/commit/c5312d48366a5eb003b9ae7b1e58b00b511563ac", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:38:23Z", "type": "commit"}, {"oid": "659b001f31ca14083fa4c7041a0920b31cd9e45f", "url": "https://github.com/elastic/elasticsearch/commit/659b001f31ca14083fa4c7041a0920b31cd9e45f", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:38:47Z", "type": "commit"}, {"oid": "4f56669408c3d480c9d436720936db26365bd6a3", "url": "https://github.com/elastic/elasticsearch/commit/4f56669408c3d480c9d436720936db26365bd6a3", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:39:03Z", "type": "commit"}, {"oid": "cabe53cde4267db303b8e59ffdf5a042e8753e8f", "url": "https://github.com/elastic/elasticsearch/commit/cabe53cde4267db303b8e59ffdf5a042e8753e8f", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:39:19Z", "type": "commit"}, {"oid": "c58164c3edbe0d18fffaef37e2cd9a55d17e66f2", "url": "https://github.com/elastic/elasticsearch/commit/c58164c3edbe0d18fffaef37e2cd9a55d17e66f2", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:39:31Z", "type": "commit"}, {"oid": "0a9146c6a33cd2c812d6a209d91bea1f31825de3", "url": "https://github.com/elastic/elasticsearch/commit/0a9146c6a33cd2c812d6a209d91bea1f31825de3", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:39:43Z", "type": "commit"}, {"oid": "3d2fe725dfdc1c2ec059ec51b54027c87664a990", "url": "https://github.com/elastic/elasticsearch/commit/3d2fe725dfdc1c2ec059ec51b54027c87664a990", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:40:12Z", "type": "commit"}, {"oid": "c5d017dd3a322e639451322120195b2e9906302a", "url": "https://github.com/elastic/elasticsearch/commit/c5d017dd3a322e639451322120195b2e9906302a", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:40:25Z", "type": "commit"}, {"oid": "97f41694178ca050382e45986c7a9069287ad6c5", "url": "https://github.com/elastic/elasticsearch/commit/97f41694178ca050382e45986c7a9069287ad6c5", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:40:44Z", "type": "commit"}, {"oid": "306e71132c005e3b417d4545d15289bcda57ee61", "url": "https://github.com/elastic/elasticsearch/commit/306e71132c005e3b417d4545d15289bcda57ee61", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:40:58Z", "type": "commit"}, {"oid": "5fef61b4eb196b0e7714ab1b646984f99f60bcce", "url": "https://github.com/elastic/elasticsearch/commit/5fef61b4eb196b0e7714ab1b646984f99f60bcce", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:41:21Z", "type": "commit"}, {"oid": "9ca4755bccd1f57be2c24140c895ce9812d19581", "url": "https://github.com/elastic/elasticsearch/commit/9ca4755bccd1f57be2c24140c895ce9812d19581", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:42:16Z", "type": "commit"}, {"oid": "7c3a33a652517dd4ddc4033b757191f4c2418e4f", "url": "https://github.com/elastic/elasticsearch/commit/7c3a33a652517dd4ddc4033b757191f4c2418e4f", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:42:29Z", "type": "commit"}, {"oid": "f65c791d7595f4e97b7dc8184afee062041563fc", "url": "https://github.com/elastic/elasticsearch/commit/f65c791d7595f4e97b7dc8184afee062041563fc", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:43:32Z", "type": "commit"}, {"oid": "fccb30eee5f007005a33d018dc4ebd8516241365", "url": "https://github.com/elastic/elasticsearch/commit/fccb30eee5f007005a33d018dc4ebd8516241365", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:43:51Z", "type": "commit"}, {"oid": "05cf93650cba3b2f7970f2ffb91114c902dedb4c", "url": "https://github.com/elastic/elasticsearch/commit/05cf93650cba3b2f7970f2ffb91114c902dedb4c", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:44:38Z", "type": "commit"}, {"oid": "752050bb616cfe0304c06771ac5e9e79810607b1", "url": "https://github.com/elastic/elasticsearch/commit/752050bb616cfe0304c06771ac5e9e79810607b1", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:44:57Z", "type": "commit"}, {"oid": "ba0998c1a2a5f44af70071de815cda61cef52e2a", "url": "https://github.com/elastic/elasticsearch/commit/ba0998c1a2a5f44af70071de815cda61cef52e2a", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:45:29Z", "type": "commit"}, {"oid": "97e9aa92ecfba0e12bd2821b4635b83716e6cc20", "url": "https://github.com/elastic/elasticsearch/commit/97e9aa92ecfba0e12bd2821b4635b83716e6cc20", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: James Rodewig <james.rodewig@elastic.co>", "committedDate": "2020-04-20T20:45:43Z", "type": "commit"}, {"oid": "82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "url": "https://github.com/elastic/elasticsearch/commit/82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "message": "Address James' suggestions", "committedDate": "2020-04-20T22:03:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3NzM1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411777357", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            existing index down to a fewer shards.\n          \n          \n            \n            existing index down to fewer shards.", "author": "debadair", "createdAt": "2020-04-21T00:13:48Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,148 @@\n+[[avoid-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the\n+same amount of data will lead to a more effective use of system resources\n+(CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d.\n+\n+The number of shards where this inflection point occurs depends on a variety\n+of factors, including:\n+\n+* available hardware\n+* indexing load\n+* data volume\n+* the types of queries executed against the clusters\n+* the rate of these queries being issued\n+* the volume of data being queried\n+\n+Testing against production data with production queries on production hardware\n+is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB\n+are commonly used, and this may be a useful starting point from which to\n+experiment. {kib}'s {kibana-ref}/elasticsearch-metrics.html[{es} monitoring]\n+provides a useful view of historical cluster performance when evaluating the\n+impact of different shard sizes.\n+\n+[discrete]\n+[[oversharding-inefficient]]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in heap memory. These include\n+lists of fields, the number of documents, and terms dictionaries. As a shard\n+grows in size, the size of its segments generally grow because smaller segments\n+are <<index-modules-merge,merged>> into fewer, larger segments. This typically\n+reduces the amount of heap required by a shard\u2019s segment metadata for a given\n+data volume. At a bare minimum shards should be at least larger than 1GB to\n+make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB,\n+a cluster full of 1GB shards will likely still perform poorly. This is because\n+having many small shards can also have a negative impact on search and\n+indexing operations. Each query or indexing operation is executed in a single\n+thread per shard of indices being queried or indexed to. The node receiving\n+a request from a client becomes responsible for distributing that request to\n+the appropriate shards as well as reducing the results from those individual\n+shards into a single response. Even assuming that a cluster has sufficient\n+<<modules-threadpool,search threadpool threads>> available to immediately\n+process the requested action against all shards required by the request, the\n+overhead associated with making network requests to the nodes holding those\n+shards and with having to merge the results of results from many small shards\n+can lead to increased latency. This in turn can lead to exhaustion of the\n+threadpool and, as a result, decreased throughput.\n+\n+[discrete]\n+[[reduce-shard-counts-increase-shard-size]]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these methods to reduce oversharding.\n+\n+[discrete]\n+[[reduce-shards-for-new-indices]]\n+==== Reduce the number of shards for new indices\n+\n+You can specify the `index.number_of_shards` setting  for new indices created\n+with the <<indices-create-index,create index API>> or as part of\n+<<indices-templates,index templates>> for indices automatically created by\n+<<index-lifecycle-management,{ilm} ({ilm-init})>>.\n+\n+You can override the `index.number_of_shards`  when rolling over an index\n+using the <<rollover-index-api-example,rollover index API>>.\n+\n+[discrete]\n+[[create-larger-shards-by-increasing-rollover-thresholds]]\n+==== Create larger shards by increasing rollover thresholds\n+\n+You can roll over indices using the\n+<<indices-rollover-index,rollover index API>> or by specifying the\n+<<ilm-rollover-action,rollover action>> in an {ilm-init} policy. If using an\n+{ilm-init} policy, increase the rollover condition thresholds (`max_age`,\n+  `max_docs`, `max_size`)  to allow the indices to grow to a larger size\n+  before being rolled over, which creates larger shards.\n+\n+Take special note of any empty indices. These may be managed by an {ilm-init}\n+policy that is rolling over the indices because the `max_age` threshold is met.\n+In this case, you may need to adjust the policy to make use of the `max_docs`\n+or `max_size` properties to prevent the creation of these empty indices. One\n+example where this may happen is if one or more {beats} stop sending data. If\n+the {ilm-init}-managed indices for those {beats{ are configured to rollover\n+  daily, then new, empty indices will be generated each day. Empty indices can\n+  be identified using the <<cat-count,cat count API>>.\n+\n+[discrete]\n+[[create-larger-shards-with-index-patterns]]\n+==== Create larger shards by using index patterns spanning longer time periods\n+\n+Creating indices covering longer time periods reduces index and shard counts\n+while increasing index sizes. For example, instead of daily indices, you can\n+create monthly, or even yearly indices.\n+\n+If creating indices using {ls}, the \n+{logstash-ref}/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-index[index]\n+property of the {es} output can be modified to a\n+<<date-math-index-names,date math expression>> covering a longer time period.\n+For example, use `logstash-%{+YYYY.MM}`` instead of `logstash-%{+YYYY.MM.dd}``\n+to create monthly, rather than daily, indices. {beats} also lets you change the\n+date math expression defined in the `index` property of the {es} output, such\n+as for {filebeat-ref}/elasticsearch-output.html#index-option-es[Filebeat].\n+\n+[discrete]\n+[[shrink-existing-index-to-fewer-shards]]\n+==== Shrink an existing index to fewer shards\n+\n+You can use the <<indices-shrink-index,shrink index API>> to shrink an\n+existing index down to a fewer shards.", "originalCommit": "82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3NzQ1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411777459", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the {ilm-init}-managed indices for those {beats{ are configured to rollover\n          \n          \n            \n            the {ilm-init}-managed indices for those {beats} are configured to roll over", "author": "debadair", "createdAt": "2020-04-21T00:14:04Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,148 @@\n+[[avoid-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the\n+same amount of data will lead to a more effective use of system resources\n+(CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d.\n+\n+The number of shards where this inflection point occurs depends on a variety\n+of factors, including:\n+\n+* available hardware\n+* indexing load\n+* data volume\n+* the types of queries executed against the clusters\n+* the rate of these queries being issued\n+* the volume of data being queried\n+\n+Testing against production data with production queries on production hardware\n+is the only way to calibrate optimal shard sizes. Shard sizes of tens of GB\n+are commonly used, and this may be a useful starting point from which to\n+experiment. {kib}'s {kibana-ref}/elasticsearch-metrics.html[{es} monitoring]\n+provides a useful view of historical cluster performance when evaluating the\n+impact of different shard sizes.\n+\n+[discrete]\n+[[oversharding-inefficient]]\n+=== Why oversharding is inefficient\n+\n+Each segment has metadata that needs to be kept in heap memory. These include\n+lists of fields, the number of documents, and terms dictionaries. As a shard\n+grows in size, the size of its segments generally grow because smaller segments\n+are <<index-modules-merge,merged>> into fewer, larger segments. This typically\n+reduces the amount of heap required by a shard\u2019s segment metadata for a given\n+data volume. At a bare minimum shards should be at least larger than 1GB to\n+make the most efficient use of memory. \n+\n+However, even though shards start to be more memory efficient at around 1GB,\n+a cluster full of 1GB shards will likely still perform poorly. This is because\n+having many small shards can also have a negative impact on search and\n+indexing operations. Each query or indexing operation is executed in a single\n+thread per shard of indices being queried or indexed to. The node receiving\n+a request from a client becomes responsible for distributing that request to\n+the appropriate shards as well as reducing the results from those individual\n+shards into a single response. Even assuming that a cluster has sufficient\n+<<modules-threadpool,search threadpool threads>> available to immediately\n+process the requested action against all shards required by the request, the\n+overhead associated with making network requests to the nodes holding those\n+shards and with having to merge the results of results from many small shards\n+can lead to increased latency. This in turn can lead to exhaustion of the\n+threadpool and, as a result, decreased throughput.\n+\n+[discrete]\n+[[reduce-shard-counts-increase-shard-size]]\n+=== How to reduce shard counts and increase shard size\n+\n+Try these methods to reduce oversharding.\n+\n+[discrete]\n+[[reduce-shards-for-new-indices]]\n+==== Reduce the number of shards for new indices\n+\n+You can specify the `index.number_of_shards` setting  for new indices created\n+with the <<indices-create-index,create index API>> or as part of\n+<<indices-templates,index templates>> for indices automatically created by\n+<<index-lifecycle-management,{ilm} ({ilm-init})>>.\n+\n+You can override the `index.number_of_shards`  when rolling over an index\n+using the <<rollover-index-api-example,rollover index API>>.\n+\n+[discrete]\n+[[create-larger-shards-by-increasing-rollover-thresholds]]\n+==== Create larger shards by increasing rollover thresholds\n+\n+You can roll over indices using the\n+<<indices-rollover-index,rollover index API>> or by specifying the\n+<<ilm-rollover-action,rollover action>> in an {ilm-init} policy. If using an\n+{ilm-init} policy, increase the rollover condition thresholds (`max_age`,\n+  `max_docs`, `max_size`)  to allow the indices to grow to a larger size\n+  before being rolled over, which creates larger shards.\n+\n+Take special note of any empty indices. These may be managed by an {ilm-init}\n+policy that is rolling over the indices because the `max_age` threshold is met.\n+In this case, you may need to adjust the policy to make use of the `max_docs`\n+or `max_size` properties to prevent the creation of these empty indices. One\n+example where this may happen is if one or more {beats} stop sending data. If\n+the {ilm-init}-managed indices for those {beats{ are configured to rollover", "originalCommit": "82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3ODE3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411778175", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Sometimes reducing the number of shards in a cluster while maintaining the\n          \n          \n            \n            same amount of data will lead to a more effective use of system resources\n          \n          \n            \n            In some cases, reducing the number of shards in a cluster while maintaining the\n          \n          \n            \n            same amount of data leads to a more effective use of system resources", "author": "debadair", "createdAt": "2020-04-21T00:16:14Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,148 @@\n+[[avoid-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the\n+same amount of data will lead to a more effective use of system resources", "originalCommit": "82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3ODI5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55480#discussion_r411778291", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            (CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d.\n          \n          \n            \n            (CPU, RAM, IO). In these situations, we consider the cluster _oversharded_.", "author": "debadair", "createdAt": "2020-04-21T00:16:33Z", "path": "docs/reference/how-to/avoid-oversharding.asciidoc", "diffHunk": "@@ -0,0 +1,148 @@\n+[[avoid-oversharding]]\n+== Avoid oversharding\n+\n+Sometimes reducing the number of shards in a cluster while maintaining the\n+same amount of data will lead to a more effective use of system resources\n+(CPU, RAM, IO). In these situations, we consider the cluster \u201coversharded\u201d.", "originalCommit": "82277745210bb2b0cef58e95723bf7ef9b4c2ba0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fa85da50d83b7b311d49c61291dc8516c6dd44f1", "url": "https://github.com/elastic/elasticsearch/commit/fa85da50d83b7b311d49c61291dc8516c6dd44f1", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: debadair <debadair@elastic.co>", "committedDate": "2020-04-21T12:29:13Z", "type": "commit"}, {"oid": "87c4671e277dd07d9200f38eec1997859eb31351", "url": "https://github.com/elastic/elasticsearch/commit/87c4671e277dd07d9200f38eec1997859eb31351", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: debadair <debadair@elastic.co>", "committedDate": "2020-04-21T12:29:23Z", "type": "commit"}, {"oid": "79a9878e077f4f3b8d7311cc1852290515432bb1", "url": "https://github.com/elastic/elasticsearch/commit/79a9878e077f4f3b8d7311cc1852290515432bb1", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: debadair <debadair@elastic.co>", "committedDate": "2020-04-21T12:29:36Z", "type": "commit"}, {"oid": "4723d5319bd355ec05dc7bbccd26ba8bac526e34", "url": "https://github.com/elastic/elasticsearch/commit/4723d5319bd355ec05dc7bbccd26ba8bac526e34", "message": "Update docs/reference/how-to/avoid-oversharding.asciidoc\n\nCo-Authored-By: debadair <debadair@elastic.co>", "committedDate": "2020-04-21T12:29:45Z", "type": "commit"}]}