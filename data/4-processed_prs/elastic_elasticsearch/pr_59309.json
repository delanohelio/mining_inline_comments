{"pr_number": 59309, "pr_title": "[ML] Autoscaling for machine learning", "pr_createdAt": "2020-07-09T15:30:07Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/59309", "timeline": [{"oid": "c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "url": "https://github.com/elastic/elasticsearch/commit/c4e1f9acb86b50fbe9dc0b4154249e4aa0bf4adb", "message": "[ML] Adding ML autoscaling decider integration", "committedDate": "2020-08-25T12:06:16Z", "type": "forcePushed"}, {"oid": "e5f75e57a799454d623f5ebc1a3acbca56388c1a", "url": "https://github.com/elastic/elasticsearch/commit/e5f75e57a799454d623f5ebc1a3acbca56388c1a", "message": "[ML] Adding ML autoscaling decider integration", "committedDate": "2020-08-25T12:21:24Z", "type": "forcePushed"}, {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "url": "https://github.com/elastic/elasticsearch/commit/4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "message": "[ML] Adding ML autoscaling decider integration", "committedDate": "2020-08-27T18:20:54Z", "type": "commit"}, {"oid": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "url": "https://github.com/elastic/elasticsearch/commit/4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "message": "[ML] Adding ML autoscaling decider integration", "committedDate": "2020-08-27T18:20:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyMjY2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482922663", "bodyText": "I don't see why this is being changed.  The default model memory limit is 1GB, but if a user wants to set 2GB for one of their jobs then we shouldn't out-of-the-box stop them doing that.", "author": "droberts195", "createdAt": "2020-09-03T11:58:49Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/MachineLearningField.java", "diffHunk": "@@ -21,8 +23,9 @@\n     public static final Setting<Boolean> AUTODETECT_PROCESS =\n             Setting.boolSetting(\"xpack.ml.autodetect_process\", true, Setting.Property.NodeScope);\n     public static final Setting<ByteSizeValue> MAX_MODEL_MEMORY_LIMIT =\n-            Setting.memorySizeSetting(\"xpack.ml.max_model_memory_limit\", ByteSizeValue.ZERO,\n-                    Setting.Property.Dynamic, Setting.Property.NodeScope);\n+            Setting.memorySizeSetting(\"xpack.ml.max_model_memory_limit\",\n+                new ByteSizeValue(AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB, ByteSizeUnit.MB),", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyNDQ1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482924459", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);\n          \n          \n            \n                private static final TimeValue DEFAULT_ANALYTICS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);", "author": "droberts195", "createdAt": "2020-09-03T12:02:06Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderConfiguration;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    private static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_MIN_NUM_NODES = 1;\n+\n+    private static final TimeValue DEFAULT_ANOMALY_JOB_TIME_IN_QUEUE = TimeValue.ZERO;\n+    private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjkyNDYxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482924619", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final ParseField ANALYSIS_JOB_TIME_IN_QUEUE = new ParseField(\"analysis_job_time_in_queue\");\n          \n          \n            \n                private static final ParseField ANALYTICS_JOB_TIME_IN_QUEUE = new ParseField(\"analytics_job_time_in_queue\");", "author": "droberts195", "createdAt": "2020-09-03T12:02:23Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderConfiguration;\n+import org.elasticsearch.xpack.core.ml.utils.ExceptionsHelper;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    private static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_MIN_NUM_NODES = 1;\n+\n+    private static final TimeValue DEFAULT_ANOMALY_JOB_TIME_IN_QUEUE = TimeValue.ZERO;\n+    private static final TimeValue DEFAULT_ANALYSIS_JOB_TIME_IN_QUEUE = TimeValue.timeValueMinutes(5);\n+\n+    private static final ParseField MIN_NUM_NODES = new ParseField(\"min_num_nodes\");\n+    private static final ParseField ANOMALY_JOB_TIME_IN_QUEUE = new ParseField(\"anomaly_job_time_in_queue\");\n+    private static final ParseField ANALYSIS_JOB_TIME_IN_QUEUE = new ParseField(\"analysis_job_time_in_queue\");", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482947547", "bodyText": "I think maxJobMemoryBytes here should really be max_machine_memory_percent * biggestPossibleMlNodeSize rather than max_model_memory_limit.  In Cloud the two should be approximately the same, but in self managed they won't be.  Yet this logic is being used for both Cloud and self managed.  Obviously in self-managed we don't know biggestPossibleMlNodeSize if we think the user might add more nodes, so we'll have to assume they won't.\nOverall I would recommend:\n\nRevert the addition of a default value for max_model_memory_limit, so that it stays with the default for on-prem being unlimited and the default for Cloud being set by Cloud.\nAssume that if autoscaling is being used then max_model_memory_limit has been set sensibly by whatever is orchestrating the autoscaling.  Make it an error for autoscaling to be used if max_model_memory_limit is unlimited.\nIf autoscaling is being used then maxJobMemoryBytes can be set equal to max_model_memory_limit as it is currently.\nIf autoscaling is not being used then maxJobMemoryBytes can be calculated from the cluster state by taking the max of max_machine_memory_percent * ml.machine_memory over all nodes that have a value for ml.machine_memory in their node attributes.\n\nThis implies we need a way to tell if autoscaling is being used without waiting to see if the autoscaling APIs are called.  If there currently isn't a way then we need to raise this at the next meeting.\nWhat is going on here also needs to be commented in the code, as there is now a very complex yet subtle interaction with the Cloud orchestration code that future readers of the code won't understand.", "author": "droberts195", "createdAt": "2020-09-03T12:42:41Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportOpenJobAction.java", "diffHunk": "@@ -438,7 +443,7 @@ public OpenJobPersistentTasksExecutor(Settings settings, ClusterService clusterS\n             JobNodeSelector jobNodeSelector = new JobNodeSelector(clusterState, jobId, MlTasks.JOB_TASK_NAME, memoryTracker,\n                 job.allowLazyOpen() ? Integer.MAX_VALUE : maxLazyMLNodes, node -> nodeFilter(node, job));\n             return jobNodeSelector.selectNode(\n-                maxOpenJobs, maxConcurrentJobAllocations, maxMachineMemoryPercent, isMemoryTrackerRecentlyRefreshed);\n+                maxOpenJobs, maxConcurrentJobAllocations, maxMachineMemoryPercent, maxJobMemoryBytes, isMemoryTrackerRecentlyRefreshed);", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzE3NzUyMg==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r483177522", "bodyText": "The check of\n\nif autoscaling is being used\n\nIs tricky. I wonder if it is as simple as to check if the plugin is enabled or not.\n@henningandersen what do you think?", "author": "benwtrent", "createdAt": "2020-09-03T18:31:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQ2ODY1MA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r483468650", "bodyText": "I would prefer to decouple the two concerns. We should assume autoscaling is always \"enabled\". It now (or rather after #61575) returns a capacity that could be useful for alerting on-prem too and while the actual scaling in that case may not be automatic, the detection of a need to scale could be.\nI am no expert on the ML code, but AFAICS, if we pass down Long.MAX_VALUE when max_model_memory_limit is unset, the code in selectNode should behave as it did prior to this change?\nOn this specific part:\n\nif we think the user might add more nodes, so we'll have to assume they won't.\n\nI think you meant \"larger nodes\"? It is my understanding that hitting the limit for the recommended 64GB ML production nodes is unexpected. If a user started smaller (which could be fine), they might simply want to use bigger nodes even on-prem (which could be in a cloud) when the job cannot run?\nAbout 2:\n\nMake it an error for autoscaling to be used if max_model_memory_limit is unlimited.\n\nI think this is coupling the concerns too tightly. In our cloud, things would be configured correctly. Other users of the autoscaling API (like on-prem alerting) would work fine, with the reaction resulting in either setting the limit or upgrading the nodes?\nAbout 4:\n\nIf autoscaling is not being used then ...\n\nWould this not have to use the max_model_memory_limit if set and only if not revert to max_machine_memory_percent * ml.machine_memory? I fail to see how this would also not work out fine in both cloud and on-prem?\nMy response to 2 and 4 are clearly contradictory \ud83d\ude42 , I think either way works fine with my autoscaling glasses on.", "author": "henningandersen", "createdAt": "2020-09-04T08:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU3OTExOQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r483579119", "bodyText": "The way things work at the moment, if you try to open an ML job that has a model memory requirement of X and no node in the cluster has space for an ML job of size X then unless that specific job config says it's allowed to open lazily the open request will fail.\nOne of the changes in this PR is trying to move from \"no node in the cluster has space for an ML job of size X\" to \"no node in the cluster or that might be added to the cluster has space for an ML job of size X\".\n\nI think you meant \"larger nodes\"?\n\nIt could be larger nodes or more nodes.  Larger nodes in the case where somebody is trying to open a job that wouldn't fit on any current ML node even if no other jobs were running.  But there is also the case where somebody is trying to open a job that would fit on an existing ML node if it was empty, but will not fit anywhere due to already-running ML jobs.\n\nIt is my understanding that hitting the limit for the recommended 64GB ML production nodes is unexpected.\n\nThis is related - it's unexpected that somebody would open a single job that wouldn't fit on an empty 64GB ML node.  But if they already have 10 jobs running on that node then quite a small job might not fit on it.  Also, although we have a best practices doc that says it's best to run 64GB ML nodes in production I know there are quite big ML users who don't run such big nodes in their self-managed clusters.\n\nWould this not have to use the max_model_memory_limit if set and only if not revert to max_machine_memory_percent * ml.machine_memory? I fail to see how this would also not work out fine in both cloud and on-prem?\n\nThere are edge cases outside of Cloud.  In Cloud the intention is that max_model_memory_limit is set such that jobs cannot be created with a model memory limit that would be too big to ever run.  In self-managed it is possible that this setting is used for another reason, for example an administrator might want to prevent users creating single jobs that can use all their ML capacity.  The setting only applies when the model memory limit configuration is changed.  So the following scenario is possible:\n\nCluster has a single 8GB ML node\nCluster starts off with max_model_memory_limit unset\nUser creates a job with a model memory limit of 2GB\nmax_model_memory_limit is set to 1GB\nUser opens their job with a model memory limit of 2GB and it runs fine\n\nBut this scenario wouldn't work if we are using max_model_memory_limit as a proxy for \"what is the biggest job that could ever be run in this cluster\".\nAll these problems come from trying to avoid having a new setting for \"what is the biggest ML node that could ever be added to this cluster\".  Maybe it's best to just introduce that setting, have Cloud set it, and if it's unset make the default \"the biggest ML node currently in the cluster\".  I guess it could vary between AWS, GCP and Azure anyway, as the nominal 64GB node might be 58GB or 62GB in some regions.", "author": "droberts195", "createdAt": "2020-09-04T12:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgwMzU4NA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r485803584", "bodyText": "I think we should stick to decoupling this as originally planned.\nAdding a new setting is OK, IMO. Its lower cognitive overhead and by default it is current maximum node size in the cluster.", "author": "benwtrent", "createdAt": "2020-09-09T17:44:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk0NzU0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1MTMyOA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482951328", "bodyText": "I wonder if we could make a rule that autoscaling cannot be used in a mixed version cluster where some of the nodes are on a version that doesn't implement autoscaling?  If we can then this can be hardcoded to true even now.", "author": "droberts195", "createdAt": "2020-09-03T12:48:59Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {\n+            logger.debug(\"job memory tracker is stale. Request refresh before making scaling decision.\");\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+            return new AutoscalingDecision(\n+                name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"job memory tracker is stale. Unable to make accurate scale down recommendation\");\n+        }\n+        // TODO: remove in 8.0.0\n+        boolean allNodesHaveDynamicMaxWorkers = clusterState.getNodes().getMinNodeVersion().onOrAfter(Version.V_7_2_0);", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzE3ODU2OA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r483178568", "bodyText": "I think this is the case. Especially the \"normal\" way to upgrade is to upgrade everything BUT master and then upgrade master last.\nSo, the \"oldest\" node should be master. Indicating that all nodes need to be \"at least\" the version of master and thus the version that has this service installed.\nI think it should be safe to say that a mixed cluster environment of nodes that do and do not support autoscaling is not possible.\nRight @henningandersen ?", "author": "benwtrent", "createdAt": "2020-09-03T18:33:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1MTMyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQzOTgzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r483439839", "bodyText": "I agree that this case should be handled generally. And with masters being upgraded last, it should even be a rare problem already. I opened #61958 to clarify this and also what to do in general about mixed version clusters. I think above check can be removed also in 7.x.", "author": "henningandersen", "createdAt": "2020-09-04T07:31:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1MTMyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1OTM2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482959366", "bodyText": "One nasty thing is that in Cloud the value of max_machine_memory_percent will change as the nodes are scaled: https://github.com/elastic/cloud/blob/65712c90f8b1bc67df949c7b711d5aff5e1f43ea/scala-services/constructor/src/main/scala/no/found/constructor/steps/ml/OptimizeMlClusterSettings.scala#L191-L194\nAs the ML nodes get bigger the JVM occupies a smaller proportion of the node, so a bigger proportion is available for ML native processes.  So as the nodes get bigger max_machine_memory_percent increases.  This is potentially suboptimal but not a disaster when scaling up - we might scale up one step more than necessary.  But after scaling down it might mean the jobs don't fit in the permitted memory. \ud83d\ude31\nIdeally we would do the calculation with the value of max_machine_memory_percent that would apply at the size we were scaling to, both when scaling up and down.  But this requires that we know how max_machine_memory_percent changes as the node size changes.  So, either we have to duplicate the Cloud formula in ES code (and assume only Cloud will use autoscaling), or add a setting that is an alternative way to specify max_machine_memory as an arbitrary function of node size and JVM heap size instead of as a percentage of node size.  That would actually make the Cloud code simpler in the future, as they could then just put their formula in a static setting instead of having to dynamically change the value for different node sizes.", "author": "droberts195", "createdAt": "2020-09-03T13:01:32Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {\n+            logger.debug(\"job memory tracker is stale. Request refresh before making scaling decision.\");\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+            return new AutoscalingDecision(\n+                name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"job memory tracker is stale. Unable to make accurate scale down recommendation\");\n+        }\n+        // TODO: remove in 8.0.0\n+        boolean allNodesHaveDynamicMaxWorkers = clusterState.getNodes().getMinNodeVersion().onOrAfter(Version.V_7_2_0);\n+        List<NodeLoadDetector.NodeLoad> nodeLoads = new ArrayList<>();\n+        boolean isMemoryAccurateFlag = true;\n+        for (DiscoveryNode node : nodes) {\n+            NodeLoadDetector.NodeLoad nodeLoad = nodeLoadDetector.detectNodeLoad(clusterState,\n+                allNodesHaveDynamicMaxWorkers,\n+                node,\n+                maxOpenJobs,\n+                maxMachineMemoryPercent,", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgwODA0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r485808041", "bodyText": "or add a setting that is an alternative way to specify max_machine_memory as an arbitrary function of node size and JVM heap size instead of as a percentage of node size.\n\nYeesh...this sounds error prone :(.\nI also don't think this is possible given the current ways we parse settings. How would users even provide the function for use to use? Just a string setting that we parse into a script D: ???\nThis is a pickle...", "author": "benwtrent", "createdAt": "2020-09-09T17:52:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk1OTM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk2Mzc5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r482963796", "bodyText": "The definition of isRecentlyRefreshed() is currently \"in the last 90 seconds\".  We should probably change that so it's greater than the expected interval at which the autoscaling decider is called.  Otherwise we'll get situations where the memory tracker is stale on every single call to autoscaling.  I expect this won't happen as we scale up, because we refresh the memory tracker when attempting to open jobs.  But for scaling down when jobs are being closed and no new jobs opened we could easily get into the state where we repeatedly say \"no scale\" because the memory tracker is stale.  Maybe there is a way to tie the two intervals together in code instead of hardcoding MlMemoryTracker.RECENT_UPDATE_THRESHOLD to some value in the Cloud code.", "author": "droberts195", "createdAt": "2020-09-03T13:08:41Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecision;\n+import org.elasticsearch.xpack.autoscaling.decision.AutoscalingDecisionType;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements\n+    AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final Map<String, Long> anomalyJobsTimeInQueue;\n+    private final Map<String, Long> analyticsJobsTimeInQueue;\n+    private final Supplier<Long> timeSupplier;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.analyticsJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.anomalyJobsTimeInQueue = new ConcurrentHashMap<>();\n+        this.timeSupplier = timeSupplier;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            nodeLoadDetector.getMlMemoryTracker().asyncRefresh();\n+        }\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDecision scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            previousTimeStamp = lastTimeToScale;\n+        }\n+        final long timeDiff = Math.max(0L, this.lastTimeToScale - previousTimeStamp);\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        List<DiscoveryNode> nodes = clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+\n+        final AutoscalingDecision scaleUpDecision = checkForScaleUp(decider,\n+            nodes,\n+            anomalyDetectionTasks,\n+            dataframeAnalyticsTasks,\n+            timeDiff);\n+        if (AutoscalingDecisionType.SCALE_UP.equals(scaleUpDecision.type())) {\n+            return scaleUpDecision;\n+        }\n+\n+        final AutoscalingDecision scaleDownDecision = checkForScaleDown(decider, nodes, clusterState);\n+        if (AutoscalingDecisionType.SCALE_DOWN.equals(scaleDownDecision.type())) {\n+            return scaleDownDecision;\n+        }\n+\n+        return new AutoscalingDecision(name(),\n+            AutoscalingDecisionType.NO_SCALE,\n+            scaleUpDecision.reason() + \"|\" + scaleDownDecision.reason());\n+    }\n+\n+    AutoscalingDecision checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                        List<DiscoveryNode> nodes,\n+                                        Collection<PersistentTask<?>> anomalyDetectionTasks,\n+                                        Collection<PersistentTask<?>> dataframeAnalyticsTasks,\n+                                        long timeSinceLastCheckMs) {\n+        Set<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toSet());\n+        Set<String> waitingAnalysisJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toSet());\n+        anomalyJobsTimeInQueue.keySet().retainAll(waitingAnomalyJobs);\n+        analyticsJobsTimeInQueue.keySet().retainAll(waitingAnalysisJobs);\n+\n+        if (waitingAnomalyJobs.isEmpty() == false || waitingAnalysisJobs.isEmpty() == false || nodes.size() < decider.getMinNumNodes()) {\n+            if (nodes.size() < decider.getMinNumNodes() || nodes.isEmpty()) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"number of machine learning nodes [\"\n+                        + nodes.size()\n+                        + \"] is below the configured minimum number of [\"\n+                        + decider.getMinNumNodes()\n+                        + \"] or is zero\");\n+            }\n+            Set<String> timedUpJobs = new HashSet<>();\n+            for (String jobId : waitingAnomalyJobs) {\n+                long time = anomalyJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnomalyJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            for (String jobId : waitingAnalysisJobs) {\n+                long time = analyticsJobsTimeInQueue.compute(jobId, (k, v) -> v == null ? 0L : v + timeSinceLastCheckMs);\n+                if (time >= decider.getAnalysisJobTimeInQueue().getMillis()) {\n+                    timedUpJobs.add(jobId);\n+                }\n+            }\n+            if (timedUpJobs.isEmpty() == false) {\n+                return new AutoscalingDecision(name(),\n+                    AutoscalingDecisionType.SCALE_UP,\n+                    \"jobs \" + timedUpJobs + \" have been waiting for assignment too long\");\n+            }\n+        }\n+        return new AutoscalingDecision(name(), AutoscalingDecisionType.NO_SCALE, \"no jobs have waited long enough for assignment\");\n+    }\n+\n+    AutoscalingDecision checkForScaleDown(MlAutoscalingDeciderConfiguration decider,\n+                                          List<DiscoveryNode> nodes,\n+                                          ClusterState clusterState) {\n+        if (nodes.size() == decider.getMinNumNodes()) {\n+            return new AutoscalingDecision(name(),\n+                AutoscalingDecisionType.NO_SCALE,\n+                \"| already at configured minimum node count [\"\n+                    + decider.getMinNumNodes()\n+                    +\"]\");\n+        }\n+        // kick of a refresh if our data around memory usage is stale.\n+        if (nodeLoadDetector.getMlMemoryTracker().isRecentlyRefreshed() == false) {", "originalCommit": "4ceb88163e9ed7759ccdc6aeedf2a147d185f18f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxMjQwMg==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r485812402", "bodyText": "I see two possibilities:\n\nHave this aging rate configurable via a setting. This way consumers of this API can set it to some value larger than the polling interval\nHave this refresh rate tied to some value included in the polling itself. Initially, if it is stale given the interval of 90 sec, we say its stale. Then we keep track of the last API call time and if we have refreshed since that time (barring any other changes), we say we have fresh data.\n\nI think the second option is doable and the nicest with very little configuration for the user. We will just have to think about what it means for job values to be stale and scaling down.", "author": "benwtrent", "createdAt": "2020-09-09T18:00:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk2Mzc5Ng=="}], "type": "inlineReview"}, {"oid": "dacbffc98700f630d39e45d2b7a32e04a3519d80", "url": "https://github.com/elastic/elasticsearch/commit/dacbffc98700f630d39e45d2b7a32e04a3519d80", "message": "Merge branch 'master' into feature/ml-autoscaling-integration", "committedDate": "2020-09-14T13:25:08Z", "type": "commit"}, {"oid": "f5a1d8031fec0c9e157ee8ea559243bf33335694", "url": "https://github.com/elastic/elasticsearch/commit/f5a1d8031fec0c9e157ee8ea559243bf33335694", "message": "Merge branch 'master' into feature/ml-autoscaling-integration", "committedDate": "2020-09-22T19:11:31Z", "type": "commit"}, {"oid": "bb639c608f778d55c3f2f465886b91f224232837", "url": "https://github.com/elastic/elasticsearch/commit/bb639c608f778d55c3f2f465886b91f224232837", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-09-28T12:09:28Z", "type": "commit"}, {"oid": "040b16d7239743505b678df7eb60db1d365dd6ab", "url": "https://github.com/elastic/elasticsearch/commit/040b16d7239743505b678df7eb60db1d365dd6ab", "message": "adjusting autoscaling decider", "committedDate": "2020-09-28T16:57:49Z", "type": "commit"}, {"oid": "d199dcac1756ddb091456b104f38591165cdac59", "url": "https://github.com/elastic/elasticsearch/commit/d199dcac1756ddb091456b104f38591165cdac59", "message": "fixing setting handling", "committedDate": "2020-09-28T17:37:43Z", "type": "commit"}, {"oid": "1394ca18bac61340d98480907f09730ae02d25b4", "url": "https://github.com/elastic/elasticsearch/commit/1394ca18bac61340d98480907f09730ae02d25b4", "message": "addressing scale up", "committedDate": "2020-09-28T19:54:34Z", "type": "commit"}, {"oid": "5b267e01c69d781be7149bb7ce0a66f08acaa2d7", "url": "https://github.com/elastic/elasticsearch/commit/5b267e01c69d781be7149bb7ce0a66f08acaa2d7", "message": "finalizing scale up logic", "committedDate": "2020-09-29T20:26:36Z", "type": "commit"}, {"oid": "8b8462ee547d4c8168de7eb0d4eec30cd8de135f", "url": "https://github.com/elastic/elasticsearch/commit/8b8462ee547d4c8168de7eb0d4eec30cd8de135f", "message": "adding downscale delay option", "committedDate": "2020-10-13T14:55:41Z", "type": "commit"}, {"oid": "0aa0301841f7b33606462c488db825d47916964f", "url": "https://github.com/elastic/elasticsearch/commit/0aa0301841f7b33606462c488db825d47916964f", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-10-13T14:59:17Z", "type": "commit"}, {"oid": "b2139a584e24c3e0110d0ed2fe22dd13604ff9b5", "url": "https://github.com/elastic/elasticsearch/commit/b2139a584e24c3e0110d0ed2fe22dd13604ff9b5", "message": "adding native memory calculator for special dynamic case", "committedDate": "2020-10-14T13:45:36Z", "type": "commit"}, {"oid": "1e8183b865758df1758a60c916c9576c23bd1897", "url": "https://github.com/elastic/elasticsearch/commit/1e8183b865758df1758a60c916c9576c23bd1897", "message": "adjusting how we calculate memory percentage", "committedDate": "2020-10-14T20:42:23Z", "type": "commit"}, {"oid": "453d4a7914fd658f379ab65ce7b6c58eefa3f670", "url": "https://github.com/elastic/elasticsearch/commit/453d4a7914fd658f379ab65ce7b6c58eefa3f670", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-10-14T20:57:46Z", "type": "commit"}, {"oid": "8e846e41ad47a51c7124c326cf45ef916036db94", "url": "https://github.com/elastic/elasticsearch/commit/8e846e41ad47a51c7124c326cf45ef916036db94", "message": "handling native size to node size and scale down", "committedDate": "2020-10-15T18:32:52Z", "type": "commit"}, {"oid": "28f2257e31961dcbe6dd5d66f042b6a921cb59a3", "url": "https://github.com/elastic/elasticsearch/commit/28f2257e31961dcbe6dd5d66f042b6a921cb59a3", "message": "Merge branch 'master' into feature/ml-autoscaling-integration", "committedDate": "2020-10-21T17:36:24Z", "type": "commit"}, {"oid": "a6652454f38af740c163f27d0859f49559dab453", "url": "https://github.com/elastic/elasticsearch/commit/a6652454f38af740c163f27d0859f49559dab453", "message": "updating from master", "committedDate": "2020-10-21T18:16:41Z", "type": "commit"}, {"oid": "6e45e6aec124d317b8d130afe1ae0574c2c67686", "url": "https://github.com/elastic/elasticsearch/commit/6e45e6aec124d317b8d130afe1ae0574c2c67686", "message": "undo bad delete", "committedDate": "2020-10-21T18:17:20Z", "type": "commit"}, {"oid": "d5e065b526dcef8f2e7bb54edacb8f8bf01dbcc2", "url": "https://github.com/elastic/elasticsearch/commit/d5e065b526dcef8f2e7bb54edacb8f8bf01dbcc2", "message": "minor adjustments", "committedDate": "2020-10-21T18:21:31Z", "type": "commit"}, {"oid": "a4a7f237410cfe9ccaa7124070e2779ff35256fa", "url": "https://github.com/elastic/elasticsearch/commit/a4a7f237410cfe9ccaa7124070e2779ff35256fa", "message": "Merge branch 'master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-10T19:54:25Z", "type": "commit"}, {"oid": "f4ed9829300f5e943cb99646588a400c5816fea9", "url": "https://github.com/elastic/elasticsearch/commit/f4ed9829300f5e943cb99646588a400c5816fea9", "message": "fixing tests", "committedDate": "2020-11-10T21:07:02Z", "type": "commit"}, {"oid": "a00e1290b07f854311b2e1679035e262ff9156c1", "url": "https://github.com/elastic/elasticsearch/commit/a00e1290b07f854311b2e1679035e262ff9156c1", "message": "fixing tests", "committedDate": "2020-11-11T16:42:46Z", "type": "commit"}, {"oid": "e47a136b8a3aa9f0b4e03e60d627d625324281a6", "url": "https://github.com/elastic/elasticsearch/commit/e47a136b8a3aa9f0b4e03e60d627d625324281a6", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-11T16:43:07Z", "type": "commit"}, {"oid": "7fbe30e60f24a14d7ba0f84bb6c572bf77eeafc2", "url": "https://github.com/elastic/elasticsearch/commit/7fbe30e60f24a14d7ba0f84bb6c572bf77eeafc2", "message": "adding scaledown tests, refactoring nodeload class", "committedDate": "2020-11-11T21:23:58Z", "type": "commit"}, {"oid": "8cf46edcbe8a2707a1dda235694e9359bec086f3", "url": "https://github.com/elastic/elasticsearch/commit/8cf46edcbe8a2707a1dda235694e9359bec086f3", "message": "adding tests and validations", "committedDate": "2020-11-12T15:25:56Z", "type": "commit"}, {"oid": "66843332894db3833d1851b0ec0bffea9b6f7453", "url": "https://github.com/elastic/elasticsearch/commit/66843332894db3833d1851b0ec0bffea9b6f7453", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-12T15:26:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjI5Mg==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r522206292", "bodyText": "nit: it can just be Property.NodeScope like in the setting above", "author": "droberts195", "createdAt": "2020-11-12T15:45:01Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/MachineLearning.java", "diffHunk": "@@ -478,6 +482,17 @@\n             Property.NodeScope\n         );\n \n+    /**\n+     * This is the maximum possible node size for a machine learning node. It is useful when determining if a job could ever be opened\n+     * on the cluster.\n+     *\n+     * If the value is the default special case of `0b`, that means the value is ignored when assigning jobs.\n+     */\n+    public static final Setting<ByteSizeValue> MAX_ML_NODE_SIZE = Setting.byteSizeSetting(\n+        \"xpack.ml.max_ml_node_size\",\n+        ByteSizeValue.ZERO,\n+        Setting.Property.NodeScope);", "originalCommit": "66843332894db3833d1851b0ec0bffea9b6f7453", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM2Mzg3MA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r522363870", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Setting.Property.NodeScope);\n          \n          \n            \n                    Property.NodeScope);", "author": "benwtrent", "createdAt": "2020-11-12T19:28:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjIwNjI5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjI2MDMxMg==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r522260312", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        this.downScaleDelay = downScaleDelay;\n          \n          \n            \n                        this.downScaleDelay = Objects.requireNonNull(downScaleDelay);", "author": "droberts195", "createdAt": "2020-11-12T16:55:10Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderConfiguration.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderConfiguration;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+public class MlAutoscalingDeciderConfiguration implements AutoscalingDeciderConfiguration {\n+    static final String NAME = \"ml\";\n+\n+    private static final int DEFAULT_ANOMALY_JOBS_IN_QUEUE = 0;\n+    private static final int DEFAULT_ANALYTICS_JOBS_IN_QUEUE = 0;\n+\n+    private static final ParseField NUM_ANOMALY_JOBS_IN_QUEUE = new ParseField(\"num_anomaly_jobs_in_queue\");\n+    private static final ParseField NUM_ANALYTICS_JOBS_IN_QUEUE = new ParseField(\"num_analytics_jobs_in_queue\");\n+    private static final ParseField DOWN_SCALE_DELAY = new ParseField(\"down_scale_delay\");\n+\n+    private static final ObjectParser<MlAutoscalingDeciderConfiguration.Builder, Void> PARSER = new ObjectParser<>(NAME,\n+        MlAutoscalingDeciderConfiguration.Builder::new);\n+\n+    static {\n+        PARSER.declareInt(MlAutoscalingDeciderConfiguration.Builder::setNumAnomalyJobsInQueue, NUM_ANOMALY_JOBS_IN_QUEUE);\n+        PARSER.declareInt(MlAutoscalingDeciderConfiguration.Builder::setNumAnalyticsJobsInQueue, NUM_ANALYTICS_JOBS_IN_QUEUE);\n+        PARSER.declareString(MlAutoscalingDeciderConfiguration.Builder::setDownScaleDelay, DOWN_SCALE_DELAY);\n+    }\n+\n+    public static MlAutoscalingDeciderConfiguration parse(final XContentParser parser) {\n+        return PARSER.apply(parser, null).build();\n+    }\n+\n+    private final int numAnomalyJobsInQueue;\n+    private final int numAnalyticsJobsInQueue;\n+    private final TimeValue downScaleDelay;\n+\n+    MlAutoscalingDeciderConfiguration(int numAnomalyJobsInQueue, int numAnalyticsJobsInQueue, TimeValue downScaleDelay) {\n+        if (numAnomalyJobsInQueue < 0) {\n+            throw new IllegalArgumentException(\"[\" + NUM_ANOMALY_JOBS_IN_QUEUE.getPreferredName() + \"] must be non-negative\");\n+        }\n+        if (numAnalyticsJobsInQueue < 0) {\n+            throw new IllegalArgumentException(\"[\" + NUM_ANALYTICS_JOBS_IN_QUEUE.getPreferredName() + \"] must be non-negative\");\n+        }\n+        this.numAnalyticsJobsInQueue = numAnalyticsJobsInQueue;\n+        this.numAnomalyJobsInQueue = numAnomalyJobsInQueue;\n+        this.downScaleDelay = downScaleDelay;\n+    }\n+\n+    public MlAutoscalingDeciderConfiguration(StreamInput in) throws IOException {\n+        numAnomalyJobsInQueue = in.readVInt();\n+        numAnalyticsJobsInQueue = in.readVInt();\n+        downScaleDelay = in.readTimeValue();\n+    }\n+\n+    @Override\n+    public String name() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public String getWriteableName() {\n+        return NAME;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeVInt(numAnomalyJobsInQueue);\n+        out.writeVInt(numAnalyticsJobsInQueue);\n+        out.writeTimeValue(downScaleDelay);\n+    }\n+\n+    public int getNumAnomalyJobsInQueue() {\n+        return numAnomalyJobsInQueue;\n+    }\n+\n+    public int getNumAnalyticsJobsInQueue() {\n+        return numAnalyticsJobsInQueue;\n+    }\n+\n+    public TimeValue getDownScaleDelay() {\n+        return downScaleDelay;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        MlAutoscalingDeciderConfiguration that = (MlAutoscalingDeciderConfiguration) o;\n+        return numAnomalyJobsInQueue == that.numAnomalyJobsInQueue &&\n+            numAnalyticsJobsInQueue == that.numAnalyticsJobsInQueue &&\n+            Objects.equals(downScaleDelay, that.downScaleDelay);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(numAnomalyJobsInQueue, numAnalyticsJobsInQueue, downScaleDelay);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(NUM_ANOMALY_JOBS_IN_QUEUE .getPreferredName(), numAnomalyJobsInQueue);\n+        builder.field(NUM_ANALYTICS_JOBS_IN_QUEUE.getPreferredName(), numAnalyticsJobsInQueue);\n+        builder.field(DOWN_SCALE_DELAY.getPreferredName(), downScaleDelay.getStringRep());\n+        builder.endObject();\n+        return builder;\n+    }\n+\n+    public static Builder builder() {\n+        return new Builder();\n+    }\n+\n+    public static class Builder {\n+\n+        private int numAnomalyJobsInQueue = DEFAULT_ANOMALY_JOBS_IN_QUEUE;\n+        private int numAnalyticsJobsInQueue = DEFAULT_ANALYTICS_JOBS_IN_QUEUE;\n+        private TimeValue downScaleDelay = TimeValue.ZERO;\n+\n+        public Builder setNumAnomalyJobsInQueue(int numAnomalyJobsInQueue) {\n+            this.numAnomalyJobsInQueue = numAnomalyJobsInQueue;\n+            return this;\n+        }\n+\n+        public Builder setNumAnalyticsJobsInQueue(int numAnalyticsJobsInQueue) {\n+            this.numAnalyticsJobsInQueue = numAnalyticsJobsInQueue;\n+            return this;\n+        }\n+\n+        Builder setDownScaleDelay(String unparsedTimeValue) {\n+            return setDownScaleDelay(TimeValue.parseTimeValue(unparsedTimeValue, DOWN_SCALE_DELAY.getPreferredName()));\n+        }\n+\n+        public Builder setDownScaleDelay(TimeValue downScaleDelay) {\n+            this.downScaleDelay = downScaleDelay;", "originalCommit": "66843332894db3833d1851b0ec0bffea9b6f7453", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6dc17aa48d205e1d9fc7128cf5efaf57dd0c884f", "url": "https://github.com/elastic/elasticsearch/commit/6dc17aa48d205e1d9fc7128cf5efaf57dd0c884f", "message": "Apply suggestions from code review\n\nCo-authored-by: David Roberts <dave.roberts@elastic.co>", "committedDate": "2020-11-12T19:28:48Z", "type": "commit"}, {"oid": "f023e77f700d178b604ae26625de7d35bc2b10e5", "url": "https://github.com/elastic/elasticsearch/commit/f023e77f700d178b604ae26625de7d35bc2b10e5", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-16T13:51:09Z", "type": "commit"}, {"oid": "b181b409a5b8135d95086177b416e8b3935ac2d8", "url": "https://github.com/elastic/elasticsearch/commit/b181b409a5b8135d95086177b416e8b3935ac2d8", "message": "fixing compilation", "committedDate": "2020-11-16T14:02:35Z", "type": "commit"}, {"oid": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "url": "https://github.com/elastic/elasticsearch/commit/94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-16T14:02:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MDEzMw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524290133", "bodyText": "I think this is dangerous.  It means we could scale the cluster significantly due to the memory tracker not having the required information.  It would also be difficult to diagnose this based on a user complaint that their cluster scaled wildly.  For example, suppose there are 50 jobs each with a memory requirement of 0.1GB, total requirement 5GB, but then due to a glitch in the memory tracker we treat that as 50GB and scale up to a much more costly cluster.\nSince this is for scaling up, I think we should use 0 for jobs where we have no memory information, so we'll only scale up if the sum of the memory requirements for jobs that we know the memory requirement for imply a scale up.\n(Same a few lines below.)", "author": "droberts195", "createdAt": "2020-11-16T14:04:42Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,561 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param defaultSize    The default memory size (if the sizeFunction returns null)\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            long defaultSize,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? defaultSize : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build());\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build());\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                // TODO Better default???\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,", "originalCommit": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMwMjE3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524302179", "bodyText": "Since this is for scaling up, I think we should use 0 for jobs where we have no memory information\n\nThe reason I did do this was this caused a scale_up to be delayed. If we are fine with the scale up being delayed, then its fine.", "author": "benwtrent", "createdAt": "2020-11-16T14:21:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MDEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMDUyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524330521", "bodyText": "I think this should only happen once per master node though, if the Cloud infrastructure asks for a scaling decision immediately after that master node became master.  In that rare situation it might be better not to make a hasty decision that will need to get adjusted again at the next scaling decision.\nWhen a job is opened/started via the open/start API, we put a value for its memory requirement in the memory tracker.  And when a new master takes over, we recalculate the memory requirement of all jobs.\nSo the memory tracker should only return null for a job\u2019s memory requirement in the period between a new master being elected and the recalculation of all jobs\u2019 memory requirements completing.\nI think it would be good to log a warning if the default is used during a scale up decision.  We'll need this to debug the situation where a cluster doesn't scale at all due to some unforeseen problem.  But unless I am mistaken I think it will be very rare to see that log message.  In the case of a new master node being elected and an autoscaling decision being requested soon afterwards we should see the log message once.  And only in the case of some unforeseen problem will we see the log message repeatedly.", "author": "droberts195", "createdAt": "2020-11-16T14:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MDEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI5MjMxMg==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r524292312", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Even if we verify that memory usage is up today before checking node capacity, we could still run into stale information.\n          \n          \n            \n                    // Even if we verify that memory usage is up to date before checking node capacity, we could still run into stale information.", "author": "droberts195", "createdAt": "2020-11-16T14:07:55Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,561 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param defaultSize    The default memory size (if the sizeFunction returns null)\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            long defaultSize,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? defaultSize : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build());\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build());\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                // TODO Better default???\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                AnalysisLimits.DEFAULT_MODEL_MEMORY_LIMIT_MB,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(PersistentTask<?> task) {\n+        return getAnalyticsMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(String anomalyId) {\n+        return mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyId);\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(PersistentTask<?> task) {\n+        return getAnomalyMemoryRequirement(MlTasks.jobId(task.getId()));\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleDown(List<DiscoveryNode> nodes,\n+                                                         ClusterState clusterState,\n+                                                         long largestJob,\n+                                                         NativeMemoryCapacity currentCapacity,\n+                                                         MlScalingReason.Builder reasonBuilder) {\n+        List<NodeLoad> nodeLoads = new ArrayList<>();\n+        boolean isMemoryAccurateFlag = true;\n+        for (DiscoveryNode node : nodes) {\n+            NodeLoad nodeLoad = nodeLoadDetector.detectNodeLoad(clusterState,\n+                true,\n+                node,\n+                maxOpenJobs,\n+                maxMachineMemoryPercent,\n+                true,\n+                useAuto);\n+            if (nodeLoad.getError() != null) {\n+                logger.warn(\"[{}] failed to gather node load limits, failure [{}]\", node.getId(), nodeLoad.getError());\n+                return Optional.empty();\n+            }\n+            nodeLoads.add(nodeLoad);\n+            isMemoryAccurateFlag = isMemoryAccurateFlag && nodeLoad.isUseMemory();\n+        }\n+        // Even if we verify that memory usage is up today before checking node capacity, we could still run into stale information.", "originalCommit": "94a969e1b2b23eaa6c7e7b8814dc1471f0dde032", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "15a8a7b613ef44fd7cad1484d35a6b59eb6f56b0", "url": "https://github.com/elastic/elasticsearch/commit/15a8a7b613ef44fd7cad1484d35a6b59eb6f56b0", "message": "addressing PR concerns", "committedDate": "2020-11-16T14:55:00Z", "type": "commit"}, {"oid": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "url": "https://github.com/elastic/elasticsearch/commit/6536073d0983c0cc4c49f30c41392a2b2a23fff0", "message": "Merge remote-tracking branch 'upstream/master' into feature/ml-autoscaling-integration", "committedDate": "2020-11-16T14:55:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1Nzk2Nw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525057967", "bodyText": "Am I correct that getting here should be incredibly rare, because we checked isRecentlyRefreshed near the beginning of the scale method, so we'll only get here if the definition of \"recently\" was breached while the code of the scale method was running?\nIf this is correct then we should log a warning here, because if some strange bug causes us to get here more often then we'll want to know when dealing with the \"why doesn't my cluster scale\" support case.", "author": "droberts195", "createdAt": "2020-11-17T10:46:46Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);", "originalCommit": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTEzNDE5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525134197", "bodyText": "Am I correct that getting here should be incredibly rare\n\nWe will hit this predicate if scale_up fails due to memory being stale.\nWe will also hit this predicate if scale_down fails due to memory being stale (more rare).", "author": "benwtrent", "createdAt": "2020-11-17T12:59:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE1OTYzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525159635", "bodyText": "Oh yes, sorry.  When I wrote this I was thinking that scale was checking for staleness before both scale up and scale down, but it checks in between.", "author": "droberts195", "createdAt": "2020-11-17T13:40:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA1Nzk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2MDg1OA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525060858", "bodyText": "Please remove this comment, as staleness is already being checked elsewhere.", "author": "droberts195", "createdAt": "2020-11-17T10:51:16Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?", "originalCommit": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTgxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525069819", "bodyText": "Given that we check staleness at the beginning of the scale method I think it should impossible to return null here given where it's called from.  So:\n\nPlease add a Javadoc comment saying this method must only be called after checking the that memory tracker is recently refreshed\nAdd an assertion that the return value isn't null so we can catch unexpected situations in tests\nAdd a warning log so that if we return null in production and it causes a support case that autoscaling isn't working we have something in the log", "author": "droberts195", "createdAt": "2020-11-17T11:05:56Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        } else {\n+            return potentialResult;\n+        }\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);", "originalCommit": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE0NjE3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525146179", "bodyText": "Given that we check staleness at the beginning of the scale method I think it should impossible to return null here given where it's called from.\n\nThis method is also used in scale_up, which does not directly check for memory to not be stale.\nBut, in one place where this method is used (right before scale down), we have recently checked. I will put an assert there.", "author": "benwtrent", "createdAt": "2020-11-17T13:19:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE0ODAxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525148011", "bodyText": "I will also put an assert in scale down. If the node load returns saying that memory is stale, that is also a weird scenario and we should fail as we just recently checked.", "author": "benwtrent", "createdAt": "2020-11-17T13:22:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2OTkwNA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525069904", "bodyText": "Given that we check staleness at the beginning of the scale method I think it should impossible to return null here given where it's called from.  So:\n\nPlease add a Javadoc comment saying this method must only be called after checking the that memory tracker is recently refreshed\nAdd an assertion that the return value isn't null so we can catch unexpected situations in tests\nAdd a warning log so that if we return null in production and it causes a support case that autoscaling isn't working we have something in the log", "author": "droberts195", "createdAt": "2020-11-17T11:06:07Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderService.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.ml.autoscaling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.LocalNodeMasterListener;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.component.LifecycleListener;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata;\n+import org.elasticsearch.persistent.PersistentTasksCustomMetadata.PersistentTask;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderContext;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderResult;\n+import org.elasticsearch.xpack.autoscaling.capacity.AutoscalingDeciderService;\n+import org.elasticsearch.xpack.core.ml.MlTasks;\n+import org.elasticsearch.xpack.core.ml.action.StartDatafeedAction.DatafeedParams;\n+import org.elasticsearch.xpack.core.ml.dataframe.DataFrameAnalyticsState;\n+import org.elasticsearch.xpack.core.ml.job.config.JobState;\n+import org.elasticsearch.xpack.ml.MachineLearning;\n+import org.elasticsearch.xpack.ml.job.NodeLoad;\n+import org.elasticsearch.xpack.ml.job.NodeLoadDetector;\n+import org.elasticsearch.xpack.ml.process.MlMemoryTracker;\n+import org.elasticsearch.xpack.ml.utils.NativeMemoryCalculator;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.OptionalLong;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getDataFrameAnalyticsState;\n+import static org.elasticsearch.xpack.core.ml.MlTasks.getJobStateModifiedForReassignments;\n+import static org.elasticsearch.xpack.ml.job.JobNodeSelector.AWAITING_LAZY_ASSIGNMENT;\n+\n+public class MlAutoscalingDeciderService implements AutoscalingDeciderService<MlAutoscalingDeciderConfiguration>,\n+    LocalNodeMasterListener {\n+\n+    private static final Logger logger = LogManager.getLogger(MlAutoscalingDeciderService.class);\n+    private static final Duration DEFAULT_MEMORY_REFRESH_RATE = Duration.ofMinutes(15);\n+    private static final String MEMORY_STALE = \"unable to make scaling decision as job memory requirements are stale\";\n+    private static final long NO_SCALE_DOWN_POSSIBLE = -1L;\n+\n+    private final NodeLoadDetector nodeLoadDetector;\n+    private final MlMemoryTracker mlMemoryTracker;\n+    private final Supplier<Long> timeSupplier;\n+    private final boolean useAuto;\n+\n+    private volatile boolean isMaster;\n+    private volatile boolean running;\n+    private volatile int maxMachineMemoryPercent;\n+    private volatile int maxOpenJobs;\n+    private volatile long lastTimeToScale;\n+    private volatile long scaleDownDetected;\n+\n+    public MlAutoscalingDeciderService(MlMemoryTracker memoryTracker, Settings settings, ClusterService clusterService) {\n+        this(new NodeLoadDetector(memoryTracker), settings, clusterService, System::currentTimeMillis);\n+    }\n+\n+    MlAutoscalingDeciderService(NodeLoadDetector nodeLoadDetector,\n+                                Settings settings,\n+                                ClusterService clusterService,\n+                                Supplier<Long> timeSupplier) {\n+        this.nodeLoadDetector = nodeLoadDetector;\n+        this.mlMemoryTracker = nodeLoadDetector.getMlMemoryTracker();\n+        this.maxMachineMemoryPercent = MachineLearning.MAX_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.maxOpenJobs = MachineLearning.MAX_OPEN_JOBS_PER_NODE.get(settings);\n+        this.useAuto = MachineLearning.USE_AUTO_MACHINE_MEMORY_PERCENT.get(settings);\n+        this.timeSupplier = timeSupplier;\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_MACHINE_MEMORY_PERCENT,\n+            this::setMaxMachineMemoryPercent);\n+        clusterService.getClusterSettings().addSettingsUpdateConsumer(MachineLearning.MAX_OPEN_JOBS_PER_NODE, this::setMaxOpenJobs);\n+        clusterService.addLocalNodeMasterListener(this);\n+        clusterService.addLifecycleListener(new LifecycleListener() {\n+            @Override\n+            public void afterStart() {\n+                running = true;\n+                if (isMaster) {\n+                    mlMemoryTracker.asyncRefresh();\n+                }\n+            }\n+\n+            @Override\n+            public void beforeStop() {\n+                running = false;\n+            }\n+        });\n+    }\n+\n+    static OptionalLong getNodeJvmSize(DiscoveryNode node) {\n+        Map<String, String> nodeAttributes = node.getAttributes();\n+        OptionalLong value = OptionalLong.empty();\n+        String valueStr = nodeAttributes.get(MachineLearning.MAX_JVM_SIZE_NODE_ATTR);\n+        try {\n+            value = OptionalLong.of(Long.parseLong(valueStr));\n+        } catch (NumberFormatException e) {\n+            logger.debug(() -> new ParameterizedMessage(\n+                \"could not parse stored string value [{}] in node attribute [{}]\",\n+                valueStr,\n+                MachineLearning.MAX_JVM_SIZE_NODE_ATTR));\n+        }\n+        return value;\n+    }\n+\n+    static List<DiscoveryNode> getNodes(final ClusterState clusterState) {\n+        return clusterState.nodes()\n+            .mastersFirstStream()\n+            .filter(MachineLearning::isMlNode)\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * @param unassignedJobs The list of unassigned jobs\n+     * @param sizeFunction   Function providing the memory required for a job\n+     * @param maxNumInQueue  The number of unassigned jobs allowed.\n+     * @return The capacity needed to reduce the length of `unassignedJobs` to `maxNumInQueue`\n+     */\n+    static Optional<NativeMemoryCapacity> requiredCapacityForUnassignedJobs(List<String> unassignedJobs,\n+                                                                            Function<String, Long> sizeFunction,\n+                                                                            int maxNumInQueue) {\n+        List<Long> jobSizes = unassignedJobs\n+            .stream()\n+            // TODO do we want to verify memory requirements aren't stale? Or just consider `null` a fastpath?\n+            .map(sizeFunction)\n+            .map(l -> l == null ? 0L : l)\n+            .collect(Collectors.toList());\n+        // Only possible if unassignedJobs was empty.\n+        if (jobSizes.isEmpty()) {\n+            return Optional.empty();\n+        }\n+        jobSizes.sort(Comparator.comparingLong(Long::longValue).reversed());\n+        long tierMemory = 0L;\n+        long nodeMemory = jobSizes.get(0);\n+        Iterator<Long> iter = jobSizes.iterator();\n+        while (jobSizes.size() > maxNumInQueue && iter.hasNext()) {\n+            tierMemory += iter.next();\n+            iter.remove();\n+        }\n+        return Optional.of(new NativeMemoryCapacity(tierMemory, nodeMemory));\n+    }\n+\n+    private static Collection<PersistentTask<?>> anomalyDetectionTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.JOB_TASK_NAME,\n+            t -> getJobStateModifiedForReassignments(t).isAnyOf(JobState.OPENED, JobState.OPENING));\n+    }\n+\n+    private static Collection<PersistentTask<?>> dataframeAnalyticsTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATA_FRAME_ANALYTICS_TASK_NAME,\n+            t -> getDataFrameAnalyticsState(t).isAnyOf(DataFrameAnalyticsState.STARTED, DataFrameAnalyticsState.STARTING));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Collection<PersistentTask<DatafeedParams>> datafeedTasks(PersistentTasksCustomMetadata tasksCustomMetadata) {\n+        if (tasksCustomMetadata == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return tasksCustomMetadata.findTasks(MlTasks.DATAFEED_TASK_NAME, t -> true)\n+            .stream()\n+            .map(p -> (PersistentTask<DatafeedParams>) p)\n+            .collect(Collectors.toList());\n+    }\n+\n+    void setMaxMachineMemoryPercent(int maxMachineMemoryPercent) {\n+        this.maxMachineMemoryPercent = maxMachineMemoryPercent;\n+    }\n+\n+    void setMaxOpenJobs(int maxOpenJobs) {\n+        this.maxOpenJobs = maxOpenJobs;\n+    }\n+\n+    @Override\n+    public void onMaster() {\n+        isMaster = true;\n+        if (running) {\n+            mlMemoryTracker.asyncRefresh();\n+        }\n+    }\n+\n+    private void resetScaleDownCoolDown() {\n+        this.scaleDownDetected = NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    private boolean canScaleDown(TimeValue coolDown) {\n+        if (this.scaleDownDetected == NO_SCALE_DOWN_POSSIBLE) {\n+            return false;\n+        }\n+        return timeSupplier.get() - scaleDownDetected >= coolDown.millis();\n+    }\n+\n+    private boolean newScaleDownCheck() {\n+        return scaleDownDetected == NO_SCALE_DOWN_POSSIBLE;\n+    }\n+\n+    NativeMemoryCapacity currentScale(final List<DiscoveryNode> machineLearningNodes) {\n+        long[] mlMemory = machineLearningNodes.stream()\n+            .mapToLong(node -> NativeMemoryCalculator.allowedBytesForMl(node, maxMachineMemoryPercent, useAuto).orElse(0L))\n+            .toArray();\n+\n+        return new NativeMemoryCapacity(\n+            Arrays.stream(mlMemory).sum(),\n+            Arrays.stream(mlMemory).max().orElse(0L),\n+            // We assume that JVM size is universal, at least, the largest JVM indicates the largest node\n+            machineLearningNodes.stream()\n+                .map(MlAutoscalingDeciderService::getNodeJvmSize)\n+                .mapToLong(l -> l.orElse(0L))\n+                .boxed()\n+                .max(Long::compare)\n+                .orElse(null)\n+        );\n+    }\n+\n+    @Override\n+    public void offMaster() {\n+        isMaster = false;\n+    }\n+\n+    @Override\n+    public AutoscalingDeciderResult scale(MlAutoscalingDeciderConfiguration decider, AutoscalingDeciderContext context) {\n+        if (isMaster == false) {\n+            throw new IllegalArgumentException(\"request for scaling information is only allowed on the master node\");\n+        }\n+        final Duration memoryTrackingStale;\n+        long previousTimeStamp = this.lastTimeToScale;\n+        this.lastTimeToScale = this.timeSupplier.get();\n+        if (previousTimeStamp == 0L) {\n+            memoryTrackingStale = DEFAULT_MEMORY_REFRESH_RATE;\n+        } else {\n+            memoryTrackingStale = Duration.ofMillis(TimeValue.timeValueMinutes(1).millis() + this.lastTimeToScale - previousTimeStamp);\n+        }\n+\n+        final ClusterState clusterState = context.state();\n+\n+        PersistentTasksCustomMetadata tasks = clusterState.getMetadata().custom(PersistentTasksCustomMetadata.TYPE);\n+        Collection<PersistentTask<?>> anomalyDetectionTasks = anomalyDetectionTasks(tasks);\n+        Collection<PersistentTask<?>> dataframeAnalyticsTasks = dataframeAnalyticsTasks(tasks);\n+        final List<DiscoveryNode> nodes = getNodes(clusterState);\n+        Optional<NativeMemoryCapacity> futureFreedCapacity = calculateFutureFreedCapacity(tasks, memoryTrackingStale);\n+\n+        final List<String> waitingAnomalyJobs = anomalyDetectionTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.jobId(t.getId()))\n+            .collect(Collectors.toList());\n+        final List<String> waitingAnalyticsJobs = dataframeAnalyticsTasks.stream()\n+            .filter(t -> AWAITING_LAZY_ASSIGNMENT.equals(t.getAssignment()))\n+            .map(t -> MlTasks.dataFrameAnalyticsId(t.getId()))\n+            .collect(Collectors.toList());\n+\n+        final NativeMemoryCapacity currentScale = currentScale(nodes);\n+        final MlScalingReason.Builder reasonBuilder = MlScalingReason.builder()\n+            .setWaitingAnomalyJobs(waitingAnomalyJobs)\n+            .setWaitingAnalyticsJobs(waitingAnalyticsJobs)\n+            .setCurrentMlCapacity(currentScale.autoscalingCapacity(maxMachineMemoryPercent, useAuto))\n+            .setPassedConfiguration(decider);\n+\n+        final Optional<AutoscalingDeciderResult> scaleUpDecision = checkForScaleUp(decider,\n+            waitingAnomalyJobs,\n+            waitingAnalyticsJobs,\n+            futureFreedCapacity.orElse(null),\n+            currentScale,\n+            reasonBuilder);\n+\n+        if (scaleUpDecision.isPresent()) {\n+            resetScaleDownCoolDown();\n+            return scaleUpDecision.get();\n+        }\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // We don't want to continue to consider a scale down if there are now waiting jobs\n+            resetScaleDownCoolDown();\n+            noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\"Passing currently perceived capacity as there are analytics and anomaly jobs in the queue, \" +\n+                        \"but the number in the queue is less than the configured maximum allowed.\")\n+                    .build()));\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        }\n+\n+        long largestJob = Math.max(\n+            anomalyDetectionTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnomalyMemoryRequirement)\n+                .max()\n+                .orElse(0L),\n+            dataframeAnalyticsTasks.stream()\n+                .filter(PersistentTask::isAssigned)\n+                // Memory SHOULD be recently refreshed, so in our current state, we should at least have an idea of the memory used\n+                .mapToLong(this::getAnalyticsMemoryRequirement)\n+                .max()\n+                .orElse(0L));\n+\n+        final Optional<AutoscalingDeciderResult> scaleDownDecision =\n+            checkForScaleDown(nodes, clusterState, largestJob, currentScale, reasonBuilder);\n+\n+        if (scaleDownDecision.isPresent()) {\n+            if (newScaleDownCheck()) {\n+                scaleDownDetected = timeSupplier.get();\n+            }\n+            if (canScaleDown(decider.getDownScaleDelay())) {\n+                return scaleDownDecision.get();\n+            }\n+            return new AutoscalingDeciderResult(\n+                context.currentCapacity(),\n+                reasonBuilder\n+                    .setSimpleReason(\n+                        \"Passing currently perceived capacity as configured down scale delay has not be satisfied; configured delay [\"\n+                            + decider.getDownScaleDelay().millis()\n+                            + \"] last detected scale down event [\"\n+                            + scaleDownDetected\n+                            + \"]\")\n+                    .build());\n+        }\n+\n+        return noScaleResultOrRefresh(reasonBuilder, memoryTrackingStale, new AutoscalingDeciderResult(context.currentCapacity(),\n+            reasonBuilder\n+                .setSimpleReason(\"Passing currently perceived capacity as no scaling changes were detected to be possible\")\n+                .build()));\n+    }\n+\n+    AutoscalingDeciderResult noScaleResultOrRefresh(MlScalingReason.Builder reasonBuilder,\n+                                                    Duration memoryTrackingStale,\n+                                                    AutoscalingDeciderResult potentialResult) {\n+        if (mlMemoryTracker.isRecentlyRefreshed(memoryTrackingStale) == false) {\n+            return buildDecisionAndRequestRefresh(reasonBuilder);\n+        } else {\n+            return potentialResult;\n+        }\n+    }\n+\n+    Optional<AutoscalingDeciderResult> checkForScaleUp(MlAutoscalingDeciderConfiguration decider,\n+                                                       List<String> waitingAnomalyJobs,\n+                                                       List<String> waitingAnalyticsJobs,\n+                                                       @Nullable NativeMemoryCapacity futureFreedCapacity,\n+                                                       NativeMemoryCapacity currentScale,\n+                                                       MlScalingReason.Builder reasonBuilder) {\n+\n+        // Are we in breach of maximum waiting jobs?\n+        if (waitingAnalyticsJobs.size() > decider.getNumAnalyticsJobsInQueue()\n+            || waitingAnomalyJobs.size() > decider.getNumAnomalyJobsInQueue()) {\n+            NativeMemoryCapacity updatedCapacity = NativeMemoryCapacity.from(currentScale);\n+            Optional<NativeMemoryCapacity> analyticsCapacity = requiredCapacityForUnassignedJobs(waitingAnalyticsJobs,\n+                this::getAnalyticsMemoryRequirement,\n+                decider.getNumAnalyticsJobsInQueue());\n+            Optional<NativeMemoryCapacity> anomalyCapacity = requiredCapacityForUnassignedJobs(waitingAnomalyJobs,\n+                this::getAnomalyMemoryRequirement,\n+                decider.getNumAnomalyJobsInQueue());\n+\n+            updatedCapacity.merge(anomalyCapacity.orElse(NativeMemoryCapacity.ZERO))\n+                .merge(analyticsCapacity.orElse(NativeMemoryCapacity.ZERO));\n+            return Optional.of(new AutoscalingDeciderResult(\n+                updatedCapacity.autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                reasonBuilder.setSimpleReason(\"requesting scale up as number of jobs in queues exceeded configured limit\").build()));\n+        }\n+\n+        // Could the currently waiting jobs ever be assigned?\n+        if (waitingAnalyticsJobs.isEmpty() == false || waitingAnomalyJobs.isEmpty() == false) {\n+            // we are unable to determine new tier size, but maybe we can see if our nodes are big enough.\n+            if (futureFreedCapacity == null) {\n+                Optional<Long> maxSize = Stream.concat(\n+                    waitingAnalyticsJobs.stream().map(mlMemoryTracker::getDataFrameAnalyticsJobMemoryRequirement),\n+                    waitingAnomalyJobs.stream().map(mlMemoryTracker::getAnomalyDetectorJobMemoryRequirement))\n+                    .filter(Objects::nonNull)\n+                    .max(Long::compareTo);\n+                if (maxSize.isPresent() && maxSize.get() > currentScale.getNode()) {\n+                    return Optional.of(new AutoscalingDeciderResult(\n+                        new NativeMemoryCapacity(Math.max(currentScale.getTier(), maxSize.get()), maxSize.get())\n+                            .autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                        reasonBuilder.setSimpleReason(\"requesting scale up as there is no node large enough to handle queued jobs\")\n+                            .build()));\n+                }\n+                // we have no info, allow the caller to make the appropriate action, probably returning a no_scale\n+                return Optional.empty();\n+            }\n+            long newTierNeeded = 0L;\n+            // could any of the nodes actually run the job?\n+            long newNodeMax = currentScale.getNode();\n+            for (String analyticsJob : waitingAnalyticsJobs) {\n+                Long requiredMemory = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            for (String anomalyJob : waitingAnomalyJobs) {\n+                Long requiredMemory = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyJob);\n+                // it is OK to continue here as we have not breached our queuing limit\n+                if (requiredMemory == null) {\n+                    continue;\n+                }\n+                // Is there \"future capacity\" on a node that could run this job? If not, we need that much more in the tier.\n+                if (futureFreedCapacity.getNode() < requiredMemory) {\n+                    newTierNeeded = Math.max(requiredMemory, newTierNeeded);\n+                }\n+                newNodeMax = Math.max(newNodeMax, requiredMemory);\n+            }\n+            if (newNodeMax > currentScale.getNode() || newTierNeeded > 0L) {\n+                NativeMemoryCapacity newCapacity = new NativeMemoryCapacity(newTierNeeded, newNodeMax);\n+                return Optional.of(new AutoscalingDeciderResult(\n+                    // We need more memory in the tier, or our individual node size requirements has increased\n+                    NativeMemoryCapacity.from(currentScale).merge(newCapacity).autoscalingCapacity(maxMachineMemoryPercent, useAuto),\n+                    reasonBuilder\n+                        .setSimpleReason(\"scaling up as adequate space would not automatically become available when running jobs finish\")\n+                        .build()\n+                ));\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    // This calculates the the following the potentially automatically free capacity of sometime in the future\n+    // Since jobs with lookback only datafeeds, and data frame analytics jobs all have some potential future end date\n+    // we can assume (without user intervention) that these will eventually stop and free their currently occupied resources.\n+    //\n+    // The capacity is as follows:\n+    //  tier: The sum total of the resources that will be removed\n+    //  node: The largest block of memory that will be freed on a given node.\n+    //      - If > 1 \"batch\" ml tasks are running on the same node, we sum their resources.\n+    Optional<NativeMemoryCapacity> calculateFutureFreedCapacity(PersistentTasksCustomMetadata tasks, Duration jobMemoryExpiry) {\n+        final List<PersistentTask<DatafeedParams>> jobsWithLookbackDatafeeds = datafeedTasks(tasks).stream()\n+            .filter(t -> t.getParams().getEndTime() != null && t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+        final List<PersistentTask<?>> assignedAnalyticsJobs = dataframeAnalyticsTasks(tasks).stream()\n+            .filter(t -> t.getExecutorNode() != null)\n+            .collect(Collectors.toList());\n+\n+        if (jobsWithLookbackDatafeeds.isEmpty() && assignedAnalyticsJobs.isEmpty()) {\n+            return Optional.of(NativeMemoryCapacity.ZERO);\n+        }\n+        if (mlMemoryTracker.isRecentlyRefreshed(jobMemoryExpiry) == false) {\n+            return Optional.empty();\n+        }\n+        // What is the largest chunk of memory that could be freed on a node in the future\n+        Map<String, Long> maxNodeBytes = new HashMap<>();\n+        for (PersistentTask<DatafeedParams> lookbackOnlyDf : jobsWithLookbackDatafeeds) {\n+            Long jobSize = mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(lookbackOnlyDf.getParams().getJobId());\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(lookbackOnlyDf.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        for (PersistentTask<?> task : assignedAnalyticsJobs) {\n+            Long jobSize = mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+            if (jobSize == null) {\n+                return Optional.empty();\n+            }\n+            maxNodeBytes.compute(task.getExecutorNode(), (_k, v) -> v == null ? jobSize : jobSize + v);\n+        }\n+        return Optional.of(new NativeMemoryCapacity(\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).sum(),\n+            maxNodeBytes.values().stream().mapToLong(Long::longValue).max().orElse(0L)));\n+    }\n+\n+    private AutoscalingDeciderResult buildDecisionAndRequestRefresh(MlScalingReason.Builder reasonBuilder) {\n+        mlMemoryTracker.asyncRefresh();\n+        return new AutoscalingDeciderResult(null, reasonBuilder.setSimpleReason(MEMORY_STALE).build());\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(String analyticsId) {\n+        return mlMemoryTracker.getDataFrameAnalyticsJobMemoryRequirement(analyticsId);\n+    }\n+\n+    private Long getAnalyticsMemoryRequirement(PersistentTask<?> task) {\n+        return getAnalyticsMemoryRequirement(MlTasks.dataFrameAnalyticsId(task.getId()));\n+    }\n+\n+    private Long getAnomalyMemoryRequirement(String anomalyId) {\n+        return mlMemoryTracker.getAnomalyDetectorJobMemoryRequirement(anomalyId);", "originalCommit": "6536073d0983c0cc4c49f30c41392a2b2a23fff0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ac8ca5913958f4e7ff245ea91ae0dd9f307856e9", "url": "https://github.com/elastic/elasticsearch/commit/ac8ca5913958f4e7ff245ea91ae0dd9f307856e9", "message": "adding logging", "committedDate": "2020-11-17T13:30:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE2Njc5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525166797", "bodyText": "Does expectThrows work with AssertionError?  If it does then that would be the more idiomatic way to write this.", "author": "droberts195", "createdAt": "2020-11-17T13:47:51Z", "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/autoscaling/MlAutoscalingDeciderServiceTests.java", "diffHunk": "@@ -213,11 +213,17 @@ public void testScaleDown_WhenMemoryIsInaccurate() {\n \n         MlAutoscalingDeciderService service = buildService();\n         MlScalingReason.Builder reasonBuilder = new MlScalingReason.Builder();\n-        assertThat(service.checkForScaleDown(nodes,\n-            ClusterState.EMPTY_STATE,\n-            Long.MAX_VALUE,\n-            new NativeMemoryCapacity(ByteSizeValue.ofGb(3).getBytes(), ByteSizeValue.ofGb(1).getBytes()),\n-            reasonBuilder).isEmpty(), is(true));\n+        try {\n+            service.checkForScaleDown(nodes,\n+                ClusterState.EMPTY_STATE,\n+                Long.MAX_VALUE,\n+                new NativeMemoryCapacity(ByteSizeValue.ofGb(3).getBytes(), ByteSizeValue.ofGb(1).getBytes()),\n+                reasonBuilder);\n+        } catch (AssertionError ae) {\n+            // the scale method should thrown an assertion failure\n+            return;\n+        }\n+        assert false : \"call for scale down should have thrown an assertion error\";", "originalCommit": "ac8ca5913958f4e7ff245ea91ae0dd9f307856e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE5ODgyNA==", "url": "https://github.com/elastic/elasticsearch/pull/59309#discussion_r525198824", "bodyText": "I can try that, but iirc, it doesn't as expectThrows has an assertion in it itself.", "author": "benwtrent", "createdAt": "2020-11-17T14:31:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE2Njc5Nw=="}], "type": "inlineReview"}, {"oid": "29adb17ec25ea87f349cfb756412a058990b68f0", "url": "https://github.com/elastic/elasticsearch/commit/29adb17ec25ea87f349cfb756412a058990b68f0", "message": "fixing test", "committedDate": "2020-11-17T14:40:36Z", "type": "commit"}]}