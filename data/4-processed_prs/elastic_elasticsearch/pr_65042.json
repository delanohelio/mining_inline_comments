{"pr_number": 65042, "pr_title": "Fix Two Snapshot Clone State Machine Bugs", "pr_createdAt": "2020-11-15T19:14:57Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/65042", "timeline": [{"oid": "6f0214b62db03b4c3189016e068cf73b59b8c626", "url": "https://github.com/elastic/elasticsearch/commit/6f0214b62db03b4c3189016e068cf73b59b8c626", "message": "Fix Two Snapshot Clone State Machine Bugs\n\nThere are two separate but closely related bug fixes in this PR:\n1. When two snapshot clones would initialize concurrently we could get into a state\nwhere one is in front of the other in the queue snapshots array but its shard states\nare in fact queued behind the other snapshot or clone. Tightly linked to this,\nsnapshot cloning would not account for snapshot deletes when queueing shard snapshots\nwhich could lead to races where both delete and clone are running concurrently for a shard.\n2. As a result of fixing the first issue and writing a test for it, it also became obvious\nthat a finished delete was not properly accounted for when it comes to starting snapshot\nclones that could now queue behind a delete.", "committedDate": "2020-11-15T19:12:21Z", "type": "commit"}, {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9", "url": "https://github.com/elastic/elasticsearch/commit/3d57e5da3cf8466ff06584b89649cb19d88d16c9", "message": "drier", "committedDate": "2020-11-15T19:14:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3ODkwMA==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525078900", "bodyText": "This is the only place where the clone is re-queued right? I was wondering if this could lead to starvation in some really bad scenarios, but I think that's not possible?", "author": "fcofdez", "createdAt": "2020-11-17T11:21:34Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -476,25 +476,50 @@ public ClusterState execute(ClusterState currentState) {\n                 final String repoName = cloneEntry.repository();\n                 final ShardGenerations shardGenerations = repoData.shardGenerations();\n                 for (int i = 0; i < updatedEntries.size(); i++) {\n-                    if (cloneEntry.snapshot().equals(updatedEntries.get(i).snapshot())) {\n+                    final SnapshotsInProgress.Entry entry = updatedEntries.get(i);\n+                    if (cloneEntry.repository().equals(entry.repository()) == false) {\n+                        // different repo => just continue without modification\n+                        continue;\n+                    }\n+                    if (cloneEntry.snapshot().getSnapshotId().equals(entry.snapshot().getSnapshotId())) {\n                         final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder =\n                                 ImmutableOpenMap.builder();\n-                        final InFlightShardSnapshotStates inFlightShardStates =\n-                            InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        final boolean readyToExecute = currentState.custom(\n+                                SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).getEntries().stream()\n+                                .noneMatch(e -> e.repository().equals(repoName) && e.state() == SnapshotDeletionsInProgress.State.STARTED);\n+                        final InFlightShardSnapshotStates inFlightShardStates;\n+                        if (readyToExecute) {\n+                            inFlightShardStates = InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        } else {\n+                            // no need to compute these, we'll mark all shards as queued anyway because we wait for the delete\n+                            inFlightShardStates = null;\n+                        }\n+                        boolean queuedShards = false;\n                         for (Tuple<IndexId, Integer> count : counts) {\n                             for (int shardId = 0; shardId < count.v2(); shardId++) {\n                                 final RepositoryShardId repoShardId = new RepositoryShardId(count.v1(), shardId);\n                                 final String indexName = repoShardId.indexName();\n-                                if (inFlightShardStates.isActive(indexName, shardId)) {\n+                                if (readyToExecute == false || inFlightShardStates.isActive(indexName, shardId)) {\n                                     clonesBuilder.put(repoShardId, ShardSnapshotStatus.UNASSIGNED_QUEUED);\n+                                    queuedShards = true;\n                                 } else {\n                                     clonesBuilder.put(repoShardId, new ShardSnapshotStatus(localNodeId,\n                                         inFlightShardStates.generationForShard(repoShardId.index(), shardId, shardGenerations)));\n                                 }\n                             }\n                         }\n                         updatedEntry = cloneEntry.withClones(clonesBuilder.build());\n-                        updatedEntries.set(i, updatedEntry);\n+                        if (queuedShards) {\n+                            // We queued up some shards based on the in-flight operations found in all snapshots for the current\n+                            // repository, so in order to make sure we don't set a shard to QUEUED before (as in before it in the\n+                            // `updatedEntries` list) one that is actively executing we just put it to the back of the list as if we had\n+                            // just created the entry\n+                            // TODO: If we could eventually drop the snapshot clone init phase we don't need this any longer\n+                            updatedEntries.remove(i);\n+                            updatedEntries.add(updatedEntry);", "originalCommit": "3d57e5da3cf8466ff06584b89649cb19d88d16c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE1MDU5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525150595", "bodyText": "This is the only place where the clone is re-queued right\n\nRight, we're only doing this here.\n\nI was wondering if this could lead to starvation in some really bad scenarios, but I think that's not possible?\n\nI think we're good here even in theory. We do all the shard count fetching on the SNAPSHOT pool and any operation that would keep adding more and more operations to the list so quickly that we never get our turn would have to do some work on the SNAPSHOT pool eventually so we're safe on that front :)", "author": "original-brownbear", "createdAt": "2020-11-17T13:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3ODkwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3OTczNQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525079735", "bodyText": "Is this related to this pr?", "author": "fcofdez", "createdAt": "2020-11-17T11:22:54Z", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -471,6 +472,25 @@ public XContentBuilder toXContent(XContentBuilder builder, Params params) throws\n                 }\n             }\n             builder.endArray();\n+            if (isClone()) {", "originalCommit": "3d57e5da3cf8466ff06584b89649cb19d88d16c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE0OTMxMg==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525149312", "bodyText": "Sort of, it's of necessary to make assertion failure messages useful, that's where I noticed this was missing.", "author": "original-brownbear", "createdAt": "2020-11-17T13:24:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3OTczNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzODYwMg==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525238602", "bodyText": "That looks strange that Entry implements ToXContent just for rendering assertion error messages? I guess it's fine but I find it strange (and it should implement ToXContentObject)", "author": "tlrx", "createdAt": "2020-11-17T15:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3OTczNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA4MTczMg==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525081732", "bodyText": "Maybe we could assert here that the shard snapshots are queued?", "author": "fcofdez", "createdAt": "2020-11-17T11:26:28Z", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "diffHunk": "@@ -571,6 +575,40 @@ public void testStartCloneWithSuccessfulShardSnapshotPendingFinalization() throw\n         assertEquals(getSnapshot(repoName, cloneName).state(), SnapshotState.SUCCESS);\n     }\n \n+    public void testStartCloneDuringRunningDelete() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"test-idx\";\n+        createIndexWithContent(indexName);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final List<String> snapshotNames = createNSnapshots(repoName, randomIntBetween(1, 5));\n+        blockMasterOnWriteIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteSnapshot(repoName, randomFrom(snapshotNames));\n+        waitForBlock(masterName, repoName);\n+        awaitNDeletionsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, \"target-snapshot\", indexName);\n+        logger.info(\"--> waiting for snapshot clone to be fully initialized\");\n+        awaitClusterState(state -> {\n+            for (SnapshotsInProgress.Entry entry : state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n+                if (entry.clones().isEmpty() == false) {\n+                    assertEquals(sourceSnapshot, entry.source().getName());", "originalCommit": "3d57e5da3cf8466ff06584b89649cb19d88d16c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE0ODM2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525148369", "bodyText": "++ I added that now. Technically this is ensured by assertions we have on the cluster state in SnapshotsService but practically this makes the test a lot clearer in its intent :)", "author": "original-brownbear", "createdAt": "2020-11-17T13:22:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA4MTczMg=="}], "type": "inlineReview"}, {"oid": "1cca1970528597f0381ec4bdb04199661e0bd44e", "url": "https://github.com/elastic/elasticsearch/commit/1cca1970528597f0381ec4bdb04199661e0bd44e", "message": "Merge remote-tracking branch 'elastic/master' into fix-clone-bug", "committedDate": "2020-11-17T12:57:45Z", "type": "commit"}, {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "url": "https://github.com/elastic/elasticsearch/commit/f7b19344a1b1c239f6bbd5db749e43189e0efea8", "message": "add assertion", "committedDate": "2020-11-17T13:02:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI0NTgxMA==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525245810", "bodyText": "readyToExecute kind of lack of meaning, maybe startedSnapshotDeletion (+ anyMatch)?", "author": "tlrx", "createdAt": "2020-11-17T15:26:43Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -476,25 +476,50 @@ public ClusterState execute(ClusterState currentState) {\n                 final String repoName = cloneEntry.repository();\n                 final ShardGenerations shardGenerations = repoData.shardGenerations();\n                 for (int i = 0; i < updatedEntries.size(); i++) {\n-                    if (cloneEntry.snapshot().equals(updatedEntries.get(i).snapshot())) {\n+                    final SnapshotsInProgress.Entry entry = updatedEntries.get(i);\n+                    if (cloneEntry.repository().equals(entry.repository()) == false) {\n+                        // different repo => just continue without modification\n+                        continue;\n+                    }\n+                    if (cloneEntry.snapshot().getSnapshotId().equals(entry.snapshot().getSnapshotId())) {\n                         final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder =\n                                 ImmutableOpenMap.builder();\n-                        final InFlightShardSnapshotStates inFlightShardStates =\n-                            InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        final boolean readyToExecute = currentState.custom(", "originalCommit": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3NjEzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525276131", "bodyText": "++ this is 100% the better name, I'll leave it as is for a sec though so it's less confusing to review when drying things up (we use the same mechanics and var name in the normal snapshot shard assignments path ... and it's equally weird there in hindsight :))", "author": "original-brownbear", "createdAt": "2020-11-17T15:57:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI0NTgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI0OTQ4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525249481", "bodyText": "\ud83d\udc4d", "author": "tlrx", "createdAt": "2020-11-17T15:29:05Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -797,11 +822,15 @@ private static boolean assertNoDanglingSnapshots(ClusterState state) {\n         final Set<String> reposSeen = new HashSet<>();\n         for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n             if (reposSeen.add(entry.repository())) {\n-                for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                for (ObjectCursor<ShardSnapshotStatus> value : (entry.isClone() ? entry.clones() : entry.shards()).values()) {\n                     if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)) {\n                         assert reposWithRunningDelete.contains(entry.repository())\n                                 : \"Found shard snapshot waiting to be assigned in [\" + entry +\n                                 \"] but it is not blocked by any running delete\";\n+                    } else if (value.value.isActive()) {\n+                        assert reposWithRunningDelete.contains(entry.repository()) == false\n+                                : \"Found shard snapshot actively executing in [\" + entry +\n+                                \"] when it should be blocked by a running delete\";", "originalCommit": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1NTYxNw==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525255617", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n          \n          \n            \n                                        // Collect waiting shards from that entry that we can assign now that we are done with the deletion", "author": "tlrx", "createdAt": "2020-11-17T15:33:07Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1929,50 +1961,89 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n \n             // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n             // them to multiple snapshots by accident\n-            final Set<ShardId> reassignedShardIds = new HashSet<>();\n+            final Map<String, Set<Integer>> reassignedShardIds = new HashMap<>();\n \n             boolean changed = false;\n \n+            final String localNodeId = currentState.nodes().getLocalNodeId();\n             final String repoName = deleteEntry.repository();\n             // Computing the new assignments can be quite costly, only do it once below if actually needed\n             ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardAssignments = null;\n+            InFlightShardSnapshotStates inFlightShardStates = null;\n             for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n                 if (entry.repository().equals(repoName)) {\n                     if (entry.state().completed() == false) {\n-                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n-                        final List<ShardId> canBeUpdated = new ArrayList<>();\n-                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> value : entry.shards()) {\n-                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n-                                    && reassignedShardIds.contains(value.key) == false) {\n-                                canBeUpdated.add(value.key);\n+                        // TODO: dry up redundant computation and code between clone and non-clone case, in particular reuse\n+                        //  `inFlightShardStates` across both clone and standard snapshot code\n+                        if (entry.isClone()) {\n+                            // Collect waiting shards that in entry that we can assign now that we are done with the deletion", "originalCommit": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODAxMw==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525258013", "bodyText": "Maybe evaluate canBeUpdated.isEmpty() first and then only compute readyToExecute if needed?", "author": "tlrx", "createdAt": "2020-11-17T15:35:50Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1929,50 +1961,89 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n \n             // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n             // them to multiple snapshots by accident\n-            final Set<ShardId> reassignedShardIds = new HashSet<>();\n+            final Map<String, Set<Integer>> reassignedShardIds = new HashMap<>();\n \n             boolean changed = false;\n \n+            final String localNodeId = currentState.nodes().getLocalNodeId();\n             final String repoName = deleteEntry.repository();\n             // Computing the new assignments can be quite costly, only do it once below if actually needed\n             ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardAssignments = null;\n+            InFlightShardSnapshotStates inFlightShardStates = null;\n             for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n                 if (entry.repository().equals(repoName)) {\n                     if (entry.state().completed() == false) {\n-                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n-                        final List<ShardId> canBeUpdated = new ArrayList<>();\n-                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> value : entry.shards()) {\n-                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n-                                    && reassignedShardIds.contains(value.key) == false) {\n-                                canBeUpdated.add(value.key);\n+                        // TODO: dry up redundant computation and code between clone and non-clone case, in particular reuse\n+                        //  `inFlightShardStates` across both clone and standard snapshot code\n+                        if (entry.isClone()) {\n+                            // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n+                            final List<RepositoryShardId> canBeUpdated = new ArrayList<>();\n+                            for (ObjectObjectCursor<RepositoryShardId, ShardSnapshotStatus> value : entry.clones()) {\n+                                if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n+                                        && alreadyReassigned(value.key.indexName(), value.key.shardId(), reassignedShardIds) == false) {\n+                                    canBeUpdated.add(value.key);\n+                                }\n+                            }\n+                            // TODO: the below logic is very similar to that in #startCloning and both could be dried up against each other\n+                            //       also the code for standard snapshots could make use of this breakout as well\n+                            final boolean readyToExecute = updatedDeletions.getEntries().stream().noneMatch(\n+                                    e -> e.repository().equals(repoName) && e.state() == SnapshotDeletionsInProgress.State.STARTED);\n+                            if (readyToExecute == false || canBeUpdated.isEmpty()) {", "originalCommit": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3NDIxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525274215", "bodyText": "++", "author": "original-brownbear", "createdAt": "2020-11-17T15:55:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1OTE5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525259196", "bodyText": "Maybe add some message in case it occurs in tests?", "author": "tlrx", "createdAt": "2020-11-17T15:37:12Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1987,6 +2058,15 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n             }\n             return changed ? SnapshotsInProgress.of(snapshotEntries) : null;\n         }\n+\n+        private void markShardReassigned(String indexName, int shardId, Map<String, Set<Integer>> reassignments) {\n+            final boolean added = reassignments.computeIfAbsent(indexName, k -> new HashSet<>()).add(shardId);\n+            assert added;", "originalCommit": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3OTI1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525279257", "bodyText": "++", "author": "original-brownbear", "createdAt": "2020-11-17T16:01:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1OTE5Ng=="}], "type": "inlineReview"}, {"oid": "2f03a53c0ca5be2fc7a7fa953c4500b3d026ae1e", "url": "https://github.com/elastic/elasticsearch/commit/2f03a53c0ca5be2fc7a7fa953c4500b3d026ae1e", "message": "Update server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java\n\nCo-authored-by: Tanguy Leroux <tlrx.dev@gmail.com>", "committedDate": "2020-11-17T15:53:49Z", "type": "commit"}, {"oid": "f524930d2f53d368fb1509c47924d2b93b88dfc2", "url": "https://github.com/elastic/elasticsearch/commit/f524930d2f53d368fb1509c47924d2b93b88dfc2", "message": "Merge remote-tracking branch 'elastic/master' into fix-clone-bug", "committedDate": "2020-11-17T15:54:46Z", "type": "commit"}, {"oid": "9b1ade6ecf0dfa95eb1500ab6911252e7c43c0a0", "url": "https://github.com/elastic/elasticsearch/commit/9b1ade6ecf0dfa95eb1500ab6911252e7c43c0a0", "message": "CR: comments", "committedDate": "2020-11-17T16:00:51Z", "type": "commit"}]}