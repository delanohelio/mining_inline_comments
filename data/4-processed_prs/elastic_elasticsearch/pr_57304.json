{"pr_number": 57304, "pr_title": "Save memory when auto_date_histogram is not on top", "pr_createdAt": "2020-05-28T17:48:37Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/57304", "timeline": [{"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "url": "https://github.com/elastic/elasticsearch/commit/342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "message": "Save memory when auto_date_histogram is not on top\n\nThis rebuilds `auto_date_histogram`'s aggregator to function without\n`asMultiBucketAggregator` which should save a significant amount of\nmemory when it is not the top most aggregator.\n\nIt isn't possible to \"just port the aggregator\" without taking a pretty\nsignificant performance hit because we used to rewrite all of the\nbuckets every time we switched to a coarser and coarser rounding\nconfiguration. Without some major surgery to how to delay sub-aggs\nwe'd end up rewriting the delay list zillions of time if there are many\nbuckets.\n\nThis change replaces the constant rewrites with a \"budget\" of \"wasted\"\nbuckets and only rewrites all of the buckets when we exceed that budget.\nNow that we don't rebucket every time we increase the rounding we can no\nlonger get an accurate count of the number of buckets! So instead the\naggregator uses an esimate of the number of buckets to trigger switching\nto a coarser rounding. This estimate is likely to be *terrible* when\nbuckets are far apart compared to the rounding. So it also uses the\ndifference between the first and last bucket to trigger switching to a\ncoarser rounding. Which covers for the shortcomings of the bucket\nestimation technique pretty well. It also causes the aggregator to emit\nfewer buckets in cases where they'd be reduced together on the\ncoordinating node. This is wonderful! But probably fairly rare.\n\nAfter all that, it amounts to about the same performance, in the\nbenchmarks that I've run. But the memory savings is totaly still at\nthing!\n\nRelates to #56487", "committedDate": "2020-05-28T17:45:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1Mzk2OA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432153968", "bodyText": "remove the ending \"it\"?", "author": "talevy", "createdAt": "2020-05-28T22:14:29Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NDQxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432154411", "bodyText": "remove the leading \"it\"?", "author": "talevy", "createdAt": "2020-05-28T22:15:41Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NDcyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432154721", "bodyText": "drop the \"is\"?", "author": "talevy", "createdAt": "2020-05-28T22:16:28Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are\n+ * quite a spread out compared to the rounding.\n+ * <p>\n+ * The second heuristic it uses to trigger promotion to a coarser rounding is\n+ * the distance between the min and max bucket. When that distance is greater\n+ * than what the current rounding supports it promotes. This is heuristic", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5Mjg1NA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432192854", "bodyText": "should this be AutoDateHistogramAggregator#rebucket? Intellij no-likey", "author": "talevy", "createdAt": "2020-05-29T00:21:12Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are\n+ * quite a spread out compared to the rounding.\n+ * <p>\n+ * The second heuristic it uses to trigger promotion to a coarser rounding is\n+ * the distance between the min and max bucket. When that distance is greater\n+ * than what the current rounding supports it promotes. This is heuristic\n+ * isn't good at limiting the number of buckets but is great when the buckets\n+ * are spread out compared to the rounding. So it should complement the first\n+ * heuristic.\n+ * <p>\n+ * When promoting a rounding we keep the old buckets around because it is\n+ * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+ * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+ * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+ * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+ * \"wasting\". When we promote the rounding and our estimate of the number of\n+ * \"dead\" buckets that have data but have yet to be merged into the buckets\n+ * that are valid for the current rounding exceeds the budget then we rebucket\n+ * the entire aggregation and double the budget.\n+ * <p>\n+ * Once we're done collecting and we know exactly which buckets we'll be\n+ * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+ * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+ * collect and picking the rounding based on a real, accurate count and the\n+ * min and max.\n  */\n class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n-\n     private final ValuesSource.Numeric valuesSource;\n     private final DocValueFormat formatter;\n     private final RoundingInfo[] roundingInfos;\n     private final Function<Rounding, Rounding.Prepared> roundingPreparer;\n-    private int roundingIdx = 0;\n-    private Rounding.Prepared preparedRounding;\n-\n-    private LongHash bucketOrds;\n-    private int targetBuckets;\n+    private final int targetBuckets;\n+    private final boolean collectsFromSingleBucket;\n+    /**\n+     * An array of prepared roundings in the same order as\n+     * {@link #roundingInfos}. The 0th entry is prepared initially,\n+     * and other entries are null until first needed.\n+     */\n+    private final Rounding.Prepared[] preparedRoundings;\n+    /**\n+     * Map from {@code owningBucketOrd, roundedDate} to {@code bucketOrdinal}.\n+     */\n+    private LongKeyedBucketOrds bucketOrds;\n+    /**\n+     * The index of the rounding that each {@code owningBucketOrd} is\n+     * currently using.\n+     * <p>\n+     * During collection we use overestimates for how much buckets are save\n+     * by bumping to the next rounding index. So we end up bumping less\n+     * aggressively than a \"perfect\" algorithm. That is fine because we\n+     * correct the error when we merge the buckets together all the way\n+     * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+     * on final reduce we bump the rounding until it we appropriately\n+     * cover the date range across all of the results returned by all of\n+     * the {@link AutoDateHistogramAggregator}s. \n+     */\n+    private ByteArray roundingIndices;\n+    /**\n+     * The min and max of each bucket's keys. min lives in indices of the form\n+     * {@code 2n} and max in {@code 2n + 1}.\n+     */\n+    private LongArray bounds;\n+    /**\n+     * A reference to the collector so we can\n+     * {@link MergingBucketsDeferringCollector#mergeBuckets(long[])}.\n+     */\n     private MergingBucketsDeferringCollector deferringCollector;\n+    /**\n+     * An underestimate of the number of buckets that are \"live\" in the\n+     * current rounding for each {@code owningBucketOrdinal}. \n+     */\n+    private IntArray liveBucketCountUnderestimate;\n+    /**\n+     * An over estimate of the number of wasted buckets. When this gets\n+     * too high we {@link #rebucket()} which sets it to 0.", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5NTcwMA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432195700", "bodyText": "should there be unit tests for when:\n(max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()) == false\n&& (newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()))\n\n?", "author": "talevy", "createdAt": "2020-05-29T00:32:33Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3NTQxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r433375411", "bodyText": "We do cover this case in the test case, but there isn't a unit test for this exactly. do you think it is worth pulling this out somehow?", "author": "nik9000", "createdAt": "2020-06-01T17:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5NTcwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDA3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432580072", "bodyText": "owningBucketOrd * 2 and owningBucketOrd * 2 + 1 are used a lot here and make it a bit difficult to read... could we just set some lowerBound/upperBound (or whatever name makes sense) variables to make it easier to read?", "author": "polyfractal", "createdAt": "2020-05-29T15:54:52Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MTg1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432581851", "bodyText": "I'm not really sure I understand what this is doing?", "author": "polyfractal", "createdAt": "2020-05-29T15:57:25Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5NTM0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432695343", "bodyText": "Setting the min and max properly. It'd all go away if I make too arrays like you asked above. While I'll do.", "author": "nik9000", "createdAt": "2020-05-29T19:38:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MTg1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NDM4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432584389", "bodyText": "I think we can move these two bounds.set() inside the if() { ... } down below, to avoid unnecessary setting when we have to fall through and increase the rounding.  Not that it's terribly expensive, but if we can avoid always good :)", "author": "polyfractal", "createdAt": "2020-05-29T16:01:39Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NzkyNw==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432587927", "bodyText": "What's the rationale behind doubling the rebucketing threshold?  Shouldn't it stay constant since the goal of the agg is to produce a relatively constant number of buckets?  As we rebucket and move to coarser intervals, the number of buckets will reduce (and wasted buckets should get cleaned up over time)?", "author": "polyfractal", "createdAt": "2020-05-29T16:07:41Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5OTY0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432699646", "bodyText": "My idea was to always produce good buckets in the end but to allow more and more \"wasted\" buckets as we end up cleaning up more and more. With the default as it stands when the agg is at the top level we don't clean up any of the \"wasted\" buckets until right before we return the aggs.\nThis bumping is more to defend against having large numbers of owningBucketOrds that cause us to have to rebucket many many times. To sort of blunt the O(n^2) nature of it all. We'd still to the O(n) rebucketing many times. Just fewer times as you accumulate more and more buckets.\nI did think about making it a constant budget. And that works pretty well too because we naturally rebucket less and less as we promote rounding because the buckets are bigger. But if we have a ton of owningBucketOrds I'm kind of worried.", "author": "nik9000", "createdAt": "2020-05-29T19:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NzkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NzY3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432597672", "bodyText": "Don't we have the min/max in bounds at this point?  Could we use that to help skip to the correct rounding, so we don't have to potentially re-build the ords several times if max-min > the threshold?", "author": "polyfractal", "createdAt": "2020-05-29T16:25:31Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMDM4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432700385", "bodyText": "We do. Let me think about that. My instinct is that this check here is largely symbolic because if the min and max would have pushed us up then we'd, well, already be on a bigger rounding anyway.", "author": "nik9000", "createdAt": "2020-05-29T19:45:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NzY3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432598751", "bodyText": "Is it safe to add to the bucketOrds while also iterating over it?  Haven't looked super closely at the internals.\nOn that note, do we need to clear out the bucketOrdsEnum before adding new keys?  E.g. I'm thinking of the case where we might need to go around this do...while loop a few times, and it looks like each time through we'll just keep appending to the ordsEnum so we'll end up with multiple levels of intervals?  I might be misreading though", "author": "polyfractal", "createdAt": "2020-05-29T16:27:30Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {\n+                newRounding++;\n+                try (LongHash perfect = new LongHash(liveBucketCountUnderestimate.get(owningBucketOrd), context.bigArrays())) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {", "originalCommit": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMTQ4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432701486", "bodyText": "Oh shit! This shld be adding to perfect. How did this ever work?!", "author": "nik9000", "createdAt": "2020-05-29T19:47:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMTc5MA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432701790", "bodyText": "Wow. Ok then. I wonder if I can make a test case that fails because of this......", "author": "nik9000", "createdAt": "2020-05-29T19:47:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc2MTM4OA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432761388", "bodyText": "It turns out that the heuristics we had were doing a pretty ok job making the right buckets anyway. So I can just drop this method entirely!", "author": "nik9000", "createdAt": "2020-05-29T22:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}], "type": "inlineReview"}, {"oid": "8b67b27560f4aa84fa947b49bb012ffe964d5661", "url": "https://github.com/elastic/elasticsearch/commit/8b67b27560f4aa84fa947b49bb012ffe964d5661", "message": "What happens if we, like, never correct the rounding?", "committedDate": "2020-05-29T20:54:09Z", "type": "commit"}, {"oid": "35291d4816efa294ba9479ac674b307cb017abe6", "url": "https://github.com/elastic/elasticsearch/commit/35291d4816efa294ba9479ac674b307cb017abe6", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-01T17:17:34Z", "type": "commit"}, {"oid": "962bb33610a81e2c8a36da1d5951e68033d10850", "url": "https://github.com/elastic/elasticsearch/commit/962bb33610a81e2c8a36da1d5951e68033d10850", "message": "Fixup", "committedDate": "2020-06-01T17:44:27Z", "type": "commit"}, {"oid": "fd49345e6e27edf98e155ac05425d315e3f16f78", "url": "https://github.com/elastic/elasticsearch/commit/fd49345e6e27edf98e155ac05425d315e3f16f78", "message": "Well that was totally wrong", "committedDate": "2020-06-01T18:20:09Z", "type": "commit"}, {"oid": "ba36e41812c240576b68893c6927e9eb2f4bc6ed", "url": "https://github.com/elastic/elasticsearch/commit/ba36e41812c240576b68893c6927e9eb2f4bc6ed", "message": "Remove nocommit", "committedDate": "2020-06-01T19:11:44Z", "type": "commit"}, {"oid": "e8fb47947bd7b797b488ffbe465d9b52edd6698b", "url": "https://github.com/elastic/elasticsearch/commit/e8fb47947bd7b797b488ffbe465d9b52edd6698b", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-01T21:42:42Z", "type": "commit"}, {"oid": "4895af7c95b333b18b71275626644ab40fb88c88", "url": "https://github.com/elastic/elasticsearch/commit/4895af7c95b333b18b71275626644ab40fb88c88", "message": "Maybe skip rebucketing", "committedDate": "2020-06-02T14:09:36Z", "type": "commit"}, {"oid": "2bf4618f370325866f9953bb8843805272a75b1d", "url": "https://github.com/elastic/elasticsearch/commit/2bf4618f370325866f9953bb8843805272a75b1d", "message": "Update comment", "committedDate": "2020-06-02T14:38:16Z", "type": "commit"}, {"oid": "43b3512fa17a565c369ddc46b87fc95a3f945710", "url": "https://github.com/elastic/elasticsearch/commit/43b3512fa17a565c369ddc46b87fc95a3f945710", "message": "Use int", "committedDate": "2020-06-02T23:14:57Z", "type": "commit"}, {"oid": "ec12da27fa5f31b08ec938abf5469716f1e201ec", "url": "https://github.com/elastic/elasticsearch/commit/ec12da27fa5f31b08ec938abf5469716f1e201ec", "message": "Rounding strat", "committedDate": "2020-06-03T14:02:51Z", "type": "commit"}, {"oid": "6aca7a5eec6bdf9d949c04d68d96baa4408c8d84", "url": "https://github.com/elastic/elasticsearch/commit/6aca7a5eec6bdf9d949c04d68d96baa4408c8d84", "message": "Time", "committedDate": "2020-06-03T18:44:24Z", "type": "commit"}, {"oid": "94f84732648cbad65f3fc09bcb50bd41f9366352", "url": "https://github.com/elastic/elasticsearch/commit/94f84732648cbad65f3fc09bcb50bd41f9366352", "message": "Single values", "committedDate": "2020-06-03T19:01:55Z", "type": "commit"}, {"oid": "77ca03a06fe18b14e008dad20cfdf3ad3aa87600", "url": "https://github.com/elastic/elasticsearch/commit/77ca03a06fe18b14e008dad20cfdf3ad3aa87600", "message": "Revert \"Single values\"\n\nThis reverts commit 94f84732648cbad65f3fc09bcb50bd41f9366352.", "committedDate": "2020-06-04T11:58:41Z", "type": "commit"}, {"oid": "6411589cfb8fd4be25bc8c2d4656a6a269b2aea9", "url": "https://github.com/elastic/elasticsearch/commit/6411589cfb8fd4be25bc8c2d4656a6a269b2aea9", "message": "Revert \"Time\"\n\nThis reverts commit 6aca7a5eec6bdf9d949c04d68d96baa4408c8d84.", "committedDate": "2020-06-04T11:58:43Z", "type": "commit"}, {"oid": "2e67acd7b1c7d9bbea748fef05346210a320aae1", "url": "https://github.com/elastic/elasticsearch/commit/2e67acd7b1c7d9bbea748fef05346210a320aae1", "message": "Revert \"Rounding strat\"\n\nThis reverts commit ec12da27fa5f31b08ec938abf5469716f1e201ec.", "committedDate": "2020-06-04T11:58:44Z", "type": "commit"}, {"oid": "068f028e77f250f4e5dbeb58fbf7a3f164fffc8a", "url": "https://github.com/elastic/elasticsearch/commit/068f028e77f250f4e5dbeb58fbf7a3f164fffc8a", "message": "How about this?", "committedDate": "2020-06-04T14:50:32Z", "type": "commit"}, {"oid": "d3efe68b02d332fbc341e3d78dddd9d5c49e4466", "url": "https://github.com/elastic/elasticsearch/commit/d3efe68b02d332fbc341e3d78dddd9d5c49e4466", "message": "Monomorphic", "committedDate": "2020-06-04T15:19:35Z", "type": "commit"}, {"oid": "54f8c6612746057cfa80e4211016e4247a8eba6a", "url": "https://github.com/elastic/elasticsearch/commit/54f8c6612746057cfa80e4211016e4247a8eba6a", "message": "Javadoc", "committedDate": "2020-06-04T19:37:42Z", "type": "commit"}, {"oid": "c005c155c2579ea81a081c51f04f31ccec3eb6e3", "url": "https://github.com/elastic/elasticsearch/commit/c005c155c2579ea81a081c51f04f31ccec3eb6e3", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-04T19:48:36Z", "type": "commit"}, {"oid": "0dcbd2597d1ef73bc12fef0b340ed9d06c35da98", "url": "https://github.com/elastic/elasticsearch/commit/0dcbd2597d1ef73bc12fef0b340ed9d06c35da98", "message": "Oh checkstyle, you loveable scamp", "committedDate": "2020-06-04T22:04:48Z", "type": "commit"}, {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324", "url": "https://github.com/elastic/elasticsearch/commit/518cdfa7e76b22b676d8339eda92c76000f9a324", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-07T21:58:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU2NTE1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439565157", "bodyText": "Huh, TIL.", "author": "polyfractal", "createdAt": "2020-06-12T17:56:47Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,80 +40,253 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n+import java.util.function.LongToIntFunction;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * This class is abstract because there is a simple implementation for when the\n+ * aggregator only collects from a single bucket and a more complex\n+ * implementation when it doesn't. This ain't great from a test coverage\n+ * standpoint but the simpler implementation is between 7% and 15% faster\n+ * when you can use it. This is an important aggregation and we need that\n+ * performance.\n  */\n-class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n+abstract class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n+    static AutoDateHistogramAggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        int targetBuckets,\n+        RoundingInfo[] roundingInfos,\n+        Function<Rounding, Rounding.Prepared> roundingPreparer,\n+        @Nullable ValuesSource valuesSource,\n+        DocValueFormat formatter,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        return collectsFromSingleBucket\n+            ? new FromSingle(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            )\n+            : new FromMany(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            );\n+    }\n \n     private final ValuesSource.Numeric valuesSource;\n     private final DocValueFormat formatter;\n-    private final RoundingInfo[] roundingInfos;\n     private final Function<Rounding, Rounding.Prepared> roundingPreparer;\n-    private int roundingIdx = 0;\n-    private Rounding.Prepared preparedRounding;\n-\n-    private LongHash bucketOrds;\n-    private int targetBuckets;\n+    /**\n+     * A reference to the collector so we can\n+     * {@link MergingBucketsDeferringCollector#mergeBuckets(long[])}.\n+     */\n     private MergingBucketsDeferringCollector deferringCollector;\n \n-    AutoDateHistogramAggregator(String name, AggregatorFactories factories, int numBuckets, RoundingInfo[] roundingInfos,\n-        Function<Rounding, Rounding.Prepared> roundingPreparer, @Nullable ValuesSource valuesSource, DocValueFormat formatter,\n-        SearchContext aggregationContext, Aggregator parent, Map<String, Object> metadata) throws IOException {\n+    protected final RoundingInfo[] roundingInfos;\n+    protected final int targetBuckets;\n+\n+    private AutoDateHistogramAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        int targetBuckets,\n+        RoundingInfo[] roundingInfos,\n+        Function<Rounding, Rounding.Prepared> roundingPreparer,\n+        @Nullable ValuesSource valuesSource,\n+        DocValueFormat formatter,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n \n         super(name, factories, aggregationContext, parent, metadata);\n-        this.targetBuckets = numBuckets;\n+        this.targetBuckets = targetBuckets;\n         this.valuesSource = (ValuesSource.Numeric) valuesSource;\n         this.formatter = formatter;\n         this.roundingInfos = roundingInfos;\n         this.roundingPreparer = roundingPreparer;\n-        preparedRounding = roundingPreparer.apply(roundingInfos[roundingIdx].rounding);\n-\n-        bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n-\n     }\n \n     @Override\n-    public ScoreMode scoreMode() {\n+    public final ScoreMode scoreMode() {\n         if (valuesSource != null && valuesSource.needsScores()) {\n             return ScoreMode.COMPLETE;\n         }\n         return super.scoreMode();\n     }\n \n     @Override\n-    protected boolean shouldDefer(Aggregator aggregator) {\n+    protected final boolean shouldDefer(Aggregator aggregator) {\n         return true;\n     }\n \n     @Override\n-    public DeferringBucketCollector getDeferringCollector() {\n+    public final DeferringBucketCollector getDeferringCollector() {\n         deferringCollector = new MergingBucketsDeferringCollector(context, descendsFromGlobalAggregator(parent()));\n         return deferringCollector;\n     }\n \n+    protected abstract LeafBucketCollector getLeafCollector(SortedNumericDocValues values, LeafBucketCollector sub) throws IOException;\n+\n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public final LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n-        return new LeafBucketCollectorBase(sub, values) {\n-            @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n-                if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+        return getLeafCollector(valuesSource.longValues(ctx), sub);\n+    }\n+\n+    protected final InternalAggregation[] buildAggregations(\n+        LongKeyedBucketOrds bucketOrds,\n+        LongToIntFunction roundingIndexFor,\n+        long[] owningBucketOrds\n+    ) throws IOException {\n+        return buildAggregationsForVariableBuckets(\n+            owningBucketOrds,\n+            bucketOrds,\n+            (bucketValue, docCount, subAggregationResults) -> new InternalAutoDateHistogram.Bucket(\n+                bucketValue,\n+                docCount,\n+                formatter,\n+                subAggregationResults\n+            ),\n+            (owningBucketOrd, buckets) -> {\n+                // the contract of the histogram aggregation is that shards must return\n+                // buckets ordered by key in ascending order\n+                CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+                // value source will be null for unmapped fields\n+                InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(\n+                    roundingInfos,\n+                    roundingIndexFor.applyAsInt(owningBucketOrd),\n+                    buildEmptySubAggregations()\n+                );\n+\n+                return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n+            }\n+        );\n+    }\n+\n+    @Override\n+    public final InternalAggregation buildEmptyAggregation() {\n+        InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(\n+            roundingInfos,\n+            0,\n+            buildEmptySubAggregations()\n+        );\n+        return new InternalAutoDateHistogram(name, Collections.emptyList(), targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n+    }\n+\n+    protected final Rounding.Prepared prepareRounding(int index) {\n+        return roundingPreparer.apply(roundingInfos[index].rounding);\n+    }\n+\n+    protected final void merge(long[] mergeMap, long newNumBuckets) {\n+        mergeBuckets(mergeMap, newNumBuckets);\n+        if (deferringCollector != null) {\n+            deferringCollector.mergeBuckets(mergeMap);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible\n+     * but as more data arrives it rebuckets the data until it \"fits\" in the\n+     * aggregation rounding. Similar to {@link FromMany} this checks both the\n+     * bucket count and range of the aggregation, but unlike\n+     * {@linkplain FromMany} it keeps an accurate count of the buckets and it\n+     * doesn't delay rebucketing.\n+     * <p>\n+     * Rebucketing is roughly {@code O(number_of_hits_collected_so_far)} but we\n+     * rebucket roughly {@code O(log number_of_hits_collected_so_far)} because\n+     * the \"shape\" of the roundings is <strong>roughly</strong>\n+     * logarithmically increasing.\n+     */\n+    private static class FromSingle extends AutoDateHistogramAggregator {\n+        private int roundingIdx;\n+        private Rounding.Prepared preparedRounding;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}. You'd\n+         * think that it wouldn't matter, but its seriously 7%-15% performance\n+         * difference for the aggregation. Yikes.\n+         */", "originalCommit": "518cdfa7e76b22b676d8339eda92c76000f9a324", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NDU1NA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439584554", "bodyText": "Hmm, we might need to guard here in the situation where the agg doesn't actually generate any buckets (none of the collected docs end up having the field, no collected docs, etc), and return an empty agg instead.\nOr alternatively, put the guard in rebucket().\nI think BucketsAggregator#mergeBuckets() would be fine since doc counts would also be empty... but I'm less confident about MergingBucketsDeferringCollector#mergeBuckets().  Might be ok though, only skimmed it :)  We've ran into this kind of issue in the past though where buildAgg broke when the agg runs and is effectively \"unmapped\" after running, so thought I'd mention :)", "author": "polyfractal", "createdAt": "2020-06-12T18:40:00Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;\n+\n+        FromMany(\n+            String name,\n+            AggregatorFactories factories,\n+            int targetBuckets,\n+            RoundingInfo[] roundingInfos,\n+            Function<Rounding, Rounding.Prepared> roundingPreparer,\n+            @Nullable ValuesSource valuesSource,\n+            DocValueFormat formatter,\n+            SearchContext aggregationContext,\n+            Aggregator parent,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+\n+            super(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            );\n+            assert roundingInfos.length < 127 : \"Rounding must fit in a signed byte\";\n+            roundingIndices = context.bigArrays().newByteArray(1, true);\n+            mins = context.bigArrays().newLongArray(1, false);\n+            mins.set(0, Long.MAX_VALUE);\n+            maxes = context.bigArrays().newLongArray(1, false);\n+            maxes.set(0, Long.MIN_VALUE);\n+            preparedRoundings = new Rounding.Prepared[roundingInfos.length];\n+            // Prepare the first rounding because we know we'll need it.\n+            preparedRoundings[0] = roundingPreparer.apply(roundingInfos[0].rounding);\n+            bucketOrds = new LongKeyedBucketOrds.FromMany(context.bigArrays());\n+            liveBucketCountUnderestimate = context.bigArrays().newIntArray(1, true);\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(SortedNumericDocValues values, LeafBucketCollector sub) throws IOException {\n+            return new LeafBucketCollectorBase(sub, values) {\n+                @Override\n+                public void collect(int doc, long owningBucketOrd) throws IOException {\n+                    if (false == values.advanceExact(doc)) {\n+                        return;\n+                    }\n+                    int valuesCount = values.docValueCount();\n+\n+                    long previousRounded = Long.MIN_VALUE;\n+                    int roundingIdx = roundingIndexFor(owningBucketOrd);\n+                    for (int i = 0; i < valuesCount; ++i) {\n+                        long value = values.nextValue();\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n+                        assert rounded >= previousRounded;\n+                        if (rounded == previousRounded) {\n+                            continue;\n                         }\n+                        roundingIdx = collectValue(owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n-            }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n+                private int collectValue(long owningBucketOrd, int roundingIdx, int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return roundingIdx;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                    int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                    return increaseRoundingIfNeeded(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+                }\n+\n+                /**\n+                 * Increase the rounding of {@code owningBucketOrd} using\n+                 * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+                 * buckets if the estimated number of wasted buckets is too high.\n+                 */\n+                private int increaseRoundingIfNeeded(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, int oldRounding) {\n+                    if (oldRounding >= roundingInfos.length - 1) {\n+                        return oldRounding;\n+                    }\n+                    if (mins.size() < owningBucketOrd + 1) {\n+                        long oldSize = mins.size();\n+                        mins = context.bigArrays().grow(mins, owningBucketOrd + 1);\n+                        mins.fill(oldSize, mins.size(), Long.MAX_VALUE);\n+                    }\n+                    if (maxes.size() < owningBucketOrd + 1) {\n+                        long oldSize = maxes.size();\n+                        maxes = context.bigArrays().grow(maxes, owningBucketOrd + 1);\n+                        maxes.fill(oldSize, maxes.size(), Long.MIN_VALUE);\n+                    }\n+\n+                    long min = Math.min(mins.get(owningBucketOrd), newKey);\n+                    mins.set(owningBucketOrd, min);\n+                    long max = Math.max(maxes.get(owningBucketOrd), newKey);\n+                    maxes.set(owningBucketOrd, max);\n+                    if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                        return oldRounding;\n+                    }\n+                    long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                    int newRounding = oldRounding;\n+                    int newEstimatedBucketCount;\n+                    do {\n+                        newRounding++;\n+                        double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                        newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                    } while (newRounding < roundingInfos.length - 1\n+                        && (newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                    setRounding(owningBucketOrd, newRounding);\n+                    mins.set(owningBucketOrd, preparedRoundings[newRounding].round(mins.get(owningBucketOrd)));\n+                    maxes.set(owningBucketOrd, preparedRoundings[newRounding].round(maxes.get(owningBucketOrd)));\n+                    wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                    if (wastedBucketsOverestimate > nextRebucketAt) {\n+                        rebucket();\n+                        // Bump the threshold for the next rebucketing\n+                        wastedBucketsOverestimate = 0;\n+                        nextRebucketAt *= 2;\n+                    } else {\n+                        liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n                     }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+                    return newRounding;\n+                }\n+            };\n+        }\n+\n+        private void rebucket() {\n+            rebucketCount++;\n+            try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                bucketOrds = new LongKeyedBucketOrds.FromMany(context.bigArrays());\n+                for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {\n+                        long oldKey = ordsEnum.value();\n+                        long newKey = preparedRounding.round(oldKey);\n+                        long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                        mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                     }\n-                    bucketOrds = newBucketOrds;\n+                    liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n                 }\n+                merge(mergeMap, bucketOrds.size());\n             }\n-        };\n-    }\n+        }\n \n-    @Override\n-    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n-        return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n-                (bucketValue, docCount, subAggregationResults) ->\n-                    new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n-                    // the contract of the histogram aggregation is that shards must return\n-                    // buckets ordered by key in ascending order\n-                    CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n-\n-                    // value source will be null for unmapped fields\n-                    InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n-\n-                    return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n-                });\n-    }\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            /*\n+             * Rebucket before building the aggregation to build as small as result\n+             * as possible.\n+             *\n+             * TODO it'd be faster if we could apply the merging on the fly as we\n+             * replay the hits and build the buckets. How much faster is not clear,\n+             * but it does have the advantage of only touching the buckets that we\n+             * want to collect.\n+             */\n+            rebucket();", "originalCommit": "518cdfa7e76b22b676d8339eda92c76000f9a324", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439587345", "bodyText": "Exposing rebucketing info (count, rebucketAt, wasted, etc) via collectDebugInfo would be super interesting from a profiling perspective.  Especially if a user comes to us with performance issues, we'd have a better idea if they are hitting some kind of antagonistic scenario or something\nNot super important, just a thought :)", "author": "polyfractal", "createdAt": "2020-06-12T18:46:09Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;", "originalCommit": "518cdfa7e76b22b676d8339eda92c76000f9a324", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NzgyNA==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439597824", "bodyText": "I already exposed it all! I made this in the first place because I wanted to see how the algorithm did with certain bits of data with the profiler.", "author": "nik9000", "createdAt": "2020-06-12T19:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMwODQ3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r440308471", "bodyText": "Wow I totally overlooked those.  Ignore me! :)", "author": "polyfractal", "createdAt": "2020-06-15T16:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ=="}], "type": "inlineReview"}, {"oid": "268503e29658ca754a56876292295a3f44044bcf", "url": "https://github.com/elastic/elasticsearch/commit/268503e29658ca754a56876292295a3f44044bcf", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-12T19:54:07Z", "type": "commit"}, {"oid": "88bb61faceb5c3fd182d65bd4fb54b43208437ff", "url": "https://github.com/elastic/elasticsearch/commit/88bb61faceb5c3fd182d65bd4fb54b43208437ff", "message": "Merge branch 'master' into auto_date_histo_mem", "committedDate": "2020-06-15T17:44:45Z", "type": "commit"}]}