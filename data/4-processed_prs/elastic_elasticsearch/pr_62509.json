{"pr_number": 62509, "pr_title": "Faster sequential access for stored fields", "pr_createdAt": "2020-09-16T22:33:40Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62509", "timeline": [{"oid": "7ce055ba46ee082580bc7efa49ffbea315e080e8", "url": "https://github.com/elastic/elasticsearch/commit/7ce055ba46ee082580bc7efa49ffbea315e080e8", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T00:04:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA0NTI2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490045261", "bodyText": "No need to clone, it's a thread-local instance already.", "author": "jpountz", "createdAt": "2020-09-17T07:56:49Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -114,7 +118,17 @@ public void execute(SearchContext context) {\n                 int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n                 if (currentReaderIndex != readerIndex) {\n                     currentReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n+                    storedFieldsReader = context.searcher().getCodecReader(readerIndex).getFieldsReader();\n                     currentReaderIndex = readerIndex;\n+                    if (hasSequentialDocs && docs.length >= 10) {\n+                        // All the docs to fetch are adjacent but Lucene stored fields are optimized\n+                        // for random access and don't optimize for sequential access - except for merging.\n+                        // So we do a little hack here and pretend we're going to do merges in order to\n+                        // get better sequential access.\n+                        storedFieldsReader = storedFieldsReader.getMergeInstance();\n+                    } else {\n+                        storedFieldsReader = storedFieldsReader.clone();", "originalCommit": "3377e71c3aade39debdfc194125ddc067d2357e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI1OTU5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490259593", "bodyText": "got it, thanks", "author": "jimczi", "createdAt": "2020-09-17T13:47:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA0NTI2MQ=="}], "type": "inlineReview"}, {"oid": "6ed45fae4d36290930cf56b76473433738ac84e9", "url": "https://github.com/elastic/elasticsearch/commit/6ed45fae4d36290930cf56b76473433738ac84e9", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T13:45:51Z", "type": "commit"}, {"oid": "6ed45fae4d36290930cf56b76473433738ac84e9", "url": "https://github.com/elastic/elasticsearch/commit/6ed45fae4d36290930cf56b76473433738ac84e9", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T13:45:51Z", "type": "forcePushed"}, {"oid": "c06020d37bf3214997c257d766526c18f5ce6241", "url": "https://github.com/elastic/elasticsearch/commit/c06020d37bf3214997c257d766526c18f5ce6241", "message": "fix SeqIdGeneratingFilterReader", "committedDate": "2020-09-17T14:11:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3MzcwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490273705", "bodyText": "Can you add javadocs?", "author": "jpountz", "createdAt": "2020-09-17T14:05:08Z", "path": "server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.common.lucene.index;\n+\n+import org.apache.lucene.codecs.StoredFieldsReader;\n+import org.apache.lucene.index.CodecReader;\n+import org.apache.lucene.index.FilterLeafReader;\n+import org.apache.lucene.index.LeafReader;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A {@link FilterLeafReader} that exposes a {@link StoredFieldsReader}\n+ * optimized for sequential access. This class should be used by custom\n+ * {@link FilterLeafReader} that are used at search time in order to\n+ * leverage sequential access when retrieving stored fields in queries,\n+ * aggregations or during the fetch phase.\n+ */\n+public abstract class SequentialStoredFieldsLeafReader extends FilterLeafReader {\n+    /**\n+     * <p>Construct a StoredFieldsFilterLeafReader based on the specified base reader.\n+     * <p>Note that base reader is closed if this FilterLeafReader is closed.</p>\n+     *\n+     * @param in specified base reader.\n+     */\n+    public SequentialStoredFieldsLeafReader(LeafReader in) {\n+        super(in);\n+    }\n+\n+    protected abstract StoredFieldsReader doGetSequentialStoredFieldsReader(StoredFieldsReader reader);", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNjg4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490326886", "bodyText": "I pushed 9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:05:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3MzcwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NTk3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490275977", "bodyText": "nit: since doc ids are sorted, we should be able to do something like return docs.length > 0 && docs[docs.length-1].docId - docs[0].docId == docs.length - 1;", "author": "jpountz", "createdAt": "2020-09-17T14:08:03Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -524,4 +550,18 @@ private static void fillDocAndMetaFields(SearchContext context, FieldsVisitor fi\n             }\n         }\n     }\n+\n+    /**\n+     * Returns <code>true</code> if the provided <code>docs</code> are\n+     * stored sequentially (Dn = Dn-1 + 1).\n+     */\n+    static boolean hasSequentialDocs(DocIdToIndex[] docs) {\n+        for (int i = 1; i < docs.length; i++) {\n+            assert docs[i].docId >= docs[i-1].docId : \"doc ids out of order\";\n+            if (docs[i].docId - docs[i-1].docId > 1) {\n+                return false;\n+            }\n+        }\n+        return true;", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzc2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327766", "bodyText": "++, I pushed 9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NTk3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NjQyMw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490276423", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Make sure that the engine produces a CodecReader.\n          \n          \n            \n                    // Make sure that the engine produces a SequentialStoredFieldsLeafReader.", "author": "jpountz", "createdAt": "2020-09-17T14:08:37Z", "path": "server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java", "diffHunk": "@@ -5940,4 +5941,25 @@ public void testNotWarmUpSearcherInEngineCtor() throws Exception {\n             }\n         }\n     }\n+\n+    public void testProducesStoredFieldsReader() throws Exception {\n+        // Make sure that the engine produces a CodecReader.", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzg2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327869", "bodyText": "9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NjQyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3OTk3MA==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490279970", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testProducesCodecReader() throws Exception {\n          \n          \n            \n                public void testProducesSequentialStoredFieldsLeafReader() throws Exception {", "author": "jpountz", "createdAt": "2020-09-17T14:13:00Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java", "diffHunk": "@@ -215,6 +218,41 @@ public void testCoreCacheKey() throws Exception {\n         IOUtils.close(ir, ir2, iw, dir);\n     }\n \n+    public void testProducesCodecReader() throws Exception {", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzk1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327950", "bodyText": "9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3OTk3MA=="}], "type": "inlineReview"}, {"oid": "9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "url": "https://github.com/elastic/elasticsearch/commit/9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "message": "feedback", "committedDate": "2020-09-17T15:04:48Z", "type": "commit"}, {"oid": "2bee5fc87014a881cc46f7fc1d79bb33be886818", "url": "https://github.com/elastic/elasticsearch/commit/2bee5fc87014a881cc46f7fc1d79bb33be886818", "message": "unused import", "committedDate": "2020-09-17T15:56:53Z", "type": "commit"}]}