{"pr_number": 51230, "pr_title": "Optimize sequential reads in SearchableSnapshotIndexInput", "pr_createdAt": "2020-01-20T16:37:26Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/51230", "timeline": [{"oid": "a265c302f73bc940ff66a00ed7ad758d3af6856a", "url": "https://github.com/elastic/elasticsearch/commit/a265c302f73bc940ff66a00ed7ad758d3af6856a", "message": "Optimize sequential reads in SearchableSnapshotIndexInput\n\nToday `SearchableSnapshotIndexInput` translates each `readBytesInternal` call\nto one or more calls to `readBlob` on the underlying repository. We make a lot\nof small `readBytesInternal` calls since they are used to fill a small\nin-memory buffer. Calls to `readBlob` are expensive: blob storage providers\nlike AWS S3 charge money per API call.\n\nA common usage pattern is to take a brand-new `IndexInput`, seek to a\nparticular location, and then sequentially read a substantial amount of data\nand stream it to disk.\n\nThis commit optimizes the implementation for that specific usage pattern.\nRather than calling `readBlob` each time the internal buffer needs filling we\ninstead request a (potentially much larger) range of the blob and consume the\nresponse bit-by-bit as needed by a sequentially-reading client.", "committedDate": "2020-01-20T16:22:49Z", "type": "commit"}, {"oid": "b8a425b253124bb1ba8f367bb5d9839a523b9904", "url": "https://github.com/elastic/elasticsearch/commit/b8a425b253124bb1ba8f367bb5d9839a523b9904", "message": "Post-PR-opening blues", "committedDate": "2020-01-20T16:45:06Z", "type": "commit"}, {"oid": "6b22442d48cc1afa065482f9ef4a6aae721a6896", "url": "https://github.com/elastic/elasticsearch/commit/6b22442d48cc1afa065482f9ef4a6aae721a6896", "message": "Introduce constant to clarify that clones and slices are not optimized", "committedDate": "2020-01-20T17:09:26Z", "type": "commit"}, {"oid": "855b661c37381a7f79e171aad17798ce1f9bdc5e", "url": "https://github.com/elastic/elasticsearch/commit/855b661c37381a7f79e171aad17798ce1f9bdc5e", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead", "committedDate": "2020-01-21T10:26:40Z", "type": "commit"}, {"oid": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "url": "https://github.com/elastic/elasticsearch/commit/4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "message": "Fix merge conflict", "committedDate": "2020-01-21T10:27:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370017273", "bodyText": "perhaps use new ByteSizeValue(32, ByteSizeUnit.MB).getBytes().\nIt auto-documents the value :)", "author": "ywelsch", "createdAt": "2020-01-23T09:45:02Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -36,6 +37,7 @@\n \n     private final BlobStoreIndexShardSnapshot snapshot;\n     private final BlobContainer blobContainer;\n+    private static final long BLOB_STORE_SEQUENTIAL_READ_SIZE = 1L<<25; // 32MB", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODQ3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058479", "bodyText": "D'oh.", "author": "DaveCTurner", "createdAt": "2020-01-23T11:11:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA5MzM5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370093391", "bodyText": "Addressed in 9359e70.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:38:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDU2NA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100564", "bodyText": "Addressed in 9359e70.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:55:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxNzI3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370018583", "bodyText": "I wonder if choosing the BLOB_STORE_SEQUENTIAL_READ_SIZE should be left to the underlying blob store implementation. For a shared file-system, it might not make sense to have BLOB_STORE_SEQUENTIAL_READ_SIZE at all (but should use Long.MAX_VALUE instead), as it requires opening the files multiple times.", "author": "ywelsch", "createdAt": "2020-01-23T09:47:29Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -68,7 +70,8 @@ public long fileLength(final String name) throws IOException {\n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n         ensureOpen();\n-        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name));\n+        return new SearchableSnapshotIndexInput(blobContainer, fileInfo(name), BLOB_STORE_SEQUENTIAL_READ_SIZE,", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODQzMA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058430", "bodyText": "Yes it probably should. Maybe even expose this as a setting on the repository, although with sensible defaults as you note.", "author": "DaveCTurner", "createdAt": "2020-01-23T11:11:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA5MzcyOA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370093728", "bodyText": "Allowing this to depend on the blob container in 9359e70 - we can expose a setting in a followup if needed.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAxODU4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzMTY3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370031676", "bodyText": "something missing here", "author": "ywelsch", "createdAt": "2020-01-23T10:13:01Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDQyOA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100428", "bodyText": "Fixed in 78087ab.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzMTY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNTUxNA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370035514", "bodyText": "should we put this logic into StreamForSequentialReads? Perhaps that class could enforce that only sequential reads are possible from the stream (and offer a method to say isSequentialReadPossible), with the logic in this class here just trying to call these methods.", "author": "ywelsch", "createdAt": "2020-01-23T10:21:02Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDAyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100021", "bodyText": "Moved this around in 1d69293 and 184c9b1.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:54:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNTUxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNjI0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370036241", "bodyText": "just leave off the != false", "author": "ywelsch", "createdAt": "2020-01-23T10:22:38Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDA4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100083", "bodyText": "33a1362", "author": "DaveCTurner", "createdAt": "2020-01-23T12:54:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNjI0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370037307", "bodyText": "Do we ever expect concurrent activity here? If that was the case, our bookkeeping logic (position, pos, offset, length) would not work at all?", "author": "ywelsch", "createdAt": "2020-01-23T10:24:51Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NjI3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370056275", "bodyText": "We don't expect concurrent reading, but we can be concurrently closed. In an earlier iteration we also dropped the optimisation if cloned or sliced, which happens on a separate thread, but in fact that shouldn't be happening concurrently with reading either, so concurrent closing is the only thing to handle. I'll adjust the comments.", "author": "DaveCTurner", "createdAt": "2020-01-23T11:06:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDE3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100176", "bodyText": "Adjusted in 84d12b6.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:54:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzNzMwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370038800", "bodyText": "maybe put everything above this into a readOptimized() method that returns a boolean (denoting whether it read or not). This will allow having so many explicit returns in the above code (and the deliberate fall-through logic).", "author": "ywelsch", "createdAt": "2020-01-23T10:27:55Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1ODAzNA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370058034", "bodyText": "Sounds good.", "author": "DaveCTurner", "createdAt": "2020-01-23T11:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDEwMDM1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370100355", "bodyText": "See 1d69293 and 184c9b1.", "author": "DaveCTurner", "createdAt": "2020-01-23T12:54:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzODgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzOTM2NA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370039364", "bodyText": "I guess this is to handle some kind of concurrency. I don't really understand the concurrency here though.", "author": "ywelsch", "createdAt": "2020-01-23T10:28:58Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NzA0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370057041", "bodyText": "Just closing.", "author": "DaveCTurner", "createdAt": "2020-01-23T11:08:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAzOTM2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0MjM5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370042393", "bodyText": "Should we not use this existing open stream as much as possible? We might not be able to read the full bytes from this stream, but perhaps we can use it to read everything up to streamLength, and subsequently request a new stream for the rest? This might avoid redownloading data in case where the buffer size is not a proper divisor of sequentialReadSize?", "author": "ywelsch", "createdAt": "2020-01-23T10:35:27Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,34 +105,126 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        final long currentSequentialReadSize = sequentialReadSize;\n+        if (currentSequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            final StreamForSequentialReads streamForSequentialReads = streamForSequentialReadsRef.get();\n+            if (streamForSequentialReads == null) {\n+                // start a new sequential read\n+                if (tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                    return;\n+                }\n+            } else if (streamForSequentialReads.part == part && streamForSequentialReads.pos == pos) {\n+                // continuing a sequential read that we started previously\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                int read = streamForSequentialReads.inputStream.read(b, offset, length);\n+                assert read <= length : read + \" vs \" + length;\n+                streamForSequentialReads.pos += read;\n+                position += read;\n+                pos += read;\n+                offset += read;\n+                length -= read;\n+\n+                if (streamForSequentialReads.isFullyRead()) {\n+                    if (streamForSequentialReadsRef.compareAndSet(streamForSequentialReads, null) != false) {\n+                        streamForSequentialReads.close();\n+                    } else {\n+                        // something happened concurrently, defensively stop optimizing\n+                        sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                    }\n+\n+                    if (length == 0) {\n+                        // the current stream contained precisely enough data for this read, so we're good.\n+                        return;\n+                    } else {\n+                        // the current stream didn't contain enough data for this read, so we must read more\n+                        if (sequentialReadSize != NO_SEQUENTIAL_READ_OPTIMIZATION\n+                            && tryReadAndKeepStreamOpen(part, pos, b, offset, length, currentSequentialReadSize)) {\n+                            return;\n+                        }\n+                    }\n+                } else {\n+                    // the current stream contained enough data for this read and more besides, so we leave it alone.\n+                    assert length == 0 : length + \" remaining\";\n+                    return;\n+                }\n+            } else {\n+                // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+                assert streamForSequentialReads.isFullyRead() == false;\n+                sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                IOUtils.close(streamForSequentialReadsRef.getAndSet(null));\n+            }\n+        }\n+\n+        // read part of a blob directly; the code above falls through to this case where there is no optimization possible\n         try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n+            final int read = inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n             position += read;\n         }\n     }\n \n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it. Returns whether this happened or not;\n+     * if it did not happen then nothing was read, and the caller should perform the read directly.\n+     */\n+    private boolean tryReadAndKeepStreamOpen(int part, long pos, byte[] b, int offset, int length, long currentSequentialReadSize)\n+        throws IOException {\n+\n+        assert streamForSequentialReadsRef.get() == null : \"should only be called when a new stream is needed\";\n+        assert currentSequentialReadSize > 0L : \"should not be called if \";\n+\n+        final long streamLength = Math.min(currentSequentialReadSize, fileInfo.partBytes(part) - pos);\n+        if (length < streamLength) {\n+            // if we open a stream of length streamLength then it will not be completely consumed by this read, so it is worthwhile to open\n+            // it and keep it open for future reads\n+            final InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, streamLength);\n+            final StreamForSequentialReads newStreamForSequentialReads\n+                = new StreamForSequentialReads(inputStream, part, pos, streamLength);\n+            if (streamForSequentialReadsRef.compareAndSet(null, newStreamForSequentialReads) == false) {\n+                // something happened concurrently, defensively stop optimizing and fall through to the unoptimized behaviour\n+                this.sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+                inputStream.close();\n+                return false;\n+            }\n+\n+            final int read = newStreamForSequentialReads.inputStream.read(b, offset, length);\n+            assert read == length : read + \" vs \" + length;\n+            position += read;\n+            newStreamForSequentialReads.pos += read;\n+            assert newStreamForSequentialReads.isFullyRead() == false;\n+            return true;\n+        } else {\n+            // streamLength <= length so this single read will consume the entire stream, so there is no need to keep hold of it, so we can\n+            // tell the caller to read the data directly", "originalCommit": "4e0cfcf442ccc0ca75fbd476594320ef6cdcba57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA1NzgxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r370057819", "bodyText": "At this point we don't have an existing open stream, we're trying to create a new one. If we can satisfy part of a read from the existing stream then we do so (see comment containing the string the current stream didn't contain enough data for this read, so we must read more).", "author": "DaveCTurner", "createdAt": "2020-01-23T11:10:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDA0MjM5Mw=="}], "type": "inlineReview"}, {"oid": "9359e70c6e9f19ee918e967d85868527293abf58", "url": "https://github.com/elastic/elasticsearch/commit/9359e70c6e9f19ee918e967d85868527293abf58", "message": "Sequential read size depends on container", "committedDate": "2020-01-23T11:39:37Z", "type": "commit"}, {"oid": "78087ab4e1c08c13002abfa5a37a1606cd4d7f98", "url": "https://github.com/elastic/elasticsearch/commit/78087ab4e1c08c13002abfa5a37a1606cd4d7f98", "message": "Fix partial assertion message", "committedDate": "2020-01-23T11:40:47Z", "type": "commit"}, {"oid": "33a1362ce45b035158f1243a5935548c8a5e782a", "url": "https://github.com/elastic/elasticsearch/commit/33a1362ce45b035158f1243a5935548c8a5e782a", "message": "(!= false) == (== true)", "committedDate": "2020-01-23T11:41:23Z", "type": "commit"}, {"oid": "84d12b606f06c3d37e2dd9bb24549356079d913a", "url": "https://github.com/elastic/elasticsearch/commit/84d12b606f06c3d37e2dd9bb24549356079d913a", "message": "Adjust behaviour now that the only concurrent thing we support is closing", "committedDate": "2020-01-23T11:47:26Z", "type": "commit"}, {"oid": "1d692930391c32e0ca4f2bf0fd6d0e6fbbce7ec6", "url": "https://github.com/elastic/elasticsearch/commit/1d692930391c32e0ca4f2bf0fd6d0e6fbbce7ec6", "message": "readOptimized()", "committedDate": "2020-01-23T12:37:11Z", "type": "commit"}, {"oid": "184c9b13764642baa3a6daf9ccabce54e4ef24d5", "url": "https://github.com/elastic/elasticsearch/commit/184c9b13764642baa3a6daf9ccabce54e4ef24d5", "message": "More refactoring into StreamForSequentialReads etc.", "committedDate": "2020-01-23T12:51:57Z", "type": "commit"}, {"oid": "70b44c3667d6a515d2a9da07c56767d8d48179bb", "url": "https://github.com/elastic/elasticsearch/commit/70b44c3667d6a515d2a9da07c56767d8d48179bb", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead", "committedDate": "2020-01-23T12:53:26Z", "type": "commit"}, {"oid": "f4094c7eca2756d9d6b1efb7ea04af1b9cd6a843", "url": "https://github.com/elastic/elasticsearch/commit/f4094c7eca2756d9d6b1efb7ea04af1b9cd6a843", "message": "Imports", "committedDate": "2020-01-23T13:57:35Z", "type": "commit"}, {"oid": "4a88bb4c8060d5f93cb351d9ad208e3c6a9b5f6a", "url": "https://github.com/elastic/elasticsearch/commit/4a88bb4c8060d5f93cb351d9ad208e3c6a9b5f6a", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead", "committedDate": "2020-02-03T16:20:46Z", "type": "commit"}, {"oid": "eeab774cc90806282b81ac163e5373558973d361", "url": "https://github.com/elastic/elasticsearch/commit/eeab774cc90806282b81ac163e5373558973d361", "message": "No need to handle concurrent closing", "committedDate": "2020-02-03T16:46:41Z", "type": "commit"}, {"oid": "a92ced530401cece9969728d48d17b1196f1ec0b", "url": "https://github.com/elastic/elasticsearch/commit/a92ced530401cece9969728d48d17b1196f1ec0b", "message": "Merge branch 'feature/searchable-snapshots' into 2020-01-20-searchable-snapshot-readahead", "committedDate": "2020-02-03T17:03:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NjEyNw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374696127", "bodyText": "Maybe update the class javadoc to explain how/why we use this?", "author": "tlrx", "createdAt": "2020-02-04T14:19:06Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -41,20 +44,31 @@\n     private final long length;\n \n     private long position;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SearchableSnapshotIndexInput(final BlobContainer blobContainer, final FileInfo fileInfo) {\n-        this(\"SearchableSnapshotIndexInput(\" + fileInfo.physicalName() + \")\", blobContainer, fileInfo, 0L, 0L, fileInfo.length());\n+    // optimisation for the case where we perform a single seek, then read a large block of data sequentially, then close the input\n+    @Nullable // if not currently reading sequentially\n+    private StreamForSequentialReads streamForSequentialReads;", "originalCommit": "a92ced530401cece9969728d48d17b1196f1ec0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758066", "bodyText": "sure, done in f18251a", "author": "DaveCTurner", "createdAt": "2020-02-04T15:55:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NjEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NzIyOA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374697228", "bodyText": "We're closing + nullify the streamForSequentialReads many times, maybe it deserves its own closeSequentialStream() method?", "author": "tlrx", "createdAt": "2020-02-04T14:21:05Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "originalCommit": "a92ced530401cece9969728d48d17b1196f1ec0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODI0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758241", "bodyText": "Good point, this is no longer a one-liner. Done in c9cf7bc.", "author": "DaveCTurner", "createdAt": "2020-02-04T15:55:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5NzIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5ODQ0MA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374698440", "bodyText": "The method signature can fit on a single line", "author": "tlrx", "createdAt": "2020-02-04T14:23:09Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -93,12 +107,87 @@ protected void readInternal(byte[] b, int offset, int length) throws IOException\n         }\n     }\n \n-    private void readInternalBytes(final long part, final long pos, byte[] b, int offset, int length) throws IOException {\n-        try (InputStream inputStream = blobContainer.readBlob(fileInfo.partName(part), pos, length)) {\n-            int read = inputStream.read(b, offset, length);\n-            assert read == length;\n-            position += read;\n+    private void readInternalBytes(final int part, long pos, final byte[] b, int offset, int length) throws IOException {\n+        int optimizedReadSize = readOptimized(part, pos, b, offset, length);\n+        assert optimizedReadSize <= length;\n+        position += optimizedReadSize;\n+\n+        if (optimizedReadSize < length) {\n+            // we did not read everything in an optimized fashion, so read the remainder directly\n+            try (InputStream inputStream\n+                     = blobContainer.readBlob(fileInfo.partName(part), pos + optimizedReadSize, length - optimizedReadSize)) {\n+                final int directReadSize = inputStream.read(b, offset + optimizedReadSize, length - optimizedReadSize);\n+                assert optimizedReadSize + directReadSize == length : optimizedReadSize + \" and \" + directReadSize + \" vs \" + length;\n+                position += directReadSize;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempt to satisfy this read in an optimized fashion using {@code streamForSequentialReadsRef}.\n+     * @return the number of bytes read\n+     */\n+    private int readOptimized(int part, long pos, byte[] b, int offset, int length) throws IOException {\n+        if (sequentialReadSize == NO_SEQUENTIAL_READ_OPTIMIZATION) {\n+            return 0;\n+        }\n+\n+        int read = 0;\n+        if (streamForSequentialReads == null) {\n+            // starting a new sequential read\n+            read = readFromNewSequentialStream(part, pos, b, offset, length);\n+        } else if (streamForSequentialReads.canContinueSequentialRead(part, pos)) {\n+            // continuing a sequential read that we started previously\n+            read = streamForSequentialReads.read(b, offset, length);\n+            if (streamForSequentialReads.isFullyRead()) {\n+                // the current stream was exhausted by this read, so it should be closed\n+                streamForSequentialReads.close();\n+                streamForSequentialReads = null;\n+            } else {\n+                // the current stream contained enough data for this read and more besides, so we leave it in place\n+                assert read == length : length + \" remaining\";\n+            }\n+\n+            if (read < length) {\n+                // the current stream didn't contain enough data for this read, so we must read more\n+                read += readFromNewSequentialStream(part, pos + read, b, offset + read, length - read);\n+            }\n+        } else {\n+            // not a sequential read, so stop optimizing for this usage pattern and fall through to the unoptimized behaviour\n+            assert streamForSequentialReads.isFullyRead() == false;\n+            sequentialReadSize = NO_SEQUENTIAL_READ_OPTIMIZATION;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n         }\n+        return read;\n+    }\n+\n+    /**\n+     * If appropriate, open a new stream for sequential reading and satisfy the given read using it.\n+     * @return the number of bytes read; if a new stream wasn't opened then nothing was read so the caller should perform the read directly.\n+     */\n+    private int readFromNewSequentialStream(int part, long pos, byte[] b, int offset, int length)\n+        throws IOException {", "originalCommit": "a92ced530401cece9969728d48d17b1196f1ec0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODM5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758395", "bodyText": "It didn't always ;) Fixed in 51d2af5", "author": "DaveCTurner", "createdAt": "2020-02-04T15:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5ODQ0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTQ4NA==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699484", "bodyText": "Maybe nullify in a finally block, just in case", "author": "tlrx", "createdAt": "2020-02-04T14:24:56Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;", "originalCommit": "a92ced530401cece9969728d48d17b1196f1ec0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODQ4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758489", "bodyText": "Oh good point, done in c9cf7bc.", "author": "DaveCTurner", "createdAt": "2020-02-04T15:55:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTc5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374699799", "bodyText": "Maybe add a small word on why we can't read optimized for clones?", "author": "tlrx", "createdAt": "2020-02-04T14:25:31Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotIndexInput.java", "diffHunk": "@@ -108,19 +197,24 @@ protected void seekInternal(long pos) throws IOException {\n         } else if (pos < 0L) {\n             throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n         }\n-        this.position = offset + pos;\n+        if (position != offset + pos) {\n+            position = offset + pos;\n+            IOUtils.close(streamForSequentialReads);\n+            streamForSequentialReads = null;\n+        }\n     }\n \n     @Override\n     public BufferedIndexInput clone() {\n-        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length);\n+        return new SearchableSnapshotIndexInput(\"clone(\" + this + \")\", blobContainer, fileInfo, position, offset, length,\n+            NO_SEQUENTIAL_READ_OPTIMIZATION, getBufferSize());", "originalCommit": "a92ced530401cece9969728d48d17b1196f1ec0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1ODYxMw==", "url": "https://github.com/elastic/elasticsearch/pull/51230#discussion_r374758613", "bodyText": "Comments added in 2fee6b8.", "author": "DaveCTurner", "createdAt": "2020-02-04T15:55:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY5OTc5OQ=="}], "type": "inlineReview"}, {"oid": "c9cf7bcd11d91a676f60d3a6bdb4c9b44858c368", "url": "https://github.com/elastic/elasticsearch/commit/c9cf7bcd11d91a676f60d3a6bdb4c9b44858c368", "message": "Extract closeStreamForSequentialReads and use finally", "committedDate": "2020-02-04T15:41:18Z", "type": "commit"}, {"oid": "51d2af507dde139718a13d865cb2522b3f0f6a8e", "url": "https://github.com/elastic/elasticsearch/commit/51d2af507dde139718a13d865cb2522b3f0f6a8e", "message": "Whitespace", "committedDate": "2020-02-04T15:41:35Z", "type": "commit"}, {"oid": "2fee6b851ac4482dd2ec43c46c87debdbeb71113", "url": "https://github.com/elastic/elasticsearch/commit/2fee6b851ac4482dd2ec43c46c87debdbeb71113", "message": "Comment on why slices/clones are not optimized", "committedDate": "2020-02-04T15:45:09Z", "type": "commit"}, {"oid": "f18251a597595b441b09ac46432623163a25caf0", "url": "https://github.com/elastic/elasticsearch/commit/f18251a597595b441b09ac46432623163a25caf0", "message": "Add Javadoc", "committedDate": "2020-02-04T15:54:06Z", "type": "commit"}, {"oid": "f9d4def49d7976011413d08e16de7023b279670c", "url": "https://github.com/elastic/elasticsearch/commit/f9d4def49d7976011413d08e16de7023b279670c", "message": "typo", "committedDate": "2020-02-04T15:56:53Z", "type": "commit"}]}