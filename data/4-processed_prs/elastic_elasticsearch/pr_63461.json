{"pr_number": 63461, "pr_title": "Use Pooled Byte Arrays in BlobStoreRepository Serialization", "pr_createdAt": "2020-10-08T10:40:30Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/63461", "timeline": [{"oid": "54186e745ca7d4566e24388712432b2e8625aaff", "url": "https://github.com/elastic/elasticsearch/commit/54186e745ca7d4566e24388712432b2e8625aaff", "message": "Use Pooled Byte Arrays in BlobStoreRepository Serialization\n\nMany of the metadata blobs we handle in the changed spots can grow\nup in size up to `O(1M)`. Not using recycled bytes when working with\nthem causes significant spikes in memory use for larger repositories.", "committedDate": "2020-10-08T10:35:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTYzOTQxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r501639415", "bodyText": "It's not exclusively laziness that I didn't make this path use pooled bytes now as well. Caching here turned out to be somewhat bogus in practice and needs to be changed (will do over the next few days when I find some time). I just went with the simplest possible change here for now as it doesn't really matter.", "author": "original-brownbear", "createdAt": "2020-10-08T11:14:40Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1340,8 +1348,8 @@ private void doGetRepositoryData(ActionListener<RepositoryData> listener) {\n                     // We can cache serialized in the most recent version here without regard to the actual repository metadata version\n                     // since we're only caching the information that we just wrote and thus won't accidentally cache any information that\n                     // isn't safe\n-                    cacheRepositoryData(\n-                            BytesReference.bytes(loaded.snapshotsToXContent(XContentFactory.jsonBuilder(), Version.CURRENT)), genToLoad);\n+                    cacheRepositoryData(compressRepoDataForCache(BytesReference.bytes(", "originalCommit": "54186e745ca7d4566e24388712432b2e8625aaff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY0MDQ5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r501640493", "bodyText": "Creating the serialized repo data to cache here early is a win even without pooled buffers for the serialization. It wasn't necessary to keep the uncompressed version around for the duration of the below cluster state update.\nThis is especially true when it turns out to be very large (as in multiple MB or so) and won't be cached to begin with.", "author": "original-brownbear", "createdAt": "2020-10-08T11:16:46Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1630,9 +1648,15 @@ public void onFailure(Exception e) {\n             }\n             final String indexBlob = INDEX_FILE_PREFIX + Long.toString(newGen);\n             logger.debug(\"Repository [{}] writing new index generational blob [{}]\", metadata.name(), indexBlob);\n-            final BytesReference serializedRepoData =\n-                    BytesReference.bytes(newRepositoryData.snapshotsToXContent(XContentFactory.jsonBuilder(), version));\n-            writeAtomic(blobContainer(), indexBlob, serializedRepoData, true);\n+            final BytesReference repoDataToCache;\n+            try (ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays)) {\n+                try (XContentBuilder xContentBuilder = XContentFactory.jsonBuilder(Streams.noCloseStream(out))) {\n+                    newRepositoryData.snapshotsToXContent(xContentBuilder, version);\n+                }\n+                final BytesReference serializedRepoData = out.bytes();\n+                writeAtomic(blobContainer(), indexBlob, serializedRepoData, true);\n+                repoDataToCache = compressRepoDataForCache(serializedRepoData);", "originalCommit": "54186e745ca7d4566e24388712432b2e8625aaff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY0MTA1OA==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r501641058", "bodyText": "Gotta do the non-closing copy here and close the in via try because we can only close out once (multiple closes aren't idempotent and trip assertions.", "author": "original-brownbear", "createdAt": "2020-10-08T11:17:56Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java", "diffHunk": "@@ -101,9 +104,14 @@ public ChecksumBlobStoreFormat(String codec, String blobNameFormat, CheckedFunct\n      * @param name          name to be translated into\n      * @return parsed blob object\n      */\n-    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry) throws IOException {\n+    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry,\n+                  BigArrays bigArrays) throws IOException {\n         String blobName = blobName(name);\n-        return deserialize(blobName, namedXContentRegistry, Streams.readFully(blobContainer.readBlob(blobName)));\n+        try (ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays);\n+             InputStream in = blobContainer.readBlob(blobName)) {\n+            Streams.copy(in, out, false);", "originalCommit": "54186e745ca7d4566e24388712432b2e8625aaff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA5Njk2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503096965", "bodyText": "+1, bytes() should not be used once ReleasableBytesStreamOutput is closed", "author": "tlrx", "createdAt": "2020-10-12T07:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY0MTA1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA5NTE5MA==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503095190", "bodyText": "extra space", "author": "tlrx", "createdAt": "2020-10-12T07:38:25Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java", "diffHunk": "@@ -101,9 +104,14 @@ public ChecksumBlobStoreFormat(String codec, String blobNameFormat, CheckedFunct\n      * @param name          name to be translated into\n      * @return parsed blob object\n      */\n-    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry) throws IOException {\n+    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry,\n+                  BigArrays bigArrays) throws IOException {\n         String blobName = blobName(name);\n-        return deserialize(blobName, namedXContentRegistry, Streams.readFully(blobContainer.readBlob(blobName)));\n+        try (ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays);\n+             InputStream in = blobContainer.readBlob(blobName)) {", "originalCommit": "54186e745ca7d4566e24388712432b2e8625aaff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExNTc0MA==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503115740", "bodyText": "? where :)", "author": "original-brownbear", "createdAt": "2020-10-12T08:14:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA5NTE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExNzI4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503117282", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                         InputStream in = blobContainer.readBlob(blobName)) {\n          \n          \n            \n                        Streams.copy(in, out, false);\n          \n          \n            \n                        InputStream in = blobContainer.readBlob(blobName)) {\n          \n          \n            \n                        Streams.copy(in, out, false);", "author": "tlrx", "createdAt": "2020-10-12T08:17:33Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java", "diffHunk": "@@ -101,9 +104,14 @@ public ChecksumBlobStoreFormat(String codec, String blobNameFormat, CheckedFunct\n      * @param name          name to be translated into\n      * @return parsed blob object\n      */\n-    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry) throws IOException {\n+    public T read(BlobContainer blobContainer, String name, NamedXContentRegistry namedXContentRegistry,\n+                  BigArrays bigArrays) throws IOException {\n         String blobName = blobName(name);\n-        return deserialize(blobName, namedXContentRegistry, Streams.readFully(blobContainer.readBlob(blobName)));\n+        try (ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays);\n+             InputStream in = blobContainer.readBlob(blobName)) {\n+            Streams.copy(in, out, false);", "originalCommit": "54186e745ca7d4566e24388712432b2e8625aaff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExODQ3OA==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503118478", "bodyText": "But InputStream in is part of the try-with-resources so it should align with the ReleasableBytesStreamOutput out above it shouldn't it?", "author": "original-brownbear", "createdAt": "2020-10-12T08:19:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExNzI4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExOTM3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63461#discussion_r503119375", "bodyText": "Raaah yes, I misread this line :( Sorry for the noise.", "author": "tlrx", "createdAt": "2020-10-12T08:20:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzExNzI4Mg=="}], "type": "inlineReview"}]}