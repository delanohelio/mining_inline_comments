{"pr_number": 51270, "pr_title": "Fix Overly Aggressive Request DeDuplication", "pr_createdAt": "2020-01-21T20:13:51Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/51270", "timeline": [{"oid": "a2c7a31d97cc3713025562492948aecdc2ae7516", "url": "https://github.com/elastic/elasticsearch/commit/a2c7a31d97cc3713025562492948aecdc2ae7516", "message": "Fix Overly Optimistic Request Deduplication\n\nOn master failover we have to resent all the shard failed messages,\nbut the transport requests remain the same in the eyes of `equals`.\nIf the master failover is registered and the requests to the new master\nare sent before all the callbacks have executed and the request to the\nold master removed from the deduplicator then the requuests to the new\nmaster will incorrectly fail and the snapshot get stuck.\n\nCloses #51253", "committedDate": "2020-01-21T20:05:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTIyMTI5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369221291", "bodyText": "I know there's just one use case for this now, but just dropping all deduplication seems to be the right approach to master failovers in general if we ever want to  reuse this for e.g. put mapping calls or so.", "author": "original-brownbear", "createdAt": "2020-01-21T20:17:07Z", "path": "server/src/main/java/org/elasticsearch/transport/TransportRequestDeduplicator.java", "diffHunk": "@@ -53,6 +53,14 @@ public void executeOnce(T request, ActionListener<Void> listener, BiConsumer<T,\n         }\n     }\n \n+    /**\n+     * Remove all tracked requests from this instance so that the first time {@link #executeOnce} is invoked with any request it triggers\n+     * an actual request execution. Use this e.g. for requests to master that need to be sent again on master failover.\n+     */\n+    public void clear() {", "originalCommit": "a2c7a31d97cc3713025562492948aecdc2ae7516", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "02a8a3e1e0448b317c1f07ffca4382de29a510ef", "url": "https://github.com/elastic/elasticsearch/commit/02a8a3e1e0448b317c1f07ffca4382de29a510ef", "message": "Merge remote-tracking branch 'elastic/master' into 51253", "committedDate": "2020-01-21T20:30:39Z", "type": "commit"}, {"oid": "700184e526c3af77725bfecc7465b7a57d794ae5", "url": "https://github.com/elastic/elasticsearch/commit/700184e526c3af77725bfecc7465b7a57d794ae5", "message": "Merge remote-tracking branch 'elastic/master' into 51253", "committedDate": "2020-01-22T08:40:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369476216", "bodyText": "I don't understand why this is needed. UpdateSnapshotStatusAction is a TransportMasterNodeAction and should resend the request in case of master failover.", "author": "ywelsch", "createdAt": "2020-01-22T10:21:32Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -358,6 +358,9 @@ private void syncShardStatsOnNewMaster(ClusterChangedEvent event) {\n             return;\n         }\n \n+        // Clear request deduplicator since we need to send all requests that were potentially not handled by the previous\n+        // master again\n+        remoteFailedRequestDeduplicator.clear();", "originalCommit": "700184e526c3af77725bfecc7465b7a57d794ae5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3OTU2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369479566", "bodyText": "I think currently we're only retrying on FailedToCommitClusterStateException and NotMasterException but not on things like the node closed exception. That's why we have the manual retry/re-send of these messages in SnapshotShardsService in the first place don't we?", "author": "original-brownbear", "createdAt": "2020-01-22T10:28:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ4ODI1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369488256", "bodyText": "I'm confused by this stack trace. It shows that the node that's sending the request is closed, no? Why would we want to retry sending a request from a node that's shut down?", "author": "ywelsch", "createdAt": "2020-01-22T10:46:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ5Mzk4OA==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369493988", "bodyText": "Sorry pasted the wrong trace:\n  1> [2020-01-21T11:49:53,151][WARN ][o.e.s.SnapshotShardsService] [node_td3] [test-repo:test-snap/v_2yOVZsQHu6gk_aFsmkKg] [ShardSnapshotStatus[state=SUCCESS, nodeId=8MquPimwRTCtgnobCaxJlw, reason=null, generation=T-25oH4kSg62xXEtINmviQ]] failed to update snapshot state\n  1> org.elasticsearch.transport.RemoteTransportException: [node_tm0][127.0.0.1:41909][internal:cluster/snapshot/update_snapshot_status]\n  1> Caused by: org.elasticsearch.node.NodeClosedException: node closed {node_tm0}{VGisb-JmTiOkIc_vJP8FhA}{Ucpv57LoTEy1J_cuDT1L8A}{127.0.0.1}{127.0.0.1:41909}{im}\n  1> \tat org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$2.onClusterServiceClose(TransportMasterNodeAction.java:213) ~[main/:?]\n  1> \tat org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onClusterServiceClose(ClusterStateObserver.java:318) ~[main/:?]\n  1> \tat org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:237) ~[main/:?]\n  1> \tat org.elasticsearch.cluster.service.ClusterApplierService.doStop(ClusterApplierService.java:186) ~[main/:?]\n  1> \tat org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:79) ~[main/:?]\n  1> \tat org.elasticsearch.cluster.service.ClusterService.doStop(ClusterService.java:96) ~[main/:?]\n  1> \tat org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:79) ~[main/:?]\n  1> \tat org.elasticsearch.node.Node.stop(Node.java:805) ~[main/:?]\n  1> \tat org.elasticsearch.node.Node.close(Node.java:829) ~[main/:?]\n  1> \tat org.elasticsearch.test.InternalTestCluster$NodeAndClient.close(InternalTestCluster.java:954) ~[framework-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\n  1> \tat org.elasticsearch.test.InternalTestCluster.stopNodesAndClients(InternalTestCluster.java:1573) ~[framework-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\n  1> \tat org.elasticsearch.test.InternalTestCluster.stopNodesAndClient(InternalTestCluster.java:1563) ~[framework-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\n  1> \tat org.elasticsearch.test.InternalTestCluster.stopCurrentMasterNode(InternalTestCluster.java:1493) ~[framework-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]\n  1> \tat org.elasticsearch.snapshots.DedicatedClusterSnapshotRestoreIT.testMasterShutdownDuringSnapshot(DedicatedClusterSnapshotRestoreIT.java:849) ~[test/:?]\n  1> \tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n  1> \tat jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n  1> \tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n  1> \tat java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n  1> \tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750) ~[randomizedtesting-runner-2.7.4.jar:?]\n  1> \tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938) ~[randomizedtesting-runner-2.7.4.jar:?]\n  1> \tat com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974) ~[randomizedtesting-runner-2.7.4.jar:?]\n  1> \tat com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988) ~[randomizedtesting-runner-2.7.4.jar:?]\n\nWe have the same for the remote end as well. I'm not 100% sure we can get that exception outside of a test (because of the shut down order, but as you can see at least in the test requests are clearly not retried on the master node restart tm_0 and just go to the listener's onFailure). We could improve the master action to retry this case as well, but as long as we have the fail-safe retry on master failover in the snapshot shard's service already, we should make that actually work?", "author": "original-brownbear", "createdAt": "2020-01-22T10:58:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTU0NjU1MA==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369546550", "bodyText": "This looks like a bug in TransportMasterNodeAction, then. In case where a node is shut down, we first shut down the cluster service, and only then do we shut down the transport service. This means that a node that has sent a MasterNodeRequest to a master node that is in the process of shutting down can receive a TransportException with the cause being a NodeClosedException, for which we don't trigger a retry (we only do that for a ConnectTransportException). Can we fix that instead of this hack here (which also fixes this situation for any other master-level request)? Long term, the logic in syncShardStatsOnNewMaster can go away. I think it's mainly still here because the ShardSnapshotStatus action was not a TransportMasterNodeAction in some older versions.", "author": "ywelsch", "createdAt": "2020-01-22T13:04:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTU0OTEwMA==", "url": "https://github.com/elastic/elasticsearch/pull/51270#discussion_r369549100", "bodyText": "@ywelsch sounds good, I'll look into fixing the TransportMasterNodeAction and removing the retry in the snapshot shards service then shortly :)\nStill, so long as we have syncShardStatsOnNewMaster I feel like we should have this hack in place as without it that method doesn't really make sense?", "author": "original-brownbear", "createdAt": "2020-01-22T13:10:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTQ3NjIxNg=="}], "type": "inlineReview"}]}