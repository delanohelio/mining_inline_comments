{"pr_number": 57227, "pr_title": "Remove Overly Strict Safety Mechnism in Shard Snapshot Logic", "pr_createdAt": "2020-05-27T15:29:16Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/57227", "timeline": [{"oid": "dc4bbb11e7dbfce3f544e21018aa47a4c787d893", "url": "https://github.com/elastic/elasticsearch/commit/dc4bbb11e7dbfce3f544e21018aa47a4c787d893", "message": "Remove Overly Strict Safety Mechnism in Shard Snapshot Logic\n\nUnfortunately, we cannot have a safety mechnism like this where we throw whenever we find unreadable\ndata in a shard.\nThis breaks in the case of an older ES version (without shard generations enabled) having failed to snapshot\na shard snapshot after writing some data to its path and having finalized it for example.\nAnother example of where we can't support this check is the test I added, if we snapshot an index with a name\nthat already exists in the repository and more shards than the existing index, fail doing that and then retry snapshotting it\nwe will also see unexpected data in the path.\n\nWe could technically do deeper inspections on the unexpected data but I don't think it's worth it really. In the end if we are\nunable to read the data here it's broken anyway. By moving to a new `index-` blob in the shard directory I don't see us ever\ncorrupting existing data and since we (by virtue of moving to an empty generation) won't do any incremental work on top of\npotentially corrupt data we also do not risk creating broken snapshots going forward.\n=> Just logging a warning in this very unlikely case is the best we can do I think", "committedDate": "2020-05-27T15:13:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI0MzM1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/57227#discussion_r431243353", "bodyText": "I thought about whether this is ok or not quite a bit. I think we're not introducing any new risk qualitatively really. If we can't read what is in this directory it's broken, period.\nFor snapshot creation:\nWe can have some freak concurrency issues here if we aren't yet using shard generations and two data nodes try to write to the same path at the same time. But those we resolve either (on S3) by using the 3 minute wait or by not allowing overwrites for index-N on other types of repos. With shard generations being used, this is a non-issue anyway.\nFor deletions we are in the clear also, because everything runs on the master.", "author": "original-brownbear", "createdAt": "2020-05-27T15:43:22Z", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -2128,8 +2128,7 @@ public BlobStoreIndexShardSnapshot loadShardSnapshot(BlobContainer shardContaine\n             return new Tuple<>(shardSnapshots, latest);\n         } else if (blobs.stream().anyMatch(b -> b.startsWith(SNAPSHOT_PREFIX) || b.startsWith(INDEX_FILE_PREFIX)\n                                                                               || b.startsWith(UPLOADED_DATA_BLOB_PREFIX))) {\n-            throw new IllegalStateException(\n-                \"Could not find a readable index-N file in a non-empty shard snapshot directory [\" + shardContainer.path() + \"]\");\n+            logger.warn(\"Could not find a readable index-N file in a non-empty shard snapshot directory [\" + shardContainer.path() + \"]\");", "originalCommit": "dc4bbb11e7dbfce3f544e21018aa47a4c787d893", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}