{"pr_number": 65140, "pr_title": "Upgrade Azure repository SDK to v12", "pr_createdAt": "2020-11-17T14:47:50Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/65140", "timeline": [{"oid": "e6c23969dcc812711ad99c0a04ba397a91bb0a4f", "url": "https://github.com/elastic/elasticsearch/commit/e6c23969dcc812711ad99c0a04ba397a91bb0a4f", "message": "Upgrade azure sdk", "committedDate": "2020-11-17T14:45:57Z", "type": "commit"}, {"oid": "01c744ef169c19956792f86686c6439d209c46b1", "url": "https://github.com/elastic/elasticsearch/commit/01c744ef169c19956792f86686c6439d209c46b1", "message": "Fix tests", "committedDate": "2020-11-17T19:18:35Z", "type": "commit"}, {"oid": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "url": "https://github.com/elastic/elasticsearch/commit/f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "message": "Delegate to regular scheduler instead of into a custom one", "committedDate": "2020-11-18T09:18:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0MTE4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525941182", "bodyText": "It's possible that the command is executed some time after the specified period if the thread pool backed by delegate is over subscribed. This is used for timeouts and retries, I guess we could live with a less precise timers but still control the executor in which that code is executed.", "author": "fcofdez", "createdAt": "2020-11-18T09:36:13Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/executors/ReactorScheduledExecutorService.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure.executors;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;\n+import org.elasticsearch.threadpool.Scheduler;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.AbstractExecutorService;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.Delayed;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Wrapper around {@link ThreadPool} that provides the necessary scheduling methods for a {@link reactor.core.scheduler.Scheduler} to\n+ * function. This allows injecting a custom Executor to the reactor schedulers factory and get fine grained control over the\n+ * thread resources used.\n+ */\n+@SuppressForbidden(reason = \"It wraps a ThreadPool and delegates all the work\")\n+public class ReactorScheduledExecutorService extends AbstractExecutorService implements ScheduledExecutorService {\n+    private final ThreadPool threadPool;\n+    private final String executorName;\n+    private final ExecutorService delegate;\n+    private final Logger logger = LogManager.getLogger(ReactorScheduledExecutorService.class);\n+\n+    public ReactorScheduledExecutorService(ThreadPool threadPool, String executorName) {\n+        this.threadPool = threadPool;\n+        this.executorName = executorName;\n+        this.delegate = threadPool.executor(executorName);\n+    }\n+\n+    @Override\n+    public <V> ScheduledFuture<V> schedule(Callable<V> callable, long delay, TimeUnit unit) {\n+        Scheduler.ScheduledCancellable schedule = threadPool.schedule(() -> {\n+            try {\n+                decorateCallable(callable).call();\n+            } catch (Exception e) {\n+                throw new RuntimeException(e);\n+            }\n+        }, new TimeValue(delay, unit), executorName);\n+\n+        return new ReactorFuture<>(schedule);\n+    }\n+\n+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit) {\n+        Runnable decoratedCommand = decorateRunnable(command);\n+        Scheduler.ScheduledCancellable schedule = threadPool.schedule(decoratedCommand, new TimeValue(delay, unit), executorName);\n+        return new ReactorFuture<>(schedule);\n+    }\n+\n+    @Override\n+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {\n+        Runnable decoratedCommand = decorateRunnable(command);\n+\n+        return threadPool.scheduler().scheduleAtFixedRate(() -> {\n+            try {\n+                delegate.execute(decoratedCommand);", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0MzUyOA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525943528", "bodyText": "This test fails right now, since the SDK triggers a HEAD request to get the blob size. The problem comes when a range that goes beyond the blob end the API would return an error 416 (Requested Range Not Satisfiable). My guess is that the expectation is that client would know the blob length beforehand and won't request past that boundary, is that correct? If that's the expectation that we have, I'll write a custom InputStream that just bypass the HEAD request as we were doing before.", "author": "fcofdez", "createdAt": "2020-11-18T09:39:51Z", "path": "plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobContainerRetriesTests.java", "diffHunk": "@@ -215,11 +234,49 @@ public void testReadBlobWithRetries() throws Exception {\n         }\n     }\n \n+    public void testReadBlobInSmallChunks() throws Exception {\n+        final int maxRetries = randomIntBetween(1, 5);\n+        final int chunkSize = randomIntBetween(4096, 4096 * 2);\n+        final byte[] bytes = randomBlobContent();\n+        httpServer.createContext(\"/account/container/read_blob_small_chunks\", exchange -> {\n+            try {\n+                Streams.readFully(exchange.getRequestBody());\n+                if (\"HEAD\".equals(exchange.getRequestMethod())) {\n+                    exchange.getResponseHeaders().add(\"Content-Type\", \"application/octet-stream\");\n+                    exchange.getResponseHeaders().add(\"x-ms-blob-content-length\", String.valueOf(bytes.length));\n+                    exchange.getResponseHeaders().add(\"Content-Length\", String.valueOf(bytes.length));\n+                    exchange.getResponseHeaders().add(\"x-ms-blob-type\", \"blockblob\");\n+                    exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1);\n+                } else if (\"GET\".equals(exchange.getRequestMethod())) {\n+                    final int rangeStart = getRangeStart(exchange);\n+                    assertThat(rangeStart, lessThan(bytes.length));\n+                    int rangeSize = Math.min(chunkSize, bytes.length - rangeStart);\n+                    exchange.getResponseHeaders().add(\"Content-Type\", \"application/octet-stream\");\n+                    exchange.getResponseHeaders().add(\"Content-Range\",\n+                        \"bytes \" + rangeStart + \"-\" + (rangeStart + rangeSize) + \"/\" + bytes.length);\n+                    exchange.getResponseHeaders().add(\"x-ms-blob-content-length\", String.valueOf(rangeSize));\n+                    exchange.getResponseHeaders().add(\"Content-Length\", String.valueOf(rangeSize));\n+                    exchange.getResponseHeaders().add(\"x-ms-blob-type\", \"blockblob\");\n+                    exchange.getResponseHeaders().add(\"ETag\", UUIDs.base64UUID());\n+                    exchange.sendResponseHeaders(RestStatus.PARTIAL_CONTENT.getStatus(), rangeSize);\n+                    exchange.getResponseBody().write(bytes, rangeStart, rangeSize);\n+                }\n+            } finally {\n+                exchange.close();\n+            }\n+        });\n+\n+        final BlobContainer blobContainer = createBlobContainer(maxRetries);\n+        try (InputStream inputStream = blobContainer.readBlob(\"read_blob_small_chunks\")) {\n+            assertArrayEquals(bytes, BytesReference.toBytes(Streams.readFully(inputStream)));\n+        }\n+    }\n+\n     public void testReadRangeBlobWithRetries() throws Exception {", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3NTcwNg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525975706", "bodyText": "Additionally, we could be a little bit more efficient and avoid a copy + allocation per chunk read request if we implement it ourselves.", "author": "fcofdez", "createdAt": "2020-11-18T10:27:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0MzUyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI1MjMwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r526252301", "bodyText": "As discussed, let's do it and try to avoid the extra HEAD + copying if we can.", "author": "original-brownbear", "createdAt": "2020-11-18T17:01:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0MzUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0NTA1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525945059", "bodyText": "This is really unfortunate, the SDK loads a default http client provider during the class construction and since the dependency jars are loaded in lexicographical order, the netty http client is not loaded at that time. This is a bug on the blob batch client, that instead of reusing the existing http client, it creates a new one. I've opened a PR and it's already approved, so this is just a temporal workaround.", "author": "fcofdez", "createdAt": "2020-11-18T09:42:09Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepositoryPlugin.java", "diffHunk": "@@ -100,6 +150,32 @@ public void reload(Settings settings) {\n         if (clientsSettings.isEmpty()) {\n             throw new SettingsException(\"If you want to use an azure repository, you need to define a client configuration.\");\n         }\n-        azureStoreService.refreshAndClearCache(clientsSettings);\n+        AzureStorageService storageService = azureStoreService.get();\n+        assert storageService != null;\n+        storageService.refreshSettings(clientsSettings);\n+    }\n+\n+    @SuppressForbidden(reason = \"needs to overcome the eager class loading problem\")\n+    private void setUpDefaultHttpClientProvider() {", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk0OTA5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525949091", "bodyText": "As I point out here most of the response handling code is executed in the netty event loop executor, meaning that we should provide a privileged environment to the tasks executed in that event loop.\nI tried different approaches to solve this issue, one of them was to create a wrapper around\ncom.azure.core.http.HttpClient that would publish the emitted response in a different thread pool:\nMono<HttpResponse> send(HttpRequest request) {\n         return delegate.send(request).publishOn(privilegedExecutor);\n}\nBut it seems like the HttpResponse is parsed before, so that didn't help.\nMaybe I'm missing something here and we can provide a more fine-grained solution.", "author": "fcofdez", "createdAt": "2020-11-18T09:48:05Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import com.azure.core.http.HttpClient;\n+import com.azure.core.http.HttpMethod;\n+import com.azure.core.http.HttpPipelineCallContext;\n+import com.azure.core.http.HttpPipelineNextPolicy;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.HttpResponse;\n+import com.azure.core.http.ProxyOptions;\n+import com.azure.core.http.netty.NettyAsyncHttpClientBuilder;\n+import com.azure.core.http.policy.HttpPipelinePolicy;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.implementation.connectionstring.StorageConnectionString;\n+import com.azure.storage.common.policy.RequestRetryOptions;\n+import io.netty.buffer.ByteBufAllocator;\n+import io.netty.buffer.PooledByteBufAllocator;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.EventLoopGroup;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.settings.SettingsException;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.repositories.azure.executors.PrivilegedExecutor;\n+import org.elasticsearch.repositories.azure.executors.ReactorScheduledExecutorService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.core.scheduler.Schedulers;\n+import reactor.netty.resources.ConnectionProvider;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.time.Duration;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.NETTY_EVENT_LOOP_THREAD_POOL_NAME;\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.REPOSITORY_THREAD_POOL_NAME;\n+\n+class AzureClientProvider extends AbstractLifecycleComponent {\n+    private static final TimeValue DEFAULT_CONNECTION_TIMEOUT = TimeValue.timeValueSeconds(30);\n+    private static final TimeValue DEFAULT_MAX_CONNECTION_IDLE_TIME = TimeValue.timeValueSeconds(60);\n+    private static final int DEFAULT_MAX_CONNECTIONS = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+    private static final int DEFAULT_EVENT_LOOP_THREAD_COUNT = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+\n+    static final Setting<String> EVENT_LOOP_EXECUTOR = Setting.simpleString(\n+        \"repository.azure.http_client.event_loop_executor_name\",\n+        NETTY_EVENT_LOOP_THREAD_POOL_NAME,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<Integer> EVENT_LOOP_THREAD_COUNT = Setting.intSetting(\n+        \"repository.azure.http_client.event_loop_executor_thread_count\",\n+        DEFAULT_EVENT_LOOP_THREAD_COUNT,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<Integer> MAX_OPEN_CONNECTIONS = Setting.intSetting(\n+        \"repository.azure.http_client.max_open_connections\",\n+        DEFAULT_MAX_CONNECTIONS,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<TimeValue> OPEN_CONNECTION_TIMEOUT = Setting.timeSetting(\n+        \"repository.azure.http_client.connection_timeout\",\n+        DEFAULT_CONNECTION_TIMEOUT,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<TimeValue> MAX_IDLE_TIME = Setting.timeSetting(\n+        \"repository.azure.http_client.connection_max_idle_time\",\n+        DEFAULT_MAX_CONNECTION_IDLE_TIME,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<String> REACTOR_SCHEDULER_EXECUTOR_NAME = Setting.simpleString(\n+        \"repository.azure.http_client.reactor_executor_name\",\n+        REPOSITORY_THREAD_POOL_NAME,\n+        Setting.Property.NodeScope);\n+\n+    private final ThreadPool threadPool;\n+    private final String reactorExecutorName;\n+    private final EventLoopGroup eventLoopGroup;\n+    private final ConnectionProvider connectionProvider;\n+    private final ByteBufAllocator byteBufAllocator;\n+    private final ClientLogger clientLogger = new ClientLogger(AzureClientProvider.class);\n+    private volatile boolean closed = false;\n+\n+    AzureClientProvider(ThreadPool threadPool,\n+                        String reactorExecutorName,\n+                        EventLoopGroup eventLoopGroup,\n+                        ConnectionProvider connectionProvider,\n+                        ByteBufAllocator byteBufAllocator) {\n+        this.threadPool = threadPool;\n+        this.reactorExecutorName = reactorExecutorName;\n+        this.eventLoopGroup = eventLoopGroup;\n+        this.connectionProvider = connectionProvider;\n+        this.byteBufAllocator = byteBufAllocator;\n+    }\n+\n+    static int eventLoopThreadsFromSettings(Settings settings) {\n+        return EVENT_LOOP_THREAD_COUNT.get(settings);\n+    }\n+\n+    static AzureClientProvider create(ThreadPool threadPool, Settings settings) {\n+        final ExecutorService executorService;\n+        try {\n+            executorService = threadPool.executor(EVENT_LOOP_EXECUTOR.get(settings));\n+        } catch (IllegalArgumentException e) {\n+            throw new SettingsException(\"Unable to find executor [\" + EVENT_LOOP_EXECUTOR.get(settings) + \"]\");\n+        }\n+        // Most of the code that needs special permissions (i.e. jackson serializers generation) is executed\n+        // in the event loop executor. That's the reason why we should provide an executor that allows the\n+        // execution of privileged code\n+        final EventLoopGroup eventLoopGroup = new NioEventLoopGroup(eventLoopThreadsFromSettings(settings),", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1MjQ4OA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525952488", "bodyText": "In te latest SDK if the endpoint uses an ip host, it expects the account in the URI path, see com.azure.storage.blob.BlobUrlParts#parseIpUrl (it doesn't validate it there, but it fails later on as it expects the blob name to be on the 3 part of the path, i.e. http://192.168.1.20:8080/account/container/blob.)", "author": "fcofdez", "createdAt": "2020-11-18T09:52:52Z", "path": "test/fixtures/azure-fixture/src/main/java/fixture/azure/AzureHttpHandler.java", "diffHunk": "@@ -50,11 +54,17 @@\n  */\n @SuppressForbidden(reason = \"Uses a HttpServer to emulate an Azure endpoint\")\n public class AzureHttpHandler implements HttpHandler {\n+    private static final Pattern BATCH_CONTENT_TYPE_MATCHER =\n+        Pattern.compile(\"multipart/mixed; boundary=(?<boundary>.+)\", Pattern.DOTALL);\n+    private static final Pattern DELETE_REQ_MATCHER = Pattern.compile(\".+DELETE (?<deleteURI>.+) HTTP/1.1.+\", Pattern.DOTALL);\n+    private static final String HTTP_NEWLINE = \"\\r\\n\";\n \n     private final Map<String, BytesReference> blobs;\n+    private final String account;\n     private final String container;\n \n-    public AzureHttpHandler(final String container) {\n+    public AzureHttpHandler(final String account, final String container) {", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1MzQwMw==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525953403", "bodyText": "All this resources are shared by all the clients, so we only need to manage the lifecycle of this component and the clients can be stateless.", "author": "fcofdez", "createdAt": "2020-11-18T09:54:17Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import com.azure.core.http.HttpClient;\n+import com.azure.core.http.HttpMethod;\n+import com.azure.core.http.HttpPipelineCallContext;\n+import com.azure.core.http.HttpPipelineNextPolicy;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.HttpResponse;\n+import com.azure.core.http.ProxyOptions;\n+import com.azure.core.http.netty.NettyAsyncHttpClientBuilder;\n+import com.azure.core.http.policy.HttpPipelinePolicy;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.implementation.connectionstring.StorageConnectionString;\n+import com.azure.storage.common.policy.RequestRetryOptions;\n+import io.netty.buffer.ByteBufAllocator;\n+import io.netty.buffer.PooledByteBufAllocator;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.EventLoopGroup;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.settings.SettingsException;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.repositories.azure.executors.PrivilegedExecutor;\n+import org.elasticsearch.repositories.azure.executors.ReactorScheduledExecutorService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.core.scheduler.Schedulers;\n+import reactor.netty.resources.ConnectionProvider;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.time.Duration;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.NETTY_EVENT_LOOP_THREAD_POOL_NAME;\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.REPOSITORY_THREAD_POOL_NAME;\n+\n+class AzureClientProvider extends AbstractLifecycleComponent {\n+    private static final TimeValue DEFAULT_CONNECTION_TIMEOUT = TimeValue.timeValueSeconds(30);\n+    private static final TimeValue DEFAULT_MAX_CONNECTION_IDLE_TIME = TimeValue.timeValueSeconds(60);\n+    private static final int DEFAULT_MAX_CONNECTIONS = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+    private static final int DEFAULT_EVENT_LOOP_THREAD_COUNT = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+\n+    static final Setting<String> EVENT_LOOP_EXECUTOR = Setting.simpleString(\n+        \"repository.azure.http_client.event_loop_executor_name\",\n+        NETTY_EVENT_LOOP_THREAD_POOL_NAME,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<Integer> EVENT_LOOP_THREAD_COUNT = Setting.intSetting(\n+        \"repository.azure.http_client.event_loop_executor_thread_count\",\n+        DEFAULT_EVENT_LOOP_THREAD_COUNT,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<Integer> MAX_OPEN_CONNECTIONS = Setting.intSetting(\n+        \"repository.azure.http_client.max_open_connections\",\n+        DEFAULT_MAX_CONNECTIONS,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<TimeValue> OPEN_CONNECTION_TIMEOUT = Setting.timeSetting(\n+        \"repository.azure.http_client.connection_timeout\",\n+        DEFAULT_CONNECTION_TIMEOUT,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<TimeValue> MAX_IDLE_TIME = Setting.timeSetting(\n+        \"repository.azure.http_client.connection_max_idle_time\",\n+        DEFAULT_MAX_CONNECTION_IDLE_TIME,\n+        Setting.Property.NodeScope);\n+\n+    static final Setting<String> REACTOR_SCHEDULER_EXECUTOR_NAME = Setting.simpleString(\n+        \"repository.azure.http_client.reactor_executor_name\",\n+        REPOSITORY_THREAD_POOL_NAME,\n+        Setting.Property.NodeScope);\n+\n+    private final ThreadPool threadPool;\n+    private final String reactorExecutorName;\n+    private final EventLoopGroup eventLoopGroup;", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525953984", "bodyText": "Is this a good way to define those settings? I think these are mostly node related so it makes sense to me to configure them at that level.", "author": "fcofdez", "createdAt": "2020-11-18T09:55:05Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import com.azure.core.http.HttpClient;\n+import com.azure.core.http.HttpMethod;\n+import com.azure.core.http.HttpPipelineCallContext;\n+import com.azure.core.http.HttpPipelineNextPolicy;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.HttpResponse;\n+import com.azure.core.http.ProxyOptions;\n+import com.azure.core.http.netty.NettyAsyncHttpClientBuilder;\n+import com.azure.core.http.policy.HttpPipelinePolicy;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.implementation.connectionstring.StorageConnectionString;\n+import com.azure.storage.common.policy.RequestRetryOptions;\n+import io.netty.buffer.ByteBufAllocator;\n+import io.netty.buffer.PooledByteBufAllocator;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.EventLoopGroup;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.settings.SettingsException;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.repositories.azure.executors.PrivilegedExecutor;\n+import org.elasticsearch.repositories.azure.executors.ReactorScheduledExecutorService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.core.scheduler.Schedulers;\n+import reactor.netty.resources.ConnectionProvider;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.time.Duration;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.NETTY_EVENT_LOOP_THREAD_POOL_NAME;\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.REPOSITORY_THREAD_POOL_NAME;\n+\n+class AzureClientProvider extends AbstractLifecycleComponent {\n+    private static final TimeValue DEFAULT_CONNECTION_TIMEOUT = TimeValue.timeValueSeconds(30);\n+    private static final TimeValue DEFAULT_MAX_CONNECTION_IDLE_TIME = TimeValue.timeValueSeconds(60);\n+    private static final int DEFAULT_MAX_CONNECTIONS = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+    private static final int DEFAULT_EVENT_LOOP_THREAD_COUNT = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+\n+    static final Setting<String> EVENT_LOOP_EXECUTOR = Setting.simpleString(", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1NTkyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r528755929", "bodyText": "The definition of the setting is fine I think :)\nBut, why do we need a setting for this?", "author": "original-brownbear", "createdAt": "2020-11-23T14:48:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3MzUzMg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542373532", "bodyText": "@fcofdez ping :) this is still an open question I have.", "author": "original-brownbear", "createdAt": "2020-12-14T13:14:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3NjM1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542376352", "bodyText": "That's to simplify testing, so we can pass a different executor there, in scenarios where we don't create the ones from the Azure plugin.", "author": "fcofdez", "createdAt": "2020-12-14T13:19:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3NjY0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542376647", "bodyText": "And sorry, I missed this comment", "author": "fcofdez", "createdAt": "2020-12-14T13:19:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ2MjQ5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542462496", "bodyText": "Addressed in 2c19d71", "author": "fcofdez", "createdAt": "2020-12-14T15:14:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1Mzk4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1NDM4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r525954385", "bodyText": "Maybe this is too much? It's capped to 16", "author": "fcofdez", "createdAt": "2020-11-18T09:55:37Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import com.azure.core.http.HttpClient;\n+import com.azure.core.http.HttpMethod;\n+import com.azure.core.http.HttpPipelineCallContext;\n+import com.azure.core.http.HttpPipelineNextPolicy;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.HttpResponse;\n+import com.azure.core.http.ProxyOptions;\n+import com.azure.core.http.netty.NettyAsyncHttpClientBuilder;\n+import com.azure.core.http.policy.HttpPipelinePolicy;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.implementation.connectionstring.StorageConnectionString;\n+import com.azure.storage.common.policy.RequestRetryOptions;\n+import io.netty.buffer.ByteBufAllocator;\n+import io.netty.buffer.PooledByteBufAllocator;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.EventLoopGroup;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.settings.SettingsException;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.repositories.azure.executors.PrivilegedExecutor;\n+import org.elasticsearch.repositories.azure.executors.ReactorScheduledExecutorService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.core.scheduler.Schedulers;\n+import reactor.netty.resources.ConnectionProvider;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.time.Duration;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.NETTY_EVENT_LOOP_THREAD_POOL_NAME;\n+import static org.elasticsearch.repositories.azure.AzureRepositoryPlugin.REPOSITORY_THREAD_POOL_NAME;\n+\n+class AzureClientProvider extends AbstractLifecycleComponent {\n+    private static final TimeValue DEFAULT_CONNECTION_TIMEOUT = TimeValue.timeValueSeconds(30);\n+    private static final TimeValue DEFAULT_MAX_CONNECTION_IDLE_TIME = TimeValue.timeValueSeconds(60);\n+    private static final int DEFAULT_MAX_CONNECTIONS = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;\n+    private static final int DEFAULT_EVENT_LOOP_THREAD_COUNT = Math.min(Runtime.getRuntime().availableProcessors(), 8) * 2;", "originalCommit": "f3ef260a1e6e71f0a9fb1c66b02b5d6f3f7aa7d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1NTQ5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r528755496", "bodyText": "I wonder if just 1 might actually be good enough here? We're not doing anything CPU intensive on the IO threads I think? (haven't bench-marked this though, maybe we can get some hard data for this from our benchmarking efforts?, my guess would be 1 is good enough and maybe we can verify that or experiment out the actual optimal value).\nI would hope that there is no hidden SDK issue that makes 16 useful here at least :)", "author": "original-brownbear", "createdAt": "2020-11-23T14:47:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1NDM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzExMDUwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r543110501", "bodyText": "Small aside, can go into a follow-up (I can deal with it later this week): I think we can make this 1 for now. The consumers that consume whatever buffers this pool produces will never be able to grind through what a single network thread can produce in throughput and the memory savings from reducing the thread count are non-trivial here. We can do a few official benchmarks to this effect though :)", "author": "original-brownbear", "createdAt": "2020-12-15T07:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk1NDM4NQ=="}], "type": "inlineReview"}, {"oid": "782ac90f3ce8e48b5f4387794f95a83fff3e6cfc", "url": "https://github.com/elastic/elasticsearch/commit/782ac90f3ce8e48b5f4387794f95a83fff3e6cfc", "message": "Update default max connections.", "committedDate": "2020-11-18T10:15:47Z", "type": "commit"}, {"oid": "1eb89fd08b5e0d08c6d35c97ce9295eed02955de", "url": "https://github.com/elastic/elasticsearch/commit/1eb89fd08b5e0d08c6d35c97ce9295eed02955de", "message": "More efficient blob reading", "committedDate": "2020-11-19T20:10:39Z", "type": "commit"}, {"oid": "61039c9eb64f3a9f1b0ec583ac7028f3d87e8ad2", "url": "https://github.com/elastic/elasticsearch/commit/61039c9eb64f3a9f1b0ec583ac7028f3d87e8ad2", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-11-19T20:11:18Z", "type": "commit"}, {"oid": "9bd24556dcbc72e6e6432533b96431b322586673", "url": "https://github.com/elastic/elasticsearch/commit/9bd24556dcbc72e6e6432533b96431b322586673", "message": "Take into account retries during reads", "committedDate": "2020-11-20T11:05:34Z", "type": "commit"}, {"oid": "532599738f7c43d3a4034ba7460a46c3dde91d23", "url": "https://github.com/elastic/elasticsearch/commit/532599738f7c43d3a4034ba7460a46c3dde91d23", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-11-20T12:25:41Z", "type": "commit"}, {"oid": "f38044365be92138b6c409512996a2d6870a9282", "url": "https://github.com/elastic/elasticsearch/commit/f38044365be92138b6c409512996a2d6870a9282", "message": "Fix failing test", "committedDate": "2020-11-20T13:23:21Z", "type": "commit"}, {"oid": "d9f2e88fcb3907632d8ff08ebd16c959391160a9", "url": "https://github.com/elastic/elasticsearch/commit/d9f2e88fcb3907632d8ff08ebd16c959391160a9", "message": "Grant permissions to tasks executed in reactor schedulers.", "committedDate": "2020-11-23T12:07:12Z", "type": "commit"}, {"oid": "b839a28c6c88686a4eb2c4f72e3526d36cd87806", "url": "https://github.com/elastic/elasticsearch/commit/b839a28c6c88686a4eb2c4f72e3526d36cd87806", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-11-23T12:07:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1MjA3MA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r528752070", "bodyText": "I think we can't use batches with SAS token based auth which seems to be a service and not an SDK issue (see Azure/azure-storage-java#538 and linked issues). Did you try running this code with token auth?", "author": "original-brownbear", "createdAt": "2020-11-23T14:43:09Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -153,213 +200,263 @@ public LocationMode getLocationMode() {\n \n     @Override\n     public BlobContainer blobContainer(BlobPath path) {\n-        return new AzureBlobContainer(path, this, threadPool);\n+        return new AzureBlobContainer(path, this);\n     }\n \n     @Override\n     public void close() {\n     }\n \n-    public boolean blobExists(String blob) throws URISyntaxException, StorageException {\n-        // Container name must be lower case.\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        return SocketAccess.doPrivilegedException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            return azureBlob.exists(null, null, context);\n-        });\n-    }\n+    public boolean blobExists(String blob) {\n+        final BlobServiceClient client = client();\n \n-    public void deleteBlob(String blob) throws URISyntaxException, StorageException {\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        // Container name must be lower case.\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        logger.trace(() -> new ParameterizedMessage(\"delete blob for container [{}], blob [{}]\", container, blob));\n-        SocketAccess.doPrivilegedVoidException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            logger.trace(() -> new ParameterizedMessage(\"container [{}]: blob [{}] found. removing.\", container, blob));\n-            azureBlob.delete(DeleteSnapshotsOption.NONE, null, null, client.v2().get());\n-        });\n+        try {\n+            Boolean blobExists = SocketAccess.doPrivilegedException(() -> {\n+                final BlobClient azureBlob = client.getBlobContainerClient(container).getBlobClient(blob);\n+                return azureBlob.exists();\n+            });\n+            return blobExists != null ? blobExists : false;\n+        } catch (Exception e) {\n+            logger.warn(\"can not access [{}] in container {{}}: {}\", blob, container, e.getMessage());\n+        }\n+        return false;\n     }\n \n-    public DeleteResult deleteBlobDirectory(String path, Executor executor)\n-            throws URISyntaxException, StorageException, IOException {\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        final OperationContext context = hookMetricCollector(client.v2().get(), listMetricsCollector);\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        final Collection<Exception> exceptions = Collections.synchronizedList(new ArrayList<>());\n-        final AtomicLong outstanding = new AtomicLong(1L);\n-        final PlainActionFuture<Void> result = PlainActionFuture.newFuture();\n-        final AtomicLong blobsDeleted = new AtomicLong();\n-        final AtomicLong bytesDeleted = new AtomicLong();\n-        SocketAccess.doPrivilegedVoidException(() -> {\n-            for (final ListBlobItem blobItem : blobContainer.listBlobs(path, true,\n-                EnumSet.noneOf(BlobListingDetails.class), null, context)) {\n-                // uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/\n-                // this requires 1 + container.length() + 1, with each 1 corresponding to one of the /\n-                final String blobPath = blobItem.getUri().getPath().substring(1 + container.length() + 1);\n-                outstanding.incrementAndGet();\n-                executor.execute(new AbstractRunnable() {\n-                    @Override\n-                    protected void doRun() throws Exception {\n-                        final long len;\n-                        if (blobItem instanceof CloudBlob) {\n-                            len = ((CloudBlob) blobItem).getProperties().getLength();\n+    public DeleteResult deleteBlobDirectory(String path) throws IOException {\n+        final AtomicInteger blobsDeleted = new AtomicInteger(0);\n+        final AtomicLong bytesDeleted = new AtomicLong(0);\n+\n+        try {\n+            final BlobServiceClient client = client();\n+            SocketAccess.doPrivilegedVoidException(() -> {\n+                final BlobContainerClient blobContainerClient = client.getBlobContainerClient(container);\n+                final List<String> blobURLs = new ArrayList<>();\n+                final Queue<String> directories = new ArrayDeque<>();\n+                directories.offer(path);\n+                String directoryName;\n+                while ((directoryName = directories.poll()) != null) {\n+                    final BlobListDetails blobListDetails = new BlobListDetails()\n+                        .setRetrieveMetadata(true);\n+\n+                    final ListBlobsOptions options = new ListBlobsOptions()\n+                        .setPrefix(directoryName)\n+                        .setDetails(blobListDetails);\n+\n+                    for (BlobItem blobItem : blobContainerClient.listBlobsByHierarchy(\"/\", options, null)) {\n+                        if (blobItem.isPrefix() != null && blobItem.isPrefix()) {\n+                            directories.offer(blobItem.getName());\n                         } else {\n-                            len = -1L;\n-                        }\n-                        deleteBlob(blobPath);\n-                        blobsDeleted.incrementAndGet();\n-                        if (len >= 0) {\n-                            bytesDeleted.addAndGet(len);\n+                            BlobClient blobClient = blobContainerClient.getBlobClient(blobItem.getName());\n+                            blobURLs.add(blobClient.getBlobUrl());\n+                            bytesDeleted.addAndGet(blobItem.getProperties().getContentLength());\n+                            blobsDeleted.incrementAndGet();\n                         }\n                     }\n+                }\n+                deleteBlobsInBatches(blobURLs);\n+            });\n+        } catch (Exception e) {\n+            throw new IOException(\"Deleting directory [\" + path + \"] failed\", e);\n+        }\n \n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        exceptions.add(e);\n-                    }\n+        return new DeleteResult(blobsDeleted.get(), bytesDeleted.get());\n+    }\n \n-                    @Override\n-                    public void onAfter() {\n-                        if (outstanding.decrementAndGet() == 0) {\n-                            result.onResponse(null);\n-                        }\n-                    }\n-                });\n-            }\n-        });\n-        if (outstanding.decrementAndGet() == 0) {\n-            result.onResponse(null);\n+    void deleteBlobList(List<String> blobs) throws IOException {\n+        if (blobs.isEmpty()) {\n+            return;\n         }\n-        result.actionGet();\n-        if (exceptions.isEmpty() == false) {\n-            final IOException ex = new IOException(\"Deleting directory [\" + path + \"] failed\");\n-            exceptions.forEach(ex::addSuppressed);\n-            throw ex;\n+\n+        final List<String> blobURLs = new ArrayList<>(blobs.size());\n+        try {\n+            final BlobServiceClient client = client();\n+            SocketAccess.doPrivilegedVoidException(() -> {\n+                // The delete batch API expects the full blob URL, so we need to build it.\n+                // This operation won't perform any API call, it just relies on the\n+                // sdk client to build the entire URL.\n+                final BlobContainerClient blobContainerClient = client.getBlobContainerClient(container);\n+                for (String blob : blobs) {\n+                    blobURLs.add(blobContainerClient.getBlobClient(blob).getBlobUrl());\n+                }\n+            });\n+        } catch (Exception e) {\n+            throw new IOException(\"Unable to delete blobs \" + blobs, e);\n         }\n-        return new DeleteResult(blobsDeleted.get(), bytesDeleted.get());\n+\n+        deleteBlobsInBatches(blobURLs);\n     }\n \n-    public InputStream getInputStream(String blob, long position, @Nullable Long length) throws URISyntaxException, StorageException {\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n-        final CloudBlockBlob blockBlobReference = client.v1().getContainerReference(container).getBlockBlobReference(blob);\n-        logger.trace(() -> new ParameterizedMessage(\"reading container [{}], blob [{}]\", container, blob));\n-        final long limit;\n-        if (length == null) {\n-            // Loading the blob attributes so we can get its length\n-            SocketAccess.doPrivilegedVoidException(() -> blockBlobReference.downloadAttributes(null, null, context));\n-            limit = blockBlobReference.getProperties().getLength() - position;\n+    private void deleteBlobsInBatches(List<String> blobUrls) throws IOException {\n+        if (blobUrls.isEmpty()) {\n+            return;\n         }\n-        else {\n-            limit = length;\n-        }\n-        final BlobInputStream blobInputStream = new BlobInputStream(limit, blockBlobReference, position, context);\n-        if (length != null) {\n-            // pre-filling the buffer in case of ranged reads so this method throws a 404 storage exception right away in case the blob\n-            // does not exist\n-            blobInputStream.fill();\n+\n+        try {\n+            SocketAccess.doPrivilegedVoidException(() -> {\n+                final BlobBatchAsyncClient blobBatchClient =\n+                    new BlobBatchClientBuilder(asyncClient())\n+                        .buildAsyncClient();\n+\n+                int numBatches = (int) Math.ceil((double) blobUrls.size() / (double) MAX_ELEMENTS_PER_BATCH);\n+                List<BlobBatch> batches = new ArrayList<>(numBatches);\n+                for (int batchNumber = 0; batchNumber < numBatches; batchNumber++) {\n+                    final BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+                    int rangeStart = batchNumber * MAX_ELEMENTS_PER_BATCH;\n+                    for (int i = rangeStart; i < Math.min(rangeStart + MAX_ELEMENTS_PER_BATCH, blobUrls.size()); i++) {\n+                        blobBatch.deleteBlob(blobUrls.get(i));\n+                    }\n+                    batches.add(blobBatch);\n+                }\n+\n+                List<Mono<Response<Void>>> batchResponses = new ArrayList<>(batches.size());\n+                for (BlobBatch batch : batches) {\n+                    batchResponses.add(blobBatchClient.submitBatchWithResponse(batch, false));", "originalCommit": "b839a28c6c88686a4eb2c4f72e3526d36cd87806", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTM5NTk3MA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r529395970", "bodyText": "You're right, I've just tested this using a SAS token and I got a:\ncom.azure.storage.blob.models.BlobStorageException: Status code 403, \"<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationResourceTypeMismatch</Code><Message>This request is not authorized to perform this operation using this resource type.\n\nI'll implement a different path that triggers all the delete requests when a SAS token is used :(.", "author": "fcofdez", "createdAt": "2020-11-24T10:09:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1MjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQwNTU1NA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r529405554", "bodyText": "I'll implement a different path that triggers all the delete requests when a SAS token is used :(.\n\nLet's just use a single path IMO. No need to optimize for non-token credentials, those are pretty rare in production.\nI think we can still get into a much better state than what we had with the v8 SDK because we can just fire off all the deletes at once with the async API?", "author": "original-brownbear", "createdAt": "2020-11-24T10:17:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1MjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQwNzY5NA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r529407694", "bodyText": "\ud83d\udc4d, yes that was my plan.", "author": "fcofdez", "createdAt": "2020-11-24T10:19:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1MjA3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc2NDMzMw==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r528764333", "bodyText": "I wonder if we actually need/want this executor still? This was just used for running deletes in parallel. (I think we still have to keep it around for BwC in 7.x but should probably add a TODO to drop it in master after the backport?", "author": "original-brownbear", "createdAt": "2020-11-23T14:59:24Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepositoryPlugin.java", "diffHunk": "@@ -86,11 +131,16 @@ AzureStorageService createAzureStoreService(final Settings settings) {\n \n     @Override\n     public List<ExecutorBuilder<?>> getExecutorBuilders(Settings settings) {\n-        return Collections.singletonList(executorBuilder());\n+        return List.of(executorBuilder(), nettyEventLoopExecutorBuilder(settings));\n     }\n \n     public static ExecutorBuilder<?> executorBuilder() {\n-        return new ScalingExecutorBuilder(REPOSITORY_THREAD_POOL_NAME, 0, 32, TimeValue.timeValueSeconds(30L));\n+        return new ScalingExecutorBuilder(REPOSITORY_THREAD_POOL_NAME, 0, 5, TimeValue.timeValueSeconds(30L));", "originalCommit": "b839a28c6c88686a4eb2c4f72e3526d36cd87806", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYxMDk3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r532610971", "bodyText": "We use this thread pool to execute some of the reactor tasks, such as timeout or retries handlers (the scheduling is done using org.elasticsearch.threadpool.ThreadPool#scheduler but the tasks itself are executed in this thread pool)", "author": "fcofdez", "createdAt": "2020-11-30T13:52:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc2NDMzMw=="}], "type": "inlineReview"}, {"oid": "dded8220b73a9d2667d514fae547c70bdbdc542e", "url": "https://github.com/elastic/elasticsearch/commit/dded8220b73a9d2667d514fae547c70bdbdc542e", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-11-24T10:12:46Z", "type": "commit"}, {"oid": "0e8049f4b71cc93cfa902b9a2673cbc24d1dbf82", "url": "https://github.com/elastic/elasticsearch/commit/0e8049f4b71cc93cfa902b9a2673cbc24d1dbf82", "message": "Use regular delete requests instead of the batch API", "committedDate": "2020-11-24T14:00:24Z", "type": "commit"}, {"oid": "c47fcf44b659691ce146f8061cd487a12128abf7", "url": "https://github.com/elastic/elasticsearch/commit/c47fcf44b659691ce146f8061cd487a12128abf7", "message": "Initialize jackson parser during AzureRepositoryPlugin class\ninitialization", "committedDate": "2020-11-26T11:00:20Z", "type": "commit"}, {"oid": "381b33df504812a434e4cfc82abb28901678bcce", "url": "https://github.com/elastic/elasticsearch/commit/381b33df504812a434e4cfc82abb28901678bcce", "message": "Force azure impl to be public\n\nThe azure sdk internally dynamically constructs its client classes.\nHowever, one of these, for the blob storage is private. This has been\nfixed upstream, but until that is released, the client is unuseable\nwithout the newProxyInPackage permission. Rather than grant that\npermission, this commit makes the class in question public by patching\nthe azure class file when building the azure repository plugin.\n\nrelates https://github.com/Azure/azure-sdk-for-java/issues/17368", "committedDate": "2020-12-02T00:36:51Z", "type": "forcePushed"}, {"oid": "0f522ade45e68e2c17ae75335614e692ea7fe7e8", "url": "https://github.com/elastic/elasticsearch/commit/0f522ade45e68e2c17ae75335614e692ea7fe7e8", "message": "Force azure impl to be public\n\nThe azure sdk internally dynamically constructs its client classes.\nHowever, one of these, for the blob storage is private. This has been\nfixed upstream, but until that is released, the client is unuseable\nwithout the newProxyInPackage permission. Rather than grant that\npermission, this commit makes the class in question public by patching\nthe azure class file when building the azure repository plugin.\n\nrelates https://github.com/Azure/azure-sdk-for-java/issues/17368", "committedDate": "2020-12-02T00:39:33Z", "type": "commit"}, {"oid": "0f522ade45e68e2c17ae75335614e692ea7fe7e8", "url": "https://github.com/elastic/elasticsearch/commit/0f522ade45e68e2c17ae75335614e692ea7fe7e8", "message": "Force azure impl to be public\n\nThe azure sdk internally dynamically constructs its client classes.\nHowever, one of these, for the blob storage is private. This has been\nfixed upstream, but until that is released, the client is unuseable\nwithout the newProxyInPackage permission. Rather than grant that\npermission, this commit makes the class in question public by patching\nthe azure class file when building the azure repository plugin.\n\nrelates https://github.com/Azure/azure-sdk-for-java/issues/17368", "committedDate": "2020-12-02T00:39:33Z", "type": "forcePushed"}, {"oid": "49ec9f5faf7196799987af3202aefa026fb0d28f", "url": "https://github.com/elastic/elasticsearch/commit/49ec9f5faf7196799987af3202aefa026fb0d28f", "message": "Make public additional sdk interfaces to avoid permission issues", "committedDate": "2020-12-02T10:20:42Z", "type": "commit"}, {"oid": "a27c23b0a73a4a554dd57207d8c0b4129ad4ffa2", "url": "https://github.com/elastic/elasticsearch/commit/a27c23b0a73a4a554dd57207d8c0b4129ad4ffa2", "message": "Fix precommit error", "committedDate": "2020-12-02T10:32:57Z", "type": "commit"}, {"oid": "16a0dc94efc91c563732613d68a0d474cd9d3220", "url": "https://github.com/elastic/elasticsearch/commit/16a0dc94efc91c563732613d68a0d474cd9d3220", "message": "Fix wiring of tasks", "committedDate": "2020-12-02T19:52:21Z", "type": "commit"}, {"oid": "eff0b69db3306c6ba0b2a145e15779d4a1c6eaf7", "url": "https://github.com/elastic/elasticsearch/commit/eff0b69db3306c6ba0b2a145e15779d4a1c6eaf7", "message": "Handle timeout configuration correctly", "committedDate": "2020-12-04T10:05:11Z", "type": "commit"}, {"oid": "ea047b4a91b2c5e33bd1c7bf7b6290371864c032", "url": "https://github.com/elastic/elasticsearch/commit/ea047b4a91b2c5e33bd1c7bf7b6290371864c032", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-04T10:13:19Z", "type": "commit"}, {"oid": "6569f18129e90df8bf5dafff28add29b2cd90a4c", "url": "https://github.com/elastic/elasticsearch/commit/6569f18129e90df8bf5dafff28add29b2cd90a4c", "message": "Remove unused dependency", "committedDate": "2020-12-04T10:31:10Z", "type": "commit"}, {"oid": "302c366db0d286d9db2727caa2b02442c4c33648", "url": "https://github.com/elastic/elasticsearch/commit/302c366db0d286d9db2727caa2b02442c4c33648", "message": "Add tests for LocationMode configurations", "committedDate": "2020-12-04T15:19:41Z", "type": "commit"}, {"oid": "8791953f24e2fb03c259bd83baaec7eb606315df", "url": "https://github.com/elastic/elasticsearch/commit/8791953f24e2fb03c259bd83baaec7eb606315df", "message": "Make InnerClass marker public too durin the azure sdk services rewrite.", "committedDate": "2020-12-04T15:22:14Z", "type": "commit"}, {"oid": "3449e3b71133614f4bf3a494fb04f5ffd9d7943d", "url": "https://github.com/elastic/elasticsearch/commit/3449e3b71133614f4bf3a494fb04f5ffd9d7943d", "message": "Add tests for using SECONDARY_ONLY endpoints", "committedDate": "2020-12-04T15:44:58Z", "type": "commit"}, {"oid": "fde398493695ced728360c1ab52ab9de2b651ab1", "url": "https://github.com/elastic/elasticsearch/commit/fde398493695ced728360c1ab52ab9de2b651ab1", "message": "Add min values to AzureClientProvider settings", "committedDate": "2020-12-04T16:05:15Z", "type": "commit"}, {"oid": "cb721041c0c712c7b6caac702278f84bf826db74", "url": "https://github.com/elastic/elasticsearch/commit/cb721041c0c712c7b6caac702278f84bf826db74", "message": "Cleanup publicifier", "committedDate": "2020-12-04T19:25:15Z", "type": "commit"}, {"oid": "f2c17b57faebb11e29b0007bade976edefa77067", "url": "https://github.com/elastic/elasticsearch/commit/f2c17b57faebb11e29b0007bade976edefa77067", "message": "restore docker maybe...", "committedDate": "2020-12-04T19:34:52Z", "type": "commit"}, {"oid": "8f9f71556c8f12eb4a2b75cd3d58872237c984d5", "url": "https://github.com/elastic/elasticsearch/commit/8f9f71556c8f12eb4a2b75cd3d58872237c984d5", "message": "Read eagerly on AzureInputStream", "committedDate": "2020-12-04T19:43:49Z", "type": "commit"}, {"oid": "eae2cee5d01e580d21c1b720df7c6dbe39d88eed", "url": "https://github.com/elastic/elasticsearch/commit/eae2cee5d01e580d21c1b720df7c6dbe39d88eed", "message": "Add tests for retry strategies", "committedDate": "2020-12-04T19:44:28Z", "type": "commit"}, {"oid": "32e01616a9767021b7552e4a99cf63b90fb4436d", "url": "https://github.com/elastic/elasticsearch/commit/32e01616a9767021b7552e4a99cf63b90fb4436d", "message": "Merge remote-tracking branch 'fcofdez/azure-sdk-upgrade-new' into azure-sdk-upgrade", "committedDate": "2020-12-04T19:45:00Z", "type": "commit"}, {"oid": "993e761e0179f8f930f072a500eab54a0c1a43bc", "url": "https://github.com/elastic/elasticsearch/commit/993e761e0179f8f930f072a500eab54a0c1a43bc", "message": "Fix checkStyle", "committedDate": "2020-12-04T19:53:43Z", "type": "commit"}, {"oid": "900111049962bcfa34165bc1b754e238b86b8413", "url": "https://github.com/elastic/elasticsearch/commit/900111049962bcfa34165bc1b754e238b86b8413", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-08T16:27:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzMjc1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r539532752", "bodyText": "NIT: revert line braeak", "author": "original-brownbear", "createdAt": "2020-12-09T18:09:48Z", "path": "plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobContainerRetriesTests.java", "diffHunk": "@@ -282,7 +365,8 @@ public void testWriteBlobWithRetries() throws Exception {\n                         Streams.readFully(exchange.getRequestBody(), new byte[randomIntBetween(1, Math.max(1, bytes.length - 1))]);\n                     } else {\n                         Streams.readFully(exchange.getRequestBody());\n-                        AzureHttpHandler.sendError(exchange, randomFrom(RestStatus.INTERNAL_SERVER_ERROR, RestStatus.SERVICE_UNAVAILABLE));\n+                        AzureHttpHandler.sendError(exchange, randomFrom(RestStatus.INTERNAL_SERVER_ERROR, RestStatus.SERVICE_UNAVAILABLE\n+                        ));", "originalCommit": "900111049962bcfa34165bc1b754e238b86b8413", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzNjgyNA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r539536824", "bodyText": "Why do we return false if we run into an exception?\nThis check is used to potentially mark a repository corrupted because an expected index-N at the repo root is not found. If we return false on some random IO/network exception then we'd potentially mark a repo as corrupted when there's nothing wrong with it?", "author": "original-brownbear", "createdAt": "2020-12-09T18:16:07Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -153,213 +194,230 @@ public LocationMode getLocationMode() {\n \n     @Override\n     public BlobContainer blobContainer(BlobPath path) {\n-        return new AzureBlobContainer(path, this, threadPool);\n+        return new AzureBlobContainer(path, this);\n     }\n \n     @Override\n     public void close() {\n     }\n \n-    public boolean blobExists(String blob) throws URISyntaxException, StorageException {\n-        // Container name must be lower case.\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        return SocketAccess.doPrivilegedException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            return azureBlob.exists(null, null, context);\n-        });\n-    }\n+    public boolean blobExists(String blob) {\n+        final BlobServiceClient client = client();\n \n-    public void deleteBlob(String blob) throws URISyntaxException, StorageException {\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        // Container name must be lower case.\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        logger.trace(() -> new ParameterizedMessage(\"delete blob for container [{}], blob [{}]\", container, blob));\n-        SocketAccess.doPrivilegedVoidException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            logger.trace(() -> new ParameterizedMessage(\"container [{}]: blob [{}] found. removing.\", container, blob));\n-            azureBlob.delete(DeleteSnapshotsOption.NONE, null, null, client.v2().get());\n-        });\n+        try {\n+            Boolean blobExists = SocketAccess.doPrivilegedException(() -> {\n+                final BlobClient azureBlob = client.getBlobContainerClient(container).getBlobClient(blob);\n+                return azureBlob.exists();\n+            });\n+            return blobExists != null ? blobExists : false;\n+        } catch (Exception e) {\n+            logger.warn(\"can not access [{}] in container {{}}: {}\", blob, container, e.getMessage());\n+        }\n+        return false;", "originalCommit": "900111049962bcfa34165bc1b754e238b86b8413", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0MTExOA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r539541118", "bodyText": "We can size this the same way we size firstReadLength so that we don't allocate 4M for a file smaller than 4M right?", "author": "original-brownbear", "createdAt": "2020-12-09T18:22:18Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,96 +449,112 @@ public void eventOccurred(RequestCompletedEvent eventArg) {\n         }\n     }\n \n-    /**\n-     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n-     * because that stream is highly inefficient in both memory and CPU use.\n-     */\n-    private static class BlobInputStream extends InputStream {\n+    private static class AzureInputStream extends StorageInputStream {\n+        private final BlobAsyncClient client;\n+        private final ByteBuffer buffer;\n+        private final int maxRetries;\n+        private final long firstReadOffset;\n+        private final int firstReadLength;\n+\n+        private AzureInputStream(final BlobAsyncClient client,\n+                                 long rangeOffset,\n+                                 Long rangeLength,\n+                                 int chunkSize,\n+                                 long contentLength,\n+                                 int maxRetries) throws IOException {\n+            super(rangeOffset, rangeLength, chunkSize, contentLength);\n+            this.client = client;\n+            this.buffer = ByteBuffer.allocate(chunkSize);", "originalCommit": "900111049962bcfa34165bc1b754e238b86b8413", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU1NzQyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r539557429", "bodyText": "Sorry for not pointing this out earlier, but I'm a little worried about this stream.\nIs it actually faster than what we currently have with the old SDK? We still have the same inefficient behavior here as far as I can tell. We fetch data into a 4M buffer lazily and copy it into the smaller read buffer that comes from other snapshot functionality like we did before, just using non-blocking IO instead of blocking IO in the background.\nThis is in contrast to the other SDKs where we just execute one request for the full file that we then read in a blocking fashion (thereby avoiding the latency of a request every 4M and getting much better performance).\nCan't we build this much more efficiently by starting a Flux on the whole stream and then blockingly iterating through it via toStream() to get similar behavior to a full blocking read triggered by a single request?", "author": "original-brownbear", "createdAt": "2020-12-09T18:46:05Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,96 +449,112 @@ public void eventOccurred(RequestCompletedEvent eventArg) {\n         }\n     }\n \n-    /**\n-     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n-     * because that stream is highly inefficient in both memory and CPU use.\n-     */\n-    private static class BlobInputStream extends InputStream {\n+    private static class AzureInputStream extends StorageInputStream {\n+        private final BlobAsyncClient client;\n+        private final ByteBuffer buffer;\n+        private final int maxRetries;\n+        private final long firstReadOffset;\n+        private final int firstReadLength;\n+\n+        private AzureInputStream(final BlobAsyncClient client,\n+                                 long rangeOffset,\n+                                 Long rangeLength,\n+                                 int chunkSize,\n+                                 long contentLength,\n+                                 int maxRetries) throws IOException {\n+            super(rangeOffset, rangeLength, chunkSize, contentLength);\n+            this.client = client;\n+            this.buffer = ByteBuffer.allocate(chunkSize);\n+            this.maxRetries = maxRetries;\n+\n+            // Read eagerly the first chunk so we can throw early if the\n+            // blob doesn't exist\n+            this.firstReadOffset = rangeOffset;\n+            this.firstReadLength = (int) Math.min(chunkSize, contentLength - rangeOffset);\n+            executeRead(firstReadLength, rangeOffset);\n+        }\n+\n+        @Override\n+        protected ByteBuffer dispatchRead(int readLength, long offset) throws IOException {\n+            // If the request is for the first chunk, don't download it again\n+            // Since we disabled marking in this InputStream, the offset only advances\n+            // and never goes back requesting the firstReadOffset again\n+            if (offset != firstReadOffset || readLength != firstReadLength) {\n+                executeRead(readLength, offset);", "originalCommit": "900111049962bcfa34165bc1b754e238b86b8413", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "904c60a40d2d4c0953257aa447598dce834d98b8", "url": "https://github.com/elastic/elasticsearch/commit/904c60a40d2d4c0953257aa447598dce834d98b8", "message": "Move to read the entire stream instead of reading it in chunks", "committedDate": "2020-12-13T18:12:15Z", "type": "commit"}, {"oid": "59f6ab3e3a48779e93ec52ac8d0c32161ca54a0a", "url": "https://github.com/elastic/elasticsearch/commit/59f6ab3e3a48779e93ec52ac8d0c32161ca54a0a", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-13T18:12:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMDEyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542000121", "bodyText": "You can make this trace again, the exception will bubble up and will be logged at warn level upstream anyway", "author": "original-brownbear", "createdAt": "2020-12-13T20:35:57Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -153,213 +195,233 @@ public LocationMode getLocationMode() {\n \n     @Override\n     public BlobContainer blobContainer(BlobPath path) {\n-        return new AzureBlobContainer(path, this, threadPool);\n+        return new AzureBlobContainer(path, this);\n     }\n \n     @Override\n     public void close() {\n     }\n \n-    public boolean blobExists(String blob) throws URISyntaxException, StorageException {\n-        // Container name must be lower case.\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        return SocketAccess.doPrivilegedException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            return azureBlob.exists(null, null, context);\n-        });\n-    }\n+    public boolean blobExists(String blob) throws IOException {\n+        final BlobServiceClient client = client();\n \n-    public void deleteBlob(String blob) throws URISyntaxException, StorageException {\n-        final Tuple<CloudBlobClient, Supplier<OperationContext>> client = client();\n-        // Container name must be lower case.\n-        final CloudBlobContainer blobContainer = client.v1().getContainerReference(container);\n-        logger.trace(() -> new ParameterizedMessage(\"delete blob for container [{}], blob [{}]\", container, blob));\n-        SocketAccess.doPrivilegedVoidException(() -> {\n-            final CloudBlockBlob azureBlob = blobContainer.getBlockBlobReference(blob);\n-            logger.trace(() -> new ParameterizedMessage(\"container [{}]: blob [{}] found. removing.\", container, blob));\n-            azureBlob.delete(DeleteSnapshotsOption.NONE, null, null, client.v2().get());\n-        });\n+        try {\n+            Boolean blobExists = SocketAccess.doPrivilegedException(() -> {\n+                final BlobClient azureBlob = client.getBlobContainerClient(container).getBlobClient(blob);\n+                return azureBlob.exists();\n+            });\n+            return Boolean.TRUE.equals(blobExists);\n+        } catch (Exception e) {\n+            logger.warn(\"can not access [{}] in container {{}}: {}\", blob, container, e.getMessage());", "originalCommit": "59f6ab3e3a48779e93ec52ac8d0c32161ca54a0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5ODUwMw==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542198503", "bodyText": "Fixed in 2c3eefa", "author": "fcofdez", "createdAt": "2020-12-14T08:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMDEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMjk0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542002949", "bodyText": "Can we document that hasNext() is where we block and wait for the IO thread to put something in the queue? This is quite subtle I think :)", "author": "original-brownbear", "createdAt": "2020-12-13T20:51:13Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/CancellableRateLimitedFluxIterator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+import java.util.ArrayDeque;\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import java.util.Queue;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Consumer;\n+\n+/**\n+ * An iterator that allows to subscribe to a reactive publisher and request more elements\n+ * in batches as the iterator is consumed so an slow consumer is not overwhelmed by a fast\n+ * producer. Additionally it provides the ability to cancel the subscription before the entire\n+ * flux is consumed, for these cases it possible to provide a cleaner function that would be\n+ * invoked for all the elements that weren't consumed before the cancellation. (i.e. it's\n+ * possible to free the memory allocated for a byte buffer).\n+ */\n+class CancellableRateLimitedFluxIterator<T> implements Subscriber<T>, Iterator<T> {\n+    private static final Subscription CANCELLED_SUBSCRIPTION = new Subscription() {\n+        @Override\n+        public void request(long n) {\n+            // no op\n+        }\n+\n+        @Override\n+        public void cancel() {\n+            // no op\n+        }\n+    };\n+\n+    private final int elementsPerBatch;\n+    private final Queue<T> queue;\n+    private final Lock lock;\n+    private final Condition condition;\n+    private final Consumer<T> cleaner;\n+    private final AtomicReference<Subscription> subscription = new AtomicReference<>();\n+    private final Logger logger = LogManager.getLogger(CancellableRateLimitedFluxIterator.class);\n+    private volatile Throwable error;\n+    private volatile boolean done;\n+    private int emittedElements;\n+\n+    /**\n+     * Creates a new CancellableRateLimitedFluxIterator that would request to it's upstream publisher\n+     * in batches as specified in {@code elementsPerBatch}. Additionally, it's possible to provide a\n+     * function that would be invoked after cancellation for possibly outstanding elements that won't by\n+     * consumed downstream but need to be cleaned in any case.\n+     * @param elementsPerBatch the number of elements to request upstream\n+     * @param cleaner the function that would be used to clean unused elements\n+     */\n+    CancellableRateLimitedFluxIterator(int elementsPerBatch, Consumer<T> cleaner) {\n+        this.elementsPerBatch = elementsPerBatch;\n+        this.queue = new ArrayDeque<>(elementsPerBatch);\n+        this.lock = new ReentrantLock();\n+        this.condition = lock.newCondition();\n+        this.cleaner = cleaner;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+        for (; ; ) {\n+            boolean isDone = done;\n+            boolean isQueueEmpty = queue.isEmpty();\n+\n+            if (isDone) {\n+                Throwable e = error;\n+                if (e != null) {\n+                    throw new RuntimeException(e);\n+                } else if (isQueueEmpty) {\n+                    return false;\n+                }\n+            }\n+\n+            if (isQueueEmpty == false) {\n+                return true;\n+            }\n+\n+            // Provide visibility guarantees for the modified queue\n+            lock.lock();\n+            try {\n+                while (done == false && queue.isEmpty()) {\n+                    condition.await();\n+                }\n+            } catch (InterruptedException e) {\n+                cancelSubscription();\n+                throw new RuntimeException(e);\n+            } finally {\n+                lock.unlock();\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public T next() {\n+        if (hasNext() == false) {", "originalCommit": "59f6ab3e3a48779e93ec52ac8d0c32161ca54a0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5ODU1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542198556", "bodyText": "Fixed in 2c3eefa", "author": "fcofdez", "createdAt": "2020-12-14T08:34:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMjk0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMzQ2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542003461", "bodyText": "How is this thread-safe when ArrayDeque isn't thread safe? Aren't we putting stuff on this queue on the Netty event-loop threads but running these checks on the SNAPSHOT or whatever pool that is reading from the blocking stream?", "author": "original-brownbear", "createdAt": "2020-12-13T20:54:02Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/CancellableRateLimitedFluxIterator.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.repositories.azure;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+import java.util.ArrayDeque;\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import java.util.Queue;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Consumer;\n+\n+/**\n+ * An iterator that allows to subscribe to a reactive publisher and request more elements\n+ * in batches as the iterator is consumed so an slow consumer is not overwhelmed by a fast\n+ * producer. Additionally it provides the ability to cancel the subscription before the entire\n+ * flux is consumed, for these cases it possible to provide a cleaner function that would be\n+ * invoked for all the elements that weren't consumed before the cancellation. (i.e. it's\n+ * possible to free the memory allocated for a byte buffer).\n+ */\n+class CancellableRateLimitedFluxIterator<T> implements Subscriber<T>, Iterator<T> {\n+    private static final Subscription CANCELLED_SUBSCRIPTION = new Subscription() {\n+        @Override\n+        public void request(long n) {\n+            // no op\n+        }\n+\n+        @Override\n+        public void cancel() {\n+            // no op\n+        }\n+    };\n+\n+    private final int elementsPerBatch;\n+    private final Queue<T> queue;\n+    private final Lock lock;\n+    private final Condition condition;\n+    private final Consumer<T> cleaner;\n+    private final AtomicReference<Subscription> subscription = new AtomicReference<>();\n+    private final Logger logger = LogManager.getLogger(CancellableRateLimitedFluxIterator.class);\n+    private volatile Throwable error;\n+    private volatile boolean done;\n+    private int emittedElements;\n+\n+    /**\n+     * Creates a new CancellableRateLimitedFluxIterator that would request to it's upstream publisher\n+     * in batches as specified in {@code elementsPerBatch}. Additionally, it's possible to provide a\n+     * function that would be invoked after cancellation for possibly outstanding elements that won't by\n+     * consumed downstream but need to be cleaned in any case.\n+     * @param elementsPerBatch the number of elements to request upstream\n+     * @param cleaner the function that would be used to clean unused elements\n+     */\n+    CancellableRateLimitedFluxIterator(int elementsPerBatch, Consumer<T> cleaner) {\n+        this.elementsPerBatch = elementsPerBatch;\n+        this.queue = new ArrayDeque<>(elementsPerBatch);\n+        this.lock = new ReentrantLock();\n+        this.condition = lock.newCondition();\n+        this.cleaner = cleaner;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+        for (; ; ) {\n+            boolean isDone = done;\n+            boolean isQueueEmpty = queue.isEmpty();", "originalCommit": "59f6ab3e3a48779e93ec52ac8d0c32161ca54a0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE3ODcyMA==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542178720", "bodyText": "Since done is volatile, in theory the writes must be visible in the SNAPSHOT threads. But I agree that this might be tricky, would you prefer to use an ArrayBlockingQueue? To be honest I was on the fence with this solution, the ArrayBlockingQueue solution would be easier to understand.", "author": "fcofdez", "createdAt": "2020-12-14T07:59:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMzQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjE5ODcxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542198719", "bodyText": "I went ahead and replaced the queue in 2c3eefa", "author": "fcofdez", "createdAt": "2020-12-14T08:34:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjAwMzQ2MQ=="}], "type": "inlineReview"}, {"oid": "2c3eefa1117b3371a21f42ea7902058f274fb746", "url": "https://github.com/elastic/elasticsearch/commit/2c3eefa1117b3371a21f42ea7902058f274fb746", "message": "Use a concurrent queue", "committedDate": "2020-12-14T08:33:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3NjczMw==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542376733", "bodyText": "Only just noticed this: why is this reduced again? We don't want to split into lots of files anymore, is there an issue with the SDK and larger files?", "author": "original-brownbear", "createdAt": "2020-12-14T13:19:46Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureStorageService.java", "diffHunk": "@@ -19,132 +19,155 @@\n \n package org.elasticsearch.repositories.azure;\n \n-import com.microsoft.azure.storage.CloudStorageAccount;\n-import com.microsoft.azure.storage.Constants;\n-import com.microsoft.azure.storage.OperationContext;\n-import com.microsoft.azure.storage.RetryPolicy;\n-import com.microsoft.azure.storage.RetryPolicyFactory;\n-import com.microsoft.azure.storage.RetryExponentialRetry;\n-import com.microsoft.azure.storage.blob.BlobRequestOptions;\n-import com.microsoft.azure.storage.blob.CloudBlobClient;\n-import org.elasticsearch.common.collect.Tuple;\n+import com.azure.core.http.ProxyOptions;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.common.implementation.connectionstring.StorageConnectionString;\n+import com.azure.storage.common.policy.RequestRetryOptions;\n+import com.azure.storage.common.policy.RetryPolicyType;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.settings.SettingsException;\n import org.elasticsearch.common.unit.ByteSizeUnit;\n import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n \n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.security.InvalidKeyException;\n+import java.net.InetSocketAddress;\n+import java.net.Proxy;\n+import java.net.URL;\n import java.util.Map;\n-import java.util.function.Supplier;\n+import java.util.function.BiConsumer;\n \n+import static com.azure.storage.blob.BlobAsyncClient.BLOB_DEFAULT_NUMBER_OF_BUFFERS;\n+import static com.azure.storage.blob.BlobClient.BLOB_DEFAULT_UPLOAD_BLOCK_SIZE;\n import static java.util.Collections.emptyMap;\n \n public class AzureStorageService {\n-\n     public static final ByteSizeValue MIN_CHUNK_SIZE = new ByteSizeValue(1, ByteSizeUnit.BYTES);\n-\n     /**\n      * Maximum allowed blob size in Azure blob store.\n      */\n-    public static final ByteSizeValue MAX_CHUNK_SIZE = new ByteSizeValue(Constants.MAX_BLOB_SIZE, ByteSizeUnit.BYTES);\n+    public static final ByteSizeValue MAX_CHUNK_SIZE = new ByteSizeValue(256, ByteSizeUnit.MB);", "originalCommit": "2c3eefa1117b3371a21f42ea7902058f274fb746", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ2MjAyMg==", "url": "https://github.com/elastic/elasticsearch/pull/65140#discussion_r542462022", "bodyText": "I've addressed it in 974bc38", "author": "fcofdez", "createdAt": "2020-12-14T15:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjM3NjczMw=="}], "type": "inlineReview"}, {"oid": "974bc38c0d18ad3321f3abb4652bb098bf84afde", "url": "https://github.com/elastic/elasticsearch/commit/974bc38c0d18ad3321f3abb4652bb098bf84afde", "message": "Use the proper max chunk size", "committedDate": "2020-12-14T14:47:07Z", "type": "commit"}, {"oid": "2c19d717a34a410b5f749bf23d0770a01e6e7b2a", "url": "https://github.com/elastic/elasticsearch/commit/2c19d717a34a410b5f749bf23d0770a01e6e7b2a", "message": "Remove unnecessary settings", "committedDate": "2020-12-14T15:12:53Z", "type": "commit"}, {"oid": "8295186d5f11dc1c83dac6e9b035711abd2df0fc", "url": "https://github.com/elastic/elasticsearch/commit/8295186d5f11dc1c83dac6e9b035711abd2df0fc", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-14T15:13:15Z", "type": "commit"}, {"oid": "5ab14d328139a47f353cf2326bdad2e0dfb0aa10", "url": "https://github.com/elastic/elasticsearch/commit/5ab14d328139a47f353cf2326bdad2e0dfb0aa10", "message": "Revert line break", "committedDate": "2020-12-15T07:18:11Z", "type": "commit"}, {"oid": "e5563f2c2f18b461faa6f7df9ed67d0006bff5df", "url": "https://github.com/elastic/elasticsearch/commit/e5563f2c2f18b461faa6f7df9ed67d0006bff5df", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-15T07:18:54Z", "type": "commit"}, {"oid": "8db3d009479767d62c8b10f925e75627cd1d1392", "url": "https://github.com/elastic/elasticsearch/commit/8db3d009479767d62c8b10f925e75627cd1d1392", "message": "Remove dead code", "committedDate": "2020-12-15T07:27:22Z", "type": "commit"}, {"oid": "9c6ea827f3ca10284d46965040bd05b96928caad", "url": "https://github.com/elastic/elasticsearch/commit/9c6ea827f3ca10284d46965040bd05b96928caad", "message": "Remove unused import", "committedDate": "2020-12-15T08:01:43Z", "type": "commit"}, {"oid": "e1ff3374353566b825b54bbe39440492ed1f38a8", "url": "https://github.com/elastic/elasticsearch/commit/e1ff3374353566b825b54bbe39440492ed1f38a8", "message": "Merge remote-tracking branch 'origin/master' into azure-sdk-upgrade", "committedDate": "2020-12-15T08:05:42Z", "type": "commit"}]}