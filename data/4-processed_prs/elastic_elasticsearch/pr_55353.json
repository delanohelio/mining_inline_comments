{"pr_number": 55353, "pr_title": "Retry failed peer recovery due to transient errors", "pr_createdAt": "2020-04-16T21:22:17Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55353", "timeline": [{"oid": "53cbbdc8ba0c6627261d3f3acabf34b1230adc28", "url": "https://github.com/elastic/elasticsearch/commit/53cbbdc8ba0c6627261d3f3acabf34b1230adc28", "message": "WIP", "committedDate": "2020-04-03T23:13:51Z", "type": "commit"}, {"oid": "395b1cfb299cdeebdfedbd2271bfd999cdd8d55f", "url": "https://github.com/elastic/elasticsearch/commit/395b1cfb299cdeebdfedbd2271bfd999cdd8d55f", "message": "Changes", "committedDate": "2020-04-04T00:19:09Z", "type": "commit"}, {"oid": "1c4b04f0368823e7c6fe699bd875ac93dbb4fe22", "url": "https://github.com/elastic/elasticsearch/commit/1c4b04f0368823e7c6fe699bd875ac93dbb4fe22", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-06T16:05:58Z", "type": "commit"}, {"oid": "2891cde4c70445d795a7e59ce3dedcea8fb3e2d1", "url": "https://github.com/elastic/elasticsearch/commit/2891cde4c70445d795a7e59ce3dedcea8fb3e2d1", "message": "Catch exceptions", "committedDate": "2020-04-06T16:59:56Z", "type": "commit"}, {"oid": "7d24bc32ad783a7bd605ed5e4d2b716ea4d89809", "url": "https://github.com/elastic/elasticsearch/commit/7d24bc32ad783a7bd605ed5e4d2b716ea4d89809", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-06T23:24:45Z", "type": "commit"}, {"oid": "62910c7820bca21589ba14bac35880eeaaeb31d9", "url": "https://github.com/elastic/elasticsearch/commit/62910c7820bca21589ba14bac35880eeaaeb31d9", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-09T20:09:45Z", "type": "commit"}, {"oid": "249c2106eaffad450a80880aff82c499f478437d", "url": "https://github.com/elastic/elasticsearch/commit/249c2106eaffad450a80880aff82c499f478437d", "message": "Changes", "committedDate": "2020-04-09T21:04:32Z", "type": "commit"}, {"oid": "8e429af0c5f525e1b0d116b40529ced1a0bfb86c", "url": "https://github.com/elastic/elasticsearch/commit/8e429af0c5f525e1b0d116b40529ced1a0bfb86c", "message": "Work on test", "committedDate": "2020-04-10T00:08:56Z", "type": "commit"}, {"oid": "4d67ad64d91273800bc2f933df3cd8d191bf6df0", "url": "https://github.com/elastic/elasticsearch/commit/4d67ad64d91273800bc2f933df3cd8d191bf6df0", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-10T00:42:01Z", "type": "commit"}, {"oid": "f36b94dd42e88bb353d8e5eb1125267830a9865c", "url": "https://github.com/elastic/elasticsearch/commit/f36b94dd42e88bb353d8e5eb1125267830a9865c", "message": "Changes", "committedDate": "2020-04-10T15:59:59Z", "type": "commit"}, {"oid": "5b6d2c597828b81e7448d5c239d09500ca445288", "url": "https://github.com/elastic/elasticsearch/commit/5b6d2c597828b81e7448d5c239d09500ca445288", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-14T17:54:53Z", "type": "commit"}, {"oid": "aa04ad03ead314d738e99d87834b6bb02f62cb6e", "url": "https://github.com/elastic/elasticsearch/commit/aa04ad03ead314d738e99d87834b6bb02f62cb6e", "message": "Changes", "committedDate": "2020-04-14T22:49:52Z", "type": "commit"}, {"oid": "3e494c249819e15767fd355fbc64cd61382d04ae", "url": "https://github.com/elastic/elasticsearch/commit/3e494c249819e15767fd355fbc64cd61382d04ae", "message": "Fix", "committedDate": "2020-04-14T23:12:01Z", "type": "commit"}, {"oid": "e21f650dc6d7048d7d639fa4dc268c179f143d76", "url": "https://github.com/elastic/elasticsearch/commit/e21f650dc6d7048d7d639fa4dc268c179f143d76", "message": "Idempotency", "committedDate": "2020-04-15T21:06:46Z", "type": "commit"}, {"oid": "65af4bcaf57e769d8821d53f233778c24174c69a", "url": "https://github.com/elastic/elasticsearch/commit/65af4bcaf57e769d8821d53f233778c24174c69a", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures", "committedDate": "2020-04-15T21:17:16Z", "type": "commit"}, {"oid": "87684ab485f0176d0617b521f22f289df5ca8000", "url": "https://github.com/elastic/elasticsearch/commit/87684ab485f0176d0617b521f22f289df5ca8000", "message": "WIP", "committedDate": "2020-04-16T20:35:53Z", "type": "commit"}, {"oid": "f6797c1f6cb5036aafd99f4edad53f8963a1ce90", "url": "https://github.com/elastic/elasticsearch/commit/f6797c1f6cb5036aafd99f4edad53f8963a1ce90", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-16T20:36:04Z", "type": "commit"}, {"oid": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "url": "https://github.com/elastic/elasticsearch/commit/46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "message": "Pull out network retries", "committedDate": "2020-04-16T21:20:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NTMzMg==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r409975332", "bodyText": "We need to use a separate buffer for each chunk request; otherwise, we will resend a chunk request with data from the other chunk. Maybe use a pool so that we do not have to allocate the buffers all the time.\nI ran the new test 100 iterations, but all of them passed. I think we should flush shards before starting recovery and aggressively change the recovery settings chunk_size_setting and  max_concurrent_file_chunks in the test so that we can catch the error.", "author": "dnhatn", "createdAt": "2020-04-17T03:43:19Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,64 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = fileChunkRequestOptions.timeout();\n+        /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n+         * see how many translog ops we accumulate while copying files across the network. A future optimization\n+         * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n+         */\n+        final RecoveryFileChunkRequest request = new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata,", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMjkxMg==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410312912", "bodyText": "good catch", "author": "ywelsch", "createdAt": "2020-04-17T15:51:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NTMzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDIwNA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410524204", "bodyText": "I was able to get consistent failures by increasing the documents and flushing in the middle.", "author": "tbrooks8", "createdAt": "2020-04-18T00:27:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NTMzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTM5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410525393", "bodyText": "I explored removing the shared ~512K buffer. But I think that is so deeply embedded in the multi file transfer thing, that it is probably a PR of its own. I just copied to a pooled big array.", "author": "tbrooks8", "createdAt": "2020-04-18T00:35:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NTMzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NzQ4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410757485", "bodyText": "I am fine with this implementation in this PR, and defer the optimization in a follow-up. I hope it won't slow down the recovery.", "author": "dnhatn", "createdAt": "2020-04-18T22:04:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NTMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NjYwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r409976609", "bodyText": "I think we only need to make translog-ops and file-chunk requests retry-able because the other requests should not fail with rejected exceptions or circuit breaking errors.", "author": "dnhatn", "createdAt": "2020-04-17T03:48:42Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -74,25 +91,32 @@ public RemoteRecoveryTargetHandler(long recoveryId, ShardId shardId, TransportSe\n                 .withType(TransportRequestOptions.Type.RECOVERY)\n                 .withTimeout(recoverySettings.internalActionTimeout())\n                 .build();\n-\n     }\n \n     @Override\n     public void prepareForTranslogOperations(int totalTranslogOps, ActionListener<Void> listener) {\n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG,\n-            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps),\n-            TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionTimeout()).build(),\n-            new ActionListenerResponseHandler<>(ActionListener.map(listener, r -> null),\n-                in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = recoverySettings.internalActionTimeout();\n+        final RecoveryPrepareForTranslogOperationsRequest request =\n+            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps);\n+        final Consumer<ActionListener<Void>> action = actionListener ->\n+            transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG, request,\n+                TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionTimeout()).build(),\n+                new ActionListenerResponseHandler<>(ActionListener.map(actionListener, r -> null),\n+                    in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        executeRetryableAction(action, listener, timeout);\n     }\n \n     @Override\n     public void finalizeRecovery(final long globalCheckpoint, final long trimAboveSeqNo, final ActionListener<Void> listener) {\n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FINALIZE,\n-            new RecoveryFinalizeRecoveryRequest(recoveryId, shardId, globalCheckpoint, trimAboveSeqNo),\n-            TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionLongTimeout()).build(),\n-            new ActionListenerResponseHandler<>(ActionListener.map(listener, r -> null),\n-                in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = recoverySettings.internalActionTimeout();", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDQ5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410524491", "bodyText": "I'm not sure I understand here. FINALIZE for example can fail due to circuit breaking. I'm not sure why we would not want to retry? I understand that FINALIZE is not resource intensive, but the node can be circuit breaking for other reasons. It seems like we would want to retry everything? Especially since I have reduced the timeout to something reasonable (1 minute).", "author": "tbrooks8", "createdAt": "2020-04-18T00:29:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NjYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NzUxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410757519", "bodyText": "ok", "author": "dnhatn", "createdAt": "2020-04-18T22:04:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk3NjYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk4MDQwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r409980409", "bodyText": "I wonder if we should make all these changes in RecoverySourceHandler instead? It might be simpler than doing it here.", "author": "dnhatn", "createdAt": "2020-04-17T04:04:12Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,64 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = fileChunkRequestOptions.timeout();\n+        /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n+         * see how many translog ops we accumulate while copying files across the network. A future optimization\n+         * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n+         */\n+        final RecoveryFileChunkRequest request = new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata,\n+            position, content, lastChunk, totalTranslogOps, throttleTimeInNanos);\n+\n+        Consumer<ActionListener<Void>> action = actionListener ->\n+            transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK, request, fileChunkRequestOptions,\n+                new ActionListenerResponseHandler<>(ActionListener.map(actionListener, r -> null),\n+                    in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+\n+        executeRetryableAction(action, listener, timeout);\n     }\n \n+    @Override\n+    public void cancel() {", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxNDA4NA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410314084", "bodyText": "The advantage there is that the cancellation logic is in one place. The advantage here is that it's clear that the retries are due to inter-node communication, and might eventually be folded into some TransportService infrastructure?", "author": "ywelsch", "createdAt": "2020-04-17T15:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk4MDQwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDU1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410524555", "bodyText": "I left as is for now.", "author": "tbrooks8", "createdAt": "2020-04-18T00:30:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk4MDQwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NjE3OA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410756178", "bodyText": "ok", "author": "dnhatn", "createdAt": "2020-04-18T21:54:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk4MDQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk4MDg4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r409980883", "bodyText": "I think we need to limit the suppressed exceptions in case we retry too many times.", "author": "dnhatn", "createdAt": "2020-04-17T04:06:25Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String retryExecutor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = Math.max(initialDelay.getMillis(), 1);\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.retryExecutor = retryExecutor;\n+    }\n+\n+    public void run() {\n+        runTryAction(new RetryingListener(initialDelayMillis, null));\n+    }\n+\n+    public void cancel(Exception e) {\n+        if (isDone.compareAndSet(false, true)) {\n+            finalListener.onFailure(e);\n+        }\n+\n+    }\n+\n+    private void runTryAction(ActionListener<Response> listener) {\n+        try {\n+            tryAction(listener);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public abstract void tryAction(ActionListener<Response> listener);\n+\n+    public abstract boolean shouldRetry(Exception e);\n+\n+    private class RetryingListener implements ActionListener<Response> {\n+\n+        private final long nextDelayMillis;\n+        private final Exception existingException;\n+\n+        private RetryingListener(long nextDelayMillis, Exception existingException) {\n+            this.nextDelayMillis = nextDelayMillis;\n+            this.existingException = existingException;\n+        }\n+\n+        @Override\n+        public void onResponse(Response response) {\n+            if (isDone.compareAndSet(false, true)) {\n+                finalListener.onResponse(response);\n+            }\n+        }\n+\n+        @Override\n+        public void onFailure(Exception e) {\n+            if (shouldRetry(e)) {\n+                final long elapsedMillis = threadPool.relativeTimeInMillis() - startMillis;\n+                if (elapsedMillis > timeoutMillis) {\n+                    logger.debug(() -> new ParameterizedMessage(\"retryable action timed out after {}\",\n+                        TimeValue.timeValueMillis(elapsedMillis)), e);\n+                    addExisting(e);\n+                    if (isDone.compareAndSet(false, true)) {\n+                        finalListener.onFailure(e);\n+                    }\n+                } else {\n+                    logger.debug(() -> new ParameterizedMessage(\"retrying action that failed in {}\",\n+                        TimeValue.timeValueMillis(nextDelayMillis)), e);\n+                    addExisting(e);\n+                    Runnable runnable = () -> runTryAction(new RetryingListener(nextDelayMillis * 2, e));\n+                    final long midpoint = (nextDelayMillis / 2);\n+                    final int randomness = Randomness.get().nextInt((int) Math.min(midpoint, Integer.MAX_VALUE));\n+                    final long delayMillis = midpoint + randomness;\n+                    if (isDone.get() == false) {\n+                        threadPool.schedule(runnable, TimeValue.timeValueMillis(delayMillis), retryExecutor);\n+                    }\n+                }\n+            } else {\n+                addExisting(e);\n+                if (isDone.compareAndSet(false,true)) {\n+                    finalListener.onFailure(e);\n+                }\n+            }\n+        }\n+\n+        private void addExisting(Exception e) {\n+            if (existingException != null) {\n+                e.addSuppressed(existingException);", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5MDkwMw==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410290903", "bodyText": "Instead of being lenient here, we could throw an IllegalArgumentException?", "author": "ywelsch", "createdAt": "2020-04-17T15:15:57Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String retryExecutor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = Math.max(initialDelay.getMillis(), 1);", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5MjQ1OA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410292458", "bodyText": "let's use an ActionRunnable here that will also notify the listener if the action can't be enqueued on the threadpool of the retryExecutor", "author": "ywelsch", "createdAt": "2020-04-17T15:18:28Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String retryExecutor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = Math.max(initialDelay.getMillis(), 1);\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.retryExecutor = retryExecutor;\n+    }\n+\n+    public void run() {\n+        runTryAction(new RetryingListener(initialDelayMillis, null));\n+    }\n+\n+    public void cancel(Exception e) {\n+        if (isDone.compareAndSet(false, true)) {\n+            finalListener.onFailure(e);\n+        }\n+\n+    }\n+\n+    private void runTryAction(ActionListener<Response> listener) {\n+        try {\n+            tryAction(listener);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public abstract void tryAction(ActionListener<Response> listener);\n+\n+    public abstract boolean shouldRetry(Exception e);\n+\n+    private class RetryingListener implements ActionListener<Response> {\n+\n+        private final long nextDelayMillis;\n+        private final Exception existingException;\n+\n+        private RetryingListener(long nextDelayMillis, Exception existingException) {\n+            this.nextDelayMillis = nextDelayMillis;\n+            this.existingException = existingException;\n+        }\n+\n+        @Override\n+        public void onResponse(Response response) {\n+            if (isDone.compareAndSet(false, true)) {\n+                finalListener.onResponse(response);\n+            }\n+        }\n+\n+        @Override\n+        public void onFailure(Exception e) {\n+            if (shouldRetry(e)) {\n+                final long elapsedMillis = threadPool.relativeTimeInMillis() - startMillis;\n+                if (elapsedMillis > timeoutMillis) {\n+                    logger.debug(() -> new ParameterizedMessage(\"retryable action timed out after {}\",\n+                        TimeValue.timeValueMillis(elapsedMillis)), e);\n+                    addExisting(e);\n+                    if (isDone.compareAndSet(false, true)) {\n+                        finalListener.onFailure(e);\n+                    }\n+                } else {\n+                    logger.debug(() -> new ParameterizedMessage(\"retrying action that failed in {}\",\n+                        TimeValue.timeValueMillis(nextDelayMillis)), e);\n+                    addExisting(e);\n+                    Runnable runnable = () -> runTryAction(new RetryingListener(nextDelayMillis * 2, e));", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI5ODU0NA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410298544", "bodyText": "info logging?", "author": "ywelsch", "createdAt": "2020-04-17T15:28:11Z", "path": "server/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java", "diffHunk": "@@ -723,6 +729,133 @@ private void validateIndexRecoveryState(RecoveryState.Index indexState) {\n         assertThat(indexState.recoveredBytesPercent(), lessThanOrEqualTo(100.0f));\n     }\n \n+    public void testTransientErrorsDuringRecoveryAreRetried() throws Exception {\n+        final String indexName = \"test\";\n+        final Settings nodeSettings = Settings.builder()\n+            .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), \"360s\")\n+            .put(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), \"10s\")\n+            .build();\n+        // start a master node\n+        internalCluster().startNode(nodeSettings);\n+\n+        final String blueNodeName = internalCluster()\n+            .startNode(Settings.builder().put(\"node.attr.color\", \"blue\").put(nodeSettings).build());\n+        final String redNodeName = internalCluster()\n+            .startNode(Settings.builder().put(\"node.attr.color\", \"red\").put(nodeSettings).build());\n+\n+        ClusterHealthResponse response = client().admin().cluster().prepareHealth().setWaitForNodes(\">=3\").get();\n+        assertThat(response.isTimedOut(), is(false));\n+\n+        client().admin().indices().prepareCreate(indexName)\n+            .setSettings(\n+                Settings.builder()\n+                    .put(IndexMetadata.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"blue\")\n+                    .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+                    .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+            ).get();\n+\n+        List<IndexRequestBuilder> requests = new ArrayList<>();\n+        int numDocs = scaledRandomIntBetween(25, 250);\n+        for (int i = 0; i < numDocs; i++) {\n+            requests.add(client().prepareIndex(indexName).setSource(\"{}\", XContentType.JSON));\n+        }\n+        indexRandom(true, requests);\n+        ensureSearchable(indexName);\n+\n+        ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get();\n+        final String blueNodeId = internalCluster().getInstance(ClusterService.class, blueNodeName).localNode().getId();\n+\n+        assertFalse(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());\n+\n+        SearchResponse searchResponse = client().prepareSearch(indexName).get();\n+        assertHitCount(searchResponse, numDocs);\n+\n+        String[] recoveryActions = new String[]{\n+            PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG,\n+            PeerRecoveryTargetService.Actions.TRANSLOG_OPS,\n+            PeerRecoveryTargetService.Actions.FILES_INFO,\n+            PeerRecoveryTargetService.Actions.FILE_CHUNK,\n+            PeerRecoveryTargetService.Actions.CLEAN_FILES,\n+            PeerRecoveryTargetService.Actions.FINALIZE\n+        };\n+        final String recoveryActionToBlock = randomFrom(recoveryActions);\n+        logger.info(\"--> will break connection between blue & red on [{}]\", recoveryActionToBlock);\n+\n+        MockTransportService blueTransportService =\n+            (MockTransportService) internalCluster().getInstance(TransportService.class, blueNodeName);\n+        MockTransportService redTransportService =\n+            (MockTransportService) internalCluster().getInstance(TransportService.class, redNodeName);\n+\n+        final SingleStartEnforcer validator = new SingleStartEnforcer();\n+        blueTransportService.addSendBehavior(redTransportService, (connection, requestId, action, request, options) -> {\n+            validator.accept(action);\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        redTransportService.addSendBehavior(blueTransportService, (connection, requestId, action, request, options) -> {\n+            validator.accept(action);\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        blueTransportService.addRequestHandlingBehavior(recoveryActionToBlock, new TransientReceiveRejected(recoveryActionToBlock));\n+        redTransportService.addRequestHandlingBehavior(recoveryActionToBlock, new TransientReceiveRejected(recoveryActionToBlock));\n+\n+        try {\n+            logger.info(\"--> starting recovery from blue to red\");\n+            client().admin().indices().prepareUpdateSettings(indexName).setSettings(\n+                Settings.builder()\n+                    .put(IndexMetadata.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"red,blue\")\n+                    .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 1)\n+            ).get();\n+\n+            ensureGreen();\n+            searchResponse = client(redNodeName).prepareSearch(indexName).setPreference(\"_local\").get();\n+            assertHitCount(searchResponse, numDocs);\n+        } finally {\n+            blueTransportService.clearAllRules();\n+            redTransportService.clearAllRules();\n+        }\n+    }\n+\n+    private class TransientReceiveRejected implements StubbableTransport.RequestHandlingBehavior<TransportRequest> {\n+\n+        private final String actionName;\n+        private final AtomicInteger blocksRemaining;\n+\n+        private TransientReceiveRejected(String actionName) {\n+            this.actionName = actionName;\n+            this.blocksRemaining = new AtomicInteger(randomIntBetween(1, 3));\n+        }\n+\n+        @Override\n+        public void messageReceived(TransportRequestHandler<TransportRequest> handler, TransportRequest request, TransportChannel channel,\n+                                    Task task) throws Exception {\n+            if (blocksRemaining.getAndDecrement() != 0) {\n+                logger.error(\"--> preventing {} response by throwing exception\", actionName);", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwMTc5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410301799", "bodyText": "using the internalActionTimout (which is 15min) as the maximum retry timeout here is I think too much to wait for a recovery to continue. I would prefer for the recovery to be failed much earlier, probably more in the range of a minute or so. Let's add a new setting in RecoverySettings (undocumented) that allows setting this for all actions here.", "author": "ywelsch", "createdAt": "2020-04-17T15:33:32Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -74,25 +91,32 @@ public RemoteRecoveryTargetHandler(long recoveryId, ShardId shardId, TransportSe\n                 .withType(TransportRequestOptions.Type.RECOVERY)\n                 .withTimeout(recoverySettings.internalActionTimeout())\n                 .build();\n-\n     }\n \n     @Override\n     public void prepareForTranslogOperations(int totalTranslogOps, ActionListener<Void> listener) {\n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG,\n-            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps),\n-            TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionTimeout()).build(),\n-            new ActionListenerResponseHandler<>(ActionListener.map(listener, r -> null),\n-                in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = recoverySettings.internalActionTimeout();\n+        final RecoveryPrepareForTranslogOperationsRequest request =\n+            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps);\n+        final Consumer<ActionListener<Void>> action = actionListener ->\n+            transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG, request,\n+                TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionTimeout()).build(),\n+                new ActionListenerResponseHandler<>(ActionListener.map(actionListener, r -> null),\n+                    in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        executeRetryableAction(action, listener, timeout);", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMwNzI2NA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410307264", "bodyText": "Can you also add unit tests for this class? Preferable using DeterministicTaskQueue and it's fake ThreadPool", "author": "ywelsch", "createdAt": "2020-04-17T15:42:48Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public abstract class RetryableAction<Response> {", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxMjc2NA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410312764", "bodyText": "I'm not a fan of these Consumers (makes the code uglier) and wonder if there's a more general concept lurking here where we want to register outgoing requests on the transport and resend those on failure. Perhaps executeRetryableAction could take all the parameters that are currently passed to transportService.sendRequest, which would avoid the need for the consumer.", "author": "ywelsch", "createdAt": "2020-04-17T15:51:32Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -74,25 +91,32 @@ public RemoteRecoveryTargetHandler(long recoveryId, ShardId shardId, TransportSe\n                 .withType(TransportRequestOptions.Type.RECOVERY)\n                 .withTimeout(recoverySettings.internalActionTimeout())\n                 .build();\n-\n     }\n \n     @Override\n     public void prepareForTranslogOperations(int totalTranslogOps, ActionListener<Void> listener) {\n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.PREPARE_TRANSLOG,\n-            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps),\n-            TransportRequestOptions.builder().withTimeout(recoverySettings.internalActionTimeout()).build(),\n-            new ActionListenerResponseHandler<>(ActionListener.map(listener, r -> null),\n-                in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = recoverySettings.internalActionTimeout();\n+        final RecoveryPrepareForTranslogOperationsRequest request =\n+            new RecoveryPrepareForTranslogOperationsRequest(recoveryId, shardId, totalTranslogOps);\n+        final Consumer<ActionListener<Void>> action = actionListener ->", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxNjA2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410316061", "bodyText": "should we only fork off if there's anything to cancel?", "author": "ywelsch", "createdAt": "2020-04-17T15:56:47Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,64 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final TimeValue timeout = fileChunkRequestOptions.timeout();\n+        /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n+         * see how many translog ops we accumulate while copying files across the network. A future optimization\n+         * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n+         */\n+        final RecoveryFileChunkRequest request = new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata,\n+            position, content, lastChunk, totalTranslogOps, throttleTimeInNanos);\n+\n+        Consumer<ActionListener<Void>> action = actionListener ->\n+            transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK, request, fileChunkRequestOptions,\n+                new ActionListenerResponseHandler<>(ActionListener.map(actionListener, r -> null),\n+                    in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+\n+        executeRetryableAction(action, listener, timeout);\n     }\n \n+    @Override\n+    public void cancel() {\n+        isCancelled = true;\n+        final RuntimeException exception = new CancellableThreads.ExecutionCancelledException(\"recovery was cancelled\");\n+        // Dispatch to generic as cancellation calls can come on the cluster state applier thread\n+        threadPool.generic().execute(() -> {", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDMxNzk0OA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410317948", "bodyText": "Can you also add some Javadocs to how this class works?", "author": "ywelsch", "createdAt": "2020-04-17T15:59:46Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public abstract class RetryableAction<Response> {", "originalCommit": "46fbcc5d581c15cccc4b2f23fadcc5568881ce5e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b981dd0ca2b857495ddc9aeb642e88143ea285f5", "url": "https://github.com/elastic/elasticsearch/commit/b981dd0ca2b857495ddc9aeb642e88143ea285f5", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-17T16:18:13Z", "type": "commit"}, {"oid": "651b576db415cc0cbe2498a125757cae35c8c7b0", "url": "https://github.com/elastic/elasticsearch/commit/651b576db415cc0cbe2498a125757cae35c8c7b0", "message": "Changes", "committedDate": "2020-04-17T17:42:30Z", "type": "commit"}, {"oid": "cd2d288d4ed55d263057e779e4963ab89e2d3e79", "url": "https://github.com/elastic/elasticsearch/commit/cd2d288d4ed55d263057e779e4963ab89e2d3e79", "message": "Fixes", "committedDate": "2020-04-17T18:12:09Z", "type": "commit"}, {"oid": "f92dfaa454f7f20b4f0325d82b6930b7be9d84de", "url": "https://github.com/elastic/elasticsearch/commit/f92dfaa454f7f20b4f0325d82b6930b7be9d84de", "message": "Changes", "committedDate": "2020-04-17T23:07:10Z", "type": "commit"}, {"oid": "826193b805cba33dd19594e949775f7b8c1571b5", "url": "https://github.com/elastic/elasticsearch/commit/826193b805cba33dd19594e949775f7b8c1571b5", "message": "Rename", "committedDate": "2020-04-17T23:08:35Z", "type": "commit"}, {"oid": "12391f3342f4902df7e9911a2da1042c38d85fc6", "url": "https://github.com/elastic/elasticsearch/commit/12391f3342f4902df7e9911a2da1042c38d85fc6", "message": "Add setting", "committedDate": "2020-04-17T23:11:08Z", "type": "commit"}, {"oid": "aaf777d07fd17a97056f30d3af0db008ef2e0e8d", "url": "https://github.com/elastic/elasticsearch/commit/aaf777d07fd17a97056f30d3af0db008ef2e0e8d", "message": "Delete thing", "committedDate": "2020-04-18T00:02:12Z", "type": "commit"}, {"oid": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "url": "https://github.com/elastic/elasticsearch/commit/fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "message": "Fix", "committedDate": "2020-04-18T00:28:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MDA5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410750099", "bodyText": "nit: remove the extra line", "author": "dnhatn", "createdAt": "2020-04-18T20:59:52Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.ArrayDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A action that will be retried on failure if {@link RetryableAction#shouldRetry(Exception)} returns true.\n+ * The executor the action will be executed on can be defined in the constructor. Otherwise, SAME is the\n+ * default. The action will be retried with exponentially increasing delay periods until the timeout period\n+ * has been reached.\n+ */\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String executor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = initialDelay.getMillis();\n+        if (initialDelayMillis < 1) {\n+            throw new IllegalArgumentException(\"Initial delay was less than 1 millisecond: \" + initialDelay);\n+        }\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.retryExecutor = executor;\n+    }\n+\n+    public void run() {\n+        final RetryingListener retryingListener = new RetryingListener(initialDelayMillis, null);\n+        final Runnable runnable = createRunnable(retryingListener);\n+        threadPool.executor(retryExecutor).execute(runnable);\n+    }\n+\n+    public void cancel(Exception e) {\n+        if (isDone.compareAndSet(false, true)) {\n+            finalListener.onFailure(e);\n+        }\n+", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MTc4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410751782", "bodyText": "The next delay here always increases. Should we randomly pick the next delay like \n  \n    \n      elasticsearch/x-pack/plugin/ccr/src/main/java/org/elasticsearch/xpack/ccr/action/ShardFollowNodeTask.java\n    \n    \n         Line 522\n      in\n      3c3ea41\n    \n    \n    \n    \n\n        \n          \n           static long computeDelay(int currentRetry, long maxRetryDelayInMillis) { \n        \n    \n  \n\n.", "author": "dnhatn", "createdAt": "2020-04-18T21:15:14Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.ArrayDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A action that will be retried on failure if {@link RetryableAction#shouldRetry(Exception)} returns true.\n+ * The executor the action will be executed on can be defined in the constructor. Otherwise, SAME is the\n+ * default. The action will be retried with exponentially increasing delay periods until the timeout period\n+ * has been reached.\n+ */\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String executor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = initialDelay.getMillis();\n+        if (initialDelayMillis < 1) {\n+            throw new IllegalArgumentException(\"Initial delay was less than 1 millisecond: \" + initialDelay);\n+        }\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.retryExecutor = executor;\n+    }\n+\n+    public void run() {\n+        final RetryingListener retryingListener = new RetryingListener(initialDelayMillis, null);\n+        final Runnable runnable = createRunnable(retryingListener);\n+        threadPool.executor(retryExecutor).execute(runnable);\n+    }\n+\n+    public void cancel(Exception e) {\n+        if (isDone.compareAndSet(false, true)) {\n+            finalListener.onFailure(e);\n+        }\n+\n+    }\n+\n+    private Runnable createRunnable(RetryingListener retryingListener) {\n+        return new ActionRunnable<>(retryingListener) {\n+\n+            @Override\n+            protected void doRun() {\n+                tryAction(listener);\n+            }\n+\n+            @Override\n+            public void onRejection(Exception e) {\n+                // Immediately fail because we were not able to schedule the action\n+                retryingListener.addException(e);\n+                finalListener.onFailure(retryingListener.buildFinalException());\n+            }\n+        };\n+    }\n+\n+    public abstract void tryAction(ActionListener<Response> listener);\n+\n+    public abstract boolean shouldRetry(Exception e);\n+\n+    private class RetryingListener implements ActionListener<Response> {\n+\n+        private static final int MAX_EXCEPTIONS = 4;\n+\n+        private final long nextDelayMillis;\n+        private ArrayDeque<Exception> caughtExceptions;\n+\n+        private RetryingListener(long nextDelayMillis, ArrayDeque<Exception> caughtExceptions) {\n+            this.nextDelayMillis = nextDelayMillis;\n+            this.caughtExceptions = caughtExceptions;\n+        }\n+\n+        @Override\n+        public void onResponse(Response response) {\n+            if (isDone.compareAndSet(false, true)) {\n+                finalListener.onResponse(response);\n+            }\n+        }\n+\n+        @Override\n+        public void onFailure(Exception e) {\n+            if (shouldRetry(e)) {\n+                final long elapsedMillis = threadPool.relativeTimeInMillis() - startMillis;\n+                if (elapsedMillis >= timeoutMillis) {\n+                    logger.debug(() -> new ParameterizedMessage(\"retryable action timed out after {}\",\n+                        TimeValue.timeValueMillis(elapsedMillis)), e);\n+                    addException(e);\n+                    if (isDone.compareAndSet(false, true)) {\n+                        finalListener.onFailure(buildFinalException());\n+                    }\n+                } else {\n+                    logger.debug(() -> new ParameterizedMessage(\"retrying action that failed in {}\",\n+                        TimeValue.timeValueMillis(nextDelayMillis)), e);\n+                    addException(e);\n+\n+                    final RetryingListener retryingListener = new RetryingListener(nextDelayMillis * 2, caughtExceptions);\n+                    final Runnable runnable = createRunnable(retryingListener);\n+                    final long midpoint = (nextDelayMillis / 2);\n+                    final int randomness = Randomness.get().nextInt((int) Math.min(midpoint, Integer.MAX_VALUE));\n+                    final long delayMillis = midpoint + randomness;", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MTk0MA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410751940", "bodyText": "I think we should not use the retryExecutor for the first run?", "author": "dnhatn", "createdAt": "2020-04-18T21:17:04Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.ArrayDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A action that will be retried on failure if {@link RetryableAction#shouldRetry(Exception)} returns true.\n+ * The executor the action will be executed on can be defined in the constructor. Otherwise, SAME is the\n+ * default. The action will be retried with exponentially increasing delay periods until the timeout period\n+ * has been reached.\n+ */\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String retryExecutor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String executor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = initialDelay.getMillis();\n+        if (initialDelayMillis < 1) {\n+            throw new IllegalArgumentException(\"Initial delay was less than 1 millisecond: \" + initialDelay);\n+        }\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.retryExecutor = executor;\n+    }\n+\n+    public void run() {\n+        final RetryingListener retryingListener = new RetryingListener(initialDelayMillis, null);\n+        final Runnable runnable = createRunnable(retryingListener);\n+        threadPool.executor(retryExecutor).execute(runnable);", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgzODI0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r411838247", "bodyText": "I renamed it to executor and made it an executor. I felt weird about the fact that the first attempt might use a different executor than the retries. Although it does not really matter for this usage since this one always uses SAME.", "author": "tbrooks8", "createdAt": "2020-04-21T03:18:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MTk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1Mjc4MA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410752780", "bodyText": "maybe reduce the lower bound to 100?", "author": "dnhatn", "createdAt": "2020-04-18T21:24:44Z", "path": "server/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java", "diffHunk": "@@ -723,6 +729,143 @@ private void validateIndexRecoveryState(RecoveryState.Index indexState) {\n         assertThat(indexState.recoveredBytesPercent(), lessThanOrEqualTo(100.0f));\n     }\n \n+    public void testTransientErrorsDuringRecoveryAreRetried() throws Exception {\n+        final String indexName = \"test\";\n+        final Settings nodeSettings = Settings.builder()\n+            .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), \"360s\")\n+            .put(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), \"10s\")\n+            .build();\n+        // start a master node\n+        internalCluster().startNode(nodeSettings);\n+\n+        final String blueNodeName = internalCluster()\n+            .startNode(Settings.builder().put(\"node.attr.color\", \"blue\").put(nodeSettings).build());\n+        final String redNodeName = internalCluster()\n+            .startNode(Settings.builder().put(\"node.attr.color\", \"red\").put(nodeSettings).build());\n+\n+        ClusterHealthResponse response = client().admin().cluster().prepareHealth().setWaitForNodes(\">=3\").get();\n+        assertThat(response.isTimedOut(), is(false));\n+\n+        client().admin().indices().prepareCreate(indexName)\n+            .setSettings(\n+                Settings.builder()\n+                    .put(IndexMetadata.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"blue\")\n+                    .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+                    .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+            ).get();\n+\n+        List<IndexRequestBuilder> requests = new ArrayList<>();\n+        int numDocs = scaledRandomIntBetween(6000, 8000);", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NjUzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410756539", "bodyText": "Discuss: Should integrate the retry with the rate limiter. I prefer \"no\" for simplicity.", "author": "dnhatn", "createdAt": "2020-04-18T21:57:06Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,85 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final String action = PeerRecoveryTargetService.Actions.FILE_CHUNK;\n+        BytesRefIterator iterator = content.iterator();\n+        BytesRef scratch;\n+        final ReleasableBytesStreamOutput output = new ReleasableBytesStreamOutput(content.length(), bigArrays);\n+        boolean actionStarted = false;\n+        try {\n+            while((scratch = iterator.next()) != null) {\n+                output.writeBytes(scratch.bytes, scratch.offset, scratch.length);\n+            }\n+            /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n+             * see how many translog ops we accumulate while copying files across the network. A future optimization\n+             * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n+             */\n+            final RecoveryFileChunkRequest request = new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata,\n+                position, output.bytes(), lastChunk, totalTranslogOps, throttleTimeInNanos);\n+            final Writeable.Reader<TransportResponse.Empty> reader = in -> TransportResponse.Empty.INSTANCE;\n+            final ActionListener<TransportResponse.Empty> responseListener = ActionListener.map(listener, r -> null);\n+            final ActionListener<TransportResponse.Empty> releaseListener = ActionListener.runBefore(responseListener, output::close);\n+            executeRetryableAction(action, request, fileChunkRequestOptions, releaseListener, reader);", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgzNzIzNA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r411837234", "bodyText": "That is why I took that approach.", "author": "tbrooks8", "createdAt": "2020-04-21T03:15:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NjUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1NjYxMg==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r410756612", "bodyText": "nit: maybe iterate over the values()?", "author": "dnhatn", "createdAt": "2020-04-18T21:57:46Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,85 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final String action = PeerRecoveryTargetService.Actions.FILE_CHUNK;\n+        BytesRefIterator iterator = content.iterator();\n+        BytesRef scratch;\n+        final ReleasableBytesStreamOutput output = new ReleasableBytesStreamOutput(content.length(), bigArrays);\n+        boolean actionStarted = false;\n+        try {\n+            while((scratch = iterator.next()) != null) {\n+                output.writeBytes(scratch.bytes, scratch.offset, scratch.length);\n+            }\n+            /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n+             * see how many translog ops we accumulate while copying files across the network. A future optimization\n+             * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n+             */\n+            final RecoveryFileChunkRequest request = new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata,\n+                position, output.bytes(), lastChunk, totalTranslogOps, throttleTimeInNanos);\n+            final Writeable.Reader<TransportResponse.Empty> reader = in -> TransportResponse.Empty.INSTANCE;\n+            final ActionListener<TransportResponse.Empty> responseListener = ActionListener.map(listener, r -> null);\n+            final ActionListener<TransportResponse.Empty> releaseListener = ActionListener.runBefore(responseListener, output::close);\n+            executeRetryableAction(action, request, fileChunkRequestOptions, releaseListener, reader);\n+            actionStarted = true;\n+        } catch (IOException e) {\n+            // Since the content data is buffer in memory, we should never get an exception.\n+            throw new AssertionError(e);\n+        } finally {\n+            if (actionStarted == false) {\n+                output.close();\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void cancel() {\n+        isCancelled = true;\n+        if (onGoingRetryableActions.isEmpty()) {\n+            return;\n+        }\n+        final RuntimeException exception = new CancellableThreads.ExecutionCancelledException(\"recovery was cancelled\");\n+        // Dispatch to generic as cancellation calls can come on the cluster state applier thread\n+        threadPool.generic().execute(() -> {\n+            for (Map.Entry<Object, RetryableAction<?>> action : onGoingRetryableActions.entrySet()) {", "originalCommit": "fe0762fed2a83a16603fc8bf9b5d76b9ef65823d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "54a1d1b56da23c02749449cd7945e7b06f511c17", "url": "https://github.com/elastic/elasticsearch/commit/54a1d1b56da23c02749449cd7945e7b06f511c17", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-21T02:44:43Z", "type": "commit"}, {"oid": "f3dbeba8cc76533da0bba22ada250d0aa10f54aa", "url": "https://github.com/elastic/elasticsearch/commit/f3dbeba8cc76533da0bba22ada250d0aa10f54aa", "message": "Review", "committedDate": "2020-04-21T03:19:26Z", "type": "commit"}, {"oid": "c4c0af9fdceac05d622600fdcdbd578b5104d7ff", "url": "https://github.com/elastic/elasticsearch/commit/c4c0af9fdceac05d622600fdcdbd578b5104d7ff", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-21T03:44:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjExMTE5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r412111199", "bodyText": "Is this correct? We schedule on the scheduler thread and wouldn't get rejected at the time the command was scheduled. If we did that on a bounded queue like the write queue and got rejected after waiting for some time on the scheduler thread, wouldn't we want to treat this just like any other temporary issue and retry running on it later?", "author": "original-brownbear", "createdAt": "2020-04-21T11:43:06Z", "path": "server/src/main/java/org/elasticsearch/action/support/RetryableAction.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.support;\n+\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.util.ArrayDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A action that will be retried on failure if {@link RetryableAction#shouldRetry(Exception)} returns true.\n+ * The executor the action will be executed on can be defined in the constructor. Otherwise, SAME is the\n+ * default. The action will be retried with exponentially increasing delay periods until the timeout period\n+ * has been reached.\n+ */\n+public abstract class RetryableAction<Response> {\n+\n+    private final Logger logger;\n+\n+    private final AtomicBoolean isDone = new AtomicBoolean(false);\n+    private final ThreadPool threadPool;\n+    private final long initialDelayMillis;\n+    private final long timeoutMillis;\n+    private final long startMillis;\n+    private final ActionListener<Response> finalListener;\n+    private final String executor;\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener) {\n+        this(logger, threadPool, initialDelay, timeoutValue, listener, ThreadPool.Names.SAME);\n+    }\n+\n+    public RetryableAction(Logger logger, ThreadPool threadPool, TimeValue initialDelay, TimeValue timeoutValue,\n+                           ActionListener<Response> listener, String executor) {\n+        this.logger = logger;\n+        this.threadPool = threadPool;\n+        this.initialDelayMillis = initialDelay.getMillis();\n+        if (initialDelayMillis < 1) {\n+            throw new IllegalArgumentException(\"Initial delay was less than 1 millisecond: \" + initialDelay);\n+        }\n+        this.timeoutMillis = Math.max(timeoutValue.getMillis(), 1);\n+        this.startMillis = threadPool.relativeTimeInMillis();\n+        this.finalListener = listener;\n+        this.executor = executor;\n+    }\n+\n+    public void run() {\n+        final RetryingListener retryingListener = new RetryingListener(initialDelayMillis, null);\n+        final Runnable runnable = createRunnable(retryingListener);\n+        threadPool.executor(executor).execute(runnable);\n+    }\n+\n+    public void cancel(Exception e) {\n+        if (isDone.compareAndSet(false, true)) {\n+            finalListener.onFailure(e);\n+        }\n+    }\n+\n+    private Runnable createRunnable(RetryingListener retryingListener) {\n+        return new ActionRunnable<>(retryingListener) {\n+\n+            @Override\n+            protected void doRun() {\n+                tryAction(listener);\n+            }\n+\n+            @Override\n+            public void onRejection(Exception e) {\n+                // Immediately fail because we were not able to schedule the action", "originalCommit": "c4c0af9fdceac05d622600fdcdbd578b5104d7ff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjExNDM3Mg==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r412114372", "bodyText": "NIT: Revert indent change?", "author": "original-brownbear", "createdAt": "2020-04-21T11:48:10Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -114,39 +144,45 @@ public void indexTranslogOperations(\n             final RetentionLeases retentionLeases,\n             final long mappingVersionOnPrimary,\n             final ActionListener<Long> listener) {\n+        final String action = PeerRecoveryTargetService.Actions.TRANSLOG_OPS;\n         final RecoveryTranslogOperationsRequest request = new RecoveryTranslogOperationsRequest(\n-                recoveryId,\n-                shardId,\n-                operations,\n-                totalTranslogOps,\n-                maxSeenAutoIdTimestampOnPrimary,\n-                maxSeqNoOfDeletesOrUpdatesOnPrimary,\n-                retentionLeases,\n-                mappingVersionOnPrimary);\n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.TRANSLOG_OPS, request, translogOpsRequestOptions,\n-            new ActionListenerResponseHandler<>(ActionListener.map(listener, r -> r.localCheckpoint),\n-                RecoveryTranslogOperationsResponse::new, ThreadPool.Names.GENERIC));\n+            recoveryId,", "originalCommit": "c4c0af9fdceac05d622600fdcdbd578b5104d7ff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjExNzI5MA==", "url": "https://github.com/elastic/elasticsearch/pull/55353#discussion_r412117290", "bodyText": "Why not just content.writeTo(output)?", "author": "original-brownbear", "createdAt": "2020-04-21T11:52:53Z", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RemoteRecoveryTargetHandler.java", "diffHunk": "@@ -174,15 +210,85 @@ public void writeFileChunk(StoreFileMetadata fileMetadata, long position, BytesR\n             throttleTimeInNanos = 0;\n         }\n \n-        transportService.sendRequest(targetNode, PeerRecoveryTargetService.Actions.FILE_CHUNK,\n-            new RecoveryFileChunkRequest(recoveryId, shardId, fileMetadata, position, content, lastChunk,\n-                totalTranslogOps,\n-                /* we send estimateTotalOperations with every request since we collect stats on the target and that way we can\n-                 * see how many translog ops we accumulate while copying files across the network. A future optimization\n-                 * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.\n-                 */\n-                throttleTimeInNanos), fileChunkRequestOptions, new ActionListenerResponseHandler<>(\n-                    ActionListener.map(listener, r -> null), in -> TransportResponse.Empty.INSTANCE, ThreadPool.Names.GENERIC));\n+        final String action = PeerRecoveryTargetService.Actions.FILE_CHUNK;\n+        BytesRefIterator iterator = content.iterator();\n+        BytesRef scratch;\n+        final ReleasableBytesStreamOutput output = new ReleasableBytesStreamOutput(content.length(), bigArrays);\n+        boolean actionStarted = false;\n+        try {\n+            while((scratch = iterator.next()) != null) {", "originalCommit": "c4c0af9fdceac05d622600fdcdbd578b5104d7ff", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f3708bce1c25d6aa62b78f50079393e3821cdbf5", "url": "https://github.com/elastic/elasticsearch/commit/f3708bce1c25d6aa62b78f50079393e3821cdbf5", "message": "Changes", "committedDate": "2020-04-21T16:13:13Z", "type": "commit"}, {"oid": "a13ebe02c5bf5fb61545e2555f307472eb342244", "url": "https://github.com/elastic/elasticsearch/commit/a13ebe02c5bf5fb61545e2555f307472eb342244", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-21T16:40:13Z", "type": "commit"}, {"oid": "5b68b0f68304d25135c8a22dc68ca21875549f03", "url": "https://github.com/elastic/elasticsearch/commit/5b68b0f68304d25135c8a22dc68ca21875549f03", "message": "Merge remote-tracking branch 'upstream/master' into retry_peer_recovery_failures_due_to_overload", "committedDate": "2020-04-21T17:14:34Z", "type": "commit"}]}