{"pr_number": 66489, "pr_title": "Reduce memory usage on Azure repository implementation", "pr_createdAt": "2020-12-17T07:48:10Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/66489", "timeline": [{"oid": "ca1c04718f74adfce780019288b124fe27643229", "url": "https://github.com/elastic/elasticsearch/commit/ca1c04718f74adfce780019288b124fe27643229", "message": "Reduce memory usage on Azure repository implementation\n\nThis commit moves the upload logic to the repository itself\ninstead of delegating into the SDK.\nMulti-block uploads are done sequentially instead of in parallel\nthat allows to bound the outstanding memory.\nAdditionally the number of i/o threads have been reduced to 1,\nto reduce the memory overhead.", "committedDate": "2020-12-17T07:43:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg3NzI5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544877291", "bodyText": "I think that we can reduce those too, wdyt @original-brownbear?", "author": "fcofdez", "createdAt": "2020-12-17T07:49:11Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureClientProvider.java", "diffHunk": "@@ -150,7 +150,6 @@ private static ByteBufAllocator createByteBufAllocator() {\n         int tinyCacheSize = PooledByteBufAllocator.defaultTinyCacheSize();\n         int smallCacheSize = PooledByteBufAllocator.defaultSmallCacheSize();\n         int normalCacheSize = PooledByteBufAllocator.defaultNormalCacheSize();\n-        boolean useCacheForAllThreads = PooledByteBufAllocator.defaultUseCacheForAllThreads();\n \n         return new PooledByteBufAllocator(false,\n             nHeapArena,", "originalCommit": "ca1c04718f74adfce780019288b124fe27643229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NzYxMg==", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544887612", "bodyText": "Yea, can't we just do 1 maybe when we only use one thread now anyway?", "author": "original-brownbear", "createdAt": "2020-12-17T08:08:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg3NzI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkxNjk4NA==", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544916984", "bodyText": "This still scares me a little. If we only do 64k at a time here, can't we use the Netty memory allocator (or manage  our own set of byte[] and recycle them on doOnComplete or do we still have no guarantees about flushing at that point?", "author": "original-brownbear", "createdAt": "2020-12-17T08:57:07Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -404,12 +403,156 @@ public void writeBlob(String blobName, InputStream inputStream, long blobSize, b\n         logger.trace(() -> new ParameterizedMessage(\"writeBlob({}, stream, {}) - done\", blobName, blobSize));\n     }\n \n-    private ParallelTransferOptions getParallelTransferOptions() {\n-        ParallelTransferOptions parallelTransferOptions = new ParallelTransferOptions();\n-        parallelTransferOptions.setBlockSizeLong(service.getUploadBlockSize())\n-            .setMaxSingleUploadSizeLong(service.getSizeThresholdForMultiBlockUpload())\n-            .setMaxConcurrency(service.getMaxUploadParallelism());\n-        return parallelTransferOptions;\n+    private void executeSingleUpload(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) {\n+        SocketAccess.doPrivilegedVoidException(() -> {\n+            final BlobServiceAsyncClient asyncClient = asyncClient();\n+\n+            final BlobAsyncClient blobAsyncClient = asyncClient.getBlobContainerAsyncClient(container).getBlobAsyncClient(blobName);\n+            final BlockBlobAsyncClient blockBlobAsyncClient = blobAsyncClient.getBlockBlobAsyncClient();\n+\n+            final Flux<ByteBuffer> byteBufferFlux =\n+                convertStreamToByteBuffer(inputStream, blobSize, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+            final BlockBlobSimpleUploadOptions options = new BlockBlobSimpleUploadOptions(byteBufferFlux, blobSize);\n+            BlobRequestConditions requestConditions = new BlobRequestConditions();\n+            if (failIfAlreadyExists) {\n+                requestConditions.setIfNoneMatch(\"*\");\n+            }\n+            options.setRequestConditions(requestConditions);\n+            blockBlobAsyncClient.uploadWithResponse(options).block();\n+        });\n+    }\n+\n+    private void executeMultipartUpload(String blobName, InputStream inputStream, long blobSize, boolean failIfAlreadyExists) {\n+        SocketAccess.doPrivilegedVoidException(() -> {\n+            final BlobServiceAsyncClient asyncClient = asyncClient();\n+            final BlobAsyncClient blobAsyncClient = asyncClient.getBlobContainerAsyncClient(container)\n+                .getBlobAsyncClient(blobName);\n+            final BlockBlobAsyncClient blockBlobAsyncClient = blobAsyncClient.getBlockBlobAsyncClient();\n+\n+            final long partSize = getUploadBlockSize();\n+            final Tuple<Long, Long> multiParts = numberOfMultiparts(blobSize, partSize);\n+            final int nbParts = multiParts.v1().intValue();\n+            final long lastPartSize = multiParts.v2();\n+            assert blobSize == (((nbParts - 1) * partSize) + lastPartSize) : \"blobSize does not match multipart sizes\";\n+\n+            final List<String> blockIds = new ArrayList<>(nbParts);\n+            for (int i = 0; i < nbParts; i++) {\n+                final long length = i < nbParts - 1 ? partSize : lastPartSize;\n+                final Flux<ByteBuffer> byteBufferFlux =\n+                    convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+\n+                final String blockId = UUIDs.base64UUID();\n+                blockBlobAsyncClient.stageBlock(blockId, byteBufferFlux, length).block();\n+                blockIds.add(blockId);\n+            }\n+\n+            blockBlobAsyncClient.commitBlockList(blockIds, failIfAlreadyExists == false).block();\n+        });\n+    }\n+\n+    /**\n+     * Converts the provided input stream into a Flux of ByteBuffer. To avoid having large amounts of outstanding\n+     * memory this Flux reads the InputStream into ByteBuffers of {@code chunkSize} size.\n+     * @param inputStream the InputStream to convert\n+     * @param length the InputStream length\n+     * @param chunkSize the chunk size in bytes\n+     * @return a Flux of ByteBuffers\n+     */\n+    private Flux<ByteBuffer> convertStreamToByteBuffer(InputStream inputStream, long length, int chunkSize) {\n+        assert inputStream.markSupported() : \"An InputStream with mark support was expected\";\n+        // We need to mark the InputStream as it's possible that we need to retry for the same chunk\n+        inputStream.mark(Integer.MAX_VALUE);\n+        return Flux.defer(() -> {\n+            final AtomicLong currentTotalLength = new AtomicLong(0);\n+            try {\n+                inputStream.reset();\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+            // This flux is subscribed by a downstream operator that finally queues the\n+            // buffers into netty output queue. Sadly we are not able to get a signal once\n+            // the buffer has been flushed, so we have to allocate those and let the GC to\n+            // reclaim them (see MonoSendMany). Additionally, that very same operator requests\n+            // 128 elements (that's hardcoded) once it's subscribed (later on, it requests\n+            // by 64 elements), that's why we provide 64kb buffers.\n+            return Flux.range(0, (int) Math.ceil((double) length / (double) chunkSize))\n+                .map(i -> i * chunkSize)\n+                .concatMap(pos -> Mono.fromCallable(() -> {\n+                    long count = pos + chunkSize > length ? length - pos : chunkSize;\n+                    int numOfBytesRead = 0;\n+                    int offset = 0;\n+                    int len = (int) count;\n+                    final byte[] buffer = new byte[len];", "originalCommit": "ca1c04718f74adfce780019288b124fe27643229", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDQyNw==", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544920427", "bodyText": "Sadly we don't have guarantees about flushing at that point", "author": "fcofdez", "createdAt": "2020-12-17T09:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkxNjk4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDk3MA==", "url": "https://github.com/elastic/elasticsearch/pull/66489#discussion_r544920970", "bodyText": "I explored a different approach where I was passing an allocator there, and recycling at the end of the request, but in that case you end up holding that memory for the entire duration of the request.", "author": "fcofdez", "createdAt": "2020-12-17T09:02:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkxNjk4NA=="}], "type": "inlineReview"}, {"oid": "d044af02e6109e7108359a5d521c0a569c92e924", "url": "https://github.com/elastic/elasticsearch/commit/d044af02e6109e7108359a5d521c0a569c92e924", "message": "Reduce the number of heap arenas", "committedDate": "2020-12-17T09:27:55Z", "type": "commit"}]}