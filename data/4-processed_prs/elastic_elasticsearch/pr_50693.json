{"pr_number": 50693, "pr_title": "Add searchable snapshots cache directory", "pr_createdAt": "2020-01-07T08:26:25Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/50693", "timeline": [{"oid": "cc58922b5e51dfba888859009384383d62e02918", "url": "https://github.com/elastic/elasticsearch/commit/cc58922b5e51dfba888859009384383d62e02918", "message": "Add basis for CacheService & CacheDirectory", "committedDate": "2020-01-06T15:51:07Z", "type": "commit"}, {"oid": "a3bd08841128a50a93df48f7b0b51c5e49eeff49", "url": "https://github.com/elastic/elasticsearch/commit/a3bd08841128a50a93df48f7b0b51c5e49eeff49", "message": "Add SparseFileTracker\n\nCo-Authored-By: David Turner <david.turner@elastic.co>", "committedDate": "2020-01-06T15:51:08Z", "type": "commit"}, {"oid": "bfbddbf38649565bdb5d24dd768c0a071587f427", "url": "https://github.com/elastic/elasticsearch/commit/bfbddbf38649565bdb5d24dd768c0a071587f427", "message": "Add length to SparseFileTracker", "committedDate": "2020-01-06T15:51:08Z", "type": "commit"}, {"oid": "95ca3dc51cb9d7db67503a51df5e8b69180421ad", "url": "https://github.com/elastic/elasticsearch/commit/95ca3dc51cb9d7db67503a51df5e8b69180421ad", "message": "Tests passed", "committedDate": "2020-01-06T15:56:06Z", "type": "commit"}, {"oid": "99194dc849c407806d9e75df5981909b23794cea", "url": "https://github.com/elastic/elasticsearch/commit/99194dc849c407806d9e75df5981909b23794cea", "message": "Add small note", "committedDate": "2020-01-06T16:25:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0MTE0MA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363741140", "bodyText": "Could we remove this and always invalidate the cache on shutdown? I see that this might be useful for some early experiments, but I think if we want to assert things about the content of the cache we can do so in an ESIntegTestCase while the node(s) are running.", "author": "DaveCTurner", "createdAt": "2020-01-07T13:14:12Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0NTUxNA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366245514", "bodyText": "Sure - I removed the setting and it now always invalidate on shutdown.", "author": "tlrx", "createdAt": "2020-01-14T09:59:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0MTE0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0MzM2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363743366", "bodyText": "I'm not very familiar with Cache but it looks like the weight is not expected to change during the lifetime of an entry which isn't the case with tracker.getLengthOfRanges(). I think for now it might be safer to use the length of the file.", "author": "DaveCTurner", "createdAt": "2020-01-07T13:19:55Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);\n+\n+                    final CacheEntry cacheEntry = new CacheEntry(path, length, source, channel, CACHE_FILE_RANGE_SIZE);\n+                    success = true;\n+                    return cacheEntry;\n+                } catch (IOException e) {\n+                    logger.error(() -> new ParameterizedMessage(\"failed to create cache file [{}]\", path), e);\n+                    throw e;\n+                } finally {\n+                    if (success == false) {\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            });\n+        } catch (ExecutionException e) {\n+            throw ExceptionsHelper.convertToElastic(e);\n+        }\n+    }\n+\n+    /**\n+     * Reads bytes from a cached file.\n+     * <p>\n+     * The cached file is identified by its path ({@code filePath}). This path is used as a cache key by the cache service to know if an\n+     * entry already exists in cache for the given file.\n+     * <p>\n+     * When no entry exists in cache, the service creates a new entry for the file. Because cached files are created as sparse files on\n+     * disk, each cache entry needs to track the ranges of bytes that have already been cached and the ranges of bytes that will need to be\n+     * retrieved from a source. This is tracked in {@link CacheEntry} by {@link SparseFileTracker} which requires to know the length of the\n+     * file to cache to correctly track ranges.\n+     * <p>\n+     * When an entry already exists in cache, the service uses the {@link SparseFileTracker} of the {@link CacheEntry} to know if it can\n+     * serve the read operations using one or more ranges of bytes already cached on local disk or if it needs to retrieve the bytes from\n+     * the source (supplied as a {@link IndexInput}) first, then writes the bytes locally before returning them as the result of the read\n+     * operation.\n+     *\n+     * @param filePath   the {@link Path} of the cached file\n+     * @param fileLength the length of the cached file (required to compute ranges of bytes)\n+     * @param fileSource supplies the {@link IndexInput} to read the bytes from in case they are not already in cache\n+     * @param position   the position in the cached file where to start reading bytes from\n+     * @param b          the array to read bytes into\n+     * @param off        the offset in the array to start storing bytes\n+     * @param len        the number of bytes to read\n+     *\n+     * @throws IOException if something went wrong\n+     */\n+    public void readFromCache(final Path filePath, final long fileLength, final CheckedSupplier<IndexInput, IOException> fileSource,\n+                              final long position, final byte[] b, int off, int len) throws IOException {\n+        long pos = position;\n+        while (len > 0 && lifecycleState() == Lifecycle.State.STARTED) {\n+            final CacheEntry cacheEntry = getOrAddToCache(filePath, fileLength, fileSource);\n+            if (cacheEntry.tryIncRef() == false) {\n+                continue;\n+            }\n+            try {\n+                final int read = cacheEntry.fetchRange(pos, b, off, len);\n+                logger.trace(() -> new ParameterizedMessage(\"read {} bytes of file [name:{}, length:{}] at position [{}] from cache\",\n+                    read, filePath.getFileName(), fileLength, position));\n+                pos += read;\n+                off += read;\n+                len -= read;\n+            } finally {\n+                cacheEntry.decRef();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Mark the given {@link CacheEntry} as evicted: no new read or write operation can be executed on the file on disk, which will be\n+     * deleted from disk once all on-going reads are processed.\n+     */\n+    private void markAsEvicted(final CacheEntry cacheEntry) {\n+        logger.trace(() -> new ParameterizedMessage(\"marking cache entry [{}] as evicted\", cacheEntry));\n+        cacheEntry.markAsEvicted();\n+    }\n+\n+    /**\n+     * Remove from cache all entries that match the given predicate.\n+     *\n+     * @param predicate the predicate to evaluate\n+     */\n+    public void removeFromCache(final Predicate<String> predicate) {\n+        if (invalidateOnShutdown) {\n+            for (String cacheKey : cache.keys()) {\n+                if (predicate.test(cacheKey)) {\n+                    cache.invalidate(cacheKey);\n+                }\n+            }\n+        }\n+        cache.refresh();\n+    }\n+\n+    /**\n+     * Computes the cache key associated to the given Lucene cached file\n+     *\n+     * @param cacheFile the cached file\n+     * @return the cache key\n+     */\n+    private static String toCacheKey(final Path cacheFile) {\n+        return cacheFile.toAbsolutePath().toString();\n+    }\n+\n+    private class CacheEntry implements RefCounted {\n+\n+        private final CheckedSupplier<IndexInput, IOException> source;\n+        private final SparseFileTracker tracker;\n+        private final FileChannel channel;\n+        private final int sizeOfRange;\n+        private final Path path;\n+\n+        private final AbstractRefCounted refCounter;\n+        private volatile boolean evicted;\n+\n+        CacheEntry(Path path, long length, CheckedSupplier<IndexInput, IOException> source, FileChannel channel, int sizeOfRange) {\n+            this.tracker = new SparseFileTracker(path.toString(), length);\n+            this.source = Objects.requireNonNull(source);\n+            this.channel = Objects.requireNonNull(channel);\n+            this.path = Objects.requireNonNull(path);\n+            this.sizeOfRange = sizeOfRange;\n+            this.refCounter = new AbstractRefCounted(path.toString()) {\n+                @Override\n+                protected void closeInternal() {\n+                    assert refCount() == 0;\n+                    IOUtils.closeWhileHandlingException(CacheEntry.this.channel);\n+                    assert evicted || invalidateOnShutdown == false;\n+                    if (evicted) {\n+                        logger.trace(() -> new ParameterizedMessage(\"deleting cache file [{}]\", path));\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            };\n+            this.evicted = false;\n+        }\n+\n+        private long estimateWeight() {", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0NjI3OA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366246278", "bodyText": "I think for now it might be safer to use the length of the file.\n\nI agree, I pushed this change", "author": "tlrx", "createdAt": "2020-01-14T10:01:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0MzM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0NDA0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363744046", "bodyText": "Whitespace nit (cannot unsee!):\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    private  int fetchRange(final long position, final byte[] buffer, final int offset, final int length) throws IOException {\n          \n          \n            \n                    private int fetchRange(final long position, final byte[] buffer, final int offset, final int length) throws IOException {", "author": "DaveCTurner", "createdAt": "2020-01-07T13:21:43Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);\n+\n+                    final CacheEntry cacheEntry = new CacheEntry(path, length, source, channel, CACHE_FILE_RANGE_SIZE);\n+                    success = true;\n+                    return cacheEntry;\n+                } catch (IOException e) {\n+                    logger.error(() -> new ParameterizedMessage(\"failed to create cache file [{}]\", path), e);\n+                    throw e;\n+                } finally {\n+                    if (success == false) {\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            });\n+        } catch (ExecutionException e) {\n+            throw ExceptionsHelper.convertToElastic(e);\n+        }\n+    }\n+\n+    /**\n+     * Reads bytes from a cached file.\n+     * <p>\n+     * The cached file is identified by its path ({@code filePath}). This path is used as a cache key by the cache service to know if an\n+     * entry already exists in cache for the given file.\n+     * <p>\n+     * When no entry exists in cache, the service creates a new entry for the file. Because cached files are created as sparse files on\n+     * disk, each cache entry needs to track the ranges of bytes that have already been cached and the ranges of bytes that will need to be\n+     * retrieved from a source. This is tracked in {@link CacheEntry} by {@link SparseFileTracker} which requires to know the length of the\n+     * file to cache to correctly track ranges.\n+     * <p>\n+     * When an entry already exists in cache, the service uses the {@link SparseFileTracker} of the {@link CacheEntry} to know if it can\n+     * serve the read operations using one or more ranges of bytes already cached on local disk or if it needs to retrieve the bytes from\n+     * the source (supplied as a {@link IndexInput}) first, then writes the bytes locally before returning them as the result of the read\n+     * operation.\n+     *\n+     * @param filePath   the {@link Path} of the cached file\n+     * @param fileLength the length of the cached file (required to compute ranges of bytes)\n+     * @param fileSource supplies the {@link IndexInput} to read the bytes from in case they are not already in cache\n+     * @param position   the position in the cached file where to start reading bytes from\n+     * @param b          the array to read bytes into\n+     * @param off        the offset in the array to start storing bytes\n+     * @param len        the number of bytes to read\n+     *\n+     * @throws IOException if something went wrong\n+     */\n+    public void readFromCache(final Path filePath, final long fileLength, final CheckedSupplier<IndexInput, IOException> fileSource,\n+                              final long position, final byte[] b, int off, int len) throws IOException {\n+        long pos = position;\n+        while (len > 0 && lifecycleState() == Lifecycle.State.STARTED) {\n+            final CacheEntry cacheEntry = getOrAddToCache(filePath, fileLength, fileSource);\n+            if (cacheEntry.tryIncRef() == false) {\n+                continue;\n+            }\n+            try {\n+                final int read = cacheEntry.fetchRange(pos, b, off, len);\n+                logger.trace(() -> new ParameterizedMessage(\"read {} bytes of file [name:{}, length:{}] at position [{}] from cache\",\n+                    read, filePath.getFileName(), fileLength, position));\n+                pos += read;\n+                off += read;\n+                len -= read;\n+            } finally {\n+                cacheEntry.decRef();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Mark the given {@link CacheEntry} as evicted: no new read or write operation can be executed on the file on disk, which will be\n+     * deleted from disk once all on-going reads are processed.\n+     */\n+    private void markAsEvicted(final CacheEntry cacheEntry) {\n+        logger.trace(() -> new ParameterizedMessage(\"marking cache entry [{}] as evicted\", cacheEntry));\n+        cacheEntry.markAsEvicted();\n+    }\n+\n+    /**\n+     * Remove from cache all entries that match the given predicate.\n+     *\n+     * @param predicate the predicate to evaluate\n+     */\n+    public void removeFromCache(final Predicate<String> predicate) {\n+        if (invalidateOnShutdown) {\n+            for (String cacheKey : cache.keys()) {\n+                if (predicate.test(cacheKey)) {\n+                    cache.invalidate(cacheKey);\n+                }\n+            }\n+        }\n+        cache.refresh();\n+    }\n+\n+    /**\n+     * Computes the cache key associated to the given Lucene cached file\n+     *\n+     * @param cacheFile the cached file\n+     * @return the cache key\n+     */\n+    private static String toCacheKey(final Path cacheFile) {\n+        return cacheFile.toAbsolutePath().toString();\n+    }\n+\n+    private class CacheEntry implements RefCounted {\n+\n+        private final CheckedSupplier<IndexInput, IOException> source;\n+        private final SparseFileTracker tracker;\n+        private final FileChannel channel;\n+        private final int sizeOfRange;\n+        private final Path path;\n+\n+        private final AbstractRefCounted refCounter;\n+        private volatile boolean evicted;\n+\n+        CacheEntry(Path path, long length, CheckedSupplier<IndexInput, IOException> source, FileChannel channel, int sizeOfRange) {\n+            this.tracker = new SparseFileTracker(path.toString(), length);\n+            this.source = Objects.requireNonNull(source);\n+            this.channel = Objects.requireNonNull(channel);\n+            this.path = Objects.requireNonNull(path);\n+            this.sizeOfRange = sizeOfRange;\n+            this.refCounter = new AbstractRefCounted(path.toString()) {\n+                @Override\n+                protected void closeInternal() {\n+                    assert refCount() == 0;\n+                    IOUtils.closeWhileHandlingException(CacheEntry.this.channel);\n+                    assert evicted || invalidateOnShutdown == false;\n+                    if (evicted) {\n+                        logger.trace(() -> new ParameterizedMessage(\"deleting cache file [{}]\", path));\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            };\n+            this.evicted = false;\n+        }\n+\n+        private long estimateWeight() {\n+            final long lengthOfRanges = tracker.getLengthOfRanges();\n+            return (lengthOfRanges == 0L) ? sizeOfRange : lengthOfRanges;\n+        }\n+\n+        private synchronized void markAsEvicted() {\n+            if (evicted == false) {\n+                evicted = true;\n+                decRef();\n+            }\n+        }\n+\n+        @Override\n+        public synchronized boolean tryIncRef() {\n+            if (evicted) {\n+                return false;\n+            }\n+            return refCounter.tryIncRef();\n+        }\n+\n+        @Override\n+        public synchronized void incRef() {\n+            if (tryIncRef() == false) {\n+                throw new IllegalStateException(\"Failed to increment reference counter, cache entry is already evicted\");\n+            }\n+        }\n+\n+        @Override\n+        public synchronized void decRef() {\n+            refCounter.decRef();\n+        }\n+\n+        /**\n+         * Computes the start and the end of a range to which the given {@code position} belongs.\n+         *\n+         * @param position the reading position\n+         * @return the start and end range positions to fetch\n+         */\n+        private Tuple<Long, Long> computeRange(final long position) {\n+            final long start = (position / sizeOfRange) * sizeOfRange;\n+            return Tuple.tuple(start, Math.min(start + sizeOfRange, tracker.getLength()));\n+        }\n+\n+        /**\n+         * Fetch the range corresponding to the position from the cache.\n+         *\n+         * @param position the position to start reading bytes from.\n+         * @param buffer   the array to read bytes into\n+         * @param offset   the offset in the array to start storing bytes\n+         * @param length   the number of bytes to read\n+         * @return the number of bytes read\n+         *\n+         * @throws IOException if something went wrong\n+         */\n+        private  int fetchRange(final long position, final byte[] buffer, final int offset, final int length) throws IOException {", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc0ODA4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363748082", "bodyText": "If I understand correctly, this means every CacheEntry corresponds with an open file, even if the corresponding IndexInput is closed. I think this might cause problems at scale. Could we close the FileChannel whenever the IndexInput is closed?", "author": "DaveCTurner", "createdAt": "2020-01-07T13:32:25Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc1NDE0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363754149", "bodyText": "Will this work ok with non-filesystem blob stores? Ideally we'd send a single 32MB GET request and then write it locally in 8k chunks, but it's not obvious how we will tell input to prepare for a large sequential read like that.", "author": "DaveCTurner", "createdAt": "2020-01-07T13:47:16Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);\n+\n+                    final CacheEntry cacheEntry = new CacheEntry(path, length, source, channel, CACHE_FILE_RANGE_SIZE);\n+                    success = true;\n+                    return cacheEntry;\n+                } catch (IOException e) {\n+                    logger.error(() -> new ParameterizedMessage(\"failed to create cache file [{}]\", path), e);\n+                    throw e;\n+                } finally {\n+                    if (success == false) {\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            });\n+        } catch (ExecutionException e) {\n+            throw ExceptionsHelper.convertToElastic(e);\n+        }\n+    }\n+\n+    /**\n+     * Reads bytes from a cached file.\n+     * <p>\n+     * The cached file is identified by its path ({@code filePath}). This path is used as a cache key by the cache service to know if an\n+     * entry already exists in cache for the given file.\n+     * <p>\n+     * When no entry exists in cache, the service creates a new entry for the file. Because cached files are created as sparse files on\n+     * disk, each cache entry needs to track the ranges of bytes that have already been cached and the ranges of bytes that will need to be\n+     * retrieved from a source. This is tracked in {@link CacheEntry} by {@link SparseFileTracker} which requires to know the length of the\n+     * file to cache to correctly track ranges.\n+     * <p>\n+     * When an entry already exists in cache, the service uses the {@link SparseFileTracker} of the {@link CacheEntry} to know if it can\n+     * serve the read operations using one or more ranges of bytes already cached on local disk or if it needs to retrieve the bytes from\n+     * the source (supplied as a {@link IndexInput}) first, then writes the bytes locally before returning them as the result of the read\n+     * operation.\n+     *\n+     * @param filePath   the {@link Path} of the cached file\n+     * @param fileLength the length of the cached file (required to compute ranges of bytes)\n+     * @param fileSource supplies the {@link IndexInput} to read the bytes from in case they are not already in cache\n+     * @param position   the position in the cached file where to start reading bytes from\n+     * @param b          the array to read bytes into\n+     * @param off        the offset in the array to start storing bytes\n+     * @param len        the number of bytes to read\n+     *\n+     * @throws IOException if something went wrong\n+     */\n+    public void readFromCache(final Path filePath, final long fileLength, final CheckedSupplier<IndexInput, IOException> fileSource,\n+                              final long position, final byte[] b, int off, int len) throws IOException {\n+        long pos = position;\n+        while (len > 0 && lifecycleState() == Lifecycle.State.STARTED) {\n+            final CacheEntry cacheEntry = getOrAddToCache(filePath, fileLength, fileSource);\n+            if (cacheEntry.tryIncRef() == false) {\n+                continue;\n+            }\n+            try {\n+                final int read = cacheEntry.fetchRange(pos, b, off, len);\n+                logger.trace(() -> new ParameterizedMessage(\"read {} bytes of file [name:{}, length:{}] at position [{}] from cache\",\n+                    read, filePath.getFileName(), fileLength, position));\n+                pos += read;\n+                off += read;\n+                len -= read;\n+            } finally {\n+                cacheEntry.decRef();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Mark the given {@link CacheEntry} as evicted: no new read or write operation can be executed on the file on disk, which will be\n+     * deleted from disk once all on-going reads are processed.\n+     */\n+    private void markAsEvicted(final CacheEntry cacheEntry) {\n+        logger.trace(() -> new ParameterizedMessage(\"marking cache entry [{}] as evicted\", cacheEntry));\n+        cacheEntry.markAsEvicted();\n+    }\n+\n+    /**\n+     * Remove from cache all entries that match the given predicate.\n+     *\n+     * @param predicate the predicate to evaluate\n+     */\n+    public void removeFromCache(final Predicate<String> predicate) {\n+        if (invalidateOnShutdown) {\n+            for (String cacheKey : cache.keys()) {\n+                if (predicate.test(cacheKey)) {\n+                    cache.invalidate(cacheKey);\n+                }\n+            }\n+        }\n+        cache.refresh();\n+    }\n+\n+    /**\n+     * Computes the cache key associated to the given Lucene cached file\n+     *\n+     * @param cacheFile the cached file\n+     * @return the cache key\n+     */\n+    private static String toCacheKey(final Path cacheFile) {\n+        return cacheFile.toAbsolutePath().toString();\n+    }\n+\n+    private class CacheEntry implements RefCounted {\n+\n+        private final CheckedSupplier<IndexInput, IOException> source;\n+        private final SparseFileTracker tracker;\n+        private final FileChannel channel;\n+        private final int sizeOfRange;\n+        private final Path path;\n+\n+        private final AbstractRefCounted refCounter;\n+        private volatile boolean evicted;\n+\n+        CacheEntry(Path path, long length, CheckedSupplier<IndexInput, IOException> source, FileChannel channel, int sizeOfRange) {\n+            this.tracker = new SparseFileTracker(path.toString(), length);\n+            this.source = Objects.requireNonNull(source);\n+            this.channel = Objects.requireNonNull(channel);\n+            this.path = Objects.requireNonNull(path);\n+            this.sizeOfRange = sizeOfRange;\n+            this.refCounter = new AbstractRefCounted(path.toString()) {\n+                @Override\n+                protected void closeInternal() {\n+                    assert refCount() == 0;\n+                    IOUtils.closeWhileHandlingException(CacheEntry.this.channel);\n+                    assert evicted || invalidateOnShutdown == false;\n+                    if (evicted) {\n+                        logger.trace(() -> new ParameterizedMessage(\"deleting cache file [{}]\", path));\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            };\n+            this.evicted = false;\n+        }\n+\n+        private long estimateWeight() {\n+            final long lengthOfRanges = tracker.getLengthOfRanges();\n+            return (lengthOfRanges == 0L) ? sizeOfRange : lengthOfRanges;\n+        }\n+\n+        private synchronized void markAsEvicted() {\n+            if (evicted == false) {\n+                evicted = true;\n+                decRef();\n+            }\n+        }\n+\n+        @Override\n+        public synchronized boolean tryIncRef() {\n+            if (evicted) {\n+                return false;\n+            }\n+            return refCounter.tryIncRef();\n+        }\n+\n+        @Override\n+        public synchronized void incRef() {\n+            if (tryIncRef() == false) {\n+                throw new IllegalStateException(\"Failed to increment reference counter, cache entry is already evicted\");\n+            }\n+        }\n+\n+        @Override\n+        public synchronized void decRef() {\n+            refCounter.decRef();\n+        }\n+\n+        /**\n+         * Computes the start and the end of a range to which the given {@code position} belongs.\n+         *\n+         * @param position the reading position\n+         * @return the start and end range positions to fetch\n+         */\n+        private Tuple<Long, Long> computeRange(final long position) {\n+            final long start = (position / sizeOfRange) * sizeOfRange;\n+            return Tuple.tuple(start, Math.min(start + sizeOfRange, tracker.getLength()));\n+        }\n+\n+        /**\n+         * Fetch the range corresponding to the position from the cache.\n+         *\n+         * @param position the position to start reading bytes from.\n+         * @param buffer   the array to read bytes into\n+         * @param offset   the offset in the array to start storing bytes\n+         * @param length   the number of bytes to read\n+         * @return the number of bytes read\n+         *\n+         * @throws IOException if something went wrong\n+         */\n+        private  int fetchRange(final long position, final byte[] buffer, final int offset, final int length) throws IOException {\n+            final CompletableFuture<Integer> future = new CompletableFuture<>();\n+            assert refCounter.refCount() > 0;\n+\n+            final Tuple<Long, Long> range = computeRange(position);\n+            assert range.v2() - range.v1() <= sizeOfRange;\n+\n+            logger.trace(() -> new ParameterizedMessage(\"fetching range [{}-{}] of [{}]\", range.v1(), range.v2(), path.getFileName()));\n+\n+            // wait for the range to be available and read it from disk\n+            final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(range.v1(), range.v2(), new ActionListener<>() {\n+                @Override\n+                public void onResponse(Void ignored) {\n+                    try {\n+                        final ByteBuffer dst = ByteBuffer.wrap(buffer);\n+                        dst.position(offset);\n+                        dst.limit(Math.toIntExact(offset + Math.min(length, range.v2() - position)));\n+                        final int read = Channels.readFromFileChannel(channel, position, dst);\n+                        logger.trace(() -> new ParameterizedMessage(\"read [{}] bytes from [{}]\", read, path.getFileName()));\n+                        future.complete(read);\n+                    } catch (IOException e) {\n+                        future.completeExceptionally(e);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    logger.error(() ->\n+                        new ParameterizedMessage(\"failed to fetch range [{}-{}] of [{}]\", range.v1(), range.v2(), path.getFileName()), e);\n+                    future.completeExceptionally(e);\n+                }\n+            });\n+\n+            if (gaps.isEmpty() == false) {\n+                fetchMissingRanges(gaps);\n+            }\n+\n+            try {\n+                return future.get();\n+            } catch (InterruptedException| ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+                throw new IOException(\"Failed to fetch range [{}-{}] of [{}]: \" + toString(), e);\n+            }\n+        }\n+\n+        /**\n+         * Fetches all missing ranges\n+         *\n+         * @param gaps the missing ranges to fetch\n+         */\n+        private void fetchMissingRanges(final List<SparseFileTracker.Gap> gaps) {\n+            assert gaps.isEmpty() || gaps.size() == 1;\n+            for (SparseFileTracker.Gap gap : gaps) {\n+                threadPool.executor(ThreadPool.Names.GENERIC).execute(new AbstractRunnable() {\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        gap.onFailure(e);\n+                    }\n+\n+                    @Override\n+                    protected void doRun() throws Exception {\n+                        fetchAndWriteRange(gap.start, gap.end);\n+                        gap.onResponse(null);\n+                    }\n+                });\n+            }\n+        }\n+\n+        /**\n+         * Fetches a missing range from the cache entry's source and writes it into the sparse cached file.\n+         * <p>\n+         * Even though {@link SparseFileTracker} prevents the same range to be concurrently fetched and written to disk, this method\n+         * is synchronized for extra safety.\n+         *\n+         * @param start the range start to fetch\n+         * @param end   the range end to fetch\n+         * @throws IOException if something went wrong\n+         */\n+        private synchronized void fetchAndWriteRange(final long start, final long end) throws IOException {\n+            logger.trace(() -> new ParameterizedMessage(\"fetching missing range [{}-{}] from source [{}]\", start, end, source));\n+\n+            final int copyBufferSize = 8192;\n+            byte[] copyBuffer = new byte[copyBufferSize];\n+\n+            try (IndexInput input = source.get()) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                long remaining = end - start;\n+                long pos = start;\n+                while (remaining > 0) {\n+                    int len = (remaining < copyBufferSize) ? (int) remaining : copyBufferSize;\n+                    logger.trace(() -> new ParameterizedMessage(\"reading {} bytes from [{}]\", len, input));\n+                    input.readBytes(copyBuffer, 0, len);", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI1Mjc3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366252775", "bodyText": "I agree this is defeating the purpose of requesting large chunks of data from the underlying SearchableSnapshotIndexInput, which is today implemented as a BufferedIndexInput. With the current implementation only chunks of <<BufferedIndexInput's internal buffer size>> are requested (ie 1024).\nI think that the right think to do is to have SearchableSnapshotIndexInput extends IndexInput directly and maintains an internal InputStream which is opened from the BlobContainer's readBlob(name, position, len) method where len is 32Mb.\nThe SearchableSnapshotIndexInput should take care of opening the InputStream from the required position (by using range bytes S3 requests), closing it and reopening it on IndexInput's seekings. It should also take care of draining/aborting the InputStream in case not all bytes were read.", "author": "tlrx", "createdAt": "2020-01-14T10:14:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc1NDE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc1ODIxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363758219", "bodyText": "We might exit this loop with len > 0; if we do so I think we have to throw an exception.", "author": "DaveCTurner", "createdAt": "2020-01-07T13:56:41Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);\n+\n+                    final CacheEntry cacheEntry = new CacheEntry(path, length, source, channel, CACHE_FILE_RANGE_SIZE);\n+                    success = true;\n+                    return cacheEntry;\n+                } catch (IOException e) {\n+                    logger.error(() -> new ParameterizedMessage(\"failed to create cache file [{}]\", path), e);\n+                    throw e;\n+                } finally {\n+                    if (success == false) {\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            });\n+        } catch (ExecutionException e) {\n+            throw ExceptionsHelper.convertToElastic(e);\n+        }\n+    }\n+\n+    /**\n+     * Reads bytes from a cached file.\n+     * <p>\n+     * The cached file is identified by its path ({@code filePath}). This path is used as a cache key by the cache service to know if an\n+     * entry already exists in cache for the given file.\n+     * <p>\n+     * When no entry exists in cache, the service creates a new entry for the file. Because cached files are created as sparse files on\n+     * disk, each cache entry needs to track the ranges of bytes that have already been cached and the ranges of bytes that will need to be\n+     * retrieved from a source. This is tracked in {@link CacheEntry} by {@link SparseFileTracker} which requires to know the length of the\n+     * file to cache to correctly track ranges.\n+     * <p>\n+     * When an entry already exists in cache, the service uses the {@link SparseFileTracker} of the {@link CacheEntry} to know if it can\n+     * serve the read operations using one or more ranges of bytes already cached on local disk or if it needs to retrieve the bytes from\n+     * the source (supplied as a {@link IndexInput}) first, then writes the bytes locally before returning them as the result of the read\n+     * operation.\n+     *\n+     * @param filePath   the {@link Path} of the cached file\n+     * @param fileLength the length of the cached file (required to compute ranges of bytes)\n+     * @param fileSource supplies the {@link IndexInput} to read the bytes from in case they are not already in cache\n+     * @param position   the position in the cached file where to start reading bytes from\n+     * @param b          the array to read bytes into\n+     * @param off        the offset in the array to start storing bytes\n+     * @param len        the number of bytes to read\n+     *\n+     * @throws IOException if something went wrong\n+     */\n+    public void readFromCache(final Path filePath, final long fileLength, final CheckedSupplier<IndexInput, IOException> fileSource,\n+                              final long position, final byte[] b, int off, int len) throws IOException {\n+        long pos = position;\n+        while (len > 0 && lifecycleState() == Lifecycle.State.STARTED) {", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0NjY2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366246669", "bodyText": "Done in this change", "author": "tlrx", "createdAt": "2020-01-14T10:01:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc1ODIxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc2MTg3NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363761874", "bodyText": "I'm concerned about thrashing here. We're trying to handle the case where the entry is evicted between getOrAddToCache and tryIncRef, but if we have multiple threads all adding entries that get immediately evicted (might happen if the cache is too small and/or almost full) I think we could end up stuck here. Maybe it'd be better to give up on the cache in this case and read directly from the fileSource?", "author": "DaveCTurner", "createdAt": "2020-01-07T14:04:38Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        if (invalidateOnShutdown) { // NORELEASE Only for debug purpose as CacheEntry maintains a reference to the source dir\n+            cache.invalidateAll();\n+        }\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+    }\n+\n+    /**\n+     * Creates a new {@link CacheEntry} instance in cache which points to a specific sparse file on disk located within the cache directory.\n+     *\n+     * @param file   the Lucene cached file\n+     * @param length the length of the Lucene file to cache\n+     * @return a new {@link CacheEntry}\n+     */\n+    private CacheEntry getOrAddToCache(final Path file, final long length, final CheckedSupplier<IndexInput, IOException> source) {\n+        assert Files.notExists(file) : \"Lucene cached file exists \" + file;\n+        try {\n+            return cache.computeIfAbsent(toCacheKey(file), key -> {\n+                // generate a random UUID for the name of the cache file on disk\n+                final String uuid = UUIDs.randomBase64UUID();\n+                // resolve the cache file on disk w/ the expected cached file\n+                final Path path = file.getParent().resolve(uuid);\n+                assert Files.notExists(path) : \"cache file already exists \" + path;\n+\n+                boolean success = false;\n+                try {\n+                    logger.trace(() -> new ParameterizedMessage(\"creating new cache file for [{}] at [{}]\", file.getFileName(), path));\n+                    final FileChannel channel = FileChannel.open(path, CACHE_FILE_OPEN_OPTIONS);\n+\n+                    final CacheEntry cacheEntry = new CacheEntry(path, length, source, channel, CACHE_FILE_RANGE_SIZE);\n+                    success = true;\n+                    return cacheEntry;\n+                } catch (IOException e) {\n+                    logger.error(() -> new ParameterizedMessage(\"failed to create cache file [{}]\", path), e);\n+                    throw e;\n+                } finally {\n+                    if (success == false) {\n+                        IOUtils.deleteFilesIgnoringExceptions(path);\n+                    }\n+                }\n+            });\n+        } catch (ExecutionException e) {\n+            throw ExceptionsHelper.convertToElastic(e);\n+        }\n+    }\n+\n+    /**\n+     * Reads bytes from a cached file.\n+     * <p>\n+     * The cached file is identified by its path ({@code filePath}). This path is used as a cache key by the cache service to know if an\n+     * entry already exists in cache for the given file.\n+     * <p>\n+     * When no entry exists in cache, the service creates a new entry for the file. Because cached files are created as sparse files on\n+     * disk, each cache entry needs to track the ranges of bytes that have already been cached and the ranges of bytes that will need to be\n+     * retrieved from a source. This is tracked in {@link CacheEntry} by {@link SparseFileTracker} which requires to know the length of the\n+     * file to cache to correctly track ranges.\n+     * <p>\n+     * When an entry already exists in cache, the service uses the {@link SparseFileTracker} of the {@link CacheEntry} to know if it can\n+     * serve the read operations using one or more ranges of bytes already cached on local disk or if it needs to retrieve the bytes from\n+     * the source (supplied as a {@link IndexInput}) first, then writes the bytes locally before returning them as the result of the read\n+     * operation.\n+     *\n+     * @param filePath   the {@link Path} of the cached file\n+     * @param fileLength the length of the cached file (required to compute ranges of bytes)\n+     * @param fileSource supplies the {@link IndexInput} to read the bytes from in case they are not already in cache\n+     * @param position   the position in the cached file where to start reading bytes from\n+     * @param b          the array to read bytes into\n+     * @param off        the offset in the array to start storing bytes\n+     * @param len        the number of bytes to read\n+     *\n+     * @throws IOException if something went wrong\n+     */\n+    public void readFromCache(final Path filePath, final long fileLength, final CheckedSupplier<IndexInput, IOException> fileSource,\n+                              final long position, final byte[] b, int off, int len) throws IOException {\n+        long pos = position;\n+        while (len > 0 && lifecycleState() == Lifecycle.State.STARTED) {\n+            final CacheEntry cacheEntry = getOrAddToCache(filePath, fileLength, fileSource);\n+            if (cacheEntry.tryIncRef() == false) {\n+                continue;", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0ODYxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366248615", "bodyText": "You're right, and this is a good suggestion.\nI changed the test so that it randomly uses a smaller cache and changed the readFromCache() method so that it serves the read operation directly from the source if the picked up cache file is already evicted. It does a similar thing later in the process, if the cache file has been evicted just before writing the new range on disk.", "author": "tlrx", "createdAt": "2020-01-14T10:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc2MTg3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc2NDE2OA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r363764168", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files\n          \n          \n            \n                    // NORELEASE TODO clean up (or rebuild) cache from disk as a node crash may leave cached files", "author": "DaveCTurner", "createdAt": "2020-01-07T14:09:57Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedSupplier;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.AbstractRunnable;\n+import org.elasticsearch.common.util.concurrent.RefCounted;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(1, ByteSizeUnit.MB),                  // min\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN =\n+        Setting.boolSetting(\"searchable.snapshot.cache.invalidate_on_shutdown\", true, Setting.Property.NodeScope, Setting.Property.Dynamic);\n+\n+    private static final int CACHE_FILE_RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] CACHE_FILE_OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW, StandardOpenOption.SPARSE\n+    };\n+\n+    private static final Logger logger = LogManager.getLogger(CacheService.class);\n+\n+    private final Cache<String, CacheEntry> cache;\n+    private final ThreadPool threadPool;\n+\n+    private volatile boolean invalidateOnShutdown = SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN.get(Settings.EMPTY);\n+\n+    public CacheService(final Settings settings, final ClusterSettings clusterSettings, final ThreadPool threadPool) {\n+        this.cache = CacheBuilder.<String, CacheEntry>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.estimateWeight()) // TODO only evaluated on promotion/eviction...\n+            .removalListener(notification -> markAsEvicted(notification.getValue()))\n+            .build();\n+        clusterSettings.addSettingsUpdateConsumer(SNAPSHOT_CACHE_INVALIDATE_ON_SHUTDOWN, this::setInvalidateOnShutdown);\n+        this.threadPool = threadPool;\n+    }\n+\n+    private void setInvalidateOnShutdown(boolean invalidateOnShutdown) {\n+        this.invalidateOnShutdown = invalidateOnShutdown;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        // TODO clean up (or rebuild) cache from disk as a node crash may leave cached files", "originalCommit": "99194dc849c407806d9e75df5981909b23794cea", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bf277a3264f59c8a935204164651af4fd30147c0", "url": "https://github.com/elastic/elasticsearch/commit/bf277a3264f59c8a935204164651af4fd30147c0", "message": "Remove invalidate_on_shutdown", "committedDate": "2020-01-08T08:37:57Z", "type": "commit"}, {"oid": "e0a6d0142c4e29d5da2c135cece5a865c5c5d2d9", "url": "https://github.com/elastic/elasticsearch/commit/e0a6d0142c4e29d5da2c135cece5a865c5c5d2d9", "message": "Estimate weight using file length", "committedDate": "2020-01-08T09:19:44Z", "type": "commit"}, {"oid": "e24a74d68d670d7a46133c878c7bc01335956968", "url": "https://github.com/elastic/elasticsearch/commit/e24a74d68d670d7a46133c878c7bc01335956968", "message": "whitespaces", "committedDate": "2020-01-08T09:21:36Z", "type": "commit"}, {"oid": "f3a958bd37dde693fed8285aba4dd2a26f5e5d56", "url": "https://github.com/elastic/elasticsearch/commit/f3a958bd37dde693fed8285aba4dd2a26f5e5d56", "message": "Handle lifecycle", "committedDate": "2020-01-08T09:51:40Z", "type": "commit"}, {"oid": "43b59b442be392cde286e45e52677f3695159a65", "url": "https://github.com/elastic/elasticsearch/commit/43b59b442be392cde286e45e52677f3695159a65", "message": "Add small cache size + concurrent searches", "committedDate": "2020-01-14T09:54:42Z", "type": "commit"}, {"oid": "288fa21d87203372400b6d32a5842db9212ed0bc", "url": "https://github.com/elastic/elasticsearch/commit/288fa21d87203372400b6d32a5842db9212ed0bc", "message": "apply feedback", "committedDate": "2020-01-14T09:58:35Z", "type": "commit"}, {"oid": "b8597b86b176bccf2675a544038c97cd73878944", "url": "https://github.com/elastic/elasticsearch/commit/b8597b86b176bccf2675a544038c97cd73878944", "message": "Update x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java\n\nCo-Authored-By: David Turner <david.turner@elastic.co>", "committedDate": "2020-01-14T10:06:28Z", "type": "commit"}, {"oid": "bc783a42dd59dea89dbf1d423314eb0037e29924", "url": "https://github.com/elastic/elasticsearch/commit/bc783a42dd59dea89dbf1d423314eb0037e29924", "message": "Restore sparse flag", "committedDate": "2020-01-14T10:59:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzMTU4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366331582", "bodyText": "I think we clone and/or slice from different threads so we should make this threadsafe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        this.clones = new ArrayList<>();\n          \n          \n            \n                        this.clones = new CopyOnWriteArrayList<>();", "author": "DaveCTurner", "createdAt": "2020-01-14T13:17:54Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzMTcyNg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366331726", "bodyText": "I think we clone and/or slice from different threads so we should make this threadsafe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        clone.clones = new ArrayList<>();\n          \n          \n            \n                        clone.clones = new CopyOnWriteArrayList <>();", "author": "DaveCTurner", "createdAt": "2020-01-14T13:18:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzMjMzNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366332337", "bodyText": "Maybe use an AtomicBoolean for thread-safety, and then\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (closed == false) {\n          \n          \n            \n                        if (closed.compareAndSet(false, true)) {\n          \n      \n    \n    \n  \n\nProbably wise to assert closed.get() == false in the other methods too.", "author": "DaveCTurner", "createdAt": "2020-01-14T13:19:29Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();\n+            clone.releasable = null;\n+            clones.add(clone);\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            clones.add(slice);\n+            return slice;\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed == false) {", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzMzQzOQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366333439", "bodyText": "We can avoid this null check:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            if (releasable != null) {\n          \n          \n            \n                            Releasables.close(releasable);", "author": "DaveCTurner", "createdAt": "2020-01-14T13:21:54Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();\n+            clone.releasable = null;\n+            clones.add(clone);\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            clones.add(slice);\n+            return slice;\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed == false) {\n+                closed = true;\n+                if (releasable != null) {", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzNTg1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366335852", "bodyText": "I think this is always true.", "author": "DaveCTurner", "createdAt": "2020-01-14T13:27:05Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();\n+            clone.releasable = null;\n+            clones.add(clone);\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            clones.add(slice);\n+            return slice;\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed == false) {\n+                closed = true;\n+                if (releasable != null) {\n+                    Releasables.close(releasable);\n+                }\n+                if (clones != null) {", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzNTk4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366335986", "bodyText": "Maybe IOUtils.close(clones)?", "author": "DaveCTurner", "createdAt": "2020-01-14T13:27:22Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();\n+            clone.releasable = null;\n+            clones.add(clone);\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            clones.add(slice);\n+            return slice;\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed == false) {\n+                closed = true;\n+                if (releasable != null) {\n+                    Releasables.close(releasable);\n+                }\n+                if (clones != null) {\n+                    for (IndexInput clone : clones) {", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjMzNzM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r366337386", "bodyText": "I'm concerned that if we keep an IndexInput open for an extended period of time then we will accumulate a lot of clones here, all of which have been closed (or maybe just discarded). I think that e.g. with a compound segment we might keep it open for the life of the shard, and add at least one clone or slice for each search.", "author": "DaveCTurner", "createdAt": "2020-01-14T13:30:18Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        /** a releasable provided by the CacheService that must be released when this IndexInput is done with the cached file **/\n+        private @Nullable Releasable releasable;\n+        private List<IndexInput> clones;\n+        private boolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.clones = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] b, final int off, final int len) throws IOException {\n+            final long pos = getFilePointer() + offset;\n+            releasable =\n+                cacheService.readFromCache(file, fileLength, releasable, () -> in.openInput(fileName, ioContext), pos, b, off, len);\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.clones = new ArrayList<>();\n+            clone.releasable = null;\n+            clones.add(clone);", "originalCommit": "bc783a42dd59dea89dbf1d423314eb0037e29924", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "51852dafaba733e396c06fc7d6e75e0433274a5a", "url": "https://github.com/elastic/elasticsearch/commit/51852dafaba733e396c06fc7d6e75e0433274a5a", "message": "Apply David's feedback", "committedDate": "2020-01-20T11:50:49Z", "type": "commit"}, {"oid": "178b6c4c12ed24b67a1a97ba4bcbee62f3d4f64d", "url": "https://github.com/elastic/elasticsearch/commit/178b6c4c12ed24b67a1a97ba4bcbee62f3d4f64d", "message": "Merge branch 'feature/searchable-snapshots' into add-searchable-snapshots-cache-directory", "committedDate": "2020-01-20T11:51:58Z", "type": "commit"}, {"oid": "8645db3f4d31e747db9836a1f7b7099dde069a41", "url": "https://github.com/elastic/elasticsearch/commit/8645db3f4d31e747db9836a1f7b7099dde069a41", "message": "Merge branch 'feature/searchable-snapshots' into add-searchable-snapshots-cache-directory", "committedDate": "2020-01-20T12:58:05Z", "type": "commit"}, {"oid": "93d406496ac8e792e2fd910bf262f6eaab439756", "url": "https://github.com/elastic/elasticsearch/commit/93d406496ac8e792e2fd910bf262f6eaab439756", "message": "Fix bug in position + potential deadlock in getOrAcquire() + NPE in channel inc ref", "committedDate": "2020-01-20T16:29:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY1OTM0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368659341", "bodyText": "cacheFile may be null here if there's a concurrent eviction I think, but in that case we don't read directly.", "author": "DaveCTurner", "createdAt": "2020-01-20T17:22:16Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private volatile @Nullable CacheFile cacheFile;\n+        private volatile boolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile;\n+            if (currentCacheFile == null) {\n+                final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+                synchronized (this) {\n+                    if (cacheFile == null) {\n+                        if (newCacheFile.acquire(this)) {\n+                            cacheFile = newCacheFile;\n+                        }\n+                    }\n+                    currentCacheFile = cacheFile;\n+                }\n+            }\n+            return currentCacheFile;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            synchronized (this) {\n+                assert cacheFile == evictedCacheFile || cacheFile == null;\n+                releaseAndClear(evictedCacheFile);\n+            }\n+        }\n+\n+        private void releaseAndClear(final CacheFile cacheFile) {\n+            synchronized (this) {\n+                try {\n+                    cacheFile.release(this);\n+                } finally {\n+                    if (cacheFile == this.cacheFile) {\n+                        this.cacheFile = null;\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed) {\n+                return;\n+            }\n+            synchronized (this) {\n+                if (closed) {\n+                    return;\n+                }\n+                closed = true;\n+                if (cacheFile != null) {\n+                    releaseAndClear(cacheFile);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    final CacheFile.FileChannelRefCounted channelRef = cacheFile.getChannelRefCounter();", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODA1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868057", "bodyText": "Yes, this.cacheFile may be null and local variable not assigned as an exception should be thrown. I added an assertion to check non-nullity of the local cacheFile.", "author": "tlrx", "createdAt": "2020-01-21T08:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY1OTM0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2MDcwNA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368660704", "bodyText": "nit: always true.", "author": "DaveCTurner", "createdAt": "2020-01-20T17:26:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile FileChannelRefCounted channelRefCounter;\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannelRefCounted getChannelRefCounter() {\n+        return channelRefCounter;\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    if (newListeners.add(Objects.requireNonNull(listener)) == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = removed;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        if (channelRefCounter != null) {\n+            channelRefCounter.deleteAfterClose();\n+        } else {\n+            try {\n+                Files.deleteIfExists(file);\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        if (closed == false) {\n+            final Set<EvictionListener> evictionListeners = new HashSet<>();\n+            synchronized (this) {\n+                if (closed == false) {\n+                    closed = true;\n+                    evictionListeners.addAll(listeners);\n+                    refCounter.decRef();\n+                }\n+            }\n+            if (evictionListeners != null) {", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODA5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868099", "bodyText": "Right, this is a left over.", "author": "tlrx", "createdAt": "2020-01-21T08:40:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2MDcwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2MTE2NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368661164", "bodyText": "Can we assert this (and also that listener != null)?", "author": "DaveCTurner", "createdAt": "2020-01-20T17:27:31Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile FileChannelRefCounted channelRefCounter;\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannelRefCounted getChannelRefCounter() {\n+        return channelRefCounter;\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    if (newListeners.add(Objects.requireNonNull(listener)) == false) {", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODEyMg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868122", "bodyText": "\ud83d\udc4d", "author": "tlrx", "createdAt": "2020-01-21T08:40:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2MTE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2Mzk2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368663969", "bodyText": "Can we assert here that listeners.isEmpty() and that refCounter.refCount() == 0?", "author": "DaveCTurner", "createdAt": "2020-01-20T17:35:02Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile FileChannelRefCounted channelRefCounter;\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannelRefCounted getChannelRefCounter() {\n+        return channelRefCounter;\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    if (newListeners.add(Objects.requireNonNull(listener)) == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = removed;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        if (channelRefCounter != null) {\n+            channelRefCounter.deleteAfterClose();\n+        } else {\n+            try {\n+                Files.deleteIfExists(file);\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        if (closed == false) {\n+            final Set<EvictionListener> evictionListeners = new HashSet<>();\n+            synchronized (this) {\n+                if (closed == false) {\n+                    closed = true;\n+                    evictionListeners.addAll(listeners);\n+                    refCounter.decRef();\n+                }\n+            }\n+            if (evictionListeners != null) {\n+                evictionListeners.forEach(listener -> listener.onEviction(this));\n+            }", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODE1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868152", "bodyText": "I don't think we can? The close() method does not clean up the list of listeners but instead captures the registered listeners at closing time (end set the closed flag so that no new listeners can be registered). This is the duty of each listener to release and remove itself once it is notified of the eviction (and once the listener is done with any cache file resource)", "author": "tlrx", "createdAt": "2020-01-21T08:40:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2Mzk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2NTQzMg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368665432", "bodyText": "This doesn't quite track the cache size since we do not delete the file until all the fetchers have finished. I think it's ok for now (fetchers should complete pretty quickly) but let's note it with a // NORELEASE to make sure we don't forget.", "author": "DaveCTurner", "createdAt": "2020-01-20T17:38:58Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(0, ByteSizeUnit.BYTES),               // min // NORELEASE\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    private final Cache<String, CacheFile> cache;\n+\n+    public CacheService(final Settings settings) {\n+        this.cache = CacheBuilder.<String, CacheFile>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.getLength())\n+            .removalListener(notification -> Releasables.closeWhileHandlingException(notification.getValue()))", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODE5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868191", "bodyText": "Good suggestion.", "author": "tlrx", "createdAt": "2020-01-21T08:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2NTQzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2NzcxMA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368667710", "bodyText": "I think this should be synchronized since it accesses channelRefCounter repeatedly.", "author": "DaveCTurner", "createdAt": "2020-01-20T17:45:35Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile FileChannelRefCounted channelRefCounter;\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannelRefCounted getChannelRefCounter() {\n+        return channelRefCounter;\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    if (newListeners.add(Objects.requireNonNull(listener)) == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = removed;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        if (channelRefCounter != null) {\n+            channelRefCounter.deleteAfterClose();\n+        } else {\n+            try {\n+                Files.deleteIfExists(file);\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        if (closed == false) {\n+            final Set<EvictionListener> evictionListeners = new HashSet<>();\n+            synchronized (this) {\n+                if (closed == false) {\n+                    closed = true;\n+                    evictionListeners.addAll(listeners);\n+                    refCounter.decRef();\n+                }\n+            }\n+            if (evictionListeners != null) {\n+                evictionListeners.forEach(listener -> listener.onEviction(this));\n+            }\n+        }\n+        assert invariant();\n+    }\n+\n+    private void maybeOpenFileChannel(Set<EvictionListener> listeners) throws IOException {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 1) {\n+            assert channelRefCounter == null;\n+            channelRefCounter = new FileChannelRefCounted(file);\n+        }\n+    }\n+\n+    private void maybeCloseFileChannel(Set<EvictionListener> listeners) {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 0) {\n+            final FileChannelRefCounted oldFileChannel = channelRefCounter;\n+            channelRefCounter = null;\n+            if (oldFileChannel != null) {\n+                oldFileChannel.decRef();\n+            }\n+        }\n+    }\n+\n+    private boolean invariant() {", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODIyNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868227", "bodyText": "Good catch. I also added a few more assertions on file existence.", "author": "tlrx", "createdAt": "2020-01-21T08:40:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY2NzcxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjI3OA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368672278", "bodyText": "I got a test failure here due to WindowsFS rejecting deletes of files that are still open. I'm not quite sure why yet.", "author": "DaveCTurner", "createdAt": "2020-01-20T17:59:38Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile FileChannelRefCounted channelRefCounter;\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannelRefCounted getChannelRefCounter() {\n+        return channelRefCounter;\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    if (newListeners.add(Objects.requireNonNull(listener)) == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = removed;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        if (channelRefCounter != null) {\n+            channelRefCounter.deleteAfterClose();\n+        } else {\n+            try {\n+                Files.deleteIfExists(file);", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODY3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868671", "bodyText": "Raaaah. I ran the tests quite many times over the weekend and didn't get such failure. I'll try to reproduce and let you know my findings.", "author": "tlrx", "createdAt": "2020-01-21T08:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjU2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368672561", "bodyText": "I got a test failure here, but there was nothing to suggest why in the log:\njava.lang.AssertionError: \nExpected: <4003 hits>\n     but: was <3564 hits>\nExpected :<4003 hits>\nActual   :<3564 hits>\n <Click to see difference>\n\n\n\tat __randomizedtesting.SeedInfo.seed([D2D6A1BF6B71726C:58B5D6C4F8E623DF]:0)\n\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n\tat org.junit.Assert.assertThat(Assert.java:956)\n\tat org.junit.Assert.assertThat(Assert.java:923)\n\tat org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsIntegTests.assertRecovered(SearchableSnapshotsIntegTests.java:197)\n\tat org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsIntegTests.testCreateAndRestoreSearchableSnapshot(SearchableSnapshotsIntegTests.java:134)", "author": "DaveCTurner", "createdAt": "2020-01-20T18:00:38Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java", "diffHunk": "@@ -148,5 +185,16 @@ private void assertRecovered(String indexName, TotalHits originalAllHits, TotalH\n                     lessThanOrEqualTo(1)); // we make a new commit so we write a new `segments_n` file\n             }\n         }\n+\n+        for (int i = 0; i < threads.length; i++) {\n+            threads[i].join();\n+\n+            final TotalHits allTotalHits = allHits.get(i);\n+            final TotalHits barTotalHits = barHits.get(i);\n+\n+            logger.info(\"--> thread #{} has [{}] hits in total, of which [{}] match the query\", i, allTotalHits, barTotalHits);\n+            assertThat(allTotalHits, equalTo(originalAllHits));\n+            assertThat(barTotalHits, equalTo(originalBarHits));", "originalCommit": "93d406496ac8e792e2fd910bf262f6eaab439756", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg2ODg2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368868869", "bodyText": "Is it a different failure from #50693 (comment) or a consequence of it?", "author": "tlrx", "createdAt": "2020-01-21T08:41:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjU2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODg3MTE0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r368871143", "bodyText": "They weren't obviously related. I found this one when re-running the tests in a loop to try and reproduce the one you linked.", "author": "DaveCTurner", "createdAt": "2020-01-21T08:46:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjU2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY5OTc2NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369699764", "bodyText": "I have been unable to reproduce this and suspect it might be because I wasn't running the tests on a completely clean checkout.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:29:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODY3MjU2MQ=="}], "type": "inlineReview"}, {"oid": "7211c22bd3292ee655f9a27bc551b62acb7a932f", "url": "https://github.com/elastic/elasticsearch/commit/7211c22bd3292ee655f9a27bc551b62acb7a932f", "message": "Apply David's early feedback", "committedDate": "2020-01-21T08:39:36Z", "type": "commit"}, {"oid": "e1f5450836c13746e828337e7a7e8e69251606b3", "url": "https://github.com/elastic/elasticsearch/commit/e1f5450836c13746e828337e7a7e8e69251606b3", "message": "Really?!", "committedDate": "2020-01-21T09:06:17Z", "type": "commit"}, {"oid": "f0bf0e6d51794f87de3ac7668d56afbe6c533509", "url": "https://github.com/elastic/elasticsearch/commit/f0bf0e6d51794f87de3ac7668d56afbe6c533509", "message": "Fix setting name", "committedDate": "2020-01-22T10:33:14Z", "type": "commit"}, {"oid": "15a8698678f9b3ba2898e9fea41681276b0b9399", "url": "https://github.com/elastic/elasticsearch/commit/15a8698678f9b3ba2898e9fea41681276b0b9399", "message": "Remove FileChannelRefCount and manage FileChannel within CacheFile", "committedDate": "2020-01-22T10:33:28Z", "type": "commit"}, {"oid": "a1b4277097f89c5e0b1bdb62ed2902247ae84861", "url": "https://github.com/elastic/elasticsearch/commit/a1b4277097f89c5e0b1bdb62ed2902247ae84861", "message": "Add CacheBufferedIndexInputTests", "committedDate": "2020-01-22T10:33:36Z", "type": "commit"}, {"oid": "cea36b246a7fe96a54a2b55b716686ef975a4f16", "url": "https://github.com/elastic/elasticsearch/commit/cea36b246a7fe96a54a2b55b716686ef975a4f16", "message": "Merge branch 'feature/searchable-snapshots' into add-searchable-snapshots-cache-directory", "committedDate": "2020-01-22T11:39:14Z", "type": "commit"}, {"oid": "67f7b9a5d8c86cda5983f5aa3fc4e6ed9a89f2a4", "url": "https://github.com/elastic/elasticsearch/commit/67f7b9a5d8c86cda5983f5aa3fc4e6ed9a89f2a4", "message": "Prevent cache invalidation to trigger more gets on shutdown", "committedDate": "2020-01-22T15:03:58Z", "type": "commit"}, {"oid": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "url": "https://github.com/elastic/elasticsearch/commit/5efa91f393874efacbafa5f46c1d4519eda8a3c0", "message": "Also on removeFromCache", "committedDate": "2020-01-22T15:12:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY5NDE2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369694165", "bodyText": "This is (almost always) going to be smaller than the 1KB buffer, so we will (almost always) be avoiding doing any interesting buffer-refilling within a single input. Suggest this (and maybe fewer iterations if it turns out too slow):\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        byte[] input = randomUnicodeOfLength(randomIntBetween(1, 1000)).getBytes(StandardCharsets.UTF_8);\n          \n          \n            \n                        byte[] input = randomUnicodeOfLength(randomIntBetween(1, 100000)).getBytes(StandardCharsets.UTF_8);", "author": "DaveCTurner", "createdAt": "2020-01-22T17:18:47Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheBufferedIndexInputTests.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.apache.lucene.store.IndexOutput;\n+import org.elasticsearch.common.lucene.store.ESIndexInputTestCase;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Path;\n+\n+public class CacheBufferedIndexInputTests extends ESIndexInputTestCase {\n+\n+    private Directory directory;\n+    private CacheService cacheService;\n+    private CacheDirectory cacheDirectory;\n+\n+    @Before\n+    public void setUpCache() throws IOException {\n+        final Settings.Builder cacheSettings = Settings.builder();\n+        if (randomBoolean()) {\n+            cacheSettings.put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(),\n+                new ByteSizeValue(randomIntBetween(1, 100), randomFrom(ByteSizeUnit.BYTES, ByteSizeUnit.KB, ByteSizeUnit.MB)));\n+        }\n+        cacheService = new CacheService(cacheSettings.build());\n+        cacheService.start();\n+\n+        final Path tmpDir = createTempDir();\n+        directory = FSDirectory.open(tmpDir.resolve(\"source\"));\n+        cacheDirectory = new CacheDirectory(directory, cacheService, tmpDir.resolve(\"cache\"));\n+    }\n+\n+    @After\n+    public void tearDownCache() throws IOException {\n+        cacheDirectory.close();\n+        cacheService.close();\n+    }\n+\n+    private IndexInput createIndexInput(final byte[] input) throws IOException {\n+        final String fileName = randomAlphaOfLength(10);\n+        final IOContext context = newIOContext(random());\n+        try (IndexOutput indexOutput = directory.createOutput(fileName, context)) {\n+            indexOutput.writeBytes(input, input.length);\n+        }\n+        return cacheDirectory.openInput(fileName, context);\n+    }\n+\n+    public void testRandomReads() throws IOException {\n+        for (int i = 0; i < 100; i++) {\n+            byte[] input = randomUnicodeOfLength(randomIntBetween(1, 1000)).getBytes(StandardCharsets.UTF_8);", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MTkwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370171909", "bodyText": "I agree. I increased the length to 100000 and decreased the number of iterations to 5 in 610affd", "author": "tlrx", "createdAt": "2020-01-23T15:05:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY5NDE2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY5Nzg3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369697871", "bodyText": "Could we add some instrumentation around this Directory so we can assert some things about how the CacheFile interacts with the underlying directory - e.g. if the cache is large enough then we only download each range once, and we always read exactly RANGE_SIZE, sequentially, from each input?", "author": "DaveCTurner", "createdAt": "2020-01-22T17:25:59Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheBufferedIndexInputTests.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FSDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.apache.lucene.store.IndexOutput;\n+import org.elasticsearch.common.lucene.store.ESIndexInputTestCase;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Path;\n+\n+public class CacheBufferedIndexInputTests extends ESIndexInputTestCase {\n+\n+    private Directory directory;\n+    private CacheService cacheService;\n+    private CacheDirectory cacheDirectory;\n+\n+    @Before\n+    public void setUpCache() throws IOException {\n+        final Settings.Builder cacheSettings = Settings.builder();\n+        if (randomBoolean()) {\n+            cacheSettings.put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(),\n+                new ByteSizeValue(randomIntBetween(1, 100), randomFrom(ByteSizeUnit.BYTES, ByteSizeUnit.KB, ByteSizeUnit.MB)));\n+        }\n+        cacheService = new CacheService(cacheSettings.build());\n+        cacheService.start();\n+\n+        final Path tmpDir = createTempDir();\n+        directory = FSDirectory.open(tmpDir.resolve(\"source\"));", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MTk2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370171961", "bodyText": "This is a very good idea. I pushed 610affd: it's a bit verbose but it does check ranges fetched from the underlying index input when the file fits in the cache (when it does not fit there's no real predictable behavior about how bytes are read)", "author": "tlrx", "createdAt": "2020-01-23T15:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY5Nzg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjAxNg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369702016", "bodyText": "Let's trust our assertions here.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                final boolean added = newListeners.add(Objects.requireNonNull(listener));\n          \n          \n            \n                                final boolean added = newListeners.add(listener);", "author": "DaveCTurner", "createdAt": "2020-01-22T17:34:28Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjAxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172019", "bodyText": "I pushed 573f7ab", "author": "tlrx", "createdAt": "2020-01-23T15:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjIwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369702205", "bodyText": "Let's trust our assertions here and drop this check.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    throw new IllegalStateException(\"Cannot add the same listener twice\");", "author": "DaveCTurner", "createdAt": "2020-01-22T17:34:48Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));\n+                    assert added : \"listener already exists \" + listener;\n+                    if (added == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjA1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172052", "bodyText": "I pushed 573f7ab", "author": "tlrx", "createdAt": "2020-01-23T15:05:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjk5NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369702994", "bodyText": "This is unused. However I think the name is valuable as part of the toString() output. From my experience working with these things so far I think it would also be very useful to include the ShardId in the toString() output as well.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:36:15Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjA4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172086", "bodyText": "I removed the method in 50f36bb.\n\nFrom my experience working with these things so far I think it would also be very useful to include the ShardId in the toString() output as well.\n\nI agree. If that's OK I'd like to address this along with another TODO I have in a follow up PR.", "author": "tlrx", "createdAt": "2020-01-23T15:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjk5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkzODAwMg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r372938002", "bodyText": "From my experience working with these things so far I think it would also be very useful to include the ShardId in the toString() output as well.\n\nThis is addressed in #51669", "author": "tlrx", "createdAt": "2020-01-30T13:11:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMjk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMzE5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369703199", "bodyText": "This is unused.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:36:37Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjEyOA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172128", "bodyText": "Removed in 573f7ab", "author": "tlrx", "createdAt": "2020-01-23T15:06:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwMzE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNDkzMg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369704932", "bodyText": "I found the close and closeInternal names to be confusing given their different meanings. I think there's no need for these names and we can describe their meanings better:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private void closeInternal() {\n          \n          \n            \n                private void finishEviction() {", "author": "DaveCTurner", "createdAt": "2020-01-22T17:39:55Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));\n+                    assert added : \"listener already exists \" + listener;\n+                    if (added == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        assert listener != null;\n+\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                final boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                assert removed : \"listener does not exist \" + listener;\n+                if (removed == false) {\n+                    throw new IllegalStateException(\"Cannot remove an unknown listener\");\n+                }\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = true;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjE2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172165", "bodyText": "This is a good suggestion, I pushed 02480db", "author": "tlrx", "createdAt": "2020-01-23T15:06:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNDkzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNTE4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369705186", "bodyText": "I found the close and closeInternal names to be confusing given their different meanings. I think there's no need for these names and we can describe their meanings better. If we dropped the Releasable interface, which seems unnecessary, we can call this something else:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void close() {\n          \n          \n            \n                public void startEviction() {", "author": "DaveCTurner", "createdAt": "2020-01-22T17:40:30Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));\n+                    assert added : \"listener already exists \" + listener;\n+                    if (added == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        assert listener != null;\n+\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                final boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                assert removed : \"listener does not exist \" + listener;\n+                if (removed == false) {\n+                    throw new IllegalStateException(\"Cannot remove an unknown listener\");\n+                }\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = true;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        assert listeners.isEmpty();\n+        assert channel == null;\n+        try {\n+            Files.deleteIfExists(file);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjIwNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172207", "bodyText": "\ud83d\udc4d  I pushed 02480db", "author": "tlrx", "createdAt": "2020-01-23T15:06:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNTE4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNjUzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369706531", "bodyText": "I think this is always true, so we can assert it instead.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:43:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));\n+                    assert added : \"listener already exists \" + listener;\n+                    if (added == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        assert listener != null;\n+\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                final boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                assert removed : \"listener does not exist \" + listener;\n+                if (removed == false) {\n+                    throw new IllegalStateException(\"Cannot remove an unknown listener\");\n+                }\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = true;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        assert listeners.isEmpty();\n+        assert channel == null;\n+        try {\n+            Files.deleteIfExists(file);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        if (closed == false) {\n+            final Set<EvictionListener> evictionListeners = new HashSet<>();\n+            synchronized (this) {\n+                if (closed == false) {\n+                    closed = true;\n+                    evictionListeners.addAll(listeners);\n+                    refCounter.decRef();\n+                }\n+            }\n+            evictionListeners.forEach(listener -> listener.onEviction(this));\n+        }\n+        assert invariant();\n+    }\n+\n+    private void maybeOpenFileChannel(Set<EvictionListener> listeners) throws IOException {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 1) {\n+            assert channel == null;\n+            channel = FileChannel.open(file, OPEN_OPTIONS);\n+        }\n+    }\n+\n+    private void maybeCloseFileChannel(Set<EvictionListener> listeners) {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 0) {\n+            if (channel != null) {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjM0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172345", "bodyText": "I think so so I pushed 628e487", "author": "tlrx", "createdAt": "2020-01-23T15:06:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNjUzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNzA2NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369707064", "bodyText": "\ud83d\udc4d I like how clean this invariant is right now.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:44:16Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.closeInternal();\n+        }\n+    };\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;\n+\n+    CacheFile(String name, long length, Path file) {\n+        this.tracker = new SparseFileTracker(file.toString(), length);\n+        this.name = Objects.requireNonNull(name);\n+        this.file = Objects.requireNonNull(file);\n+        this.listeners = new HashSet<>();\n+        this.closed = false;\n+        assert invariant();\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public long getLength() {\n+        return tracker.getLength();\n+    }\n+\n+    public Path getFile() {\n+        return file;\n+    }\n+\n+    @Nullable\n+    public FileChannel getChannel() {\n+        return channel;\n+    }\n+\n+    public synchronized boolean isAcquired(final EvictionListener listener) {\n+        return listeners.contains(listener);\n+    }\n+\n+    public boolean acquire(final EvictionListener listener) throws IOException {\n+        assert listener != null;\n+\n+        ensureOpen();\n+        boolean success = false;\n+        if (refCounter.tryIncRef()) {\n+            synchronized (this) {\n+                try {\n+                    ensureOpen();\n+                    final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                    final boolean added = newListeners.add(Objects.requireNonNull(listener));\n+                    assert added : \"listener already exists \" + listener;\n+                    if (added == false) {\n+                        throw new IllegalStateException(\"Cannot add the same listener twice\");\n+                    }\n+                    maybeOpenFileChannel(newListeners);\n+                    listeners = Collections.unmodifiableSet(newListeners);\n+                    success = true;\n+                } finally {\n+                    if (success == false) {\n+                        refCounter.decRef();\n+                    }\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    public boolean release(final EvictionListener listener) {\n+        assert listener != null;\n+\n+        boolean success = false;\n+        synchronized (this) {\n+            try {\n+                final Set<EvictionListener> newListeners = new HashSet<>(listeners);\n+                final boolean removed = newListeners.remove(Objects.requireNonNull(listener));\n+                assert removed : \"listener does not exist \" + listener;\n+                if (removed == false) {\n+                    throw new IllegalStateException(\"Cannot remove an unknown listener\");\n+                }\n+                maybeCloseFileChannel(newListeners);\n+                listeners = Collections.unmodifiableSet(newListeners);\n+                success = true;\n+            } finally {\n+                if (success) {\n+                    refCounter.decRef();\n+                }\n+            }\n+        }\n+        assert invariant();\n+        return success;\n+    }\n+\n+    private void closeInternal() {\n+        assert Thread.holdsLock(this);\n+        assert listeners.isEmpty();\n+        assert channel == null;\n+        try {\n+            Files.deleteIfExists(file);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        if (closed == false) {\n+            final Set<EvictionListener> evictionListeners = new HashSet<>();\n+            synchronized (this) {\n+                if (closed == false) {\n+                    closed = true;\n+                    evictionListeners.addAll(listeners);\n+                    refCounter.decRef();\n+                }\n+            }\n+            evictionListeners.forEach(listener -> listener.onEviction(this));\n+        }\n+        assert invariant();\n+    }\n+\n+    private void maybeOpenFileChannel(Set<EvictionListener> listeners) throws IOException {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 1) {\n+            assert channel == null;\n+            channel = FileChannel.open(file, OPEN_OPTIONS);\n+        }\n+    }\n+\n+    private void maybeCloseFileChannel(Set<EvictionListener> listeners) {\n+        assert Thread.holdsLock(this);\n+        if (listeners.size() == 0) {\n+            if (channel != null) {\n+                try {\n+                    channel.close();\n+                } catch (IOException e) {\n+                    throw new UncheckedIOException(\"Exception when closing channel\", e);\n+                } finally {\n+                    channel = null;\n+                }\n+            }\n+        }\n+    }\n+\n+    private synchronized boolean invariant() {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjM4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172382", "bodyText": "Thanks!", "author": "tlrx", "createdAt": "2020-01-23T15:06:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwNzA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwODE5MA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369708190", "bodyText": "There seems a risk that a stack of exceptions that's too tall, or maybe contains a loop, will lead to a StackOverflowException. Do we need to recurse unguardedly here, or can we put a bound on the number of times we must unwrap?", "author": "DaveCTurner", "createdAt": "2020-01-22T17:46:33Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Cache file evicted soon after acquisition\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheAcquireLock.acquire()) {\n+                        final FileChannel channel = cacheFile.getChannel();\n+                        if (channel == null) {\n+                            throw new AlreadyClosedException(\"Cache file evicted before accessing the file channel\");\n+                        }\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(channel, start, end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(channel, start, end))\n+                            .get();\n+                    }\n+                } catch (Exception e) {\n+                    if (isEvictionException(e) == false) {\n+                        throw new IOException(\"Fail to read data from cache\", e);\n+                    }\n+                    // cache file was evicted during the range fetching, read bytes directly from source\n+                    bytesRead += copySource(pos, pos + len, buffer, off);\n+\n+                } finally {\n+                    if (bytesRead >= length) {\n+                        // once all bytes are read, clones immediately release the last acquired cache file\n+                        if (isClone) {\n+                            if (cacheFile != null) {\n+                                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                                    if (this.cacheFile.compareAndSet(cacheFile, null)) {\n+                                        cacheFile.release(this);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+        }\n+\n+        int readCacheFile(FileChannel fc, long start, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        int writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            int bytesCopied = 0;\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, size);\n+                    fc.write(ByteBuffer.wrap(copyBuffer, 0, size), start + bytesCopied);\n+                    bytesCopied += size;\n+                    remaining -= size;\n+                }\n+            }\n+            return bytesCopied;\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            clone.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            clone.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+            clone.cacheFile = new AtomicReference<>();\n+            clone.closed = new AtomicBoolean(false);\n+            clone.isClone = true;\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            slice.isClone = true;\n+            return slice;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"CacheBufferedIndexInput{\" +\n+                \"fileName='\" + fileName + '\\'' +\n+                \", fileLength=\" + fileLength +\n+                \", offset=\" + offset +\n+                \", end=\" + end +\n+                \", length=\" + length() +\n+                \", clone=\" + isClone +\n+                \", position=\" + getFilePointer() +\n+                '}';\n+        }\n+\n+        private int copySource(long start, long end, byte[] buffer, int offset) throws IOException {\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+\n+            int bytesCopied = 0;\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int len = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, len);\n+                    System.arraycopy(copyBuffer, 0, buffer, offset + bytesCopied, len);\n+                    bytesCopied += len;\n+                    remaining -= len;\n+                }\n+            }\n+            return bytesCopied;\n+        }\n+    }\n+\n+    private static boolean isEvictionException(final Throwable t) {\n+        if (t instanceof AlreadyClosedException) {\n+            return true;\n+        } else if (t instanceof ExecutionException) {\n+            return isEvictionException(t.getCause());", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjQxNg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172416", "bodyText": "Right. I inlined this in 4668456 as there is no need for recursion here.", "author": "tlrx", "createdAt": "2020-01-23T15:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwODE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwOTM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369709386", "bodyText": "start is unused here.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:48:57Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Cache file evicted soon after acquisition\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheAcquireLock.acquire()) {\n+                        final FileChannel channel = cacheFile.getChannel();\n+                        if (channel == null) {\n+                            throw new AlreadyClosedException(\"Cache file evicted before accessing the file channel\");\n+                        }\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(channel, start, end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(channel, start, end))\n+                            .get();\n+                    }\n+                } catch (Exception e) {\n+                    if (isEvictionException(e) == false) {\n+                        throw new IOException(\"Fail to read data from cache\", e);\n+                    }\n+                    // cache file was evicted during the range fetching, read bytes directly from source\n+                    bytesRead += copySource(pos, pos + len, buffer, off);\n+\n+                } finally {\n+                    if (bytesRead >= length) {\n+                        // once all bytes are read, clones immediately release the last acquired cache file\n+                        if (isClone) {\n+                            if (cacheFile != null) {\n+                                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                                    if (this.cacheFile.compareAndSet(cacheFile, null)) {\n+                                        cacheFile.release(this);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+        }\n+\n+        int readCacheFile(FileChannel fc, long start, long end, long position, byte[] buffer, int offset, long length) throws IOException {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjQ0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172445", "bodyText": "Removed in cdc99d8", "author": "tlrx", "createdAt": "2020-01-23T15:06:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcwOTM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMTM2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369711366", "bodyText": "I think this wants to be larger in production, but maybe it would improve coverage if we could set it smaller in tests.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:52:58Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+\n+class CacheFile implements Releasable {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    private static final int RANGE_SIZE = 1 << 15;", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjQ4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172483", "bodyText": "I think that it should be configurable through a setting (similarly to xpack.searchable.snapshot.cache.size) and also overridden in tests to allow much smaller values (like cache size too, which has a min value of 0 in tests but the production setting should not allow that).\nI think it deserves a follow up PR, what do you think?", "author": "tlrx", "createdAt": "2020-01-23T15:06:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMTM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMjI5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369712299", "bodyText": "We don't use this return value, I think, so I think it's better not to return it to make it clear that this method copies all the data or dies trying.", "author": "DaveCTurner", "createdAt": "2020-01-22T17:54:47Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Cache file evicted soon after acquisition\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheAcquireLock.acquire()) {\n+                        final FileChannel channel = cacheFile.getChannel();\n+                        if (channel == null) {\n+                            throw new AlreadyClosedException(\"Cache file evicted before accessing the file channel\");\n+                        }\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(channel, start, end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(channel, start, end))\n+                            .get();\n+                    }\n+                } catch (Exception e) {\n+                    if (isEvictionException(e) == false) {\n+                        throw new IOException(\"Fail to read data from cache\", e);\n+                    }\n+                    // cache file was evicted during the range fetching, read bytes directly from source\n+                    bytesRead += copySource(pos, pos + len, buffer, off);\n+\n+                } finally {\n+                    if (bytesRead >= length) {\n+                        // once all bytes are read, clones immediately release the last acquired cache file\n+                        if (isClone) {\n+                            if (cacheFile != null) {\n+                                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                                    if (this.cacheFile.compareAndSet(cacheFile, null)) {\n+                                        cacheFile.release(this);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+        }\n+\n+        int readCacheFile(FileChannel fc, long start, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        int writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            int bytesCopied = 0;\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, size);\n+                    fc.write(ByteBuffer.wrap(copyBuffer, 0, size), start + bytesCopied);\n+                    bytesCopied += size;\n+                    remaining -= size;\n+                }\n+            }\n+            return bytesCopied;", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjUxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172519", "bodyText": "I pushed eb514d7", "author": "tlrx", "createdAt": "2020-01-23T15:06:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMjI5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMjY0OA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369712648", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                final int size = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length;\n          \n          \n            \n                                final int size = (remaining < copyBuffer.length) ? Math.toIntExact(remaining) : copyBuffer.length;", "author": "DaveCTurner", "createdAt": "2020-01-22T17:55:33Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Cache file evicted soon after acquisition\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheAcquireLock.acquire()) {\n+                        final FileChannel channel = cacheFile.getChannel();\n+                        if (channel == null) {\n+                            throw new AlreadyClosedException(\"Cache file evicted before accessing the file channel\");\n+                        }\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(channel, start, end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(channel, start, end))\n+                            .get();\n+                    }\n+                } catch (Exception e) {\n+                    if (isEvictionException(e) == false) {\n+                        throw new IOException(\"Fail to read data from cache\", e);\n+                    }\n+                    // cache file was evicted during the range fetching, read bytes directly from source\n+                    bytesRead += copySource(pos, pos + len, buffer, off);\n+\n+                } finally {\n+                    if (bytesRead >= length) {\n+                        // once all bytes are read, clones immediately release the last acquired cache file\n+                        if (isClone) {\n+                            if (cacheFile != null) {\n+                                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                                    if (this.cacheFile.compareAndSet(cacheFile, null)) {\n+                                        cacheFile.release(this);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+        }\n+\n+        int readCacheFile(FileChannel fc, long start, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        int writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            int bytesCopied = 0;\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? (int) remaining : copyBuffer.length;", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjU2NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172564", "bodyText": "I pushed 9449563", "author": "tlrx", "createdAt": "2020-01-23T15:06:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxMjY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxNDk2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r369714969", "bodyText": "I think clones need to release the last-acquired cache file if we bail out of this method by throwing an exception before completing the read?", "author": "DaveCTurner", "createdAt": "2020-01-22T18:00:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Cache file evicted soon after acquisition\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheAcquireLock.acquire()) {\n+                        final FileChannel channel = cacheFile.getChannel();\n+                        if (channel == null) {\n+                            throw new AlreadyClosedException(\"Cache file evicted before accessing the file channel\");\n+                        }\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(channel, start, end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(channel, start, end))\n+                            .get();\n+                    }\n+                } catch (Exception e) {\n+                    if (isEvictionException(e) == false) {\n+                        throw new IOException(\"Fail to read data from cache\", e);\n+                    }\n+                    // cache file was evicted during the range fetching, read bytes directly from source\n+                    bytesRead += copySource(pos, pos + len, buffer, off);\n+\n+                } finally {\n+                    if (bytesRead >= length) {", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3MjYxNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370172617", "bodyText": "If the exception is an instance of AlreadyClosedException it should be released as part of the eviction mechanism. But if it's not, then it need to be released I agree.\nI pushed 4668456 to always release cache files for clones in case of an exception is thrown.", "author": "tlrx", "createdAt": "2020-01-23T15:06:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTcxNDk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAwODY5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370008699", "bodyText": "If this is a clone of an IndexInput over a CacheFile that was evicted then this will repopulate the cache and open the file channel, and then when we release the CacheFile later on we'll close the file channel; subsequent reads from this clone will all open and close the file channel too, which I think will be too expensive. I think we need to get the root IndexInput to acquire the CacheFile so that we keep the file channel open between reeds.", "author": "DaveCTurner", "createdAt": "2020-01-23T09:27:34Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private ReleasableLock cacheEvictionLock;\n+        private ReleasableLock cacheAcquireLock;\n+        private AtomicBoolean closed;\n+        private boolean isClone;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+\n+            final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+            this.cacheEvictionLock = new ReleasableLock(readWriteLock.writeLock());\n+            this.cacheAcquireLock = new ReleasableLock(readWriteLock.readLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                CacheFile cacheFile = null;\n+                try {\n+                    cacheFile = getOrAcquire();", "originalCommit": "5efa91f393874efacbafa5f46c1d4519eda8a3c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYwNDQ5Mg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r370604492", "bodyText": "I agree - I pushed 262d7e1 in which clones/slices now re-acquire the CacheFile using the root IndexInput. This would reduce the number of eviction listeners and should prevent too frequent opens/closes on files (unless the cache is close to be full).", "author": "tlrx", "createdAt": "2020-01-24T12:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDAwODY5OQ=="}], "type": "inlineReview"}, {"oid": "eb514d71d0fff30423b4094f24a20977fee11948", "url": "https://github.com/elastic/elasticsearch/commit/eb514d71d0fff30423b4094f24a20977fee11948", "message": "Don't return int in writeCacheFile()", "committedDate": "2020-01-23T10:20:17Z", "type": "commit"}, {"oid": "9449563ceacbf7b5bf55d6722a645dd30047053e", "url": "https://github.com/elastic/elasticsearch/commit/9449563ceacbf7b5bf55d6722a645dd30047053e", "message": "Math.toIntExact(remaining)", "committedDate": "2020-01-23T10:23:34Z", "type": "commit"}, {"oid": "573f7ab0e0ff57b497b39e7db8d185e04774fb09", "url": "https://github.com/elastic/elasticsearch/commit/573f7ab0e0ff57b497b39e7db8d185e04774fb09", "message": "Trust assertion", "committedDate": "2020-01-23T10:28:31Z", "type": "commit"}, {"oid": "02480db1bc630d282fabdd20fcdd97debb00a674", "url": "https://github.com/elastic/elasticsearch/commit/02480db1bc630d282fabdd20fcdd97debb00a674", "message": "Rename close/closeInternal", "committedDate": "2020-01-23T10:34:45Z", "type": "commit"}, {"oid": "628e4878d5e3907842e4113a2df2554a2e5461dd", "url": "https://github.com/elastic/elasticsearch/commit/628e4878d5e3907842e4113a2df2554a2e5461dd", "message": "assert channel != null", "committedDate": "2020-01-23T10:46:44Z", "type": "commit"}, {"oid": "46684563802bc5a04a4d0756c6ccd0804a7c1c52", "url": "https://github.com/elastic/elasticsearch/commit/46684563802bc5a04a4d0756c6ccd0804a7c1c52", "message": "Release for clones", "committedDate": "2020-01-23T12:13:12Z", "type": "commit"}, {"oid": "cdc99d80700e430217535d3d92c31826d6be7762", "url": "https://github.com/elastic/elasticsearch/commit/cdc99d80700e430217535d3d92c31826d6be7762", "message": "Remove unused start", "committedDate": "2020-01-23T12:27:36Z", "type": "commit"}, {"oid": "50f36bbcae3e1efe07c5e7f0b1180cdea1e19579", "url": "https://github.com/elastic/elasticsearch/commit/50f36bbcae3e1efe07c5e7f0b1180cdea1e19579", "message": "remove unused", "committedDate": "2020-01-23T14:53:22Z", "type": "commit"}, {"oid": "610affddcbf6aa485b4937172c2917ac892aea29", "url": "https://github.com/elastic/elasticsearch/commit/610affddcbf6aa485b4937172c2917ac892aea29", "message": "More assertion with underlying index input", "committedDate": "2020-01-23T14:53:38Z", "type": "commit"}, {"oid": "262d7e1cb6e123a7853682a04aab27ffb3978419", "url": "https://github.com/elastic/elasticsearch/commit/262d7e1cb6e123a7853682a04aab27ffb3978419", "message": "Clones should acquire from root IndexInput + prevent file closing", "committedDate": "2020-01-24T12:07:50Z", "type": "commit"}, {"oid": "757dc6815af14099a6df4e52af9d3f7836803285", "url": "https://github.com/elastic/elasticsearch/commit/757dc6815af14099a6df4e52af9d3f7836803285", "message": "Merge branch 'feature/searchable-snapshots' into add-searchable-snapshots-cache-directory", "committedDate": "2020-01-25T14:46:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE0OTAwMg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371149002", "bodyText": "I think we don't need a special kind of lock here any more, synchronized (this) should be ok here?", "author": "DaveCTurner", "createdAt": "2020-01-27T10:01:26Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private @Nullable CacheBufferedIndexInput parent;\n+        private ReleasableLock cacheEvictionLock;", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyNzIwMw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371227203", "bodyText": "Right, synchronized blocks should be enough. I pushed 0aece73", "author": "tlrx", "createdAt": "2020-01-27T13:02:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE0OTAwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE0OTgyMw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371149823", "bodyText": "Suggest renaming this to evicted now.", "author": "DaveCTurner", "createdAt": "2020-01-27T10:03:07Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiConsumer;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+class CacheFile {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    static final int RANGE_SIZE = 1 << 15;\n+\n+    private static final StandardOpenOption[] OPEN_OPTIONS = new StandardOpenOption[]{\n+        StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE, StandardOpenOption.SPARSE\n+    };\n+\n+    private final AbstractRefCounted refCounter = new AbstractRefCounted(\"CacheFile\") {\n+        @Override\n+        protected void closeInternal() {\n+            CacheFile.this.finishEviction();\n+        }\n+    };\n+\n+    private final ReleasableLock evictionLock;\n+    private final ReleasableLock readLock;\n+\n+    private final SparseFileTracker tracker;\n+    private final String name;\n+    private final Path file;\n+\n+    private volatile Set<EvictionListener> listeners;\n+    private volatile FileChannel channel;\n+    private volatile boolean closed;", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyNzc2Mw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371227763", "bodyText": "Makes sense, I pushed 48fa923", "author": "tlrx", "createdAt": "2020-01-27T13:04:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE0OTgyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MTAxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371151011", "bodyText": "Suggest renaming to something like readDirectly", "author": "DaveCTurner", "createdAt": "2020-01-27T10:05:36Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private @Nullable CacheBufferedIndexInput parent;\n+        private ReleasableLock cacheEvictionLock;\n+        private AtomicBoolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+            this.cacheEvictionLock = new ReleasableLock(new ReentrantLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            assert parent == null : \"should only be called on non-cloned index inputs\";\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                currentCacheFile = cacheFile.get();\n+                if (currentCacheFile != null) {\n+                    return currentCacheFile;\n+                }\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                try {\n+                    final CacheFile cacheFile = (parent == null) ? getOrAcquire() : parent.getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Failed to acquire a non-evicted cache file\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheFile.fileLock()) {\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end))\n+                            .get();\n+                    }\n+                } catch (final Exception e) {\n+                    if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                        try {\n+                            // cache file was evicted during the range fetching, read bytes directly from source\n+                            bytesRead += copySource(pos, pos + len, buffer, off);\n+                            continue;\n+                        } catch (Exception inner) {\n+                            e.addSuppressed(inner);\n+                        }\n+                    }\n+                    throw new IOException(\"Fail to read data from cache\", e);\n+\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert parent == null || cacheFile.get() == null;\n+        }\n+\n+        int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                int bytesCopied = 0;\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? Math.toIntExact(remaining) : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, size);\n+                    fc.write(ByteBuffer.wrap(copyBuffer, 0, size), start + bytesCopied);\n+                    bytesCopied += size;\n+                    remaining -= size;\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.cacheFile = new AtomicReference<>();\n+            clone.closed = new AtomicBoolean(false);\n+            clone.parent = (this.parent != null ? this.parent : this);\n+            assert clone.parent.parent == null : \"parent must not be a clone\";\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {\n+            if (offset < 0 || length < 0 || offset + length > this.length()) {\n+                throw new IllegalArgumentException(\"slice() \" + sliceDescription + \" out of bounds: offset=\" + offset\n+                    + \",length=\" + length + \",fileLength=\" + this.length() + \": \" + this);\n+            }\n+            final CacheBufferedIndexInput slice = new CacheBufferedIndexInput(fileName, fileLength, ioContext,\n+                getFullSliceDescription(sliceDescription), this.offset + offset, length);\n+            slice.parent = (this.parent != null ? this.parent : this);\n+            assert slice.parent.parent == null : \"parent must not be a clone\";\n+            return slice;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"CacheBufferedIndexInput{\" +\n+                \"fileName='\" + fileName + '\\'' +\n+                \", fileLength=\" + fileLength +\n+                \", offset=\" + offset +\n+                \", end=\" + end +\n+                \", length=\" + length() +\n+                \", clone=\" + (parent != null) +\n+                \", position=\" + getFilePointer() +\n+                '}';\n+        }\n+\n+        private int copySource(long start, long end, byte[] buffer, int offset) throws IOException {", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyNzg1OA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371227858", "bodyText": "This is a good suggestion, thanks. I pushed 391633f", "author": "tlrx", "createdAt": "2020-01-27T13:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MTAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MTUxNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371151517", "bodyText": "I think we should extract 8192L as a named constant (used here and in copySource).", "author": "DaveCTurner", "createdAt": "2020-01-27T10:06:35Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private @Nullable CacheBufferedIndexInput parent;\n+        private ReleasableLock cacheEvictionLock;\n+        private AtomicBoolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+            this.cacheEvictionLock = new ReleasableLock(new ReentrantLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            assert parent == null : \"should only be called on non-cloned index inputs\";\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                currentCacheFile = cacheFile.get();\n+                if (currentCacheFile != null) {\n+                    return currentCacheFile;\n+                }\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                try {\n+                    final CacheFile cacheFile = (parent == null) ? getOrAcquire() : parent.getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Failed to acquire a non-evicted cache file\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheFile.fileLock()) {\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end))\n+                            .get();\n+                    }\n+                } catch (final Exception e) {\n+                    if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                        try {\n+                            // cache file was evicted during the range fetching, read bytes directly from source\n+                            bytesRead += copySource(pos, pos + len, buffer, off);\n+                            continue;\n+                        } catch (Exception inner) {\n+                            e.addSuppressed(inner);\n+                        }\n+                    }\n+                    throw new IOException(\"Fail to read data from cache\", e);\n+\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert parent == null || cacheFile.get() == null;\n+        }\n+\n+        int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyNzk0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371227945", "bodyText": "Sure, I added COPY_BUFFER_SIZE in 99a9968", "author": "tlrx", "createdAt": "2020-01-27T13:04:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MTUxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MjAwNg==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371152006", "bodyText": "Apparently this doesn't throw IOException.", "author": "DaveCTurner", "createdAt": "2020-01-27T10:07:41Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private @Nullable CacheBufferedIndexInput parent;\n+        private ReleasableLock cacheEvictionLock;\n+        private AtomicBoolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+            this.cacheEvictionLock = new ReleasableLock(new ReentrantLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            assert parent == null : \"should only be called on non-cloned index inputs\";\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                currentCacheFile = cacheFile.get();\n+                if (currentCacheFile != null) {\n+                    return currentCacheFile;\n+                }\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                try {\n+                    final CacheFile cacheFile = (parent == null) ? getOrAcquire() : parent.getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Failed to acquire a non-evicted cache file\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheFile.fileLock()) {\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end))\n+                            .get();\n+                    }\n+                } catch (final Exception e) {\n+                    if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                        try {\n+                            // cache file was evicted during the range fetching, read bytes directly from source\n+                            bytesRead += copySource(pos, pos + len, buffer, off);\n+                            continue;\n+                        } catch (Exception inner) {\n+                            e.addSuppressed(inner);\n+                        }\n+                    }\n+                    throw new IOException(\"Fail to read data from cache\", e);\n+\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert parent == null || cacheFile.get() == null;\n+        }\n+\n+        int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                int bytesCopied = 0;\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? Math.toIntExact(remaining) : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, size);\n+                    fc.write(ByteBuffer.wrap(copyBuffer, 0, size), start + bytesCopied);\n+                    bytesCopied += size;\n+                    remaining -= size;\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.cacheFile = new AtomicReference<>();\n+            clone.closed = new AtomicBoolean(false);\n+            clone.parent = (this.parent != null ? this.parent : this);\n+            assert clone.parent.parent == null : \"parent must not be a clone\";\n+            return clone;\n+        }\n+\n+        @Override\n+        public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyODA4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371228085", "bodyText": "Right -> 348fac8", "author": "tlrx", "createdAt": "2020-01-27T13:05:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1MjAwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1NDcyMw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371154723", "bodyText": "I don't follow this. We invalidate the cache when stopping and expected that in that case any attempts to get things from the cache would result in exceptions. Can you clarify?", "author": "DaveCTurner", "createdAt": "2020-01-27T10:13:38Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.cache.Cache;\n+import org.elasticsearch.common.cache.CacheBuilder;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.function.Predicate;\n+\n+/**\n+ * {@link CacheService} maintains a cache entry for all files read from cached searchable snapshot directories (see {@link CacheDirectory})\n+ */\n+public class CacheService extends AbstractLifecycleComponent {\n+\n+    public static final Setting<ByteSizeValue> SNAPSHOT_CACHE_SIZE_SETTING = Setting.byteSizeSetting(\"xpack.searchable.snapshot.cache.size\",\n+        new ByteSizeValue(1, ByteSizeUnit.GB),                  // TODO: size the default value according to disk space\n+        new ByteSizeValue(0, ByteSizeUnit.BYTES),               // min // NORELEASE\n+        new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES),  // max\n+        Setting.Property.NodeScope);\n+\n+    private final ReleasableLock cacheInvalidationLock;\n+    private final ReleasableLock cacheAccessLock;\n+    private final Cache<String, CacheFile> cache;\n+\n+    public CacheService(final Settings settings) {\n+        this.cache = CacheBuilder.<String, CacheFile>builder()\n+            .setMaximumWeight(SNAPSHOT_CACHE_SIZE_SETTING.get(settings).getBytes())\n+            .weigher((key, entry) -> entry.getLength())\n+            // NORELEASE This does not immediately free space on disk, as cache file are only deleted when all index inputs\n+            // are done with reading/writing the cache file\n+            .removalListener(notification -> IOUtils.closeWhileHandlingException(() -> notification.getValue().startEviction()))\n+            .build();\n+\n+        // Prevent new CacheFile objects to be added to the cache while the cache is being fully or partially invalidated\n+        // This can happen because CacheFile objects might execute listeners at eviction time, potentially forcing more\n+        // objects (like CacheDirectory's index inputs) to get new CacheFile.", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIzMjY5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371232691", "bodyText": "Hum, I'm confused too \ud83d\ude15 I remember I added this because of some kind of deadlock in integration tests (not in manual tests, though) where the full invalidation ran concurrently with some file accesses in Lucene directory (executed by nodes list shard stores actions). The invalidation blocked the cache but the file accesses tried to get/compute new entries in cache, resulting in deadlocks. I've ran more tests without these locks and everything is fine so I now think that the deadlocks were just a manifestation of some previous buggy code.", "author": "tlrx", "createdAt": "2020-01-27T13:15:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE1NDcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE2NTA0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371165041", "bodyText": "I think we should run tests with a smaller (and non-power-of-two) here just to be certain we have good coverage, and a larger range size in production. Can we make this something that gets passed in?\n(I thought I'd already made this comment but it seems to have been lost, sorry for the duplicate if so)", "author": "DaveCTurner", "createdAt": "2020-01-27T10:34:39Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheFile.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.common.CheckedBiConsumer;\n+import org.elasticsearch.common.CheckedBiFunction;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.util.concurrent.AbstractRefCounted;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+class CacheFile {\n+\n+    @FunctionalInterface\n+    public interface EvictionListener {\n+        void onEviction(CacheFile evictedCacheFile);\n+    }\n+\n+    static final int RANGE_SIZE = 1 << 15;", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyODExMA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371228110", "bodyText": "I thought I'd already made this comment but it seems to have been lost,\n\nYes you did and no not lost :) I replied in #50693 (comment) but basically I agree with you and I think it should be a another setting (we might want to play with cache range size in tests). It requires a bit of plumbing in tests so I proposed to do it in a follow up PR (but I can do it in this one if you prefer).", "author": "tlrx", "createdAt": "2020-01-27T13:05:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE2NTA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE2NzYyNw==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371167627", "bodyText": "I think this means that clones will share locks with their parent, but slices do not. Doesn't seem to be a problem, but it is inconsistent.", "author": "DaveCTurner", "createdAt": "2020-01-27T10:40:34Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheDirectory.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.searchablesnapshots.cache;\n+\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.apache.lucene.store.BufferedIndexInput;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.FilterDirectory;\n+import org.apache.lucene.store.IOContext;\n+import org.apache.lucene.store.IndexInput;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.SuppressForbidden;\n+import org.elasticsearch.common.io.Channels;\n+import org.elasticsearch.common.util.concurrent.ReleasableLock;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * {@link CacheDirectory} uses a {@link CacheService} to cache Lucene files provided by another {@link Directory}.\n+ */\n+public class CacheDirectory extends FilterDirectory {\n+\n+    private final CacheService cacheService;\n+    private final Path cacheDir;\n+\n+    public CacheDirectory(Directory in, CacheService cacheService, Path cacheDir) throws IOException {\n+        super(in);\n+        this.cacheService = Objects.requireNonNull(cacheService);\n+        this.cacheDir = Files.createDirectories(cacheDir);\n+    }\n+\n+    public void close() throws IOException {\n+        super.close();\n+        // Ideally we could let the cache evict/remove cached files by itself after the\n+        // directory has been closed.\n+        cacheService.removeFromCache(key -> key.startsWith(cacheDir.toString()));\n+    }\n+\n+    @Override\n+    public IndexInput openInput(final String name, final IOContext context) throws IOException {\n+        ensureOpen();\n+        return new CacheBufferedIndexInput(name, fileLength(name), context);\n+    }\n+\n+    public class CacheBufferedIndexInput extends BufferedIndexInput implements CacheFile.EvictionListener {\n+\n+        private final String fileName;\n+        private final long fileLength;\n+        private final Path file;\n+        private final IOContext ioContext;\n+        private final long offset;\n+        private final long end;\n+\n+        private @Nullable AtomicReference<CacheFile> cacheFile;\n+        private @Nullable CacheBufferedIndexInput parent;\n+        private ReleasableLock cacheEvictionLock;\n+        private AtomicBoolean closed;\n+\n+        CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext) {\n+            this(fileName, fileLength, ioContext, \"CachedBufferedIndexInput(\" + fileName + \")\", 0L, fileLength);\n+        }\n+\n+        private CacheBufferedIndexInput(String fileName, long fileLength, IOContext ioContext, String desc, long offset, long length) {\n+            super(desc, ioContext);\n+            this.fileName = fileName;\n+            this.fileLength = fileLength;\n+            this.file = cacheDir.resolve(fileName);\n+            this.ioContext = ioContext;\n+            this.offset = offset;\n+            this.end = offset + length;\n+            this.cacheFile = new AtomicReference<>();\n+            this.closed = new AtomicBoolean(false);\n+            this.cacheEvictionLock = new ReleasableLock(new ReentrantLock());\n+        }\n+\n+        @Override\n+        public long length() {\n+            return end - offset;\n+        }\n+\n+        @Nullable\n+        private CacheFile getOrAcquire() throws Exception {\n+            assert parent == null : \"should only be called on non-cloned index inputs\";\n+            CacheFile currentCacheFile = cacheFile.get();\n+            if (currentCacheFile != null) {\n+                return currentCacheFile;\n+            }\n+\n+            final CacheFile newCacheFile = cacheService.get(fileName, fileLength, file);\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                currentCacheFile = cacheFile.get();\n+                if (currentCacheFile != null) {\n+                    return currentCacheFile;\n+                }\n+                if (newCacheFile.acquire(this)) {\n+                    final CacheFile previousCacheFile = cacheFile.getAndSet(newCacheFile);\n+                    assert previousCacheFile == null;\n+                    return newCacheFile;\n+                }\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public void onEviction(final CacheFile evictedCacheFile) {\n+            try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                if (cacheFile.compareAndSet(evictedCacheFile, null)) {\n+                    evictedCacheFile.release(this);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void close() {\n+            if (closed.compareAndSet(false, true)) {\n+                try (ReleasableLock ignored = cacheEvictionLock.acquire()) {\n+                    final CacheFile currentCacheFile = cacheFile.getAndSet(null);\n+                    if (currentCacheFile != null) {\n+                        currentCacheFile.release(this);\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void readInternal(final byte[] buffer, final int offset, final int length) throws IOException {\n+            final long position = getFilePointer() + this.offset;\n+\n+            int bytesRead = 0;\n+            while (bytesRead < length) {\n+                final long pos = position + bytesRead;\n+                final int off = offset + bytesRead;\n+                final int len = length - bytesRead;\n+\n+                try {\n+                    final CacheFile cacheFile = (parent == null) ? getOrAcquire() : parent.getOrAcquire();\n+                    if (cacheFile == null) {\n+                        throw new AlreadyClosedException(\"Failed to acquire a non-evicted cache file\");\n+                    }\n+\n+                    try (ReleasableLock ignored = cacheFile.fileLock()) {\n+                        bytesRead += cacheFile.fetchRange(pos,\n+                            (start, end) -> readCacheFile(cacheFile.getChannel(), end, pos, buffer, off, len),\n+                            (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end))\n+                            .get();\n+                    }\n+                } catch (final Exception e) {\n+                    if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                        try {\n+                            // cache file was evicted during the range fetching, read bytes directly from source\n+                            bytesRead += copySource(pos, pos + len, buffer, off);\n+                            continue;\n+                        } catch (Exception inner) {\n+                            e.addSuppressed(inner);\n+                        }\n+                    }\n+                    throw new IOException(\"Fail to read data from cache\", e);\n+\n+                }\n+            }\n+            assert bytesRead == length : \"partial read operation, read [\" + bytesRead + \"] bytes of [\" + length + \"]\";\n+            assert parent == null || cacheFile.get() == null;\n+        }\n+\n+        int readCacheFile(FileChannel fc, long end, long position, byte[] buffer, int offset, long length) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            return Channels.readFromFileChannel(fc, position, buffer, offset, Math.toIntExact(Math.min(length, end - position)));\n+        }\n+\n+        @SuppressForbidden(reason = \"Use positional writes on purpose\")\n+        void writeCacheFile(FileChannel fc, long start, long end) throws IOException {\n+            assert assertFileChannelOpen(fc);\n+            final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(8192L, end - start))];\n+            try (IndexInput input = in.openInput(fileName, ioContext)) {\n+                if (start > 0) {\n+                    input.seek(start);\n+                }\n+                int bytesCopied = 0;\n+                long remaining = end - start;\n+                while (remaining > 0) {\n+                    final int size = (remaining < copyBuffer.length) ? Math.toIntExact(remaining) : copyBuffer.length;\n+                    input.readBytes(copyBuffer, 0, size);\n+                    fc.write(ByteBuffer.wrap(copyBuffer, 0, size), start + bytesCopied);\n+                    bytesCopied += size;\n+                    remaining -= size;\n+                }\n+            }\n+        }\n+\n+        @Override\n+        protected void seekInternal(long pos) throws IOException {\n+            if (pos > length()) {\n+                throw new EOFException(\"Reading past end of file [position=\" + pos + \", length=\" + length() + \"] for \" + toString());\n+            } else if (pos < 0L) {\n+                throw new IOException(\"Seeking to negative position [\" + pos + \"] for \" + toString());\n+            }\n+        }\n+\n+        @Override\n+        public CacheBufferedIndexInput clone() {\n+            final CacheBufferedIndexInput clone = (CacheBufferedIndexInput) super.clone();\n+            clone.cacheFile = new AtomicReference<>();\n+            clone.closed = new AtomicBoolean(false);\n+            clone.parent = (this.parent != null ? this.parent : this);\n+            assert clone.parent.parent == null : \"parent must not be a clone\";\n+            return clone;", "originalCommit": "757dc6815af14099a6df4e52af9d3f7836803285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTIyODE1NA==", "url": "https://github.com/elastic/elasticsearch/pull/50693#discussion_r371228154", "bodyText": "You're right, cloning object might be trappy sometimes. Since we moved to synchronized blocks it is not an issue anymore but thanks for pointing this.", "author": "tlrx", "createdAt": "2020-01-27T13:05:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTE2NzYyNw=="}], "type": "inlineReview"}, {"oid": "0aece738fb7941645aefeb475853a4410893ca5d", "url": "https://github.com/elastic/elasticsearch/commit/0aece738fb7941645aefeb475853a4410893ca5d", "message": "ReleasableLock -> synchronized", "committedDate": "2020-01-27T13:01:30Z", "type": "commit"}, {"oid": "48fa92350a887c78dc45ed51c05d24aecdb9f570", "url": "https://github.com/elastic/elasticsearch/commit/48fa92350a887c78dc45ed51c05d24aecdb9f570", "message": "closed -> evicted", "committedDate": "2020-01-27T13:01:30Z", "type": "commit"}, {"oid": "391633f5150655187589c25a3b4a1c2f706587cf", "url": "https://github.com/elastic/elasticsearch/commit/391633f5150655187589c25a3b4a1c2f706587cf", "message": "readDirectly", "committedDate": "2020-01-27T13:01:30Z", "type": "commit"}, {"oid": "99a99686fe426daa450742b9ac1c33f966978bc6", "url": "https://github.com/elastic/elasticsearch/commit/99a99686fe426daa450742b9ac1c33f966978bc6", "message": "COPY_BUFFER_SIZE", "committedDate": "2020-01-27T13:01:30Z", "type": "commit"}, {"oid": "348fac8f11221c3688dce0f7b9f30f47043de2b3", "url": "https://github.com/elastic/elasticsearch/commit/348fac8f11221c3688dce0f7b9f30f47043de2b3", "message": "Remove IOE", "committedDate": "2020-01-27T13:01:31Z", "type": "commit"}, {"oid": "f01d63272b5e8929ea7a20092a952faf4098a975", "url": "https://github.com/elastic/elasticsearch/commit/f01d63272b5e8929ea7a20092a952faf4098a975", "message": "Remove locking in CacheService", "committedDate": "2020-01-27T13:01:31Z", "type": "commit"}, {"oid": "f7cc4153adb20e3d2d0134ab7f092cff8ce01895", "url": "https://github.com/elastic/elasticsearch/commit/f7cc4153adb20e3d2d0134ab7f092cff8ce01895", "message": "Add CacheFileReference", "committedDate": "2020-01-27T14:32:09Z", "type": "commit"}]}