{"pr_number": 61966, "pr_title": "Write deprecation logs to a data stream", "pr_createdAt": "2020-09-04T08:34:55Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/61966", "timeline": [{"oid": "441aef1bff47d55d39290e9e89a2f0e781859044", "url": "https://github.com/elastic/elasticsearch/commit/441aef1bff47d55d39290e9e89a2f0e781859044", "message": "Write deprecation logs to a data stream\n\nBackport of #61484.\n\nCloses #46106. Implement a new log4j appender for deprecation logging, in\norder to write logs to a dedicated data stream. This is controlled by a new\nsetting, `cluster.deprecation_indexing.enabled`.", "committedDate": "2020-09-03T16:13:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU2ODA4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/61966#discussion_r483568082", "bodyText": "Can you add this to the body of the afterBulk() method to introspect if there are bulk item failures?\nlong items = request.numberOfActions();\nif (logger.isTraceEnabled()) {\n   logger.trace(\"indexed [{}] deprecation documents into [{}]\", items,\n   Arrays.stream(response.getItems())\n      .map(BulkItemResponse::getIndex)\n      .distinct()\n      .collect(Collectors.joining(\",\")));\n}\n\nif (response.hasFailures()) {\n   Map<String, String> failures = Arrays.stream(response.getItems())\n      .filter(BulkItemResponse::isFailed)\n      .collect(Collectors.toMap(BulkItemResponse::getId, BulkItemResponse::getFailureMessage));\n   logger.error(\"failures: [{}]\", failures);\n}", "author": "martijnvg", "createdAt": "2020-09-04T11:52:36Z", "path": "x-pack/plugin/deprecation/src/main/java/org/elasticsearch/xpack/deprecation/logging/DeprecationIndexingComponent.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.deprecation.logging;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.elasticsearch.action.bulk.BackoffPolicy;\n+import org.elasticsearch.action.bulk.BulkProcessor;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.logging.ESJsonLayout;\n+import org.elasticsearch.common.logging.Loggers;\n+import org.elasticsearch.common.logging.RateLimitingFilter;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * This component manages the construction and lifecycle of the {@link DeprecationIndexingAppender}.\n+ * It also starts and stops the appender\n+ */\n+public class DeprecationIndexingComponent extends AbstractLifecycleComponent implements ClusterStateListener {\n+    private static final Logger logger = LogManager.getLogger(DeprecationIndexingComponent.class);\n+\n+    public static final Setting<Boolean> WRITE_DEPRECATION_LOGS_TO_INDEX = Setting.boolSetting(\n+        \"cluster.deprecation_indexing.enabled\",\n+        false,\n+        Setting.Property.NodeScope,\n+        Setting.Property.Dynamic\n+    );\n+\n+    private final DeprecationIndexingAppender appender;\n+    private final BulkProcessor processor;\n+    private final RateLimitingFilter filter;\n+\n+    public DeprecationIndexingComponent(Client client, Settings settings) {\n+        this.processor = getBulkProcessor(new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN), settings);\n+        final Consumer<IndexRequest> consumer = this.processor::add;\n+\n+        final LoggerContext context = (LoggerContext) LogManager.getContext(false);\n+        final Configuration configuration = context.getConfiguration();\n+\n+        final ESJsonLayout ecsLayout = ESJsonLayout.newBuilder()\n+            .setType(\"deprecation\")\n+            // This matches the additional fields in the DeprecatedMessage class\n+            .setESMessageFields(\"x-opaque-id,data_stream.type,data_stream.datatype,data_stream.namespace,ecs.version\")\n+            .setConfiguration(configuration)\n+            .build();\n+\n+        this.filter = new RateLimitingFilter();\n+        this.appender = new DeprecationIndexingAppender(\"deprecation_indexing_appender\", filter, ecsLayout, consumer);\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        this.appender.start();\n+        Loggers.addAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        Loggers.removeAppender(LogManager.getLogger(\"org.elasticsearch.deprecation\"), this.appender);\n+        this.appender.stop();\n+    }\n+\n+    @Override\n+    protected void doClose() {\n+        this.processor.close();\n+    }\n+\n+    /**\n+     * Listens for changes to the cluster state, in order to know whether to toggle indexing\n+     * and to set the cluster UUID and node ID. These can't be set in the constructor because\n+     * the initial cluster state won't be set yet.\n+     *\n+     * @param event the cluster state event to process\n+     */\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        final ClusterState state = event.state();\n+        final boolean newEnabled = WRITE_DEPRECATION_LOGS_TO_INDEX.get(state.getMetadata().settings());\n+        if (appender.isEnabled() != newEnabled) {\n+            // We've flipped from disabled to enabled. Make sure we start with a clean cache of\n+            // previously-seen keys, otherwise we won't index anything.\n+            if (newEnabled) {\n+                this.filter.reset();\n+            }\n+            appender.setEnabled(newEnabled);\n+        }\n+    }\n+\n+    /**\n+     * Constructs a bulk processor for writing documents\n+     *\n+     * @param client   the client to use\n+     * @param settings the settings to use\n+     * @return an initialised bulk processor\n+     */\n+    private BulkProcessor getBulkProcessor(Client client, Settings settings) {\n+        final OriginSettingClient originSettingClient = new OriginSettingClient(client, ClientHelper.DEPRECATION_ORIGIN);\n+        final BulkProcessor.Listener listener = new DeprecationBulkListener();\n+\n+        // This configuration disables the size count and size thresholds,\n+        // and instead uses a scheduled flush only. This means that calling\n+        // processor.add() will not block the calling thread.\n+        return BulkProcessor.builder(originSettingClient::bulk, listener)\n+            .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(1000), 3))\n+            .setConcurrentRequests(Math.max(2, EsExecutors.allocatedProcessors(settings)))\n+            .setBulkActions(-1)\n+            .setBulkSize(new ByteSizeValue(-1, ByteSizeUnit.BYTES))\n+            .setFlushInterval(TimeValue.timeValueSeconds(5))\n+            .build();\n+    }\n+\n+    private static class DeprecationBulkListener implements BulkProcessor.Listener {\n+        @Override\n+        public void beforeBulk(long executionId, BulkRequest request) {}\n+\n+        @Override\n+        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {}", "originalCommit": "441aef1bff47d55d39290e9e89a2f0e781859044", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9cc1c2cda199b9e569ed850f33136a243b11a8ef", "url": "https://github.com/elastic/elasticsearch/commit/9cc1c2cda199b9e569ed850f33136a243b11a8ef", "message": "Fixes to write documents in ECS", "committedDate": "2020-09-04T14:48:07Z", "type": "commit"}, {"oid": "2b1db5656ad611ad685e3a73f5ff1d157a4febc7", "url": "https://github.com/elastic/elasticsearch/commit/2b1db5656ad611ad685e3a73f5ff1d157a4febc7", "message": "License fix", "committedDate": "2020-09-04T15:03:15Z", "type": "commit"}, {"oid": "89f09eda99aca340ae24eb1e39b492c6f90c03bc", "url": "https://github.com/elastic/elasticsearch/commit/89f09eda99aca340ae24eb1e39b492c6f90c03bc", "message": "Logger fix", "committedDate": "2020-09-09T09:24:55Z", "type": "commit"}, {"oid": "50eb28ac07ec15e5fccf34e088c6be837304f3bf", "url": "https://github.com/elastic/elasticsearch/commit/50eb28ac07ec15e5fccf34e088c6be837304f3bf", "message": "Merge remote-tracking branch 'upstream/7.x' into 46106-index-deprecation-logs-7x", "committedDate": "2020-09-09T10:13:56Z", "type": "commit"}]}