{"pr_number": 53583, "pr_title": "add nori_number token filter in analysis-nori", "pr_createdAt": "2020-03-15T14:40:00Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/53583", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ4ODIzNg==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r393488236", "bodyText": "We also need to expose discard_punctuation in the tokenizer in order to detect floating point numbers and to split numbers correctly if they are separated with whitespaces ? We should then advise in the documentation to set discard_punctuation to false when using the nori_number filter and to remove punctuations in a subsequent filter ?", "author": "jimczi", "createdAt": "2020-03-17T07:34:39Z", "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +434,52 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter", "originalCommit": "5fa6abaa71e0b0406c69eb90edb98a17b5f74443", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE5NjcwNg==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395196706", "bodyText": "I think so.\nI added more description for that.\nPlease check the additional commits.", "author": "danmuzi", "createdAt": "2020-03-19T17:25:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ4ODIzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQyMg==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291422", "bodyText": "Can you add a small test for the new discard_punctuation option of the tokenizer ?", "author": "jimczi", "createdAt": "2020-03-19T20:10:31Z", "path": "plugins/analysis-nori/src/test/java/org/elasticsearch/index/analysis/NoriAnalysisTests.java", "diffHunk": "@@ -159,6 +162,20 @@ public void testNoriReadingForm() throws IOException {\n         assertTokenStreamContents(stream, new String[] {\"\ud5a5\uac00\"});\n     }\n \n+    public void testNoriNumber() throws IOException {\n+        Settings settings = Settings.builder()\n+            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)\n+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+            .put(\"index.analysis.filter.my_filter.type\", \"nori_number\")\n+            .build();\n+        TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new AnalysisNoriPlugin());\n+        TokenFilterFactory factory = analysis.tokenFilter.get(\"my_filter\");\n+        Tokenizer tokenizer = new KoreanTokenizer();\n+        tokenizer.setReader(new StringReader(\"\uc624\ub298 \uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31\uc6d0\uc9dc\ub9ac \uc640\uc778 \uad6c\uc785\"));\n+        TokenStream stream = factory.create(tokenizer);\n+        assertTokenStreamContents(stream, new String[] {\"\uc624\ub298\", \"102500\", \"\uc6d0\", \"\uc9dc\ub9ac\", \"\uc640\uc778\", \"\uad6c\uc785\"});\n+    }\n+", "originalCommit": "82191a3032f19729807dd2377af70a5c8272b0a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgyNTA5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395825091", "bodyText": "Done!", "author": "danmuzi", "createdAt": "2020-03-20T18:39:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQ3NA==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291474", "bodyText": "Maybe add add a NOTE: to emphasize this part ?", "author": "jimczi", "createdAt": "2020-03-19T20:10:36Z", "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +439,105 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter\n+\n+The `nori_number` token filter normalizes Korean numbers\n+to regular Arabic decimal numbers in half-width characters.\n+\n+Korean numbers are often written using a combination of Hangul and Arabic numbers with various kinds punctuation.\n+For example, \uff13\uff0e\uff12\ucc9c means 3200.\n+This filter does this kind of normalization and allows a search for 3200 to match \uff13\uff0e\uff12\ucc9c in text,\n+but can also be used to make range facets based on the normalized numbers and so on.\n+\n+Notice that this analyzer uses a token composition scheme and relies on punctuation tokens", "originalCommit": "82191a3032f19729807dd2377af70a5c8272b0a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgyNTA1OA==", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395825058", "bodyText": "Done!", "author": "danmuzi", "createdAt": "2020-03-20T18:39:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQ3NA=="}], "type": "inlineReview"}, {"oid": "b6221c5748099e6902e2ad91c5207ab5ac2005b3", "url": "https://github.com/elastic/elasticsearch/commit/b6221c5748099e6902e2ad91c5207ab5ac2005b3", "message": "add nori_number token filter", "committedDate": "2020-03-23T17:06:45Z", "type": "commit"}, {"oid": "d6dbe514fa70c8278f180a3a602a75601d588d39", "url": "https://github.com/elastic/elasticsearch/commit/d6dbe514fa70c8278f180a3a602a75601d588d39", "message": "add discard_punctuation option in nori_tokenizer", "committedDate": "2020-03-23T17:06:45Z", "type": "commit"}, {"oid": "9bd0ebe9ab119a49f6a829d308d8238d24050ea2", "url": "https://github.com/elastic/elasticsearch/commit/9bd0ebe9ab119a49f6a829d308d8238d24050ea2", "message": "add description about using discard_punctuation in nori_number", "committedDate": "2020-03-23T17:06:45Z", "type": "commit"}, {"oid": "6ed33a19b7a9f5e4c6dc9b628cf2f6202113c25d", "url": "https://github.com/elastic/elasticsearch/commit/6ed33a19b7a9f5e4c6dc9b628cf2f6202113c25d", "message": "add note in asciidoc and test cases for discard_punctuation", "committedDate": "2020-03-23T17:06:45Z", "type": "commit"}, {"oid": "1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "url": "https://github.com/elastic/elasticsearch/commit/1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "message": "fix wrong indentation in nori_number test", "committedDate": "2020-03-23T17:06:45Z", "type": "commit"}, {"oid": "1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "url": "https://github.com/elastic/elasticsearch/commit/1a4367ea993cfc34d7d034e03c5b9cb80790fe3c", "message": "fix wrong indentation in nori_number test", "committedDate": "2020-03-23T17:06:45Z", "type": "forcePushed"}]}