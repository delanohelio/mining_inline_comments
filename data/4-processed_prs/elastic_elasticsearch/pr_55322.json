{"pr_number": 55322, "pr_title": "Allow to prewarm the cache for searchable snapshot shards", "pr_createdAt": "2020-04-16T15:57:42Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55322", "timeline": [{"oid": "257a00a74e76519112dde91954ed2e791f08639d", "url": "https://github.com/elastic/elasticsearch/commit/257a00a74e76519112dde91954ed2e791f08639d", "message": "Add cache prewarming", "committedDate": "2020-04-16T15:07:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY2OTY5Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409669696", "bodyText": "Better name suggestions are welcome", "author": "tlrx", "createdAt": "2020-04-16T15:58:24Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -115,8 +135,10 @@ public SearchableSnapshotDirectory(\n         this.cacheDir = Objects.requireNonNull(cacheDir);\n         this.closed = new AtomicBoolean(false);\n         this.useCache = SNAPSHOT_CACHE_ENABLED_SETTING.get(indexSettings);\n+        this.loadCacheEagerly = useCache ? SNAPSHOT_CACHE_LOAD_EAGERLY_SETTING.get(indexSettings) : false;", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MDY1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409670656", "bodyText": "This methods blocks until the cache is fully prewarmed. It must be done before loaded is set to true so that other components of the system are not likely to trigger some caching on this directory files.", "author": "tlrx", "createdAt": "2020-04-16T15:59:47Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -142,18 +164,21 @@ protected final boolean assertCurrentThreadMayLoadSnapshot() {\n      * @return true if the snapshot was loaded by executing this method, false otherwise\n      */\n     public boolean loadSnapshot() {\n+        assert assertCurrentThreadMayLoadSnapshot();\n         boolean alreadyLoaded = this.loaded;\n         if (alreadyLoaded == false) {\n             synchronized (this) {\n                 alreadyLoaded = this.loaded;\n                 if (alreadyLoaded == false) {\n                     this.blobContainer = blobContainerSupplier.get();\n                     this.snapshot = snapshotSupplier.get();\n+                    if (loadCacheEagerly) {\n+                        prewarmCache();", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI5NDIzNw==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r411294237", "bodyText": "Is that strictly necessary? I would prefer to initiate the prewarming here, but at the same time allow the shard routing to move to started state as quickly as possible.", "author": "ywelsch", "createdAt": "2020-04-20T11:14:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MDY1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NTgwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414395809", "bodyText": "This is not strictly necessary but a misunderstanding from my part. We discussed this and I updated the PR so that cache warming now runs concurrently with the recovery.", "author": "tlrx", "createdAt": "2020-04-24T08:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MDY1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MTIwMA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409671200", "bodyText": "Splitting this method into two allows to open an IndexInput even if the snapshot is not marked as loaded yet", "author": "tlrx", "createdAt": "2020-04-16T16:00:32Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -290,17 +315,20 @@ public CacheFile getCacheFile(CacheKey cacheKey, long fileLength) throws Excepti\n \n     @Override\n     public IndexInput openInput(final String name, final IOContext context) throws IOException {\n-        ensureOpen();\n+        return openInput(fileInfo(name), context);\n+    }\n \n-        final BlobStoreIndexShardSnapshot.FileInfo fileInfo = fileInfo(name);\n+    private IndexInput openInput(final BlobStoreIndexShardSnapshot.FileInfo fileInfo, final IOContext context) {", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4NjcwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414486709", "bodyText": "I think this is no longer necessary? The private openInput method is only called in one place.", "author": "DaveCTurner", "createdAt": "2020-04-24T10:59:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MTIwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDYwNjU2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414606566", "bodyText": "Indeed - I pushed c8a1c6b to remove the method.", "author": "tlrx", "createdAt": "2020-04-24T14:11:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MTIwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3Mjc3MA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409672770", "bodyText": "This method uses an IndexInput with a specific IOContext to prewarm the cache for the given Lucene file. The IndexInput will be cloned for each part to write in cache later and closed once all parts are processed.", "author": "tlrx", "createdAt": "2020-04-16T16:02:51Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -331,12 +359,133 @@ public String toString() {\n         return this.getClass().getSimpleName() + \"@snapshotId=\" + snapshotId + \" lockFactory=\" + lockFactory;\n     }\n \n+    private void prewarmCache() {\n+        assert Thread.holdsLock(this);\n+        assert loadCacheEagerly;\n+        assert useCache;\n+\n+        final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n+        for (BlobStoreIndexShardSnapshot.FileInfo file : snapshot().indexFiles()) {\n+            final String fileName = file.physicalName();\n+            if (isExcludedFromCache(fileName) == false && file.metadata().hashEqualsContents() == false) {\n+                final long numberOfParts = file.numberOfParts();\n+                if (queue.remainingCapacity() > numberOfParts) {\n+                    logger.debug(\n+                        \"{} prewarming cache for file [{}] of length [{}] and [{}] parts\",\n+                        shardId,\n+                        fileName,\n+                        file.length(),\n+                        numberOfParts\n+                    );\n+                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+                    try {\n+                        final IndexInput input = openInput(file, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3MzIzNg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409673236", "bodyText": "This can happen in case of cache evictions or if the shard is closing", "author": "tlrx", "createdAt": "2020-04-16T16:03:31Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -331,12 +359,133 @@ public String toString() {\n         return this.getClass().getSimpleName() + \"@snapshotId=\" + snapshotId + \" lockFactory=\" + lockFactory;\n     }\n \n+    private void prewarmCache() {\n+        assert Thread.holdsLock(this);\n+        assert loadCacheEagerly;\n+        assert useCache;\n+\n+        final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n+        for (BlobStoreIndexShardSnapshot.FileInfo file : snapshot().indexFiles()) {\n+            final String fileName = file.physicalName();\n+            if (isExcludedFromCache(fileName) == false && file.metadata().hashEqualsContents() == false) {\n+                final long numberOfParts = file.numberOfParts();\n+                if (queue.remainingCapacity() > numberOfParts) {\n+                    logger.debug(\n+                        \"{} prewarming cache for file [{}] of length [{}] and [{}] parts\",\n+                        shardId,\n+                        fileName,\n+                        file.length(),\n+                        numberOfParts\n+                    );\n+                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+                    try {\n+                        final IndexInput input = openInput(file, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                        assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    final long partStartTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    assert loaded == false : \"snapshot should not be fully loaded until all prewarming tasks are completed\";\n+\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == file.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"{} part [{}/{}] of length [{}] prewarmed in cache for file [{}] in [{}] ms\",\n+                                            shardId,\n+                                            part,\n+                                            numberOfParts,\n+                                            file.partBytes(part),\n+                                            fileName,\n+                                            TimeUnit.NANOSECONDS.toMillis(statsCurrentTimeNanosSupplier.getAsLong() - partStartTimeInNanos)\n+                                        )\n+                                    );\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                        logger.debug(\n+                                            \"{} cache prewarmed for file [{}] in [{}] ms\",\n+                                            shardId,\n+                                            fileName,\n+                                            TimeUnit.NANOSECONDS.toMillis(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos)\n+                                        );\n+                                    }\n+                                }\n+                            });\n+                        }\n+                    } catch (Exception e) {\n+                        logger.warn(() -> new ParameterizedMessage(\"{} failed to prewarm cache for file [{}]\", shardId, fileName), e);\n+                    }\n+                }\n+            }\n+        }\n+\n+        final String threadPoolName = ThreadPool.Names.SNAPSHOT;\n+        final Executor executor = threadPool.executor(threadPoolName);\n+        final int workers = Math.min(threadPool.info(threadPoolName).getMax(), queue.size());\n+\n+        final CountDownLatch latch = new CountDownLatch(workers);\n+        for (int i = 0; i < workers; ++i) {\n+            executor.execute(new AbstractRunnable() {\n+                @Override\n+                protected void doRun() throws Exception {\n+                    CheckedRunnable<Exception> loader;\n+                    while (isOpen && (loader = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {\n+                        try {\n+                            loader.run();\n+                        } catch (Exception e) {\n+                            if (e instanceof AlreadyClosedException\n+                                || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n+                                continue;", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3NDI4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409674282", "bodyText": "We included the rangeSize in the CacheFile to compute the range to fetch given a specific position, but we were never asserting that the fetched ranges really matched the size.", "author": "tlrx", "createdAt": "2020-04-16T16:04:57Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -61,12 +61,11 @@ protected void closeInternal() {\n     @Nullable // if evicted, or there are no listeners\n     private volatile FileChannel channel;\n \n-    public CacheFile(String description, long length, Path file, int rangeSize) {\n+    public CacheFile(String description, long length, Path file) {", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3NTAzNg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409675036", "bodyText": "This asserts the size of the ranges written in cache, depending of the IOContext.", "author": "tlrx", "createdAt": "2020-04-16T16:06:04Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -144,6 +208,7 @@ private void writeCacheFile(FileChannel fc, long start, long end) throws IOExcep\n         final long length = end - start;\n         final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, length))];\n         logger.trace(() -> new ParameterizedMessage(\"writing range [{}-{}] to cache file [{}]\", start, end, cacheFileReference));\n+        assert assertRangeOfBytesAlignment(start, end);", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5NjQzOA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414396438", "bodyText": "We can't really assert the size of ranges now warming runs concurrently. This has been removed.", "author": "tlrx", "createdAt": "2020-04-24T08:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY3NTAzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTczMjI4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r409732281", "bodyText": "I'm wondering if this is a source of contention; I'll investigate further.", "author": "tlrx", "createdAt": "2020-04-16T17:35:53Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -131,6 +170,31 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n         lastSeekPosition = lastReadPosition;\n     }\n \n+    /**\n+     * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n+     */\n+    public int prefetchPart(final int part) throws IOException {\n+        ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n+        if (part >= fileInfo.numberOfParts()) {\n+            throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n+        }\n+        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (ReleasableLock ignored = cacheFile.fileLock()) {", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzMDk1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r411230956", "bodyText": "I see that this is taking the same approach as we use for uploading snapshot files (BlobStoreRepository). I would prefer not to hold onto workers for such a long time, as it can block the snapshot thread pool for a long time (cc: @original-brownbear).\nIn both cases (also the one BlobStoreRepository), I would prefer for the worker to process one file, then enqueue another task to the thread pool to pick up the next piece of work. This allows other operations to make progress as well, instead of waiting for a long time in the snapshot queue.", "author": "ywelsch", "createdAt": "2020-04-20T09:29:09Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -331,12 +359,133 @@ public String toString() {\n         return this.getClass().getSimpleName() + \"@snapshotId=\" + snapshotId + \" lockFactory=\" + lockFactory;\n     }\n \n+    private void prewarmCache() {\n+        assert Thread.holdsLock(this);\n+        assert loadCacheEagerly;\n+        assert useCache;\n+\n+        final BlockingQueue<CheckedRunnable<Exception>> queue = new SizeBlockingQueue<>(new LinkedBlockingQueue<>(), Integer.MAX_VALUE);\n+        for (BlobStoreIndexShardSnapshot.FileInfo file : snapshot().indexFiles()) {\n+            final String fileName = file.physicalName();\n+            if (isExcludedFromCache(fileName) == false && file.metadata().hashEqualsContents() == false) {\n+                final long numberOfParts = file.numberOfParts();\n+                if (queue.remainingCapacity() > numberOfParts) {\n+                    logger.debug(\n+                        \"{} prewarming cache for file [{}] of length [{}] and [{}] parts\",\n+                        shardId,\n+                        fileName,\n+                        file.length(),\n+                        numberOfParts\n+                    );\n+                    final long startTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+                    try {\n+                        final IndexInput input = openInput(file, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                        assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                        final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                        for (long p = 0; p < numberOfParts; p++) {\n+                            final int part = Math.toIntExact(p);\n+                            queue.add(() -> {\n+                                try {\n+                                    final long partStartTimeInNanos = statsCurrentTimeNanosSupplier.getAsLong();\n+                                    final CachedBlobContainerIndexInput cachedIndexInput = (CachedBlobContainerIndexInput) input.clone();\n+                                    assert loaded == false : \"snapshot should not be fully loaded until all prewarming tasks are completed\";\n+\n+                                    final int bytesRead = cachedIndexInput.prefetchPart(part); // TODO does not include any rate limitation\n+                                    assert bytesRead == file.partBytes(part);\n+\n+                                    logger.trace(\n+                                        () -> new ParameterizedMessage(\n+                                            \"{} part [{}/{}] of length [{}] prewarmed in cache for file [{}] in [{}] ms\",\n+                                            shardId,\n+                                            part,\n+                                            numberOfParts,\n+                                            file.partBytes(part),\n+                                            fileName,\n+                                            TimeUnit.NANOSECONDS.toMillis(statsCurrentTimeNanosSupplier.getAsLong() - partStartTimeInNanos)\n+                                        )\n+                                    );\n+                                } finally {\n+                                    if (countDown.countDown()) {\n+                                        IOUtils.closeWhileHandlingException(input);\n+                                        logger.debug(\n+                                            \"{} cache prewarmed for file [{}] in [{}] ms\",\n+                                            shardId,\n+                                            fileName,\n+                                            TimeUnit.NANOSECONDS.toMillis(statsCurrentTimeNanosSupplier.getAsLong() - startTimeInNanos)\n+                                        );\n+                                    }\n+                                }\n+                            });\n+                        }\n+                    } catch (Exception e) {\n+                        logger.warn(() -> new ParameterizedMessage(\"{} failed to prewarm cache for file [{}]\", shardId, fileName), e);\n+                    }\n+                }\n+            }\n+        }\n+\n+        final String threadPoolName = ThreadPool.Names.SNAPSHOT;\n+        final Executor executor = threadPool.executor(threadPoolName);\n+        final int workers = Math.min(threadPool.info(threadPoolName).getMax(), queue.size());\n+\n+        final CountDownLatch latch = new CountDownLatch(workers);\n+        for (int i = 0; i < workers; ++i) {\n+            executor.execute(new AbstractRunnable() {\n+                @Override\n+                protected void doRun() throws Exception {\n+                    CheckedRunnable<Exception> loader;\n+                    while (isOpen && (loader = queue.poll(0L, TimeUnit.MILLISECONDS)) != null) {", "originalCommit": "257a00a74e76519112dde91954ed2e791f08639d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzMjU0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r411232543", "bodyText": "I agree, that makes sense Yannick.", "author": "tlrx", "createdAt": "2020-04-20T09:31:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzMDk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzNTcwNg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r411235706", "bodyText": "I think for the uploading side the downside of\n\nThis allows other operations to make progress as well\n\nis that it causes the index commits to be held on for a suboptimally long time. That's why the approach of fully monopolizing the pool was consciously chosen for uploads there.", "author": "original-brownbear", "createdAt": "2020-04-20T09:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzMDk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI1MTA0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r411251049", "bodyText": "That's something to be better controlled at the SnapshotShardsService level then, though. It could limit the number of shards to be snapshotted concurrently by lazily enqueuing there.", "author": "ywelsch", "createdAt": "2020-04-20T10:00:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTIzMDk1Ng=="}], "type": "inlineReview"}, {"oid": "4ab253a7e35c1da390b6313886af9c2b7e011d86", "url": "https://github.com/elastic/elasticsearch/commit/4ab253a7e35c1da390b6313886af9c2b7e011d86", "message": "Merge branch 'master' into load-cache-eagerly", "committedDate": "2020-04-23T12:48:21Z", "type": "commit"}, {"oid": "9a67e890f0ee086ac3612e99517130cdb442edfd", "url": "https://github.com/elastic/elasticsearch/commit/9a67e890f0ee086ac3612e99517130cdb442edfd", "message": "apply feedback", "committedDate": "2020-04-23T15:56:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjIwOA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414386208", "bodyText": "This will queue up all chunks at once on the thread pool. To make this fairer among multiple recoveries, I would prefer something like https://github.com/elastic/elasticsearch/compare/master...ywelsch:para-restores?expand=1\nCan be a follow-up though if you prefer.", "author": "ywelsch", "createdAt": "2020-04-24T08:17:58Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -331,12 +353,85 @@ public String toString() {\n         return this.getClass().getSimpleName() + \"@snapshotId=\" + snapshotId + \" lockFactory=\" + lockFactory;\n     }\n \n+    private void prewarmCache() {\n+        if (loadCacheEagerly) {\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> cacheFiles = snapshot().indexFiles()\n+                .stream()\n+                .filter(file -> file.metadata().hashEqualsContents() == false)\n+                .filter(file -> isExcludedFromCache(file.physicalName()) == false)\n+                .collect(Collectors.toList());\n+\n+            final Executor executor = threadPool.executor(SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME);\n+            logger.debug(\"{} warming shard cache for [{}] files\", shardId, cacheFiles.size());\n+\n+            for (BlobStoreIndexShardSnapshot.FileInfo cacheFile : cacheFiles) {\n+                final String fileName = cacheFile.physicalName();\n+                try {\n+                    final IndexInput input = openInput(fileName, CachedBlobContainerIndexInput.CACHE_WARMING_CONTEXT);\n+                    assert input instanceof CachedBlobContainerIndexInput : \"expected cached index input but got \" + input.getClass();\n+\n+                    final long numberOfParts = cacheFile.numberOfParts();\n+                    final CountDown countDown = new CountDown(Math.toIntExact(numberOfParts));\n+                    for (long p = 0; p < numberOfParts; p++) {\n+                        final int part = Math.toIntExact(p);\n+                        executor.execute(new AbstractRunnable() {", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM5ODI5OA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414398298", "bodyText": "Yes I still have this in mind - I'd prefer to do it in a follow up PR if possible.", "author": "tlrx", "createdAt": "2020-04-24T08:36:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjIwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4NzI4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414487283", "bodyText": "Could you transfer that to a // TODO comment, just so we are sure not to lose it?", "author": "DaveCTurner", "createdAt": "2020-04-24T11:00:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjIwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTQ2MA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571460", "bodyText": "TODO added in a635c7f", "author": "tlrx", "createdAt": "2020-04-24T13:22:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjIwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTY2ODYxNA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r415668614", "bodyText": "I opened #55793 for this.", "author": "tlrx", "createdAt": "2020-04-27T09:46:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM4NjIwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4MzExNA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414483114", "bodyText": "Suggest keeping the terminology consistent around \"warming\", how about index.store.snapshot.cache.prewarm.enabled?", "author": "DaveCTurner", "createdAt": "2020-04-24T10:52:45Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshots.java", "diffHunk": "@@ -99,6 +103,11 @@\n         true,\n         Setting.Property.IndexScope\n     );\n+    public static final Setting<Boolean> SNAPSHOT_CACHE_LOAD_EAGERLY_SETTING = Setting.boolSetting(\n+        \"index.store.snapshot.cache.load.eagerly\",", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTQzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571435", "bodyText": "Much better name, thanks. I pushed 4ee96af.", "author": "tlrx", "createdAt": "2020-04-24T13:22:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4MzExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4NDc0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414484743", "bodyText": "I think we can relax the assertion here to permit the searchable_snapshots threadpool to access the repo, rather than overloading it only in CachedBlobContainerIndexInput.", "author": "DaveCTurner", "createdAt": "2020-04-24T10:55:42Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/BaseSearchableSnapshotIndexInput.java", "diffHunk": "@@ -129,7 +129,7 @@ protected InputStream openSlice(long slice) throws IOException {\n         }\n     }\n \n-    protected final boolean assertCurrentThreadMayAccessBlobStore() {\n+    protected boolean assertCurrentThreadMayAccessBlobStore() {", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTQxNA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571414", "bodyText": "Ok. I pushed 0469c7b", "author": "tlrx", "createdAt": "2020-04-24T13:22:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4NDc0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4ODkzNg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414488936", "bodyText": "A Tuple argument is a bit strange, can we pass in the two longs directly instead?", "author": "DaveCTurner", "createdAt": "2020-04-24T11:03:26Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -249,41 +248,36 @@ private void ensureOpen() {\n     }\n \n     CompletableFuture<Integer> fetchRange(\n-        long position,\n+        Tuple<Long, Long> range,", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTM4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571386", "bodyText": "Sure. I pushed 4dc76b8", "author": "tlrx", "createdAt": "2020-04-24T13:22:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4ODkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4OTIzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414489231", "bodyText": "Also invalid if the start of the range is after the end?", "author": "DaveCTurner", "createdAt": "2020-04-24T11:04:01Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -249,41 +248,36 @@ private void ensureOpen() {\n     }\n \n     CompletableFuture<Integer> fetchRange(\n-        long position,\n+        Tuple<Long, Long> range,\n         CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n         CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (position < 0 || position > tracker.getLength()) {\n-                throw new IllegalArgumentException(\"Wrong read position [\" + position + \"]\");\n+            if (range.v1() < 0 || range.v1() > tracker.getLength() || range.v2() < 0 || range.v2() > tracker.getLength()) {", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTM1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571355", "bodyText": "Right, I pushed 4dc76b8", "author": "tlrx", "createdAt": "2020-04-24T13:22:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ4OTIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ5MDE3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414490171", "bodyText": "Don't really need this guard any more.", "author": "DaveCTurner", "createdAt": "2020-04-24T11:05:46Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CacheFile.java", "diffHunk": "@@ -249,41 +248,36 @@ private void ensureOpen() {\n     }\n \n     CompletableFuture<Integer> fetchRange(\n-        long position,\n+        Tuple<Long, Long> range,\n         CheckedBiFunction<Long, Long, Integer, IOException> onRangeAvailable,\n         CheckedBiConsumer<Long, Long, IOException> onRangeMissing\n     ) {\n         final CompletableFuture<Integer> future = new CompletableFuture<>();\n         try {\n-            if (position < 0 || position > tracker.getLength()) {\n-                throw new IllegalArgumentException(\"Wrong read position [\" + position + \"]\");\n+            if (range.v1() < 0 || range.v1() > tracker.getLength() || range.v2() < 0 || range.v2() > tracker.getLength()) {\n+                throw new IllegalArgumentException(\n+                    \"Invalid range [start=\" + range.v1() + \", end=\" + range.v2() + \"] for length [\" + tracker.getLength() + ']'\n+                );\n             }\n-\n             ensureOpen();\n-            final long rangeStart = (position / rangeSize) * rangeSize;\n-            final long rangeEnd = Math.min(rangeStart + rangeSize, tracker.getLength());\n-\n             final List<SparseFileTracker.Gap> gaps = tracker.waitForRange(\n-                rangeStart,\n-                rangeEnd,\n+                range.v1(),\n+                range.v2(),\n                 ActionListener.wrap(\n-                    rangeReady -> future.complete(onRangeAvailable.apply(rangeStart, rangeEnd)),\n+                    rangeReady -> future.complete(onRangeAvailable.apply(range.v1(), range.v2())),\n                     rangeFailure -> future.completeExceptionally(rangeFailure)\n                 )\n             );\n \n             if (gaps.size() > 0) {", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTMxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571319", "bodyText": "Removed in e9831a3", "author": "tlrx", "createdAt": "2020-04-24T13:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ5MDE3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDUwMDc3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414500771", "bodyText": "Can we assert that the returned range is the whole part? I think that'd be useful documentation if nothing else.", "author": "DaveCTurner", "createdAt": "2020-04-24T11:24:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -131,6 +172,31 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n         lastSeekPosition = lastReadPosition;\n     }\n \n+    /**\n+     * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n+     */\n+    public int prefetchPart(final int part) throws IOException {\n+        ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n+        if (part >= fileInfo.numberOfParts()) {\n+            throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n+        }\n+        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());", "originalCommit": "9a67e890f0ee086ac3612e99517130cdb442edfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MTI5MA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414571290", "bodyText": "Sure. I added e01bf71, let me know what you think.", "author": "tlrx", "createdAt": "2020-04-24T13:22:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDUwMDc3MQ=="}], "type": "inlineReview"}, {"oid": "c8a1c6bf7e480d4961fe1e4a6d7fb8ab01e85534", "url": "https://github.com/elastic/elasticsearch/commit/c8a1c6bf7e480d4961fe1e4a6d7fb8ab01e85534", "message": "remove unused openinput", "committedDate": "2020-04-24T12:13:42Z", "type": "commit"}, {"oid": "a635c7f064604d74bec4c520580846042aee6310", "url": "https://github.com/elastic/elasticsearch/commit/a635c7f064604d74bec4c520580846042aee6310", "message": "add todo", "committedDate": "2020-04-24T12:16:36Z", "type": "commit"}, {"oid": "4ee96afd25d50d941bfd43e0a4c558cfded45b54", "url": "https://github.com/elastic/elasticsearch/commit/4ee96afd25d50d941bfd43e0a4c558cfded45b54", "message": "rename setting", "committedDate": "2020-04-24T12:29:27Z", "type": "commit"}, {"oid": "0469c7b582ffa55b267d082c41c29221b3e6e876", "url": "https://github.com/elastic/elasticsearch/commit/0469c7b582ffa55b267d082c41c29221b3e6e876", "message": "assert thread name", "committedDate": "2020-04-24T12:31:57Z", "type": "commit"}, {"oid": "4dc76b83c10f3881a7783896dd3610bc7baae522", "url": "https://github.com/elastic/elasticsearch/commit/4dc76b83c10f3881a7783896dd3610bc7baae522", "message": "remove tuple", "committedDate": "2020-04-24T12:38:55Z", "type": "commit"}, {"oid": "e9831a3c1c5f2035586b08bf390667a149313359", "url": "https://github.com/elastic/elasticsearch/commit/e9831a3c1c5f2035586b08bf390667a149313359", "message": "remove unnecessary condition", "committedDate": "2020-04-24T12:39:49Z", "type": "commit"}, {"oid": "e01bf710ffa6a00ab730fc3efe45b4204dadd763", "url": "https://github.com/elastic/elasticsearch/commit/e01bf710ffa6a00ab730fc3efe45b4204dadd763", "message": "assert range", "committedDate": "2020-04-24T13:16:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU4ODE3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414588179", "bodyText": "Should we avoid even creating this threadpool if prewarming is disabled?", "author": "DaveCTurner", "createdAt": "2020-04-24T13:46:03Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshots.java", "diffHunk": "@@ -252,4 +266,21 @@ public void onIndexModule(IndexModule indexModule) {\n         }\n     }\n \n+    public List<ExecutorBuilder<?>> getExecutorBuilders(Settings settings) {\n+        if (SEARCHABLE_SNAPSHOTS_FEATURE_ENABLED) {\n+            return List.of(executorBuilder());", "originalCommit": "e01bf710ffa6a00ab730fc3efe45b4204dadd763", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY0OTk0MA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414649940", "bodyText": "The prewarming is enabled or disabled on a per-index basis using a IndexScope setting. There is no mechanism to disable prewarming globally for the node. The threadpool is created globally at node construction time and I don't think we can prevent its creation as the node might be assigned later an index with cache prewarming enabled.", "author": "tlrx", "createdAt": "2020-04-24T15:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU4ODE3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY1ODk2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414658966", "bodyText": "D'oh yes good point I thought it was a global setting for some reason. Don't mind me ...", "author": "DaveCTurner", "createdAt": "2020-04-24T15:20:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU4ODE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU4ODY3OA==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414588678", "bodyText": "We could reasonably not have any active threads if there's no pre-warming going on I think.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        1,\n          \n          \n            \n                        0,", "author": "DaveCTurner", "createdAt": "2020-04-24T13:46:39Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshots.java", "diffHunk": "@@ -252,4 +266,21 @@ public void onIndexModule(IndexModule indexModule) {\n         }\n     }\n \n+    public List<ExecutorBuilder<?>> getExecutorBuilders(Settings settings) {\n+        if (SEARCHABLE_SNAPSHOTS_FEATURE_ENABLED) {\n+            return List.of(executorBuilder());\n+        } else {\n+            return List.of();\n+        }\n+    }\n+\n+    public static ExecutorBuilder<?> executorBuilder() {\n+        return new ScalingExecutorBuilder(\n+            SEARCHABLE_SNAPSHOTS_THREAD_POOL_NAME,\n+            1,", "originalCommit": "e01bf710ffa6a00ab730fc3efe45b4204dadd763", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDY0NjgxMg==", "url": "https://github.com/elastic/elasticsearch/pull/55322#discussion_r414646812", "bodyText": "Good point - I pushed baa8258", "author": "tlrx", "createdAt": "2020-04-24T15:03:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU4ODY3OA=="}], "type": "inlineReview"}, {"oid": "baa8258283eb91a45d3fc433320bce0a8aaca191", "url": "https://github.com/elastic/elasticsearch/commit/baa8258283eb91a45d3fc433320bce0a8aaca191", "message": "zero", "committedDate": "2020-04-24T15:03:18Z", "type": "commit"}]}