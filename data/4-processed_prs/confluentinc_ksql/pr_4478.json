{"pr_number": 4478, "pr_title": "Docs for primitive key support", "pr_createdAt": "2020-02-07T16:06:58Z", "pr_url": "https://github.com/confluentinc/ksql/pull/4478", "timeline": [{"oid": "32da75bf00e3383f385a86396a8508a96ff14910", "url": "https://github.com/confluentinc/ksql/commit/32da75bf00e3383f385a86396a8508a96ff14910", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:55:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNTE0Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376525143", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      will be used later allow joins against the table to use the more\n          \n          \n            \n                      to allow joins against the table to use the more", "author": "JimGalasyn", "createdAt": "2020-02-07T17:56:05Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key\n+          as is in the userId field in the value. Specifying `key='userId'`\n+          in the WITH clause above lets ksqlDB know this. This information\n+          will be used later allow joins against the table to use the more", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNTQzNA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376525434", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      on either will yield the same results. If your data does not\n          \n          \n            \n                      on either yields the same results. If your data doesn't", "author": "JimGalasyn", "createdAt": "2020-02-07T17:56:44Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key\n+          as is in the userId field in the value. Specifying `key='userId'`\n+          in the WITH clause above lets ksqlDB know this. This information\n+          will be used later allow joins against the table to use the more\n+          descriptive `userId` column name, rather than `ROWKEY`. Joining\n+          on either will yield the same results. If your data does not", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNTcwOQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376525709", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      contain a copy of the key in the value simply join on `ROWKEY`.\n          \n          \n            \n                      contain a copy of the key in the value, you can join on `ROWKEY`.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:57:22Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key\n+          as is in the userId field in the value. Specifying `key='userId'`\n+          in the WITH clause above lets ksqlDB know this. This information\n+          will be used later allow joins against the table to use the more\n+          descriptive `userId` column name, rather than `ROWKEY`. Joining\n+          on either will yield the same results. If your data does not\n+          contain a copy of the key in the value simply join on `ROWKEY`.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "97661c4c9528d997aedc718b35a6a45ce9c2ddda", "url": "https://github.com/confluentinc/ksql/commit/97661c4c9528d997aedc718b35a6a45ce9c2ddda", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:57:25Z", "type": "commit"}, {"oid": "b03f92a1e01c6e9856050e69dafe59f359711fd6", "url": "https://github.com/confluentinc/ksql/commit/b03f92a1e01c6e9856050e69dafe59f359711fd6", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:57:42Z", "type": "commit"}, {"oid": "283fff0f7b319cafc26e1b73ede314e4b10b566e", "url": "https://github.com/confluentinc/ksql/commit/283fff0f7b319cafc26e1b73ede314e4b10b566e", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:57:54Z", "type": "commit"}, {"oid": "7eb6b2cbf06628f8ea1a2712d9e1a9f0e62a866a", "url": "https://github.com/confluentinc/ksql/commit/7eb6b2cbf06628f8ea1a2712d9e1a9f0e62a866a", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNjAzNw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376526037", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                output? KsqlDB will append messages detailing any issues it\n          \n          \n            \n                output? ksqlDB appends messages that describe any issues it", "author": "JimGalasyn", "createdAt": "2020-02-07T17:58:06Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -225,66 +267,149 @@ These examples query messages from Kafka topics called `pageviews` and\n     ```\n     ksql> SHOW STREAMS;\n \n-    Stream Name              | Kafka Topic              | Format\n-    -----------------------------------------------------------------\n-    PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+     Stream Name         | Kafka Topic                 | Format    \n+    ---------------------------------------------------------------\n+     KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+     PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+    ---------------------------------------------------------------\n \n     ksql> SHOW TABLES;\n \n-    Table Name        | Kafka Topic       | Format    | Windowed\n-    --------------------------------------------------------------\n-    USERS_ORIGINAL    | users             | JSON      | false\n+     Table Name     | Kafka Topic | Format | Windowed \n+    --------------------------------------------------\n+     USERS_ORIGINAL | users       | AVRO   | false    \n+    --------------------------------------------------\n     ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cec7f713a761e8f824ce21529973af7afb41406f", "url": "https://github.com/confluentinc/ksql/commit/cec7f713a761e8f824ce21529973af7afb41406f", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:15Z", "type": "commit"}, {"oid": "167e83594901fd4aab57701e4045d68c7b03b081", "url": "https://github.com/confluentinc/ksql/commit/167e83594901fd4aab57701e4045d68c7b03b081", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:27Z", "type": "commit"}, {"oid": "3baa3df53e2751c79a4b8ed09d0e3392b3a890c8", "url": "https://github.com/confluentinc/ksql/commit/3baa3df53e2751c79a4b8ed09d0e3392b3a890c8", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNjM1MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376526351", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                encountered while processing your data. If things are not working\n          \n          \n            \n                encountered while processing your data. If things aren't working", "author": "JimGalasyn", "createdAt": "2020-02-07T17:58:45Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -225,66 +267,149 @@ These examples query messages from Kafka topics called `pageviews` and\n     ```\n     ksql> SHOW STREAMS;\n \n-    Stream Name              | Kafka Topic              | Format\n-    -----------------------------------------------------------------\n-    PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+     Stream Name         | Kafka Topic                 | Format    \n+    ---------------------------------------------------------------\n+     KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+     PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+    ---------------------------------------------------------------\n \n     ksql> SHOW TABLES;\n \n-    Table Name        | Kafka Topic       | Format    | Windowed\n-    --------------------------------------------------------------\n-    USERS_ORIGINAL    | users             | JSON      | false\n+     Table Name     | Kafka Topic | Format | Windowed \n+    --------------------------------------------------\n+     USERS_ORIGINAL | users       | AVRO   | false    \n+    --------------------------------------------------\n     ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ab731aeb283be519869990dec26756f308f502e7", "url": "https://github.com/confluentinc/ksql/commit/ab731aeb283be519869990dec26756f308f502e7", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:48Z", "type": "commit"}, {"oid": "b5f0ea4553fba561f2e6a8d93da325a84926d5ac", "url": "https://github.com/confluentinc/ksql/commit/b5f0ea4553fba561f2e6a8d93da325a84926d5ac", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:58:58Z", "type": "commit"}, {"oid": "1bd183ea72c46e97e054e174168b854dd35f521f", "url": "https://github.com/confluentinc/ksql/commit/1bd183ea72c46e97e054e174168b854dd35f521f", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:59:09Z", "type": "commit"}, {"oid": "28905f0c2fda9a026cf598c51bed748c0ea6035d", "url": "https://github.com/confluentinc/ksql/commit/28905f0c2fda9a026cf598c51bed748c0ea6035d", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:59:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNjYzNg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376526636", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                as you expect it can be worth checking the contents of this stream\n          \n          \n            \n                as you expect, check the contents of this stream", "author": "JimGalasyn", "createdAt": "2020-02-07T17:59:26Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -225,66 +267,149 @@ These examples query messages from Kafka topics called `pageviews` and\n     ```\n     ksql> SHOW STREAMS;\n \n-    Stream Name              | Kafka Topic              | Format\n-    -----------------------------------------------------------------\n-    PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+     Stream Name         | Kafka Topic                 | Format    \n+    ---------------------------------------------------------------\n+     KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+     PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+    ---------------------------------------------------------------\n \n     ksql> SHOW TABLES;\n \n-    Table Name        | Kafka Topic       | Format    | Windowed\n-    --------------------------------------------------------------\n-    USERS_ORIGINAL    | users             | JSON      | false\n+     Table Name     | Kafka Topic | Format | Windowed \n+    --------------------------------------------------\n+     USERS_ORIGINAL | users       | AVRO   | false    \n+    --------------------------------------------------\n     ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3b0ca0a9f15965d0d70a3cb9fd4696217f229cdc", "url": "https://github.com/confluentinc/ksql/commit/3b0ca0a9f15965d0d70a3cb9fd4696217f229cdc", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:59:31Z", "type": "commit"}, {"oid": "fde84807df2486ea64a1c9c6db05d6eca93bfdba", "url": "https://github.com/confluentinc/ksql/commit/fde84807df2486ea64a1c9c6db05d6eca93bfdba", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:59:41Z", "type": "commit"}, {"oid": "b58af0b921f1e784597819688a79db9da0fb3c0d", "url": "https://github.com/confluentinc/ksql/commit/b58af0b921f1e784597819688a79db9da0fb3c0d", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:59:49Z", "type": "commit"}, {"oid": "155c9912b509c661057cd03b09ec145b6b3bf132", "url": "https://github.com/confluentinc/ksql/commit/155c9912b509c661057cd03b09ec145b6b3bf132", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:00:01Z", "type": "commit"}, {"oid": "4fe8c00769870e9ee88af0ec68145292feec5537", "url": "https://github.com/confluentinc/ksql/commit/4fe8c00769870e9ee88af0ec68145292feec5537", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:00:14Z", "type": "commit"}, {"oid": "a5ae1bd6b2f3c4e9da200a06039b17b256c74832", "url": "https://github.com/confluentinc/ksql/commit/a5ae1bd6b2f3c4e9da200a06039b17b256c74832", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:00:23Z", "type": "commit"}, {"oid": "6b1ae846497efe241e2c25b2d5f2462493e37f4a", "url": "https://github.com/confluentinc/ksql/commit/6b1ae846497efe241e2c25b2d5f2462493e37f4a", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:00:44Z", "type": "commit"}, {"oid": "b31f64d79a4729a764d399e1cdea19105a1c7f53", "url": "https://github.com/confluentinc/ksql/commit/b31f64d79a4729a764d399e1cdea19105a1c7f53", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:02:14Z", "type": "commit"}, {"oid": "b8e570313d8d8087dcb534f1dc9c3ad4ac4e7713", "url": "https://github.com/confluentinc/ksql/commit/b8e570313d8d8087dcb534f1dc9c3ad4ac4e7713", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:02:47Z", "type": "commit"}, {"oid": "019e2ae32985c01d977813699121cd06ff07760d", "url": "https://github.com/confluentinc/ksql/commit/019e2ae32985c01d977813699121cd06ff07760d", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:03:07Z", "type": "commit"}, {"oid": "cd39ccd9289df92fc75f68994c1dcd2dc575efe4", "url": "https://github.com/confluentinc/ksql/commit/cd39ccd9289df92fc75f68994c1dcd2dc575efe4", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:03:28Z", "type": "commit"}, {"oid": "94b81ac46bf83a1efbf615e09c47e7633ca746d9", "url": "https://github.com/confluentinc/ksql/commit/94b81ac46bf83a1efbf615e09c47e7633ca746d9", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:06:42Z", "type": "commit"}, {"oid": "2ed7b4a3081dad2d937313d29dbcff2576bc2eec", "url": "https://github.com/confluentinc/ksql/commit/2ed7b4a3081dad2d937313d29dbcff2576bc2eec", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:07:33Z", "type": "commit"}, {"oid": "abf9bbe64d4d397efc500edd3e632169e45e13f3", "url": "https://github.com/confluentinc/ksql/commit/abf9bbe64d4d397efc500edd3e632169e45e13f3", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:08:08Z", "type": "commit"}, {"oid": "7c6ae895356ee55811a206b81cd1fe4a0258d7fb", "url": "https://github.com/confluentinc/ksql/commit/7c6ae895356ee55811a206b81cd1fe4a0258d7fb", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:08:42Z", "type": "commit"}, {"oid": "7a57b940dbc5463f27c1306cc234833832e3027d", "url": "https://github.com/confluentinc/ksql/commit/7a57b940dbc5463f27c1306cc234833832e3027d", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:09:22Z", "type": "commit"}, {"oid": "24137ff9de7c3da16312984e358b866c5c1db5b6", "url": "https://github.com/confluentinc/ksql/commit/24137ff9de7c3da16312984e358b866c5c1db5b6", "message": "Update docs-md/tutorials/basics-local.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:09:51Z", "type": "commit"}, {"oid": "98cc5832119120184a278b6372ab435d31b42183", "url": "https://github.com/confluentinc/ksql/commit/98cc5832119120184a278b6372ab435d31b42183", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:10:11Z", "type": "commit"}, {"oid": "d37ede21d67b8c048216e97b953454ea7c35bd4b", "url": "https://github.com/confluentinc/ksql/commit/d37ede21d67b8c048216e97b953454ea7c35bd4b", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:11:03Z", "type": "commit"}, {"oid": "8abde064c5e45e7cb168ecc38a321c7dd580fd3c", "url": "https://github.com/confluentinc/ksql/commit/8abde064c5e45e7cb168ecc38a321c7dd580fd3c", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:11:24Z", "type": "commit"}, {"oid": "92321002e4b85a5a1d18d06d3bbd052a4b7be5fe", "url": "https://github.com/confluentinc/ksql/commit/92321002e4b85a5a1d18d06d3bbd052a4b7be5fe", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:12:34Z", "type": "commit"}, {"oid": "06f0606afd61f23e8b3fff89380c4ab9e2248752", "url": "https://github.com/confluentinc/ksql/commit/06f0606afd61f23e8b3fff89380c4ab9e2248752", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:12:53Z", "type": "commit"}, {"oid": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "url": "https://github.com/confluentinc/ksql/commit/bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:14:08Z", "type": "commit"}, {"oid": "60a4cf51fc75bab93114d71964f0935ec395e810", "url": "https://github.com/confluentinc/ksql/commit/60a4cf51fc75bab93114d71964f0935ec395e810", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:14:31Z", "type": "commit"}, {"oid": "639a6dfb4900130fdb69420fa7d91a5aeff51056", "url": "https://github.com/confluentinc/ksql/commit/639a6dfb4900130fdb69420fa7d91a5aeff51056", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:15:04Z", "type": "commit"}, {"oid": "4075aa2f21c85a7ddb179c4fbfd4b00b7ba33ef7", "url": "https://github.com/confluentinc/ksql/commit/4075aa2f21c85a7ddb179c4fbfd4b00b7ba33ef7", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:16:04Z", "type": "commit"}, {"oid": "2eeae4c833c156bbab963c6e269a2a4bc6bff034", "url": "https://github.com/confluentinc/ksql/commit/2eeae4c833c156bbab963c6e269a2a4bc6bff034", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:16:40Z", "type": "commit"}, {"oid": "a6682da70ce97bdbe911925075d71fe0325dde2f", "url": "https://github.com/confluentinc/ksql/commit/a6682da70ce97bdbe911925075d71fe0325dde2f", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:17:08Z", "type": "commit"}, {"oid": "c5b3d81566d45b822f8720a279cd9f29c05f1cbc", "url": "https://github.com/confluentinc/ksql/commit/c5b3d81566d45b822f8720a279cd9f29c05f1cbc", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:17:31Z", "type": "commit"}, {"oid": "4070e56ef287a9a9992b91c89721f0b104d56e73", "url": "https://github.com/confluentinc/ksql/commit/4070e56ef287a9a9992b91c89721f0b104d56e73", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:17:58Z", "type": "commit"}, {"oid": "4d26d2a52e7cbb76e26892c01400e594d0f9fcfe", "url": "https://github.com/confluentinc/ksql/commit/4d26d2a52e7cbb76e26892c01400e594d0f9fcfe", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:18:17Z", "type": "commit"}, {"oid": "7ea08142aa257106fb88db0d99ddb64316988838", "url": "https://github.com/confluentinc/ksql/commit/7ea08142aa257106fb88db0d99ddb64316988838", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:19:01Z", "type": "commit"}, {"oid": "d8a91483a76dd7feb38066beb63ed7e0101352c1", "url": "https://github.com/confluentinc/ksql/commit/d8a91483a76dd7feb38066beb63ed7e0101352c1", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:19:24Z", "type": "commit"}, {"oid": "139e84e4758930f65bd1386d401acb2b1dfe7f6f", "url": "https://github.com/confluentinc/ksql/commit/139e84e4758930f65bd1386d401acb2b1dfe7f6f", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:19:47Z", "type": "commit"}, {"oid": "176f190d5394a5a17490fb72b89b51e388971878", "url": "https://github.com/confluentinc/ksql/commit/176f190d5394a5a17490fb72b89b51e388971878", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:20:05Z", "type": "commit"}, {"oid": "a86a4a3b61ab0747dc759d27c0bc952c62a1843e", "url": "https://github.com/confluentinc/ksql/commit/a86a4a3b61ab0747dc759d27c0bc952c62a1843e", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:21:40Z", "type": "commit"}, {"oid": "c91e643ebb6d044196022bf25bcaa573b9d283ee", "url": "https://github.com/confluentinc/ksql/commit/c91e643ebb6d044196022bf25bcaa573b9d283ee", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:21:59Z", "type": "commit"}, {"oid": "4d622c205f0a5605afabe66c1aeda2d5459ef493", "url": "https://github.com/confluentinc/ksql/commit/4d622c205f0a5605afabe66c1aeda2d5459ef493", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:22:30Z", "type": "commit"}, {"oid": "0c36703018d3036920f2448262da20d79cd4b823", "url": "https://github.com/confluentinc/ksql/commit/0c36703018d3036920f2448262da20d79cd4b823", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:22:55Z", "type": "commit"}, {"oid": "f78c13f463a03bdd9906431ab876c0b42cc2c891", "url": "https://github.com/confluentinc/ksql/commit/f78c13f463a03bdd9906431ab876c0b42cc2c891", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:23:16Z", "type": "commit"}, {"oid": "486513383f8f72496f1242c0e5bbccadb042fd4c", "url": "https://github.com/confluentinc/ksql/commit/486513383f8f72496f1242c0e5bbccadb042fd4c", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:23:37Z", "type": "commit"}, {"oid": "cc49fed425f1a4dcf33b130c28663b7c86df53ec", "url": "https://github.com/confluentinc/ksql/commit/cc49fed425f1a4dcf33b130c28663b7c86df53ec", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T18:23:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNTA5MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376535091", "bodyText": "would be nice to clarify what this STRING is (or link to some documentation that clarifies it)", "author": "agavra", "createdAt": "2020-02-07T18:19:15Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, ksqlDB uses the correct format.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+You must set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying {{ site.ak }} topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression, the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions, the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NDM5MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378794390", "bodyText": "Will be done in next PR!", "author": "big-andy-coates", "createdAt": "2020-02-13T11:07:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNTA5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNjcyMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376536722", "bodyText": "I think it would be good to have the example above explicitly name userID as the KEY - I know this isn't necessary for the example, but I think it's good to have an example of a stream with a key declared using the KEY syntax", "author": "agavra", "createdAt": "2020-02-07T18:22:35Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -98,26 +101,35 @@ When your inputs are co-partitioned, records with the same key, from\n both sides of the join, are delivered to the same stream task during\n processing.\n \n-### Records Have the Same Keying Scheme\n+### Records Have the Same Keying Schema\n \n-For a join to work, the keys from both sides must have the same serialized\n-binary data.\n+For a join to work, the keys from both sides must have the same SQL type.\n \n For example, you can join a stream of user clicks that's keyed on a `VARCHAR`\n user id with a table of user profiles that's also keyed on a `VARCHAR` user id.\n Records with the exact same user id on both sides will be joined.\n \n-ksqlDB requires that keys are UTF-8 encoded strings.\n+If the schema of the columns you wish to join on don't match, it may be possible\n+to `CAST` one side to match the other. For example, if one side of the join\n+had a `INT` userId column, and the other a `LONG`, then you may choose to cast\n+the `INT` side to a `LONG`:\n+\n+```sql\n+    -- stream with INT userId\n+    CREATE STREAM clicks (userId INT, url STRING) WITH(kafka_topic='clickstream', value_format='json');\n+\n+    -- table with BIGINT userId stored in they key:\n+    CREATE TABLE  users  (ROWKEY BIGINT KEY, fullName STRING) WITH(kafka_topic='users', value_format='json');", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NDMyOA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378794328", "bodyText": "I've deliberately not added the WITH KEY bit as its not required. The example only has what's needed.  Adding additional stuff if just noise IMHO and can lead to confusion.\nPlus I intend to drop the whole WITH KEY thing soon.\nSo do you mind if we leave this as it is?", "author": "big-andy-coates", "createdAt": "2020-02-13T11:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNjcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNzU0NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376537544", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            including the system columns `ROWTIME` and `ROWKEY`.\n          \n          \n            \n            including the `ROWTIME` and `ROWKEY` system columns .", "author": "agavra", "createdAt": "2020-02-07T18:24:13Z", "path": "docs-md/developer-guide/ksqldb-reference/select-push-query.md", "diffHunk": "@@ -48,7 +48,7 @@ In the previous statements, `from_item` is one of the following:\n -   `from_item LEFT JOIN from_item ON join_condition`\r\n \r\n The WHERE clause can refer to any column defined for a stream or table,\r\n-including the two implicit columns `ROWTIME` and `ROWKEY`.\r\n+including the system columns `ROWTIME` and `ROWKEY`.\r", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NDg2MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378794860", "bodyText": "Coming in the next PR.", "author": "big-andy-coates", "createdAt": "2020-02-13T11:08:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNzU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNzc3Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376537772", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            KsqlDB supports the following WINDOW types.\n          \n          \n            \n            KsqlDB supports the following WINDOW types:", "author": "agavra", "createdAt": "2020-02-07T18:24:44Z", "path": "docs-md/developer-guide/ksqldb-reference/select-push-query.md", "diffHunk": "@@ -109,8 +109,12 @@ SET 'auto.offset.reset' = 'earliest';\n \r\n The WINDOW clause lets you control how to group input records *that have\r\n the same key* into so-called *windows* for operations like aggregations\r\n-or joins. Windows are tracked per record key. ksqlDB supports the following\r\n-WINDOW types.\r\n+or joins. Windows are tracked per record key.\r\n+\r\n+Windowing adds two additional system columns to the data, which provide\r\n+the window bounds: `WINDOWSTART` and `WINDOWEND`.\r\n+\r\n+KsqlDB supports the following WINDOW types.\r", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NTA2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378795066", "bodyText": "Coming in the next PR", "author": "big-andy-coates", "createdAt": "2020-02-13T11:08:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNzc3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzODk5Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376538996", "bodyText": "all the examples have added ROWKEY <TYPE> KEY - is this now a requirement or is it just illustrative?", "author": "agavra", "createdAt": "2020-02-07T18:27:27Z", "path": "docs-md/developer-guide/syntax-reference.md", "diffHunk": "@@ -398,14 +398,16 @@ message key by setting the `KEY` property of the `WITH` clause.\n Example:\n \n ```sql\n-CREATE TABLE users (registertime BIGINT, gender VARCHAR, regionid VARCHAR, userid VARCHAR)\n+CREATE TABLE users (rowkey INT KEY, registertime BIGINT, gender VARCHAR, regionid VARCHAR, userid INT)", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NjAzMA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378796030", "bodyText": "It's not currently required. If you don't add one KSQL currently defaults to ROWKEY STRING KEY.  However, we should encourage people to be explicit, rather than rely on implicit.\nI've not documented this yet as its a bit meh.  Ideally, if you don't supply the key column it should mean there is no key column.  We can update the docs when this is the case.", "author": "big-andy-coates", "createdAt": "2020-02-13T11:10:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzODk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzOTUzNg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376539536", "bodyText": "sneaky! code changes in a docs pr \ud83d\ude1b", "author": "agavra", "createdAt": "2020-02-07T18:28:33Z", "path": "ksql-benchmark/src/main/java/io/confluent/ksql/benchmark/SerdeBenchmark.java", "diffHunk": "@@ -98,7 +98,7 @@ public void setUp() throws Exception {\n \n       final Pair<Struct, GenericRow> genericRowPair = rowGenerator.generateRow();\n       row = genericRowPair.getRight();\n-      schema = rowGenerator.schema().valueConnectSchema();\n+      schema = rowGenerator.valueSchema();", "originalCommit": "bd88d5d2b20fdf04ee0e0637b0776db021e7d3f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODc5NjIxMA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r378796210", "bodyText": "Big brother is watching!", "author": "big-andy-coates", "createdAt": "2020-02-13T11:11:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzOTUzNg=="}], "type": "inlineReview"}, {"oid": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "url": "https://github.com/confluentinc/ksql/commit/fec77204c76b39cf294d9a39c0b05ca0334f9477", "message": "feat: primitive key support\n\nksqlDB now supports the following primitive key types: `INT`, `BIGINT`, `DOUBLE` as well as the existing `STRING` type.\n\nThe key type can be defined in the CREATE TABLE or CREATE STREAM statement by including a column definition for `ROWKEY` in the form `ROWKEY <primitive-key-type> KEY,`, for example:\n\n```sql\nCREATE TABLE USERS (ROWKEY BIGINT KEY, NAME STRING, RATING DOUBLE) WITH (kafka_topic='users', VALUE_FORMAT='json');\n```\n\nksqlDB currently requires the name of the key column to be `ROWKEY`. Support for arbitrary key names is tracked by https://github.com/confluentinc/ksql/issues/3536.\n\nksqlDB currently requires keys to use the `KAFKA` format. Support for additional formats is tracked by https://github.com/confluentinc/ksql/projects/3.\n\nSchema inference currently only works with `STRING` keys, Support for additional key types is tracked by https://github.com/confluentinc/ksql/issues/4462. (Schema inference is where ksqlDB infers the schema of a CREATE TABLE and CREATE STREAM statements from the schema registered in the Schema Registry, as opposed to the user supplying the set of columns in the statement).\n\nApache Kafka Connect can be configured to output keys in the `KAFKA` format by using a Converter, e.g. `\"key.converter\": \"org.apache.kafka.connect.converters.IntegerConverter\"`. Details of which converter to use for which key type can be found here: https://docs.confluent.io/current/ksql/docs/developer-guide/serialization.html#kafka in the `Connect Converter` column.\n\n@rmoff has written an introductory blog about primitive keys: https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/\n\nBREAKING CHANGE: existing queries that perform a PARTITION BY or GROUP BY on a single column of one of the above supported primitive key types will now set the key to the appropriate type, not a `STRING` as previously.", "committedDate": "2020-02-07T16:02:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ3NjA5OA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376476098", "bodyText": "Note: changed from Primitive Keys to Keys as a primitive key means something very specific in SQL: its a unique non-null key.\nIn KSQL tables have primitive keys and streams do not. This is because stream are just a stream of facts/events. Each stands alone.  Keys are not unique: duplicate keys are allowed.\nHence, we should standardize on only using primitive keys when talking about tables and just keys when talking about streams.", "author": "big-andy-coates", "createdAt": "2020-02-07T16:12:41Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNTY2OA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376535668", "bodyText": "I assume you meant s/primitive/primary/g?", "author": "agavra", "createdAt": "2020-02-07T18:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ3NjA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ3NzQxOQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376477419", "bodyText": "Changed to join on a non-STRING column", "author": "big-andy-coates", "createdAt": "2020-02-07T16:15:15Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+- If the FROM clause contains only tables and no GROUP BY clause, the key is\n+  copied over from the key of the table(s) in the FROM clause.\n+- If the FROM clause contains only tables and has a GROUP BY clause, the\n+  grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+\n+The following example shows a `users` table joined with a `clicks` stream\n on the `userId` column. The `users` table has the correct primary key\n `userId` that coincides with the joining column. But the `clicks` stream\n doesn't have a defined key, and ksqlDB must repartition it on the joining\n-column (`userId`) and assign the primary key before performing the join.\n+column (`userId`) and assign the key before performing the join.\n \n ```sql\n     -- clicks stream, with an unknown key.\n-    -- the schema of stream clicks is: ROWTIME | ROWKEY | USERID | URL\n-    CREATE STREAM clicks (userId STRING, url STRING) WITH(kafka_topic='clickstream', value_format='json');\n+    -- the schema of stream clicks is: ROWTIME BIGINT | ROWKEY STRING | USERID BIGINT | URL STRING\n+    CREATE STREAM clicks (userId BIGINT, url STRING) WITH(kafka_topic='clickstream', value_format='json');", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ3Nzk2NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376477964", "bodyText": "note: the PRINT command is now case-sensitive. (As it being case-insensitive was causing poor UX).", "author": "big-andy-coates", "createdAt": "2020-02-07T16:16:22Z", "path": "docs-md/developer-guide/ksqldb-reference/print.md", "diffHunk": "@@ -40,7 +36,7 @@ The following statement shows how to print all of the records in a topic named\n `ksql__commands`.\r\n \r\n ```sql\r\n-PRINT 'ksql__commands' FROM BEGINNING;\r\n+PRINT ksql__commands FROM BEGINNING;\r", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ3OTUzMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376479532", "bodyText": "main change here is switching the deprecated maxInterval for its replacement: msgRate", "author": "big-andy-coates", "createdAt": "2020-02-07T16:19:24Z", "path": "docs-md/developer-guide/test-and-debug/generate-custom-test-data.md", "diffHunk": "@@ -66,20 +66,17 @@ Use the following command to generate records from one of the predefined schemas\n \n The following options apply to both the `schema` and `quickstart` options.\n \n-|                     Name                     |                       Default                       |                                                              Description                                                              |                                                      |                              |\n-| -------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- | ---------------------------- |\n-| ``bootstrap-server=<kafka-server>:<port>``   | localhost:9092                                      | IP address and port for the Kafka server to connect to.                                                                               |                                                      |                              |\n-| ``key-format=<key format>``                  | Kafka                                               | Format of generated record keys: ``avro``, ``json``, ``delimited`` or ``kafka``. Case-insensitive. Required by the ``schema`` option. |                                                      |                              |\n-| ``value-format=<value format>``              | JSON                                                | Format of generated record values: ``avro``, ``json``, or ``delimited``.                                                              | Case-insensitive. Required by the ``schema`` option. |                              |\n-| ``topic=<kafka topic name>``                 |                                                     | Name of the topic that receives generated records. Required by the ``schema`` option.                                                 |                                                      |                              |\n-| ``key=<name of key column>``                 |                                                     | Field to use as the key for generated records. Required by the ``schema`` option.                                                     |                                                      |                              |\n-| ``iterations=<number of records>``           | 1,000,000                                           | The maximum number of records to generate.                                                                                            |                                                      |                              |\n-| ``maxInterval=<max time between records>``   | 500                                                 | Longest time to wait before generating a new record, in milliseconds.                                                                 |                                                      |                              |\n-| ``propertiesFile=<path-to-properties-file>`` | ``<path-to-confluent>/etc/ksql/datagen.properties`` | Path to the ``ksql-datagen`` properties file.                                                                                         |                                                      |                              |\n-| ``schemaRegistryUrl``                        | http://localhost:8081                               | URL of {{ site.sr }} when ``format`` is ``avro``. |\n-\n-Records are generated at random intervals, with the longest interval specified\n-by the `maxInterval` option.\n+|                     Name                     |                       Default                       |                                                              Description                                                              |                                                      |\n+| -------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |\n+| ``bootstrap-server=<kafka-server>:<port>``   | localhost:9092                                      | IP address and port for the Kafka server to connect to.                                                                               |                                                      |\n+| ``key-format=<key format>``                  | Kafka                                               | Format of generated record keys: ``avro``, ``json``, ``delimited`` or ``kafka``. Case-insensitive. Required by the ``schema`` option. |                                                      |\n+| ``value-format=<value format>``              | JSON                                                | Format of generated record values: ``avro``, ``json``, or ``delimited``.                                                              | Case-insensitive. Required by the ``schema`` option. |\n+| ``topic=<kafka topic name>``                 |                                                     | Name of the topic that receives generated records. Required by the ``schema`` option.                                                 |                                                      |\n+| ``key=<name of key column>``                 |                                                     | Field to use as the key for generated records. Required by the ``schema`` option.                                                     |                                                      |\n+| ``iterations=<number of records>``           | 1,000,000                                           | The maximum number of records to generate.                                                                                            |                                                      |\n+| ``msgRate=<rate to produce in msgs/second>`` | -1 (unlimited, i.e. as fast as possible)            | The rate to produce messages at, in messages-per-second.                                                                              |                                                      |\n+| ``propertiesFile=<path-to-properties-file>`` | ``<path-to-confluent>/etc/ksql/datagen.properties`` | Path to the ``ksql-datagen`` properties file.                                                                                         |                                                      |\n+| ``schemaRegistryUrl``                        | http://localhost:8081                               | URL of {{ site.sr }} when ``format`` is ``avro``.                                                                                     |                                                      |", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MDAyNg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376480026", "bodyText": "I've switched this to Avro as I've had to switch later examples from Avro to Json as they have non-STRING keys and Avro schema inference currently only works with STRING keys :(", "author": "big-andy-coates", "createdAt": "2020-02-07T16:20:20Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -85,9 +85,9 @@ docker run --network tutorials_default --rm --name datagen-users \\\n     ksql-datagen \\\n         bootstrap-server=kafka:39092 \\\n         quickstart=users \\\n-        format=json \\\n+        format=avro \\", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MDQ5Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376480496", "bodyText": "Added details of new SHOW ALL TOPICS command!", "author": "big-andy-coates", "createdAt": "2020-02-07T16:21:04Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas \n+--------------------------------------------------------------------------\n+ __confluent.support.metrics            | 1          | 1                  \n+ _confluent-ksql-default__command_topic | 1          | 1                  \n+ _confluent-license                     | 1          | 1                  \n+ _confluent-metrics                     | 12         | 1                  \n+ default_ksql_processing_log            | 1          | 1                  \n+ pageviews                              | 1          | 1                  \n+ users                                  | 1          | 1                  \n+--------------------------------------------------------------------------", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MDk1Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376480957", "bodyText": "Call out the processing log!", "author": "big-andy-coates", "createdAt": "2020-02-07T16:21:48Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MTczNQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376481735", "bodyText": "I'm not sure when, but the default behaviour of push queries on tables has changed: they now output all data without the need to set auto.offset.reset session property.\nThere may be other docs that need updating to reflect this @JimGalasyn", "author": "big-andy-coates", "createdAt": "2020-02-07T16:23:16Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MTk0OA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376481948", "bodyText": "Note: Streams still do start from the latest offset by default.", "author": "big-andy-coates", "createdAt": "2020-02-07T16:23:37Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MjQ2Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376482463", "bodyText": "Switched the example to something more useful to demo how a user might first get the query working with a transient query, before then converting it to a persistent query in the next step.", "author": "big-andy-coates", "createdAt": "2020-02-07T16:24:31Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4Mjc0OA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376482748", "bodyText": "Added section on pull queries!", "author": "big-andy-coates", "createdAt": "2020-02-07T16:25:06Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MzE0MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376483140", "bodyText": "had to switch to Json as the key type is not a STRING, so schema inference does not work yet :(", "author": "big-andy-coates", "createdAt": "2020-02-07T16:25:51Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -522,18 +741,27 @@ docker run --network tutorials_default --rm  \\\n   confluentinc/ksql-examples:{{ site.release }} \\\n   ksql-datagen \\\n       quickstart=orders \\\n-      format=avro \\\n+      format=json \\", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MzQzNA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376483434", "bodyText": "Switched from using kafkacat to using INSERT VALUES!", "author": "big-andy-coates", "createdAt": "2020-02-07T16:26:25Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -651,10 +857,29 @@ After both `CREATE STREAM` statements, your output should resemble:\n ----------------\n ```\n \n+### 2. Populate two source topics\n+\n+Populate the streams with some sample data using the INSERT VALUES statement:\n+\n+```sql\n+-- Insert values in NEW_ORDERS:\n+-- insert supplying the list of columns to insert:\n+INSERT INTO NEW_ORDERS (ROWKEY, CUSTOMER_NAME, TOTAL_AMOUNT) \n+  VALUES (1, 'Bob Smith', 10.50);\n+  \n+-- short hand version can be used when inserting values for all columns, (except ROWTIME), in column order:\n+INSERT INTO NEW_ORDERS  VALUES (2, 3.32, 'Sarah Black');\n+INSERT INTO NEW_ORDERS  VALUES (3, 21.00, 'Emma Turner');\n+\n+-- Insert values in SHIPMENTS:\n+INSERT INTO SHIPMENTS VALUES (1, 42, 'Nashville');\n+INSERT INTO SHIPMENTS VALUES (3, 43, 'Palo Alto');\n+```\n+", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MzY2Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376483662", "bodyText": "Added steps to show real-time updates on a running push query.", "author": "big-andy-coates", "createdAt": "2020-02-07T16:26:49Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -701,25 +936,56 @@ Run the following query, which will show orders with associated\n shipments, based on a join window of 1 hour.\n \n ```sql\n-SELECT O.ORDER_ID, O.TOTAL_AMOUNT, O.CUSTOMER_NAME,\n+SELECT O.ROWKEY AS ORDER_ID, O.TOTAL_AMOUNT, O.CUSTOMER_NAME,\n S.SHIPMENT_ID, S.WAREHOUSE\n FROM NEW_ORDERS O\n INNER JOIN SHIPMENTS S\n   WITHIN 1 HOURS\n-  ON O.ORDER_ID = S.ORDER_ID;\n+  ON O.ROWKEY = S.ROWKEY\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-1 | 10.5 | Bob Smith | 42 | Nashville\n-3 | 21.0 | Emma Turner | 43 | Palo Alto\n++--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n+|ORDER_ID                  |TOTAL_AMOUNT              |CUSTOMER_NAME             |SHIPMENT_ID               |WAREHOUSE                 |\n++--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n+|1                         |10.5                      |Bob Smith                 |42                        |Nashville                 |\n+|3                         |21.0                      |Emma Turner               |43                        |Palo Alto                 |\n ```\n \n Messages with `ORDER_ID=2` have no corresponding `SHIPMENT_ID` or\n `WAREHOUSE`. This is because there's no corresponding row on the\n `SHIPMENTS` stream within the time window specified.\n \n+Start the ksqlDB CLI in a second window by running:\n+\n+```bash\n+docker run --network tutorials_default --rm --interactive --tty \\\n+    confluentinc/ksqldb-cli:{{ site.release }} ksql \\\n+    http://ksql-server:8088\n+```\n+\n+Enter the following INSERT VALUES statement to insert the shipment for\n+order id 2:\n+\n+```sql\n+INSERT INTO SHIPMENTS VALUES (2, 49, 'London');\n+```\n+\n+Switching back to your primary ksqlDB CLI window, notice that a third\n+row has now been output:\n+\n+```\n++--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n+|ORDER_ID                  |TOTAL_AMOUNT              |CUSTOMER_NAME             |SHIPMENT_ID               |WAREHOUSE                 |\n++--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n+|1                         |10.5                      |Bob Smith                 |42                        |Nashville                 |\n+|3                         |21.0                      |Emma Turner               |43                        |Palo Alto                 |\n+|2                         |3.32                      |Sarah Black               |49                        |London                    |\n+```\n+", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4Mzk4OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376483989", "bodyText": "DataGen now supports non-STRING keys.", "author": "big-andy-coates", "createdAt": "2020-02-07T16:27:27Z", "path": "ksql-examples/src/main/java/io/confluent/ksql/datagen/RowGenerator.java", "diffHunk": "@@ -38,39 +39,39 @@\n import org.apache.avro.generic.GenericRecord;\n import org.apache.kafka.connect.data.ConnectSchema;\n import org.apache.kafka.connect.data.Field;\n-import org.apache.kafka.connect.data.SchemaBuilder;\n import org.apache.kafka.connect.data.Struct;\n \n public class RowGenerator {\n \n-  static final ConnectSchema KEY_SCHEMA = (ConnectSchema) SchemaBuilder.struct()\n-      .field(\n-          SchemaUtil.ROWKEY_NAME.name(),\n-          org.apache.kafka.connect.data.Schema.OPTIONAL_STRING_SCHEMA)\n-      .build();\n-", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5MzUxNw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376493517", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            sets the timestamp of the record written to Kafka. The value of system columns\n          \n          \n            \n            sets the timestamp of the record written to {{ site.ak }}. The value of system columns", "author": "JimGalasyn", "createdAt": "2020-02-07T16:45:54Z", "path": "docs-md/concepts/stream-processing.md", "diffHunk": "@@ -27,8 +27,10 @@ collection by using the `SELECT` statement on an existing collection. The\n result of the inner `SELECT` feeds into the outer declared collection. You\n don't need to declare a schema when deriving a new collection, because ksqlDB\n infers the column names and types from the inner `SELECT` statement. The\n-`ROWKEY` and `ROWTIME` fields of each row remain, unless you override them in\n-the `SELECT` statement.\n+`ROWKEY` of the row remains the same, unless the query includes either a\n+`PARTITION BY` or `GROUP BY` clause. The value of the `ROWTIME` column\n+sets the timestamp of the record written to Kafka. The value of system columns", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NDMyOQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376494329", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ksqlDB requires keys to have been serialized using Kafka's own, or compatible,\n          \n          \n            \n            ksqlDB requires keys to have been serialized using {{ site.ak }}'s own serializers or compatible", "author": "JimGalasyn", "createdAt": "2020-02-07T16:47:26Z", "path": "docs-md/developer-guide/create-a-stream.md", "diffHunk": "@@ -40,6 +40,9 @@ data format is `DELIMITED`. Other options are `Avro`, `JSON` and `KAFKA`.\n See [Serialization Formats](serialization.md#serialization-formats) for more\n details.\n \n+ksqlDB requires keys to have been serialized using Kafka's own, or compatible,", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NDc1OA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376494758", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ksqlDB requires keys to have been serialized using Kafka's own, or compatible,\n          \n          \n            \n            ksqlDB requires keys to have been serialized using {{ site.ak }}'s own serializers or compatible", "author": "JimGalasyn", "createdAt": "2020-02-07T16:48:19Z", "path": "docs-md/developer-guide/create-a-table.md", "diffHunk": "@@ -45,6 +45,9 @@ data format is `JSON`. Other options are `Avro`, `DELIMITED` and `KAFKA`. For\n more information, see\n [Serialization Formats](serialization.md#serialization-formats).\n \n+ksqlDB requires keys to have been serialized using Kafka's own, or compatible,", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NTg0NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376495844", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            repartitioning, the correct format will be used.\n          \n          \n            \n            repartitioning, ksqlDB uses the correct format.", "author": "JimGalasyn", "createdAt": "2020-02-07T16:50:22Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NjIzMw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376496233", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It is important that you set the type of the `ROWKEY` column in the\n          \n          \n            \n            You must set the type of the `ROWKEY` column in the", "author": "JimGalasyn", "createdAt": "2020-02-07T16:51:07Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NjQ0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376496445", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            CREATE TABLE statement to match the key data in the underlying Kafka topic.\n          \n          \n            \n            CREATE TABLE statement to match the key data in the underlying {{ site.ak }} topic.", "author": "JimGalasyn", "createdAt": "2020-02-07T16:51:29Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5Njc0Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376496747", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - When grouping by a single column or expression the type of `ROWKEY` in the\n          \n          \n            \n                - When grouping by a single column or expression, the type of `ROWKEY` in the", "author": "JimGalasyn", "createdAt": "2020-02-07T16:52:07Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5Njg2Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376496862", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n          \n          \n            \n                - When grouping by multiple columns or expressions, the type of `ROWKEY` in the", "author": "JimGalasyn", "createdAt": "2020-02-07T16:52:21Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NzEzMQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376497131", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - When grouping by a single column or expression the type of `ROWKEY` in the\n          \n          \n            \n                - When grouping by a single column or expression, the type of `ROWKEY` in the", "author": "JimGalasyn", "createdAt": "2020-02-07T16:52:52Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+- If the FROM clause contains only tables and no GROUP BY clause, the key is\n+  copied over from the key of the table(s) in the FROM clause.\n+- If the FROM clause contains only tables and has a GROUP BY clause, the\n+  grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5NzIzOQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376497239", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n          \n          \n            \n                - When grouping by multiple columns or expressions, the type of `ROWKEY` in the", "author": "JimGalasyn", "createdAt": "2020-02-07T16:53:06Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+- If the FROM clause contains only tables and no GROUP BY clause, the key is\n+  copied over from the key of the table(s) in the FROM clause.\n+- If the FROM clause contains only tables and has a GROUP BY clause, the\n+  grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5Nzg0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376497845", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                -- join of users table with clicks stream, joining on the tables primary key alias and the streams userId column: \n          \n          \n            \n                -- join of users table with clicks stream, joining on the table's primary key alias and the stream's userId column:", "author": "JimGalasyn", "createdAt": "2020-02-07T16:54:13Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -16,67 +16,70 @@ records based on the joining column. To ensure that records with the same\n join column are co-located on the same stream task, the join column must\n coincide with the column that the sources are partitioned by.\n \n-Primary key\n------------\n+Keys\n+----\n \n-A *primary key*, when present, defines the partitioning column. Tables are\n+A *key*, when present, defines the partitioning column. Tables are\n always partitioned by their primary key, and ksqlDB doesn't allow repartitioning\n of tables, so you can only use a table's primary key as a join column.\n \n Streams, on the other hand, may not have a defined key or may have a key that\n-differs from the join column. In these cases, ksqlDB repartitions the stream,\n-which implicitly defines a primary key for it. The primary keys for streams\n-and tables are of data type `VARCHAR`. \n+differs from the join column. In these cases, ksqlDB internally repartitions\n+the stream, which implicitly defines a key for it.\n \n-For primary keys to match, they must have the same serialization format. For\n-example, you can't join a `VARCHAR` key encoded as JSON with one encoded as\n-AVRO.\n-\n-!!! note\n-    ksqlDB requires that keys are encoded as UTF-8 strings.\n+ksqlDB requires keys to use the `KAFKA` format. For more information, see\n+[Serialization Formats](serialization.md#serialization-formats). If internally\n+repartitioning, the correct format will be used.\n \n Because you can only use the primary key of a table as a joining column, it's\n important to understand how keys are defined. For both streams and tables, the\n-column that represents the primary key has the name `ROWKEY`.\n+column that represents the key has the name `ROWKEY`.\n \n When you create a table by using a CREATE TABLE statement, the key of the\n table is the same as that of the records in the underlying Kafka topic.\n+It is important that you set the type of the `ROWKEY` column in the\n+CREATE TABLE statement to match the key data in the underlying Kafka topic.\n \n When you create a table by using a CREATE TABLE AS SELECT statement, the key of\n the resulting table is determined as follows:\n \n-- If the FROM clause is a single source, and the source is a stream, the\n-  statement must have a GROUP BY clause, where the grouping columns determine\n-  the key of the resulting table.\n-- If the single source is a table, the key is copied over from the key of the\n-  table in the FROM clause. If the FROM clause is a join, the primary key of the\n-  resulting table is the joining column, since joins are allowed only on keys.\n-- If the statement contains a GROUP BY, the key of the resulting table\n-  comprises the grouping columns.\n-\n-When the primary key consists of multiple columns, like when it's created as\n-the result of a GROUP BY clause with multiple grouping columns, you must use\n-ROWKEY as the joining column. Even when the primary key consists of a single\n-column, we recommend using ROWKEY as the joining column to avoid confusion.\n-\n-The following example shows a `users` table joined with a `clicks` stream \n+- If the FROM clause contains a stream, the statement must have a GROUP BY clause,\n+  and the grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+- If the FROM clause contains only tables and no GROUP BY clause, the key is\n+  copied over from the key of the table(s) in the FROM clause.\n+- If the FROM clause contains only tables and has a GROUP BY clause, the\n+  grouping columns determine the key of the resulting table.\n+    - When grouping by a single column or expression the type of `ROWKEY` in the\n+    resulting stream matches the type of the column or expression.\n+    - When grouping by multiple columns or expressions the type of `ROWKEY` in the\n+    resulting stream is a `STRING`.\n+\n+The following example shows a `users` table joined with a `clicks` stream\n on the `userId` column. The `users` table has the correct primary key\n `userId` that coincides with the joining column. But the `clicks` stream\n doesn't have a defined key, and ksqlDB must repartition it on the joining\n-column (`userId`) and assign the primary key before performing the join.\n+column (`userId`) and assign the key before performing the join.\n \n ```sql\n     -- clicks stream, with an unknown key.\n-    -- the schema of stream clicks is: ROWTIME | ROWKEY | USERID | URL\n-    CREATE STREAM clicks (userId STRING, url STRING) WITH(kafka_topic='clickstream', value_format='json');\n+    -- the schema of stream clicks is: ROWTIME BIGINT | ROWKEY STRING | USERID BIGINT | URL STRING\n+    CREATE STREAM clicks (userId BIGINT, url STRING) WITH(kafka_topic='clickstream', value_format='json');\n \n-    -- the primary key of table users becomes userId that is the key of the records topic:\n-    -- the schema of table users is: ROWTIME | ROWKEY | USERID | URL\n-    CREATE TABLE  users  (userId STRING, fullName STRING) WITH(kafka_topic='users', value_format='json', key='userId');\n+    -- the primary key of table users is a BIGINT. \n+    -- The userId column in the value matches the key, so can be used as an alias for ROWKEY in queries to make them more readable.\n+    -- the schema of table users is: ROWTIME BIGINT | ROWKEY BIGINT | USERID BIGINT | FULLNAME STRING\n+    CREATE TABLE  users  (ROWKEY BIGINT KEY, userId BIGINT, fullName STRING) WITH(kafka_topic='users', value_format='json', key='userId');\n \n-    -- join using primary key of table users with newly assigned key of stream clicks\n+    -- join of users table with clicks stream, joining on the tables primary key alias and the streams userId column: ", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5OTk5OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376499999", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If the schema of the columns you wish to join on do not match it may be possible\n          \n          \n            \n            If the schema of the columns you wish to join on don't match, it may be possible", "author": "JimGalasyn", "createdAt": "2020-02-07T16:58:31Z", "path": "docs-md/developer-guide/joins/partition-data.md", "diffHunk": "@@ -98,26 +101,35 @@ When your inputs are co-partitioned, records with the same key, from\n both sides of the join, are delivered to the same stream task during\n processing.\n \n-### Records Have the Same Keying Scheme\n+### Records Have the Same Keying Schema\n \n-For a join to work, the keys from both sides must have the same serialized\n-binary data.\n+For a join to work, the keys from both sides must have the same SQL type.\n \n For example, you can join a stream of user clicks that's keyed on a `VARCHAR`\n user id with a table of user profiles that's also keyed on a `VARCHAR` user id.\n Records with the exact same user id on both sides will be joined.\n \n-ksqlDB requires that keys are UTF-8 encoded strings.\n+If the schema of the columns you wish to join on do not match it may be possible", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMTA4Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376501086", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                userid) in the message value but the actual message key in Kafka is\n          \n          \n            \n                `userid`) in the message value, but the actual message key in {{ site.ak }} is", "author": "JimGalasyn", "createdAt": "2020-02-07T17:00:46Z", "path": "docs-md/developer-guide/syntax-reference.md", "diffHunk": "@@ -458,63 +460,47 @@ Example:\n \n -   Goal: You want to create a table from a topic, which is keyed by\n     userid of type INT.\n--   Problem: The message key is present as a field/column (aptly named\n-    userid) in the message value but in the wrong format (INT instead of\n-    VARCHAR).\n+-   Problem: The required key is present as a field/column (aptly named\n+    userid) in the message value but the actual message key in Kafka is", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMTU0Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376501547", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            -- Or, if you would prefer, you can keep userId in the value of the repartitioned data\n          \n          \n            \n            -- Or, if you prefer, you can keep userId in the value of the repartitioned data", "author": "JimGalasyn", "createdAt": "2020-02-07T17:01:41Z", "path": "docs-md/developer-guide/syntax-reference.md", "diffHunk": "@@ -458,63 +460,47 @@ Example:\n \n -   Goal: You want to create a table from a topic, which is keyed by\n     userid of type INT.\n--   Problem: The message key is present as a field/column (aptly named\n-    userid) in the message value but in the wrong format (INT instead of\n-    VARCHAR).\n+-   Problem: The required key is present as a field/column (aptly named\n+    userid) in the message value but the actual message key in Kafka is\n+    not set or has some other value or format.\n \n ```sql\n -- Create a stream on the original topic\n-CREATE STREAM users_with_wrong_key_format (userid INT, username VARCHAR, email VARCHAR)\n+CREATE STREAM users_with_wrong_key (userid INT, username VARCHAR, email VARCHAR)\n   WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON');\n-\n+  \n -- Derive a new stream with the required key changes.\n -- 1) The CAST statement converts the key to the required format.\n -- 2) The PARTITION BY clause re-partitions the stream based on the new, converted key.\n+-- The resulting schema will be: ROWKEY INT, USERNAME STRING, EMAIL STRING\n+-- the userId will be stored in ROWKEY.\n CREATE STREAM users_with_proper_key\n   WITH(KAFKA_TOPIC='users-with-proper-key') AS\n-  SELECT CAST(userid as VARCHAR) as userid_string, username, email\n-  FROM users_with_wrong_key_format\n-  PARTITION BY userid_string\n+  SELECT username, email\n+  FROM users_with_wrong_key\n+  PARTITION BY userid\n   EMIT CHANGES;\n \n -- Now you can create the table on the properly keyed stream.\n-CREATE TABLE users_table (userid_string VARCHAR, username VARCHAR, email VARCHAR)\n+CREATE TABLE users_table (ROWKEY INT KEY, username VARCHAR, email VARCHAR)\n   WITH (KAFKA_TOPIC='users-with-proper-key',\n-        VALUE_FORMAT='JSON',\n-        KEY='userid_string');\n-```\n-\n-Example:\n-\n--   Goal: You want to create a table from a topic, which is keyed by\n-    userid of type INT.\n--   Problem: The message key is not present as a field/column in the\n-    topic's message values.\n-\n-```sql\n--- Create a stream on the original topic.\n--- The topic is keyed by userid, which is available as the implicit column ROWKEY\n--- in the users_with_missing_key stream. Note how the explicit column definitions\n--- only define username and email but not userid.\n-CREATE STREAM users_with_missing_key (username VARCHAR, email VARCHAR)\n-  WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON');\n-\n--- Derive a new stream with the required key changes.\n--- 1) The contents of ROWKEY (message key) are copied into the message value as the userid_string column,\n---    and the CAST statement converts the key to the required format.\n--- 2) The PARTITION BY clause re-partitions the stream based on the new, converted key.\n-CREATE STREAM users_with_proper_key\n-  WITH(KAFKA_TOPIC='users-with-proper-key') AS\n-  SELECT CAST(ROWKEY as VARCHAR) as userid_string, username, email\n-  FROM users_with_missing_key\n-  PARTITION BY userid_string\n+        VALUE_FORMAT='JSON');\n+\n+-- Or, if you would prefer, you can keep userId in the value of the repartitioned data", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMTczMQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376501731", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            -- This would allow you to use the more descriptive `userId` rather than ROWTIME.\n          \n          \n            \n            -- This enables using the more descriptive `userId` rather than ROWTIME.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:02:05Z", "path": "docs-md/developer-guide/syntax-reference.md", "diffHunk": "@@ -458,63 +460,47 @@ Example:\n \n -   Goal: You want to create a table from a topic, which is keyed by\n     userid of type INT.\n--   Problem: The message key is present as a field/column (aptly named\n-    userid) in the message value but in the wrong format (INT instead of\n-    VARCHAR).\n+-   Problem: The required key is present as a field/column (aptly named\n+    userid) in the message value but the actual message key in Kafka is\n+    not set or has some other value or format.\n \n ```sql\n -- Create a stream on the original topic\n-CREATE STREAM users_with_wrong_key_format (userid INT, username VARCHAR, email VARCHAR)\n+CREATE STREAM users_with_wrong_key (userid INT, username VARCHAR, email VARCHAR)\n   WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON');\n-\n+  \n -- Derive a new stream with the required key changes.\n -- 1) The CAST statement converts the key to the required format.\n -- 2) The PARTITION BY clause re-partitions the stream based on the new, converted key.\n+-- The resulting schema will be: ROWKEY INT, USERNAME STRING, EMAIL STRING\n+-- the userId will be stored in ROWKEY.\n CREATE STREAM users_with_proper_key\n   WITH(KAFKA_TOPIC='users-with-proper-key') AS\n-  SELECT CAST(userid as VARCHAR) as userid_string, username, email\n-  FROM users_with_wrong_key_format\n-  PARTITION BY userid_string\n+  SELECT username, email\n+  FROM users_with_wrong_key\n+  PARTITION BY userid\n   EMIT CHANGES;\n \n -- Now you can create the table on the properly keyed stream.\n-CREATE TABLE users_table (userid_string VARCHAR, username VARCHAR, email VARCHAR)\n+CREATE TABLE users_table (ROWKEY INT KEY, username VARCHAR, email VARCHAR)\n   WITH (KAFKA_TOPIC='users-with-proper-key',\n-        VALUE_FORMAT='JSON',\n-        KEY='userid_string');\n-```\n-\n-Example:\n-\n--   Goal: You want to create a table from a topic, which is keyed by\n-    userid of type INT.\n--   Problem: The message key is not present as a field/column in the\n-    topic's message values.\n-\n-```sql\n--- Create a stream on the original topic.\n--- The topic is keyed by userid, which is available as the implicit column ROWKEY\n--- in the users_with_missing_key stream. Note how the explicit column definitions\n--- only define username and email but not userid.\n-CREATE STREAM users_with_missing_key (username VARCHAR, email VARCHAR)\n-  WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON');\n-\n--- Derive a new stream with the required key changes.\n--- 1) The contents of ROWKEY (message key) are copied into the message value as the userid_string column,\n---    and the CAST statement converts the key to the required format.\n--- 2) The PARTITION BY clause re-partitions the stream based on the new, converted key.\n-CREATE STREAM users_with_proper_key\n-  WITH(KAFKA_TOPIC='users-with-proper-key') AS\n-  SELECT CAST(ROWKEY as VARCHAR) as userid_string, username, email\n-  FROM users_with_missing_key\n-  PARTITION BY userid_string\n+        VALUE_FORMAT='JSON');\n+\n+-- Or, if you would prefer, you can keep userId in the value of the repartitioned data\n+-- This would allow you to use the more descriptive `userId` rather than ROWTIME.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMjUxMw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376502513", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n          \n          \n            \n            By default, KSQL hides internal and system topics. Use the SHOW ALL TOPICS", "author": "JimGalasyn", "createdAt": "2020-02-07T17:03:46Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMjY0Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376502642", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            statement to see the full list of topics in the Kafka cluster:\n          \n          \n            \n            statement to see the full list of topics in the {{ site.ak }} cluster:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:04:03Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMjgxOA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376502818", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              Kafka Topic                            | Partitions | Partition Replicas \n          \n          \n            \n             Kafka Topic                            | Partitions | Partition Replicas", "author": "JimGalasyn", "createdAt": "2020-02-07T17:04:29Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas ", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMzMxMA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376503310", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               The PRINT statement is one of the few commands in ksqlDB that are\n          \n          \n            \n               The PRINT statement is one of the few case-sensitive commands in ksqlDB,", "author": "JimGalasyn", "createdAt": "2020-02-07T17:05:40Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas \n+--------------------------------------------------------------------------\n+ __confluent.support.metrics            | 1          | 1                  \n+ _confluent-ksql-default__command_topic | 1          | 1                  \n+ _confluent-license                     | 1          | 1                  \n+ _confluent-metrics                     | 12         | 1                  \n+ default_ksql_processing_log            | 1          | 1                  \n+ pageviews                              | 1          | 1                  \n+ users                                  | 1          | 1                  \n+--------------------------------------------------------------------------\n ```\n \n Inspect the `users` topic by using the PRINT statement:\n \n ```sql\n-PRINT 'users';\n+PRINT users;\n ```\n \n+!!! note\n+   The PRINT statement is one of the few commands in ksqlDB that are", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMzQyMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376503422", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               case-sensitive, even when the topic name is not quoted.\n          \n          \n            \n               even when the topic name is not quoted.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:05:56Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -141,25 +141,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas \n+--------------------------------------------------------------------------\n+ __confluent.support.metrics            | 1          | 1                  \n+ _confluent-ksql-default__command_topic | 1          | 1                  \n+ _confluent-license                     | 1          | 1                  \n+ _confluent-metrics                     | 12         | 1                  \n+ default_ksql_processing_log            | 1          | 1                  \n+ pageviews                              | 1          | 1                  \n+ users                                  | 1          | 1                  \n+--------------------------------------------------------------------------\n ```\n \n Inspect the `users` topic by using the PRINT statement:\n \n ```sql\n-PRINT 'users';\n+PRINT users;\n ```\n \n+!!! note\n+   The PRINT statement is one of the few commands in ksqlDB that are\n+   case-sensitive, even when the topic name is not quoted.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMzcxNA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376503714", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n          \n          \n            \n                  is Avro, and the DataGen tool publishes the Avro schema to {{ site.sr }}.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:06:33Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwNDM2Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376504363", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  ksqlDB retrieves the schema from the Schema Registry and uses this to build\n          \n          \n            \n                  ksqlDB retrieves the schema from {{ site.sr }} and uses this to build", "author": "JimGalasyn", "createdAt": "2020-02-07T17:08:01Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwNDY3OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376504679", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  The data generated has the same value in the Kafka record's key\n          \n          \n            \n                  The data generated has the same value in the {{ site.ak }} record's key", "author": "JimGalasyn", "createdAt": "2020-02-07T17:08:41Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwNTI5Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376505292", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  as is in the userId field in the value. Specifying `key='userId'`\n          \n          \n            \n                  as the `userId` field in the value. Specifying `key='userId'`", "author": "JimGalasyn", "createdAt": "2020-02-07T17:09:59Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key\n+      as is in the userId field in the value. Specifying `key='userId'`", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "79ea5a1457dda4786f6600bff3ab14e0ced10bcd", "url": "https://github.com/confluentinc/ksql/commit/79ea5a1457dda4786f6600bff3ab14e0ced10bcd", "message": "Update docs-md/concepts/stream-processing.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:10:55Z", "type": "commit"}, {"oid": "95389863e2834c1ad1b10e9a74e8383a9603466b", "url": "https://github.com/confluentinc/ksql/commit/95389863e2834c1ad1b10e9a74e8383a9603466b", "message": "Update docs-md/developer-guide/create-a-stream.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:11:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwNTg1MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376505850", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  will be used later allow joins against the table to use the more\n          \n          \n            \n                  to allow joins against the table to use the more", "author": "JimGalasyn", "createdAt": "2020-02-07T17:11:13Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key\n+      as is in the userId field in the value. Specifying `key='userId'`\n+      in the WITH clause above lets ksqlDB know this. This information\n+      will be used later allow joins against the table to use the more", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "77d19a843410f158a627e3cf4866037b542bca57", "url": "https://github.com/confluentinc/ksql/commit/77d19a843410f158a627e3cf4866037b542bca57", "message": "Update docs-md/developer-guide/create-a-table.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:11:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwNjAwNg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376506006", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  in the WITH clause above lets ksqlDB know this. This information\n          \n          \n            \n                  in the WITH clause above lets ksqlDB know this. ksqlDB uses this information", "author": "JimGalasyn", "createdAt": "2020-02-07T17:11:31Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key\n+      as is in the userId field in the value. Specifying `key='userId'`\n+      in the WITH clause above lets ksqlDB know this. This information", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dd56b0e03bd7d4459f6d110b015aec497926d65e", "url": "https://github.com/confluentinc/ksql/commit/dd56b0e03bd7d4459f6d110b015aec497926d65e", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:11:37Z", "type": "commit"}, {"oid": "07090ffed0406fa2a8e480c88ee8676091ab8d00", "url": "https://github.com/confluentinc/ksql/commit/07090ffed0406fa2a8e480c88ee8676091ab8d00", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:11:49Z", "type": "commit"}, {"oid": "c67bcc86b4be557753a7e1751ab25f21a01454a2", "url": "https://github.com/confluentinc/ksql/commit/c67bcc86b4be557753a7e1751ab25f21a01454a2", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:12:03Z", "type": "commit"}, {"oid": "20d9577a6da5f466bedebfbe2dae2fd046063058", "url": "https://github.com/confluentinc/ksql/commit/20d9577a6da5f466bedebfbe2dae2fd046063058", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:12:15Z", "type": "commit"}, {"oid": "2d5e096cbb3f122d1ada223f109544bb59ec72e0", "url": "https://github.com/confluentinc/ksql/commit/2d5e096cbb3f122d1ada223f109544bb59ec72e0", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:12:26Z", "type": "commit"}, {"oid": "f00ed9dd47a2c9be861bc20c190e0a304c09c5c0", "url": "https://github.com/confluentinc/ksql/commit/f00ed9dd47a2c9be861bc20c190e0a304c09c5c0", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:13:12Z", "type": "commit"}, {"oid": "f46ab705dce7ba3628e9a2dacb124c376b5527c7", "url": "https://github.com/confluentinc/ksql/commit/f46ab705dce7ba3628e9a2dacb124c376b5527c7", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:13:21Z", "type": "commit"}, {"oid": "35a0297889559e538d1653a1f129c998aad2937c", "url": "https://github.com/confluentinc/ksql/commit/35a0297889559e538d1653a1f129c998aad2937c", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:15:56Z", "type": "commit"}, {"oid": "9c6d1a9a0699705b749edc0de17d56ec801f4445", "url": "https://github.com/confluentinc/ksql/commit/9c6d1a9a0699705b749edc0de17d56ec801f4445", "message": "Update docs-md/developer-guide/joins/partition-data.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:16:45Z", "type": "commit"}, {"oid": "42b44710986f85f901f4b31929fae64d11063a16", "url": "https://github.com/confluentinc/ksql/commit/42b44710986f85f901f4b31929fae64d11063a16", "message": "Update docs-md/developer-guide/syntax-reference.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:16:56Z", "type": "commit"}, {"oid": "42f30e02aa3df86677fb87a8a222bd1b109644d3", "url": "https://github.com/confluentinc/ksql/commit/42f30e02aa3df86677fb87a8a222bd1b109644d3", "message": "Update docs-md/developer-guide/syntax-reference.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:17:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMDIyNA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376510224", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  on either will yield the same results. If your data does not\n          \n          \n            \n                  on either yields the same results. If your data doesn't", "author": "JimGalasyn", "createdAt": "2020-02-07T17:21:04Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key\n+      as is in the userId field in the value. Specifying `key='userId'`\n+      in the WITH clause above lets ksqlDB know this. This information\n+      will be used later allow joins against the table to use the more\n+      descriptive `userId` column name, rather than `ROWKEY`. Joining\n+      on either will yield the same results. If your data does not", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMDM5OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376510399", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  contain a copy of the key in the value simply join on `ROWKEY`.\n          \n          \n            \n                  contain a copy of the key in the value, you can join on `ROWKEY`.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:21:27Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -243,6 +266,25 @@ Your output should resemble:\n ---------------\n ```\n \n+!!! note\n+      You may have noticed the CREATE TABLE did not define the set of columns\n+      like the CREATE STREAM statement did. This is because the value format\n+      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+      the SQL schema for the table. You may still provide the schema if you wish.\n+      Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+      is complete, schema inference is only available where the key of the data\n+      is a STRING, as is the case here.\n+\n+!!! note\n+      The data generated has the same value in the Kafka record's key\n+      as is in the userId field in the value. Specifying `key='userId'`\n+      in the WITH clause above lets ksqlDB know this. This information\n+      will be used later allow joins against the table to use the more\n+      descriptive `userId` column name, rather than `ROWKEY`. Joining\n+      on either will yield the same results. If your data does not\n+      contain a copy of the key in the value simply join on `ROWKEY`.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMDY5MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376510690", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                output? KsqlDB will append messages detailing any issues it\n          \n          \n            \n                output? ksqlDB appends messages that describe any issues it", "author": "JimGalasyn", "createdAt": "2020-02-07T17:22:14Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMDg5Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376510892", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                encountered while processing your data. If things are not working\n          \n          \n            \n                encountered while processing your data. If things aren't working", "author": "JimGalasyn", "createdAt": "2020-02-07T17:22:42Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMTA0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376511045", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                as you expect it can be worth checking the contents of this stream\n          \n          \n            \n                as you expect, check the contents of this stream", "author": "JimGalasyn", "createdAt": "2020-02-07T17:23:03Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMTU3Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376511577", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               indicate we wish to stream results back. This is known as a\n          \n          \n            \n               indicate you that want to stream results back. This is known as a", "author": "JimGalasyn", "createdAt": "2020-02-07T17:24:21Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMjA3Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376512073", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               [pull query](../concepts/queries/pull.md). See the\n          \n          \n            \n               [push query](../concepts/queries/push.md). See the", "author": "JimGalasyn", "createdAt": "2020-02-07T17:25:31Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMjU3Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376512577", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               [queries](../concepts/queries/index.md) for an explanation of the\n          \n          \n            \n               [queries](../concepts/queries/index.md) section for an explanation of the", "author": "JimGalasyn", "createdAt": "2020-02-07T17:26:34Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMjk0MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376512941", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     Push queries on tables will output the full history of the table that is stored\n          \n          \n            \n                     Push queries on tables output the full history of the table that is stored", "author": "JimGalasyn", "createdAt": "2020-02-07T17:27:20Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMzE2Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376513167", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     in the Kafka changelog topic, i.e. it will output historic data, followed by the\n          \n          \n            \n                     in the {{ site.ak }} changelog topic, which means that it outputs historic data, followed by the", "author": "JimGalasyn", "createdAt": "2020-02-07T17:27:57Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMzczMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376513732", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                     stream of updates to the table. It is therefore likely that rows with matching\n          \n          \n            \n                     stream of updates to the table. So it's likely that rows with matching", "author": "JimGalasyn", "createdAt": "2020-02-07T17:29:15Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxMzg3MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376513871", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. View the data in your pageviews_original stream by issuing the following\n          \n          \n            \n            2. View the data in your `pageviews_original` stream by issuing the following", "author": "JimGalasyn", "createdAt": "2020-02-07T17:29:37Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNDEwMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376514102", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  after the query is started, i.e. historic data is not included.\n          \n          \n            \n                  after the query is started, which means that historic data isn't included.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:30:12Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNDU2NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376514565", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create query that enriches the pageviews data with the user's gender\n          \n          \n            \n            Create a query that enriches the pageviews data with the user's `gender`", "author": "JimGalasyn", "createdAt": "2020-02-07T17:31:05Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNDc0NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376514745", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            and regionid from the users table. The following query enriches the\n          \n          \n            \n            and `regionid` from the `users` table. The following query enriches the", "author": "JimGalasyn", "createdAt": "2020-02-07T17:31:30Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNDkwMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376514902", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `users_original` TABLE on the userid column.\n          \n          \n            \n            `users_original` TABLE on the `userid` column.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:31:51Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNTA5OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376515099", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              The join to the users table is on the userid column, which was identified as\n          \n          \n            \n              The join to the `users` table is on the `userid` column, which was identified as", "author": "JimGalasyn", "createdAt": "2020-02-07T17:32:19Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNTI0Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376515246", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.\n          \n          \n            \n              an alias for the table's primary key, `ROWKEY`, in the CREATE TABLE statement.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:32:36Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as\n+  an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNTQ5Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376515497", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              the stream's key. Hence, ksqlDB will first internally repartition the stream\n          \n          \n            \n              the stream's key. Hence, ksqlDB internally repartitions the stream", "author": "JimGalasyn", "createdAt": "2020-02-07T17:33:13Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as\n+  an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.\n+  `userId` and `ROWKEY` can be used interchangeably as the join criteria for\n+  the table. However, the data in `userid` on the stream side does not match\n+  the stream's key. Hence, ksqlDB will first internally repartition the stream", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNTcwMw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376515703", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              by the `userId` column.\n          \n          \n            \n              by the `userId` column before performing the join.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:33:44Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as\n+  an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.\n+  `userId` and `ROWKEY` can be used interchangeably as the join criteria for\n+  the table. However, the data in `userid` on the stream side does not match\n+  the stream's key. Hence, ksqlDB will first internally repartition the stream\n+  by the `userId` column.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNjE2OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376516169", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The results from this query are written to the `PAGEVIEWS_ENRICHED` Kafka topic.\n          \n          \n            \n            The results from this query are written to the `PAGEVIEWS_ENRICHED` {{ site.ak }} topic.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:34:50Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as\n+  an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.\n+  `userId` and `ROWKEY` can be used interchangeably as the join criteria for\n+  the table. However, the data in `userid` on the stream side does not match\n+  the stream's key. Hence, ksqlDB will first internally repartition the stream\n+  by the `userId` column.\n+\n ### 2. Create a persistent query\n \n Create a persistent query by using the `CREATE STREAM` keywords to\n-precede the `SELECT` statement. The results from this query are\n-written to the `PAGEVIEWS_ENRICHED` Kafka topic. The following query\n-enriches the `pageviews_original` STREAM by doing a `LEFT JOIN` with\n-the `users_original` TABLE on the user ID.\n+precede the `SELECT` statement, and removing the `LIMIT` clause.\n+The results from this query are written to the `PAGEVIEWS_ENRICHED` Kafka topic.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNjY0OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376516649", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            precede the `SELECT` statement, and removing the `LIMIT` clause.\n          \n          \n            \n            precede the `SELECT` statement and removing the `LIMIT` clause.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:36:02Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -252,70 +294,153 @@ Optional: Show all streams and tables.\n ```\n ksql> SHOW STREAMS;\n \n-Stream Name              | Kafka Topic              | Format\n------------------------------------------------------------------\n-PAGEVIEWS_ORIGINAL       | pageviews                | DELIMITED\n+ Stream Name         | Kafka Topic                 | Format    \n+---------------------------------------------------------------\n+ KSQL_PROCESSING_LOG | default_ksql_processing_log | JSON      \n+ PAGEVIEWS_ORIGINAL  | pageviews                   | DELIMITED \n+---------------------------------------------------------------\n \n ksql> SHOW TABLES;\n \n-Table Name        | Kafka Topic       | Format    | Windowed\n---------------------------------------------------------------\n-USERS_ORIGINAL    | users             | JSON      | false\n+ Table Name     | Kafka Topic | Format | Windowed \n+--------------------------------------------------\n+ USERS_ORIGINAL | users       | AVRO   | false    \n+--------------------------------------------------\n ```\n \n+!!! tip\n+    Notice the `KSQL_PROCESSING_LOG` stream listed in the SHOW STREAMS\n+    output? KsqlDB will append messages detailing any issues it\n+    encountered while processing your data. If things are not working\n+    as you expect it can be worth checking the contents of this stream\n+    to see if ksqlDB is encountering data errors.\n+\n+Viewing your data\n+-----------------\n+\n+1. Use `SELECT` to create a query that returns data from a TABLE. This\n+   query includes the `LIMIT` keyword to limit the number of rows\n+   returned in the query result, and the `EMIT CHANGES` keywords to\n+   indicate we wish to stream results back. This is known as a\n+   [pull query](../concepts/queries/pull.md). See the\n+   [queries](../concepts/queries/index.md) for an explanation of the\n+   different query types. Note that exact data output may vary because\n+   of the randomness of the data generation.\n+   ```sql\n+   SELECT * from users_original emit changes limit 5;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |ROWTIME             |ROWKEY        |REGISTERTIME  |GENDER   |REGIONID  |USERID       |\n+   +--------------------+--------------+--------------+---------+----------+-------------+\n+   |1581077558655       |User_9        |1513529638461 |OTHER    |Region_1  |User_9       |\n+   |1581077561454       |User_7        |1489408314958 |OTHER    |Region_2  |User_7       |\n+   |1581077561654       |User_3        |1511291005264 |MALE     |Region_2  |User_3       |\n+   |1581077561857       |User_4        |1496797956753 |OTHER    |Region_1  |User_4       |\n+   |1581077562858       |User_8        |1489169082491 |FEMALE   |Region_8  |User_8       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+   !!! note\n+         Push queries on tables will output the full history of the table that is stored\n+         in the Kafka changelog topic, i.e. it will output historic data, followed by the\n+         stream of updates to the table. It is therefore likely that rows with matching\n+         `ROWKEY` are output as existing rows in the table are updated.\n+\n+2. View the data in your pageviews_original stream by issuing the following\n+   push query:\n+   ```sql\n+   SELECT viewtime, userid, pageid FROM pageviews_original emit changes LIMIT 3;\n+   ```\n+\n+   Your output should resemble:\n+\n+   ```\n+   +--------------+--------------+--------------+\n+   |VIEWTIME      |USERID        |PAGEID        |\n+   +--------------+--------------+--------------+\n+   |1581078296791 |User_1        |Page_54       |\n+   |1581078297792 |User_8        |Page_93       |\n+   |1581078298792 |User_6        |Page_26       |\n+   Limit Reached\n+   Query terminated\n+   ```\n+\n+   !!! note\n+      By default, push queries on streams only output changes that occur\n+      after the query is started, i.e. historic data is not included.\n+      Run `set 'auto.offset.reset'='earliest';` to update your session\n+      properties if you want to see the historic data.\n+\n Write Queries\n -------------\n \n These examples write queries using ksqlDB.\n \n-!!! note\n-\tBy default ksqlDB reads the topics for streams and tables from\n-    the latest offset.\n-\n ### 1. Create a query that returns data from a ksqlDB stream\n \n-Use `SELECT` to create a query that returns data from a STREAM. This\n-query includes the `LIMIT` keyword to limit the number of rows\n-returned in the query result. Note that exact data output may vary\n-because of the randomness of the data generation.\n+Create query that enriches the pageviews data with the user's gender\n+and regionid from the users table. The following query enriches the\n+`pageviews_original` STREAM by doing a `LEFT JOIN` with the\n+`users_original` TABLE on the userid column.\n \n ```sql\n-SELECT pageid FROM pageviews_original LIMIT 3;\n+SELECT users_original.userid AS userid, pageid, regionid, gender\n+  FROM pageviews_original\n+  LEFT JOIN users_original\n+    ON pageviews_original.userid = users_original.userid\n+  EMIT CHANGES\n+  LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-Page_24\n-Page_73\n-Page_78\n-LIMIT reached\n++-------------------+-------------------+-------------------+-------------------+\n+|USERID             |PAGEID             |REGIONID           |GENDER             |\n++-------------------+-------------------+-------------------+-------------------+\n+|User_7             |Page_23            |Region_2           |OTHER              |\n+|User_3             |Page_42            |Region_2           |MALE               |\n+|User_7             |Page_87            |Region_2           |OTHER              |\n+|User_2             |Page_57            |Region_5           |FEMALE             |\n+|User_9             |Page_59            |Region_1           |OTHER              |\n+Limit Reached\n Query terminated\n ```\n \n+!!! note\n+  The join to the users table is on the userid column, which was identified as\n+  an alias for the tables primary key, `ROWKEY`, in the CREATE TABLE statement.\n+  `userId` and `ROWKEY` can be used interchangeably as the join criteria for\n+  the table. However, the data in `userid` on the stream side does not match\n+  the stream's key. Hence, ksqlDB will first internally repartition the stream\n+  by the `userId` column.\n+\n ### 2. Create a persistent query\n \n Create a persistent query by using the `CREATE STREAM` keywords to\n-precede the `SELECT` statement. The results from this query are\n-written to the `PAGEVIEWS_ENRICHED` Kafka topic. The following query\n-enriches the `pageviews_original` STREAM by doing a `LEFT JOIN` with\n-the `users_original` TABLE on the user ID.\n+precede the `SELECT` statement, and removing the `LIMIT` clause.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNzAzOA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376517038", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### 7. View query results using push query\n          \n          \n            \n            ### 7. View query results using a push query", "author": "JimGalasyn", "createdAt": "2020-02-07T17:36:58Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNzE1MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376517151", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### 8. View query results using pull query\n          \n          \n            \n            ### 8. View query results using a pull query", "author": "JimGalasyn", "createdAt": "2020-02-07T17:37:16Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNzQ0NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376517444", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n          \n          \n            \n            When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB builds an internal", "author": "JimGalasyn", "createdAt": "2020-02-07T17:37:59Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxNzkwNw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376517907", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Push queries do not have the `EMIT CHANGES` clause.\n          \n          \n            \n            Pull queries do not have the `EMIT CHANGES` clause.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:39:03Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n+table containing the results of the aggregation. ksqlDB supports pull queries against\n+such aggregation results.\n+\n+Unlike the push query used in the previous step, which _pushes_ a stream of results to you,\n+pull queries pull a result set and automatically terminate.\n+\n+Push queries do not have the `EMIT CHANGES` clause.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxODEzMg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376518132", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            View all the windows and user counts available for a specific gender and region using a pull query:\n          \n          \n            \n            View all of the windows and user counts that are available for a specific gender and region by using a pull query:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:39:35Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n+table containing the results of the aggregation. ksqlDB supports pull queries against\n+such aggregation results.\n+\n+Unlike the push query used in the previous step, which _pushes_ a stream of results to you,\n+pull queries pull a result set and automatically terminate.\n+\n+Push queries do not have the `EMIT CHANGES` clause.\n+\n+View all the windows and user counts available for a specific gender and region using a pull query:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxODQ2MA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376518460", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Pull queries on windowed tables such as pageviews_regions also supports querying a single window's result:\n          \n          \n            \n            Pull queries on windowed tables such as `pageviews_regions` also support querying a single window's result:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:40:26Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n+table containing the results of the aggregation. ksqlDB supports pull queries against\n+such aggregation results.\n+\n+Unlike the push query used in the previous step, which _pushes_ a stream of results to you,\n+pull queries pull a result set and automatically terminate.\n+\n+Push queries do not have the `EMIT CHANGES` clause.\n+\n+View all the windows and user counts available for a specific gender and region using a pull query:\n+\n+```sql\n+SELECT * FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9';\n+```\n+\n+Your output should resemble:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWKEY            |WINDOWSTART       |WINDOWEND         |ROWTIME           |GENDER            |REGIONID          |NUMUSERS          |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|OTHER|+|Region_9  |1581080490000     |1581080520000     |1581080500530     |OTHER             |Region_9          |1                 |\n+|OTHER|+|Region_9  |1581080550000     |1581080580000     |1581080576526     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080580000     |1581080610000     |1581080606525     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080610000     |1581080640000     |1581080622524     |OTHER             |Region_9          |3                 |\n+|OTHER|+|Region_9  |1581080640000     |1581080670000     |1581080667528     |OTHER             |Region_9          |6                 |\n+...\n+```\n+\n+Pull queries on windowed tables such as pageviews_regions also supports querying a single window's result:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxODczMA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376518730", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               You will need to change value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.\n          \n          \n            \n               You must change the value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:41:00Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n+table containing the results of the aggregation. ksqlDB supports pull queries against\n+such aggregation results.\n+\n+Unlike the push query used in the previous step, which _pushes_ a stream of results to you,\n+pull queries pull a result set and automatically terminate.\n+\n+Push queries do not have the `EMIT CHANGES` clause.\n+\n+View all the windows and user counts available for a specific gender and region using a pull query:\n+\n+```sql\n+SELECT * FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9';\n+```\n+\n+Your output should resemble:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWKEY            |WINDOWSTART       |WINDOWEND         |ROWTIME           |GENDER            |REGIONID          |NUMUSERS          |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|OTHER|+|Region_9  |1581080490000     |1581080520000     |1581080500530     |OTHER             |Region_9          |1                 |\n+|OTHER|+|Region_9  |1581080550000     |1581080580000     |1581080576526     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080580000     |1581080610000     |1581080606525     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080610000     |1581080640000     |1581080622524     |OTHER             |Region_9          |3                 |\n+|OTHER|+|Region_9  |1581080640000     |1581080670000     |1581080667528     |OTHER             |Region_9          |6                 |\n+...\n+```\n+\n+Pull queries on windowed tables such as pageviews_regions also supports querying a single window's result:\n+\n+```sql\n+SELECT NUMUSERS FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9' AND WINDOWSTART=1581080550000;\n+```\n+\n+!!! important\n+   You will need to change value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxODk1Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376518953", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               You will need to change value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.\n          \n          \n            \n               You must change the value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:41:25Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -400,48 +533,129 @@ format. ksqlDB registers the Avro schema with the configured\n \n ```sql\n CREATE TABLE pageviews_regions\n-    WITH (VALUE_FORMAT='avro') AS\n+ WITH (VALUE_FORMAT='avro') AS\n SELECT gender, regionid , COUNT(*) AS numusers\n FROM pageviews_enriched\n-    WINDOW TUMBLING (size 30 second)\n+  WINDOW TUMBLING (size 30 second)\n GROUP BY gender, regionid\n-HAVING COUNT(*) > 1;\n+EMIT CHANGES;\n ```\n \n Your output should resemble:\n \n ```\n-  Message\n----------------------------\n-  Table created and running\n----------------------------\n+ Message                                                                                                \n+--------------------------------------------------------------------------------------------------------\n+ Table PAGEVIEWS_REGIONS created and running. Created by query with query ID: CTAS_PAGEVIEWS_REGIONS_15 \n+--------------------------------------------------------------------------------------------------------\n ```\n \n !!! tip\n     You can run `DESCRIBE pageviews_regions;` to describe the table.\n \n-### 7. View query results\n+### 7. View query results using push query\n \n View results from the previous queries by using the `SELECT` statement.\n \n ```sql\n-SELECT gender, regionid, numusers FROM pageviews_regions LIMIT 5;\n+SELECT * FROM pageviews_regions EMIT CHANGES LIMIT 5;\n ```\n \n Your output should resemble:\n \n ```\n-FEMALE | Region_6 | 3\n-FEMALE | Region_1 | 4\n-FEMALE | Region_9 | 6\n-MALE | Region_8 | 2\n-OTHER | Region_5 | 4\n-LIMIT reached\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|ROWTIME        |ROWKEY           |WINDOWSTART    |WINDOWEND      |GENDER         |REGIONID       |NUMUSERS       |\n++---------------+-----------------+---------------+---------------+---------------+---------------+---------------+\n+|1581080500530  |OTHER|+|Region_9 |1581080490000  |1581080520000  |OTHER          |Region_9       |1              |\n+|1581080501530  |OTHER|+|Region_5 |1581080490000  |1581080520000  |OTHER          |Region_5       |2              |\n+|1581080510532  |MALE|+|Region_7  |1581080490000  |1581080520000  |MALE           |Region_7       |4              |\n+|1581080513532  |FEMALE|+|Region_1|1581080490000  |1581080520000  |FEMALE         |Region_1       |2              |\n+|1581080516533  |MALE|+|Region_2  |1581080490000  |1581080520000  |MALE           |Region_2       |3              |\n+Limit Reached\n Query terminated\n-ksql>\n ```\n \n-### 8. View persistent queries\n+!!! note\n+   Notice the addition of the WINDOWSTART and WINDOWEND columns.\n+   These are available because `pageviews_regions` is aggregating data\n+   per 30 second _window_. ksqlDB automatically adds these system columns\n+   for windowed results.\n+\n+### 8. View query results using pull query\n+\n+When a CREATE TABLE statement contains a GROUP BY clause, ksqlDB is internally building an\n+table containing the results of the aggregation. ksqlDB supports pull queries against\n+such aggregation results.\n+\n+Unlike the push query used in the previous step, which _pushes_ a stream of results to you,\n+pull queries pull a result set and automatically terminate.\n+\n+Push queries do not have the `EMIT CHANGES` clause.\n+\n+View all the windows and user counts available for a specific gender and region using a pull query:\n+\n+```sql\n+SELECT * FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9';\n+```\n+\n+Your output should resemble:\n+\n+```\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|ROWKEY            |WINDOWSTART       |WINDOWEND         |ROWTIME           |GENDER            |REGIONID          |NUMUSERS          |\n++------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n+|OTHER|+|Region_9  |1581080490000     |1581080520000     |1581080500530     |OTHER             |Region_9          |1                 |\n+|OTHER|+|Region_9  |1581080550000     |1581080580000     |1581080576526     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080580000     |1581080610000     |1581080606525     |OTHER             |Region_9          |4                 |\n+|OTHER|+|Region_9  |1581080610000     |1581080640000     |1581080622524     |OTHER             |Region_9          |3                 |\n+|OTHER|+|Region_9  |1581080640000     |1581080670000     |1581080667528     |OTHER             |Region_9          |6                 |\n+...\n+```\n+\n+Pull queries on windowed tables such as pageviews_regions also supports querying a single window's result:\n+\n+```sql\n+SELECT NUMUSERS FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9' AND WINDOWSTART=1581080550000;\n+```\n+\n+!!! important\n+   You will need to change value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.\n+   Otherwise no results will be returned.\n+\n+Your output should resemble:\n+\n+```\n++----------+\n+|NUMUSERS  |\n++----------+\n+|4         |\n+Query terminated\n+```\n+\n+Or querying a range of windows:\n+\n+```sql\n+SELECT WINDOWSTART, WINDOWEND, NUMUSERS FROM pageviews_regions WHERE ROWKEY='OTHER|+|Region_9' AND 1581080550000 <= WINDOWSTART AND WINDOWSTART <= 1581080610000;\n+```\n+\n+!!! important\n+   You will need to change value of `WINDOWSTART` in the above SQL to match one of the window boundaries in your data.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxOTUyOA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376519528", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In the ksqlDB CLI create two new streams, both streams will store their\n          \n          \n            \n            In the ksqlDB CLI create two new streams. Both streams will store their", "author": "JimGalasyn", "createdAt": "2020-02-07T17:42:51Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -600,48 +831,23 @@ common key. An example of this could be a stream of order events and\n a stream of shipment events. By joining these on the order key, you can\n see shipment information alongside the order.\n \n-### 1. Populate two source topics\n-\n-In a separate console window, populate the `orders` and `shipments`\n-topics by using the `kafkacat` utility:\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t new_orders \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n-2:{\"order_id\":2,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n-3:{\"order_id\":3,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\n-EOF\n-```\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t shipments \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"shipment_id\":42,\"warehouse\":\"Nashville\"}\n-3:{\"order_id\":3,\"shipment_id\":43,\"warehouse\":\"Palo Alto\"}\n-EOF\n-```\n-\n-### 2. Register two streams\n+### 1. Create two streams\n \n-In the ksqlDB CLI, register both topics as ksqlDB streams:\n+In the ksqlDB CLI create two new streams, both streams will store their", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxOTY0NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376519644", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            order id in ROWKEY:\n          \n          \n            \n            order ID in ROWKEY:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:43:06Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -600,48 +831,23 @@ common key. An example of this could be a stream of order events and\n a stream of shipment events. By joining these on the order key, you can\n see shipment information alongside the order.\n \n-### 1. Populate two source topics\n-\n-In a separate console window, populate the `orders` and `shipments`\n-topics by using the `kafkacat` utility:\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t new_orders \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n-2:{\"order_id\":2,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n-3:{\"order_id\":3,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\n-EOF\n-```\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t shipments \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"shipment_id\":42,\"warehouse\":\"Nashville\"}\n-3:{\"order_id\":3,\"shipment_id\":43,\"warehouse\":\"Palo Alto\"}\n-EOF\n-```\n-\n-### 2. Register two streams\n+### 1. Create two streams\n \n-In the ksqlDB CLI, register both topics as ksqlDB streams:\n+In the ksqlDB CLI create two new streams, both streams will store their\n+order id in ROWKEY:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUxOTk4NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376519984", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              ksqlDB will create the underlying topics in Kafka when these statements\n          \n          \n            \n              ksqlDB creates the underlying topics in {{ site.ak }} when it executes these statements.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:43:46Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -600,48 +831,23 @@ common key. An example of this could be a stream of order events and\n a stream of shipment events. By joining these on the order key, you can\n see shipment information alongside the order.\n \n-### 1. Populate two source topics\n-\n-In a separate console window, populate the `orders` and `shipments`\n-topics by using the `kafkacat` utility:\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t new_orders \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n-2:{\"order_id\":2,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n-3:{\"order_id\":3,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\n-EOF\n-```\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t shipments \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"shipment_id\":42,\"warehouse\":\"Nashville\"}\n-3:{\"order_id\":3,\"shipment_id\":43,\"warehouse\":\"Palo Alto\"}\n-EOF\n-```\n-\n-### 2. Register two streams\n+### 1. Create two streams\n \n-In the ksqlDB CLI, register both topics as ksqlDB streams:\n+In the ksqlDB CLI create two new streams, both streams will store their\n+order id in ROWKEY:\n \n ```sql\n-CREATE STREAM NEW_ORDERS (ORDER_ID INT, TOTAL_AMOUNT DOUBLE, CUSTOMER_NAME VARCHAR)\n-WITH (KAFKA_TOPIC='new_orders', VALUE_FORMAT='JSON');\n+CREATE STREAM NEW_ORDERS (ROWKEY INT KEY, TOTAL_AMOUNT DOUBLE, CUSTOMER_NAME VARCHAR)\n+WITH (KAFKA_TOPIC='new_orders', VALUE_FORMAT='JSON', PARTITIONS=2);\n \n-CREATE STREAM SHIPMENTS (ORDER_ID INT, SHIPMENT_ID INT, WAREHOUSE VARCHAR)\n-WITH (KAFKA_TOPIC='shipments', VALUE_FORMAT='JSON');\n+CREATE STREAM SHIPMENTS (ROWKEY INT KEY, SHIPMENT_ID INT, WAREHOUSE VARCHAR)\n+WITH (KAFKA_TOPIC='shipments', VALUE_FORMAT='JSON', PARTITIONS=2);\n ```\n \n+!!! note\n+  ksqlDB will create the underlying topics in Kafka when these statements", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMDEwMQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376520101", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              are executed. You can also specify the `REPLICAS` count.\n          \n          \n            \n              You can also specify the `REPLICAS` count.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:44:00Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -600,48 +831,23 @@ common key. An example of this could be a stream of order events and\n a stream of shipment events. By joining these on the order key, you can\n see shipment information alongside the order.\n \n-### 1. Populate two source topics\n-\n-In a separate console window, populate the `orders` and `shipments`\n-topics by using the `kafkacat` utility:\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t new_orders \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n-2:{\"order_id\":2,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n-3:{\"order_id\":3,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\n-EOF\n-```\n-\n-```bash\n-docker run --interactive --rm --network tutorials_default \\\n-  confluentinc/cp-kafkacat \\\n-  kafkacat -b kafka:39092 \\\n-          -t shipments \\\n-          -K: \\\n-          -P <<EOF\n-1:{\"order_id\":1,\"shipment_id\":42,\"warehouse\":\"Nashville\"}\n-3:{\"order_id\":3,\"shipment_id\":43,\"warehouse\":\"Palo Alto\"}\n-EOF\n-```\n-\n-### 2. Register two streams\n+### 1. Create two streams\n \n-In the ksqlDB CLI, register both topics as ksqlDB streams:\n+In the ksqlDB CLI create two new streams, both streams will store their\n+order id in ROWKEY:\n \n ```sql\n-CREATE STREAM NEW_ORDERS (ORDER_ID INT, TOTAL_AMOUNT DOUBLE, CUSTOMER_NAME VARCHAR)\n-WITH (KAFKA_TOPIC='new_orders', VALUE_FORMAT='JSON');\n+CREATE STREAM NEW_ORDERS (ROWKEY INT KEY, TOTAL_AMOUNT DOUBLE, CUSTOMER_NAME VARCHAR)\n+WITH (KAFKA_TOPIC='new_orders', VALUE_FORMAT='JSON', PARTITIONS=2);\n \n-CREATE STREAM SHIPMENTS (ORDER_ID INT, SHIPMENT_ID INT, WAREHOUSE VARCHAR)\n-WITH (KAFKA_TOPIC='shipments', VALUE_FORMAT='JSON');\n+CREATE STREAM SHIPMENTS (ROWKEY INT KEY, SHIPMENT_ID INT, WAREHOUSE VARCHAR)\n+WITH (KAFKA_TOPIC='shipments', VALUE_FORMAT='JSON', PARTITIONS=2);\n ```\n \n+!!! note\n+  ksqlDB will create the underlying topics in Kafka when these statements\n+  are executed. You can also specify the `REPLICAS` count.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMDI1NQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376520255", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Populate the streams with some sample data using the INSERT VALUES statement:\n          \n          \n            \n            Populate the streams with some sample data by using the INSERT VALUES statement:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:44:23Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -651,10 +857,29 @@ After both `CREATE STREAM` statements, your output should resemble:\n ----------------\n ```\n \n+### 2. Populate two source topics\n+\n+Populate the streams with some sample data using the INSERT VALUES statement:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMDQ3NA==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376520474", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            -- short hand version can be used when inserting values for all columns, (except ROWTIME), in column order:\n          \n          \n            \n            -- shorthand version can be used when inserting values for all columns, (except ROWTIME), in column order:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:44:55Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -651,10 +857,29 @@ After both `CREATE STREAM` statements, your output should resemble:\n ----------------\n ```\n \n+### 2. Populate two source topics\n+\n+Populate the streams with some sample data using the INSERT VALUES statement:\n+\n+```sql\n+-- Insert values in NEW_ORDERS:\n+-- insert supplying the list of columns to insert:\n+INSERT INTO NEW_ORDERS (ROWKEY, CUSTOMER_NAME, TOTAL_AMOUNT) \n+  VALUES (1, 'Bob Smith', 10.50);\n+  \n+-- short hand version can be used when inserting values for all columns, (except ROWTIME), in column order:", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f79cdc7d67bebb1b1312f3d32c7efe9d92249239", "url": "https://github.com/confluentinc/ksql/commit/f79cdc7d67bebb1b1312f3d32c7efe9d92249239", "message": "Update docs-md/developer-guide/syntax-reference.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:46:21Z", "type": "commit"}, {"oid": "381f4cfcb8ce64110c073176ba9c61973cd86123", "url": "https://github.com/confluentinc/ksql/commit/381f4cfcb8ce64110c073176ba9c61973cd86123", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:46:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMTM2Mw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376521363", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can also use `kafkacat` command line tool\n          \n          \n            \n            You can also use the `kafkacat` command line tool:", "author": "JimGalasyn", "createdAt": "2020-02-07T17:47:04Z", "path": "docs-md/tutorials/basics-docker.md", "diffHunk": "@@ -1114,7 +1396,22 @@ key3:{\"id\":\"key3\",\"col1\":\"v7\",\"col2\":\"v8\",\"col3\":\"v9\"}\n key1:{\"id\":\"key1\",\"col1\":\"v10\",\"col2\":\"v11\",\"col3\":\"v12\"}\n ```\n \n-You can also use `kafkacat` from Docker, as demonstrated in the previous\n+You can also use `kafkacat` command line tool", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMTY0Nw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376521647", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n          \n          \n            \n            By default, KSQL hides internal and system topics. Use the SHOW ALL TOPICS", "author": "JimGalasyn", "createdAt": "2020-02-07T17:47:50Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -119,25 +119,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMTg5MQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376521891", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               The PRINT statement is one of the few commands in ksqlDB that are\n          \n          \n            \n               The PRINT statement is one of the few case-sensitive commands in ksqlDB,", "author": "JimGalasyn", "createdAt": "2020-02-07T17:48:25Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -119,25 +119,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas \n+--------------------------------------------------------------------------\n+ __confluent.support.metrics            | 1          | 1                  \n+ _confluent-ksql-default__command_topic | 1          | 1                  \n+ _confluent-license                     | 1          | 1                  \n+ _confluent-metrics                     | 12         | 1                  \n+ default_ksql_processing_log            | 1          | 1                  \n+ pageviews                              | 1          | 1                  \n+ users                                  | 1          | 1                  \n+--------------------------------------------------------------------------\n ```\n \n Inspect the `users` topic by using the PRINT statement:\n \n ```sql\n-PRINT 'users';\n+PRINT users;\n ```\n \n+!!! note\n+   The PRINT statement is one of the few commands in ksqlDB that are", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMjAxNQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376522015", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               case-sensitive, even when the topic name is not quoted.\n          \n          \n            \n               even when the topic name is not quoted.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:48:45Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -119,25 +119,50 @@ SHOW TOPICS;\n Your output should resemble:\n \n ```\n- Kafka Topic        | Partitions | Partition Replicas\n-------------------------------------------------------\n- _confluent-metrics | 12         | 1\n- _schemas           | 1          | 1\n- pageviews          | 1          | 1\n- users              | 1          | 1\n-------------------------------------------------------\n+ Kafka Topic                 | Partitions | Partition Replicas\n+--------------------------------------------------------------\n+ default_ksql_processing_log | 1          | 1\n+ pageviews                   | 1          | 1\n+ users                       | 1          | 1\n+--------------------------------------------------------------\n+```\n+\n+By default, KSQL hides internal / system topics. Use the SHOW ALL TOPICS\n+statement to see the full list of topics in the Kafka cluster:\n+\n+```sql\n+SHOW ALL TOPICS;\n+```\n+\n+Your output should resemble:\n+\n+```\n+  Kafka Topic                            | Partitions | Partition Replicas \n+--------------------------------------------------------------------------\n+ __confluent.support.metrics            | 1          | 1                  \n+ _confluent-ksql-default__command_topic | 1          | 1                  \n+ _confluent-license                     | 1          | 1                  \n+ _confluent-metrics                     | 12         | 1                  \n+ default_ksql_processing_log            | 1          | 1                  \n+ pageviews                              | 1          | 1                  \n+ users                                  | 1          | 1                  \n+--------------------------------------------------------------------------\n ```\n \n Inspect the `users` topic by using the PRINT statement:\n \n ```sql\n-PRINT 'users';\n+PRINT users;\n ```\n \n+!!! note\n+   The PRINT statement is one of the few commands in ksqlDB that are\n+   case-sensitive, even when the topic name is not quoted.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMjI0OQ==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376522249", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      stream. Notice that KSQL created an additional columns, named\n          \n          \n            \n                      stream. Notice that ksqlDB created an additional column, named", "author": "JimGalasyn", "createdAt": "2020-02-07T17:49:20Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -196,16 +221,14 @@ These examples query messages from Kafka topics called `pageviews` and\n \n     !!! tip\n           You can run `DESCRIBE pageviews_original;` to see the schema for the\n-          stream. Notice that KSQL created two additional columns, named\n-          `ROWTIME`, which corresponds with the Kafka message timestamp, and\n-          `ROWKEY`, which corresponds with the Kafka message key.\n+          stream. Notice that KSQL created an additional columns, named", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMjM4Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376522386", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      `ROWTIME`, which corresponds with the Kafka message timestamp.\n          \n          \n            \n                      `ROWTIME`, which corresponds with the {{ site.ak }} message timestamp.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:49:43Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -196,16 +221,14 @@ These examples query messages from Kafka topics called `pageviews` and\n \n     !!! tip\n           You can run `DESCRIBE pageviews_original;` to see the schema for the\n-          stream. Notice that KSQL created two additional columns, named\n-          `ROWTIME`, which corresponds with the Kafka message timestamp, and\n-          `ROWKEY`, which corresponds with the Kafka message key.\n+          stream. Notice that KSQL created an additional columns, named\n+          `ROWTIME`, which corresponds with the Kafka message timestamp.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMjc2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376522766", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n          \n          \n            \n                      is Avro, and the DataGen tool publishes the Avro schema to {{ site.sr }}.", "author": "JimGalasyn", "createdAt": "2020-02-07T17:50:40Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8e62d67e3000608f2d2846c36c0b1dba06d6e6e9", "url": "https://github.com/confluentinc/ksql/commit/8e62d67e3000608f2d2846c36c0b1dba06d6e6e9", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:51:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMzI0Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376523242", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      ksqlDB retrieves the schema from the Schema Registry and uses this to build\n          \n          \n            \n                      ksqlDB retrieves the schema from {{ site.sr }} and uses this to build", "author": "JimGalasyn", "createdAt": "2020-02-07T17:51:38Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "04f27e3870884c2305a5b44749a5e6ec0da1fb3e", "url": "https://github.com/confluentinc/ksql/commit/04f27e3870884c2305a5b44749a5e6ec0da1fb3e", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:51:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyMzU0Mg==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376523542", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      The data generated has the same value in the Kafka record's key\n          \n          \n            \n                      The data generated has the same value in the {{ site.ak }} record's key", "author": "JimGalasyn", "createdAt": "2020-02-07T17:52:23Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ef72f568dd22dead43df2d78e8c3b6dd1ef36e57", "url": "https://github.com/confluentinc/ksql/commit/ef72f568dd22dead43df2d78e8c3b6dd1ef36e57", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:53:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNDAzNw==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376524037", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      as is in the userId field in the value. Specifying `key='userId'`\n          \n          \n            \n                      as the `userId` field in the value. Specifying `key='userId'`", "author": "JimGalasyn", "createdAt": "2020-02-07T17:53:33Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key\n+          as is in the userId field in the value. Specifying `key='userId'`", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a87e47ccb24fb17569e5e5c6ecef9eed5e89441f", "url": "https://github.com/confluentinc/ksql/commit/a87e47ccb24fb17569e5e5c6ecef9eed5e89441f", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:53:56Z", "type": "commit"}, {"oid": "b326546e6aef63fae0ce15856d146ecde8c3de07", "url": "https://github.com/confluentinc/ksql/commit/b326546e6aef63fae0ce15856d146ecde8c3de07", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:54:10Z", "type": "commit"}, {"oid": "78083792a99f4d5151416d6de80887c5a55370f6", "url": "https://github.com/confluentinc/ksql/commit/78083792a99f4d5151416d6de80887c5a55370f6", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:54:24Z", "type": "commit"}, {"oid": "bee53330f42af9d39b61776f22267eff6a3c1929", "url": "https://github.com/confluentinc/ksql/commit/bee53330f42af9d39b61776f22267eff6a3c1929", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:54:38Z", "type": "commit"}, {"oid": "1a0e70987013745d5f29ae36216334bac9710665", "url": "https://github.com/confluentinc/ksql/commit/1a0e70987013745d5f29ae36216334bac9710665", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:54:53Z", "type": "commit"}, {"oid": "a8fe7e58b200189c7d7fca76af0cc5c486692ad1", "url": "https://github.com/confluentinc/ksql/commit/a8fe7e58b200189c7d7fca76af0cc5c486692ad1", "message": "Update docs-md/tutorials/basics-docker.md\n\nCo-Authored-By: Jim Galasyn <jim.galasyn@confluent.io>", "committedDate": "2020-02-07T17:55:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNDg2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4478#discussion_r376524866", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      in the WITH clause above lets ksqlDB know this. This information\n          \n          \n            \n                      in the WITH clause above lets ksqlDB know this. ksqlDB uses this information", "author": "JimGalasyn", "createdAt": "2020-02-07T17:55:27Z", "path": "docs-md/tutorials/basics-local.md", "diffHunk": "@@ -217,6 +240,25 @@ These examples query messages from Kafka topics called `pageviews` and\n     ---------------\n     ```\n \n+    !!! note\n+          You may have noticed the CREATE TABLE did not define the set of columns\n+          like the CREATE STREAM statement did. This is because the value format\n+          if Avro, and the DataGen tool publishes the Avro schema to the {{ site.sr }}.\n+          ksqlDB retrieves the schema from the Schema Registry and uses this to build\n+          the SQL schema for the table. You may still provide the schema if you wish.\n+          Until [Github issue #4462](https://github.com/confluentinc/ksql/issues/4462)\n+          is complete, schema inference is only available where the key of the data\n+          is a STRING, as is the case here.\n+\n+    !!! note\n+          The data generated has the same value in the Kafka record's key\n+          as is in the userId field in the value. Specifying `key='userId'`\n+          in the WITH clause above lets ksqlDB know this. This information", "originalCommit": "fec77204c76b39cf294d9a39c0b05ca0334f9477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}